{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pommerman\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from pommerman.agents import BaseAgent, SimpleAgent\n",
    "from pommerman.configs import ffa_v0_env\n",
    "from pommerman.constants import BOARD_SIZE\n",
    "from pommerman.envs.v0 import Pomme\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "# Immitation Learning: learn a mapping from observations to actions.\n",
    "from pomm_network import PommNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "epochs = 10\n",
    "initial_rollouts = 20\n",
    "num_rollouts = 3\n",
    "iterations = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, actions, seed=0, save_path=\"./model/model.cptk\", log_path='./logs/', save_best_model=True, learning_rate=1e-3):\n",
    "        self.save_path = save_path\n",
    "        self.actions = actions\n",
    "        self.save_best_model = save_best_model\n",
    "        self.seed = seed\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.rewards = []\n",
    "        if os.path.isdir(log_path):\n",
    "            try:\n",
    "                shutil.rmtree(log_path, ignore_errors=True)\n",
    "            except:\n",
    "                print(\"Cant delete log folder\")\n",
    "\n",
    "        # TODO hardcoded\n",
    "        self.conv_ph = tf.placeholder(shape=[None, BOARD_SIZE, BOARD_SIZE, 3], name='conv_ph', dtype=tf.float32)\n",
    "        self.state_ph = tf.placeholder(shape=[None, 3], name='state_ph', dtype=tf.float32)\n",
    "        self.logits_ph = tf.placeholder(shape=[None, actions], name='logits_ph', dtype=tf.int32)\n",
    "\n",
    "        network = PommNetwork.create_network({'board': self.conv_ph, 'state': self.state_ph})\n",
    "        logits = tf.layers.dense(network, actions)\n",
    "        self.sampled_action = tf.squeeze(tf.multinomial(logits, 1), axis=[1])\n",
    "        sy_logprob_n = tf.nn.softmax_cross_entropy_with_logits(labels=self.logits_ph, logits=logits)\n",
    "        self.loss = tf.reduce_mean(sy_logprob_n)  # Loss function that we'll differentiate to get the policy gradient.\n",
    "        self.train_step = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(self.loss)\n",
    "        self.train_writer = tf.summary.FileWriter(log_path + \"train\", self.sess.graph)\n",
    "        self.test_writer = tf.summary.FileWriter(log_path + \"test\")\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.saver = tf.train.Saver()\n",
    "        if os.path.isdir(os.path.dirname(self.save_path)):\n",
    "            try:\n",
    "                print(\"Trying to load model\")\n",
    "                self.saver.restore(self.sess, self.save_path)\n",
    "                print(\"Model was loaded successful\")\n",
    "            except:\n",
    "                print(\"Model load failed\")\n",
    "\n",
    "    def __run_batch(self, X, y, batch_size, training=True):\n",
    "        accuracies = []\n",
    "        losses = []\n",
    "        size = X.shape[-1]\n",
    "        batches = int(np.ceil(size / batch_size))\n",
    "        for i in range(batches):\n",
    "            X_batch, y_batch = None, None\n",
    "            if i == batches - 1:\n",
    "                X_batch = X[i * batch_size:]\n",
    "                y_batch = y[i * batch_size:]\n",
    "            else:\n",
    "                X_batch = X[i * batch_size: (i + 1) * batch_size]\n",
    "                y_batch = y[i * batch_size: (i + 1) * batch_size]\n",
    "            board = []\n",
    "            state = []\n",
    "            for i in range(X_batch.shape[-1]):\n",
    "                val = self.featurize(X_batch[i])\n",
    "                board.append(val['board'])\n",
    "                state.append(val['state'])\n",
    "            y_batch = np.array(y_batch).reshape(-1)\n",
    "            feed_dict = {self.conv_ph: board, self.state_ph: state, self.logits_ph: to_categorical(y_batch, self.actions)}\n",
    "            if training:\n",
    "                _, loss, actions = self.sess.run([self.train_step, self.loss, self.sampled_action], feed_dict=feed_dict)\n",
    "            else:\n",
    "                loss, actions = self.sess.run([self.loss, self.sampled_action], feed_dict=feed_dict)\n",
    "            accuracies.append(accuracy_score(actions, y_batch))\n",
    "            losses.append(loss)\n",
    "        return np.mean(accuracies), np.mean(losses)\n",
    "\n",
    "    def train(self, obs, labels, batch_size, epochs):\n",
    "        print(\"Train the agent with %i training data, batch_size %i, epochs %i\" % (obs.shape[0], batch_size, epochs))\n",
    "        train_obs, val_obs, train_labels, val_labels = train_test_split(obs, labels, test_size=0.2, random_state=self.seed)\n",
    "        prev_loss = np.inf\n",
    "        for i in range(epochs):\n",
    "            train_acc, train_loss = self.__run_batch(train_obs, train_labels, batch_size)\n",
    "            val_acc, val_loss = self.__run_batch(val_obs, val_labels, batch_size, training=False)\n",
    "            print(\"Epoch %d: train_acc %f, train_loss %f, test_acc %f, test_loss: %f\" % (i + 1, train_acc, train_loss, val_acc, val_loss))\n",
    "\n",
    "            summary = tf.summary.Summary()\n",
    "            summary.value.add(tag=\"Train acc\", simple_value=train_acc)\n",
    "            summary.value.add(tag=\"Train loss\", simple_value=train_loss)\n",
    "            self.train_writer.add_summary(summary, i)\n",
    "            summary.value.add(tag=\"Val acc\", simple_value=val_acc)\n",
    "            summary.value.add(tag=\"Val loss\", simple_value=val_loss)\n",
    "            self.test_writer.add_summary(summary, i)\n",
    "            try:\n",
    "                if self.save_best_model:\n",
    "                    if val_loss < prev_loss:\n",
    "                        print(\"Saving model\")\n",
    "                        self.saver.save(self.sess, self.save_path)\n",
    "                        print(\"Model was saved successfully\")\n",
    "                else:\n",
    "                    print(\"Saving model\")\n",
    "                    self.saver.save(self.sess, self.save_path)\n",
    "                    print(\"Model was saved successfully\")\n",
    "            except:\n",
    "                print(\"Failed save model\")\n",
    "            prev_loss = val_loss\n",
    "\n",
    "    @staticmethod\n",
    "    def featurize(obs):\n",
    "        def get_matrix(dict, key):\n",
    "            res = dict[key]\n",
    "            return res.reshape(res.shape[0], res.shape[1], 1).astype(np.float32)\n",
    "\n",
    "        board = get_matrix(obs, 'board')\n",
    "        teammate_position = None\n",
    "        teammate = obs[\"teammate\"]\n",
    "        if teammate is not None:\n",
    "            teammate = teammate.value\n",
    "            if teammate > 10 and teammate < 15:\n",
    "                teammate_position = np.argwhere(board == teammate)[0]\n",
    "        else:\n",
    "            teammate = None\n",
    "        # My self - 11\n",
    "        # Team mate - 12\n",
    "        # Enemy - 13\n",
    "\n",
    "        # Everyone enemy\n",
    "        board[(board > 10) & (board < 15)] = 13\n",
    "        # I'm not enemy\n",
    "        my_position = obs['position']\n",
    "        board[my_position[0], my_position[1], 0] = 11\n",
    "        # Set teammate\n",
    "        if teammate_position is not None:\n",
    "            board[teammate_position[0], teammate_position[1], teammate_position[2]] = 12\n",
    "\n",
    "        bomb_blast_strength = get_matrix(obs, 'bomb_blast_strength')\n",
    "        bomb_life = get_matrix(obs, 'bomb_life')\n",
    "        conv_inp = np.concatenate([board, bomb_blast_strength, bomb_life], axis=2)\n",
    "        state = np.array([obs[\"ammo\"], obs[\"blast_strength\"], obs[\"can_kick\"]]).astype(np.float32)\n",
    "        return dict(board=conv_inp, state=state)\n",
    "\n",
    "    def clear_training_history(self):\n",
    "        self.history = []\n",
    "\n",
    "    def act(self, obs):\n",
    "        obs = self.featurize(obs)\n",
    "        board = np.expand_dims(obs['board'], axis=0)\n",
    "        state = np.expand_dims(obs['state'], axis=0)\n",
    "        res = self.sess.run(self.sampled_action, feed_dict={self.conv_ph: board, self.state_ph: state})\n",
    "        return res\n",
    "\n",
    "    def record_reward(self, reward):\n",
    "        self.rewards.append(np.mean(reward))\n",
    "\n",
    "    def close(self):\n",
    "        self.sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple wrapper around policy function to have an act function\n",
    "class Expert:\n",
    "    def __init__(self, config):\n",
    "        self.__agent = SimpleAgent(config)\n",
    "\n",
    "    def act(self, obs):\n",
    "        return self.__agent.act(obs, None)\n",
    "\n",
    "    def record_reward(self, reward):\n",
    "        pass\n",
    "\n",
    "\n",
    "class TensorforceAgent(BaseAgent):\n",
    "    def act(self, obs, action_space):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Environment wrapper\n",
    "class Stimulator:\n",
    "    def __init__(self, env, config):\n",
    "        self.env = env\n",
    "        self.init(config)\n",
    "\n",
    "    def init(self, config):\n",
    "        self.env.seed(0)\n",
    "        # Add 3 random agents\n",
    "        agents = []\n",
    "        for agent_id in range(3):\n",
    "            agents.append(SimpleAgent(config[\"agent\"](agent_id, config[\"game_type\"])))\n",
    "\n",
    "        # Add TensorforceAgent\n",
    "        agent_id += 1\n",
    "        agents.append(TensorforceAgent(config[\"agent\"](agent_id, config[\"game_type\"])))\n",
    "        self.env.set_agents(agents)\n",
    "        self.env.set_training_agent(agents[-1].agent_id)\n",
    "        self.env.set_init_game_state(None)\n",
    "\n",
    "    def stimulate(self, agent, num_rollouts, render):\n",
    "        returns = []\n",
    "        observations = []\n",
    "        actions = []\n",
    "        for i in range(num_rollouts):\n",
    "            print('Iteration', i)\n",
    "            obs = self.env.reset()[self.env.training_agent]\n",
    "            done = False\n",
    "            totalr = 0.\n",
    "            steps = 0\n",
    "            while not done:\n",
    "                if render:\n",
    "                    self.env.render()\n",
    "\n",
    "                action = agent.act(obs)\n",
    "                obs = self.env.get_observations()\n",
    "                all_actions = self.env.act(obs)\n",
    "                all_actions.insert(self.env.training_agent, action)\n",
    "                state, reward, done, _ = self.env.step(all_actions)\n",
    "\n",
    "                obs = state[self.env.training_agent]\n",
    "                r = reward[self.env.training_agent]\n",
    "                totalr += r\n",
    "                steps += 1\n",
    "                \n",
    "                observations.append(obs)\n",
    "                actions.append(action)\n",
    "\n",
    "            print('rollout %i/%i return=%f' % (i + 1, num_rollouts, totalr))\n",
    "            returns.append(totalr)\n",
    "        print('Return summary: mean=%f, std=%f' % (np.mean(returns), np.std(returns)))\n",
    "        agent.record_reward(returns)\n",
    "        return (np.array(observations), np.array(actions))\n",
    "\n",
    "    def label_obs(self, expert, obs):\n",
    "        actions = []\n",
    "        for o in obs:\n",
    "            actions.append(expert.act(o))\n",
    "        return np.array(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to load model\n",
      "INFO:tensorflow:Restoring parameters from ./model/model.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-03 01:45:03,697] Restoring parameters from ./model/model.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was loaded successful\n",
      "Iteration 0\n",
      "rollout 1/20 return=-1.000000\n",
      "Iteration 1\n",
      "rollout 2/20 return=-1.000000\n",
      "Iteration 2\n",
      "rollout 3/20 return=1.000000\n",
      "Iteration 3\n",
      "rollout 4/20 return=-1.000000\n",
      "Iteration 4\n",
      "rollout 5/20 return=-1.000000\n",
      "Iteration 5\n",
      "rollout 6/20 return=-1.000000\n",
      "Iteration 6\n",
      "rollout 7/20 return=1.000000\n",
      "Iteration 7\n",
      "rollout 8/20 return=-1.000000\n",
      "Iteration 8\n",
      "rollout 9/20 return=-1.000000\n",
      "Iteration 9\n",
      "rollout 10/20 return=-1.000000\n",
      "Iteration 10\n",
      "rollout 11/20 return=-1.000000\n",
      "Iteration 11\n",
      "rollout 12/20 return=-1.000000\n",
      "Iteration 12\n",
      "rollout 13/20 return=-1.000000\n",
      "Iteration 13\n",
      "rollout 14/20 return=1.000000\n",
      "Iteration 14\n",
      "rollout 15/20 return=-1.000000\n",
      "Iteration 15\n",
      "rollout 16/20 return=-1.000000\n",
      "Iteration 16\n",
      "rollout 17/20 return=1.000000\n",
      "Iteration 17\n",
      "rollout 18/20 return=-1.000000\n",
      "Iteration 18\n",
      "rollout 19/20 return=-1.000000\n",
      "Iteration 19\n",
      "rollout 20/20 return=-1.000000\n",
      "Return summary: mean=-0.600000, std=0.800000\n",
      "Train with DAgger, iter 2\n",
      "Iteration 0\n",
      "rollout 1/3 return=-1.000000\n",
      "Iteration 1\n",
      "rollout 2/3 return=-1.000000\n",
      "Iteration 2\n",
      "rollout 3/3 return=-1.000000\n",
      "Return summary: mean=-1.000000, std=0.000000\n",
      "Train the agent with 14327 training data, batch_size 1024, epochs 10\n",
      "Epoch 1: train_acc 0.270598, train_loss 3.162390, test_acc 0.253054, test_loss: 1.616782\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 2: train_acc 0.280671, train_loss 1.522856, test_acc 0.285292, test_loss: 1.469752\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 3: train_acc 0.304271, train_loss 1.434970, test_acc 0.322259, test_loss: 1.416227\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 4: train_acc 0.321621, train_loss 1.383079, test_acc 0.325932, test_loss: 1.381823\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 5: train_acc 0.340908, train_loss 1.341162, test_acc 0.341395, test_loss: 1.351467\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 6: train_acc 0.361136, train_loss 1.302872, test_acc 0.360781, test_loss: 1.323940\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 7: train_acc 0.373490, train_loss 1.269925, test_acc 0.357591, test_loss: 1.301659\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 8: train_acc 0.381627, train_loss 1.240964, test_acc 0.386919, test_loss: 1.281849\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 9: train_acc 0.395380, train_loss 1.214882, test_acc 0.386915, test_loss: 1.265578\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 10: train_acc 0.406984, train_loss 1.191256, test_acc 0.404423, test_loss: 1.251679\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Train with DAgger, iter 3\n",
      "Iteration 0\n",
      "rollout 1/3 return=-1.000000\n",
      "Iteration 1\n",
      "rollout 2/3 return=-1.000000\n",
      "Iteration 2\n",
      "rollout 3/3 return=-1.000000\n",
      "Return summary: mean=-1.000000, std=0.000000\n",
      "Train the agent with 14528 training data, batch_size 1024, epochs 10\n",
      "Epoch 1: train_acc 0.410081, train_loss 1.183408, test_acc 0.409318, test_loss: 1.223233\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 2: train_acc 0.423999, train_loss 1.160907, test_acc 0.409087, test_loss: 1.213300\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 3: train_acc 0.432511, train_loss 1.141136, test_acc 0.433984, test_loss: 1.203321\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 4: train_acc 0.439020, train_loss 1.124697, test_acc 0.427673, test_loss: 1.195273\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 5: train_acc 0.442324, train_loss 1.109858, test_acc 0.423851, test_loss: 1.189649\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 6: train_acc 0.446425, train_loss 1.095834, test_acc 0.418401, test_loss: 1.184914\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 7: train_acc 0.454741, train_loss 1.083258, test_acc 0.434540, test_loss: 1.180520\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 8: train_acc 0.455343, train_loss 1.071716, test_acc 0.422601, test_loss: 1.176386\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 9: train_acc 0.463555, train_loss 1.060671, test_acc 0.448117, test_loss: 1.173786\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 10: train_acc 0.468076, train_loss 1.050398, test_acc 0.450805, test_loss: 1.171854\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Train with DAgger, iter 4\n",
      "Iteration 0\n",
      "rollout 1/3 return=-1.000000\n",
      "Iteration 1\n",
      "rollout 2/3 return=-1.000000\n",
      "Iteration 2\n",
      "rollout 3/3 return=-1.000000\n",
      "Return summary: mean=-1.000000, std=0.000000\n",
      "Train the agent with 14653 training data, batch_size 1024, epochs 10\n",
      "Epoch 1: train_acc 0.463010, train_loss 1.066578, test_acc 0.447692, test_loss: 1.120545\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 2: train_acc 0.467879, train_loss 1.049772, test_acc 0.445348, test_loss: 1.122014\n",
      "Epoch 3: train_acc 0.471110, train_loss 1.037356, test_acc 0.454944, test_loss: 1.125131\n",
      "Epoch 4: train_acc 0.471430, train_loss 1.025493, test_acc 0.460765, test_loss: 1.122675\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 5: train_acc 0.476969, train_loss 1.013640, test_acc 0.465566, test_loss: 1.118174\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 6: train_acc 0.481250, train_loss 1.002330, test_acc 0.466791, test_loss: 1.116394\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 7: train_acc 0.487540, train_loss 0.992618, test_acc 0.459151, test_loss: 1.117076\n",
      "Epoch 8: train_acc 0.497516, train_loss 0.983935, test_acc 0.461949, test_loss: 1.118369\n",
      "Epoch 9: train_acc 0.504750, train_loss 0.975731, test_acc 0.470542, test_loss: 1.120108\n",
      "Epoch 10: train_acc 0.501289, train_loss 0.968991, test_acc 0.469970, test_loss: 1.121373\n",
      "Train with DAgger, iter 5\n",
      "Iteration 0\n",
      "rollout 1/3 return=-1.000000\n",
      "Iteration 1\n",
      "rollout 2/3 return=-1.000000\n",
      "Iteration 2\n",
      "rollout 3/3 return=-1.000000\n",
      "Return summary: mean=-1.000000, std=0.000000\n",
      "Train the agent with 15715 training data, batch_size 1024, epochs 10\n",
      "Epoch 1: train_acc 0.506471, train_loss 1.008882, test_acc 0.494089, test_loss: 1.057451\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 2: train_acc 0.512740, train_loss 0.968939, test_acc 0.499704, test_loss: 1.054768\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 3: train_acc 0.511231, train_loss 0.950318, test_acc 0.512362, test_loss: 1.051672\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 4: train_acc 0.520416, train_loss 0.939148, test_acc 0.491479, test_loss: 1.046290\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 5: train_acc 0.517168, train_loss 0.928554, test_acc 0.511141, test_loss: 1.044835\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 6: train_acc 0.522363, train_loss 0.918103, test_acc 0.513620, test_loss: 1.045766\n",
      "Epoch 7: train_acc 0.531666, train_loss 0.907849, test_acc 0.489141, test_loss: 1.047729\n",
      "Epoch 8: train_acc 0.535677, train_loss 0.897409, test_acc 0.499460, test_loss: 1.050692\n",
      "Epoch 9: train_acc 0.538520, train_loss 0.888155, test_acc 0.477986, test_loss: 1.050917\n",
      "Epoch 10: train_acc 0.549321, train_loss 0.878880, test_acc 0.502080, test_loss: 1.050780\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Train with DAgger, iter 6\n",
      "Iteration 0\n",
      "rollout 1/3 return=-1.000000\n",
      "Iteration 1\n",
      "rollout 2/3 return=-1.000000\n",
      "Iteration 2\n",
      "rollout 3/3 return=-1.000000\n",
      "Return summary: mean=-1.000000, std=0.000000\n",
      "Train the agent with 15941 training data, batch_size 1024, epochs 10\n",
      "Epoch 1: train_acc 0.541130, train_loss 0.915734, test_acc 0.540807, test_loss: 0.960362\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 2: train_acc 0.548339, train_loss 0.885638, test_acc 0.531711, test_loss: 0.960854\n",
      "Epoch 3: train_acc 0.550584, train_loss 0.871763, test_acc 0.542758, test_loss: 0.962277\n",
      "Epoch 4: train_acc 0.550812, train_loss 0.859368, test_acc 0.522981, test_loss: 0.964854\n",
      "Epoch 5: train_acc 0.549331, train_loss 0.850667, test_acc 0.545567, test_loss: 0.964457\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 6: train_acc 0.558493, train_loss 0.842910, test_acc 0.522432, test_loss: 0.966038\n",
      "Epoch 7: train_acc 0.564829, train_loss 0.835148, test_acc 0.549780, test_loss: 0.965551\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 8: train_acc 0.565013, train_loss 0.828267, test_acc 0.533418, test_loss: 0.971206\n",
      "Epoch 9: train_acc 0.571473, train_loss 0.820209, test_acc 0.542453, test_loss: 0.972818\n",
      "Epoch 10: train_acc 0.577783, train_loss 0.815351, test_acc 0.546055, test_loss: 0.977746\n",
      "Train with DAgger, iter 7\n",
      "Iteration 0\n",
      "rollout 1/3 return=-1.000000\n",
      "Iteration 1\n",
      "rollout 2/3 return=-1.000000\n",
      "Iteration 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rollout 3/3 return=-1.000000\n",
      "Return summary: mean=-1.000000, std=0.000000\n",
      "Train the agent with 16119 training data, batch_size 1024, epochs 10\n",
      "Epoch 1: train_acc 0.570773, train_loss 0.840628, test_acc 0.553737, test_loss: 0.945749\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 2: train_acc 0.572490, train_loss 0.826071, test_acc 0.545153, test_loss: 0.957939\n",
      "Epoch 3: train_acc 0.570228, train_loss 0.814939, test_acc 0.556474, test_loss: 0.960022\n",
      "Epoch 4: train_acc 0.583937, train_loss 0.806631, test_acc 0.556910, test_loss: 0.966437\n",
      "Epoch 5: train_acc 0.576898, train_loss 0.796249, test_acc 0.553081, test_loss: 0.972232\n",
      "Epoch 6: train_acc 0.579270, train_loss 0.791873, test_acc 0.562346, test_loss: 0.976642\n",
      "Epoch 7: train_acc 0.584326, train_loss 0.784014, test_acc 0.555818, test_loss: 0.981827\n",
      "Epoch 8: train_acc 0.589604, train_loss 0.777637, test_acc 0.557836, test_loss: 0.986137\n",
      "Epoch 9: train_acc 0.589984, train_loss 0.771646, test_acc 0.550820, test_loss: 0.989273\n",
      "Epoch 10: train_acc 0.589547, train_loss 0.765065, test_acc 0.557836, test_loss: 0.994999\n",
      "Train with DAgger, iter 8\n",
      "Iteration 0\n",
      "rollout 1/3 return=-1.000000\n",
      "Iteration 1\n",
      "rollout 2/3 return=-1.000000\n",
      "Iteration 2\n",
      "rollout 3/3 return=-1.000000\n",
      "Return summary: mean=-1.000000, std=0.000000\n",
      "Train the agent with 16344 training data, batch_size 1024, epochs 10\n",
      "Epoch 1: train_acc 0.591390, train_loss 0.794560, test_acc 0.554684, test_loss: 0.948753\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 2: train_acc 0.593637, train_loss 0.778138, test_acc 0.580949, test_loss: 0.942482\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 3: train_acc 0.599621, train_loss 0.761325, test_acc 0.557855, test_loss: 0.950087\n",
      "Epoch 4: train_acc 0.604440, train_loss 0.755450, test_acc 0.556634, test_loss: 0.956568\n",
      "Epoch 5: train_acc 0.598512, train_loss 0.748076, test_acc 0.580606, test_loss: 0.960401\n",
      "Epoch 6: train_acc 0.602650, train_loss 0.739765, test_acc 0.556342, test_loss: 0.966853\n",
      "Epoch 7: train_acc 0.601988, train_loss 0.735824, test_acc 0.563127, test_loss: 0.968203\n",
      "Epoch 8: train_acc 0.612994, train_loss 0.728932, test_acc 0.564444, test_loss: 0.972457\n",
      "Epoch 9: train_acc 0.614008, train_loss 0.723332, test_acc 0.566838, test_loss: 0.987265\n",
      "Epoch 10: train_acc 0.612949, train_loss 0.720519, test_acc 0.574159, test_loss: 0.994246\n",
      "Train with DAgger, iter 9\n",
      "Iteration 0\n",
      "rollout 1/3 return=-1.000000\n",
      "Iteration 1\n",
      "rollout 2/3 return=-1.000000\n",
      "Iteration 2\n",
      "rollout 3/3 return=-1.000000\n",
      "Return summary: mean=-1.000000, std=0.000000\n",
      "Train the agent with 16574 training data, batch_size 1024, epochs 10\n",
      "Epoch 1: train_acc 0.612801, train_loss 0.757553, test_acc 0.579614, test_loss: 0.888176\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 2: train_acc 0.606844, train_loss 0.740613, test_acc 0.587252, test_loss: 0.897248\n",
      "Epoch 3: train_acc 0.618446, train_loss 0.729231, test_acc 0.578934, test_loss: 0.912847\n",
      "Epoch 4: train_acc 0.618708, train_loss 0.726038, test_acc 0.568820, test_loss: 0.933991\n",
      "Epoch 5: train_acc 0.615255, train_loss 0.726920, test_acc 0.562297, test_loss: 0.995414\n",
      "Epoch 6: train_acc 0.613320, train_loss 0.743931, test_acc 0.541179, test_loss: 1.040091\n",
      "Epoch 7: train_acc 0.608361, train_loss 0.780690, test_acc 0.576265, test_loss: 0.916285\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 8: train_acc 0.605585, train_loss 0.767539, test_acc 0.574836, test_loss: 0.966542\n",
      "Epoch 9: train_acc 0.614073, train_loss 0.712674, test_acc 0.584444, test_loss: 0.918338\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 10: train_acc 0.621882, train_loss 0.689049, test_acc 0.592222, test_loss: 0.933713\n",
      "Train with DAgger, iter 10\n",
      "Iteration 0\n",
      "rollout 1/3 return=-1.000000\n",
      "Iteration 1\n",
      "rollout 2/3 return=-1.000000\n",
      "Iteration 2\n",
      "rollout 3/3 return=-1.000000\n",
      "Return summary: mean=-1.000000, std=0.000000\n",
      "Train the agent with 16826 training data, batch_size 1024, epochs 10\n",
      "Epoch 1: train_acc 0.634473, train_loss 0.704074, test_acc 0.607475, test_loss: 0.878905\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 2: train_acc 0.638968, train_loss 0.674230, test_acc 0.600074, test_loss: 0.883609\n",
      "Epoch 3: train_acc 0.642363, train_loss 0.661739, test_acc 0.585114, test_loss: 0.894582\n",
      "Epoch 4: train_acc 0.643362, train_loss 0.656306, test_acc 0.582916, test_loss: 0.911538\n",
      "Epoch 5: train_acc 0.648711, train_loss 0.650919, test_acc 0.588987, test_loss: 0.915080\n",
      "Epoch 6: train_acc 0.645642, train_loss 0.645274, test_acc 0.594105, test_loss: 0.918138\n",
      "Epoch 7: train_acc 0.649361, train_loss 0.641412, test_acc 0.581830, test_loss: 0.932093\n",
      "Epoch 8: train_acc 0.648775, train_loss 0.634698, test_acc 0.591453, test_loss: 0.955953\n",
      "Epoch 9: train_acc 0.658669, train_loss 0.632967, test_acc 0.591066, test_loss: 0.970386\n",
      "Epoch 10: train_acc 0.659023, train_loss 0.629374, test_acc 0.590594, test_loss: 0.976918\n",
      "Train with DAgger, iter 11\n",
      "Iteration 0\n",
      "rollout 1/3 return=-1.000000\n",
      "Iteration 1\n",
      "rollout 2/3 return=-1.000000\n",
      "Iteration 2\n",
      "rollout 3/3 return=-1.000000\n",
      "Return summary: mean=-1.000000, std=0.000000\n",
      "Train the agent with 16905 training data, batch_size 1024, epochs 10\n",
      "Epoch 1: train_acc 0.654797, train_loss 0.651726, test_acc 0.601734, test_loss: 0.914945\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 2: train_acc 0.657832, train_loss 0.640075, test_acc 0.609331, test_loss: 0.902424\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 3: train_acc 0.653565, train_loss 0.632159, test_acc 0.600709, test_loss: 0.911393\n",
      "Epoch 4: train_acc 0.662004, train_loss 0.627486, test_acc 0.589800, test_loss: 0.932063\n",
      "Epoch 5: train_acc 0.667688, train_loss 0.627887, test_acc 0.597870, test_loss: 0.960023\n",
      "Epoch 6: train_acc 0.662481, train_loss 0.630148, test_acc 0.584179, test_loss: 0.960454\n",
      "Epoch 7: train_acc 0.658702, train_loss 0.633947, test_acc 0.599278, test_loss: 0.951475\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 8: train_acc 0.662377, train_loss 0.628645, test_acc 0.601337, test_loss: 0.996188\n",
      "Epoch 9: train_acc 0.666747, train_loss 0.625930, test_acc 0.597019, test_loss: 1.041371\n",
      "Epoch 10: train_acc 0.659319, train_loss 0.621991, test_acc 0.588238, test_loss: 0.977786\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Train with DAgger, iter 12\n",
      "Iteration 0\n",
      "rollout 1/3 return=-1.000000\n",
      "Iteration 1\n",
      "rollout 2/3 return=-1.000000\n",
      "Iteration 2\n",
      "rollout 3/3 return=-1.000000\n",
      "Return summary: mean=-1.000000, std=0.000000\n",
      "Train the agent with 17231 training data, batch_size 1024, epochs 10\n",
      "Epoch 1: train_acc 0.652528, train_loss 0.664511, test_acc 0.618590, test_loss: 0.854093\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 2: train_acc 0.666373, train_loss 0.636763, test_acc 0.607557, test_loss: 0.863208\n",
      "Epoch 3: train_acc 0.665013, train_loss 0.619505, test_acc 0.617417, test_loss: 0.897032\n",
      "Epoch 4: train_acc 0.670608, train_loss 0.608773, test_acc 0.604834, test_loss: 0.896631\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 5: train_acc 0.675256, train_loss 0.598426, test_acc 0.621417, test_loss: 0.888328\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 6: train_acc 0.677046, train_loss 0.591251, test_acc 0.610825, test_loss: 0.893003\n",
      "Epoch 7: train_acc 0.674152, train_loss 0.587659, test_acc 0.617116, test_loss: 0.906195\n",
      "Epoch 8: train_acc 0.677525, train_loss 0.584148, test_acc 0.614646, test_loss: 0.921013\n",
      "Epoch 9: train_acc 0.679977, train_loss 0.581006, test_acc 0.604130, test_loss: 0.934550\n",
      "Epoch 10: train_acc 0.675501, train_loss 0.579111, test_acc 0.613961, test_loss: 0.943628\n",
      "Train with DAgger, iter 13\n",
      "Iteration 0\n",
      "rollout 1/3 return=-1.000000\n",
      "Iteration 1\n",
      "rollout 2/3 return=-1.000000\n",
      "Iteration 2\n",
      "rollout 3/3 return=-1.000000\n",
      "Return summary: mean=-1.000000, std=0.000000\n",
      "Train the agent with 17438 training data, batch_size 1024, epochs 10\n",
      "Epoch 1: train_acc 0.678017, train_loss 0.615770, test_acc 0.628944, test_loss: 0.861084\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 2: train_acc 0.677082, train_loss 0.599673, test_acc 0.630446, test_loss: 0.879386\n",
      "Epoch 3: train_acc 0.681005, train_loss 0.588908, test_acc 0.623422, test_loss: 0.896785\n",
      "Epoch 4: train_acc 0.676766, train_loss 0.583519, test_acc 0.623948, test_loss: 0.905978\n",
      "Epoch 5: train_acc 0.685385, train_loss 0.581019, test_acc 0.633432, test_loss: 0.911044\n",
      "Epoch 6: train_acc 0.684129, train_loss 0.580265, test_acc 0.622765, test_loss: 0.916153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: train_acc 0.681985, train_loss 0.582628, test_acc 0.624624, test_loss: 0.927492\n",
      "Epoch 8: train_acc 0.686483, train_loss 0.587632, test_acc 0.604304, test_loss: 0.954491\n",
      "Epoch 9: train_acc 0.681857, train_loss 0.593123, test_acc 0.608098, test_loss: 0.953000\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 10: train_acc 0.680574, train_loss 0.603570, test_acc 0.622089, test_loss: 0.937181\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Train with DAgger, iter 14\n",
      "Iteration 0\n",
      "rollout 1/3 return=-1.000000\n",
      "Iteration 1\n",
      "rollout 2/3 return=-1.000000\n",
      "Iteration 2\n",
      "rollout 3/3 return=-1.000000\n",
      "Return summary: mean=-1.000000, std=0.000000\n",
      "Train the agent with 17580 training data, batch_size 1024, epochs 10\n",
      "Epoch 1: train_acc 0.671994, train_loss 0.653797, test_acc 0.628156, test_loss: 0.913063\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 2: train_acc 0.670820, train_loss 0.633778, test_acc 0.643084, test_loss: 0.902197\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 3: train_acc 0.678506, train_loss 0.605154, test_acc 0.623722, test_loss: 0.883119\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 4: train_acc 0.685303, train_loss 0.579738, test_acc 0.637086, test_loss: 0.873873\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 5: train_acc 0.697223, train_loss 0.557238, test_acc 0.630798, test_loss: 0.893876\n",
      "Epoch 6: train_acc 0.693870, train_loss 0.554161, test_acc 0.633428, test_loss: 0.882011\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 7: train_acc 0.698239, train_loss 0.547243, test_acc 0.637953, test_loss: 0.901684\n",
      "Epoch 8: train_acc 0.702868, train_loss 0.539372, test_acc 0.632452, test_loss: 0.918662\n",
      "Epoch 9: train_acc 0.702094, train_loss 0.535722, test_acc 0.629991, test_loss: 0.929229\n",
      "Epoch 10: train_acc 0.705366, train_loss 0.532617, test_acc 0.625581, test_loss: 0.948510\n",
      "Train with DAgger, iter 15\n",
      "Iteration 0\n",
      "rollout 1/3 return=-1.000000\n",
      "Iteration 1\n",
      "rollout 2/3 return=-1.000000\n",
      "Iteration 2\n",
      "rollout 3/3 return=-1.000000\n",
      "Return summary: mean=-1.000000, std=0.000000\n",
      "Train the agent with 17727 training data, batch_size 1024, epochs 10\n",
      "Epoch 1: train_acc 0.690675, train_loss 0.591395, test_acc 0.638844, test_loss: 0.850537\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 2: train_acc 0.694576, train_loss 0.564859, test_acc 0.649206, test_loss: 0.851239\n",
      "Epoch 3: train_acc 0.698856, train_loss 0.557635, test_acc 0.638326, test_loss: 0.862760\n",
      "Epoch 4: train_acc 0.698320, train_loss 0.549059, test_acc 0.641861, test_loss: 0.868922\n",
      "Epoch 5: train_acc 0.698894, train_loss 0.545377, test_acc 0.641442, test_loss: 0.871571\n",
      "Epoch 6: train_acc 0.705283, train_loss 0.543644, test_acc 0.646481, test_loss: 0.891420\n",
      "Epoch 7: train_acc 0.700204, train_loss 0.542354, test_acc 0.626655, test_loss: 0.904717\n",
      "Epoch 8: train_acc 0.703425, train_loss 0.539323, test_acc 0.641960, test_loss: 0.900270\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 9: train_acc 0.701589, train_loss 0.536561, test_acc 0.634410, test_loss: 0.912678\n",
      "Epoch 10: train_acc 0.709015, train_loss 0.534128, test_acc 0.634859, test_loss: 0.944664\n",
      "Train with DAgger, iter 16\n",
      "Iteration 0\n",
      "rollout 1/3 return=-1.000000\n",
      "Iteration 1\n",
      "rollout 2/3 return=-1.000000\n",
      "Iteration 2\n",
      "rollout 3/3 return=-1.000000\n",
      "Return summary: mean=-1.000000, std=0.000000\n",
      "Train the agent with 17964 training data, batch_size 1024, epochs 10\n",
      "Epoch 1: train_acc 0.703056, train_loss 0.558653, test_acc 0.644080, test_loss: 0.880127\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 2: train_acc 0.703188, train_loss 0.567280, test_acc 0.641680, test_loss: 0.888519\n",
      "Epoch 3: train_acc 0.718065, train_loss 0.551055, test_acc 0.640064, test_loss: 0.852906\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 4: train_acc 0.713084, train_loss 0.533560, test_acc 0.643162, test_loss: 0.855274\n",
      "Epoch 5: train_acc 0.716535, train_loss 0.526639, test_acc 0.642455, test_loss: 0.864101\n",
      "Epoch 6: train_acc 0.725145, train_loss 0.512285, test_acc 0.643187, test_loss: 0.871388\n",
      "Epoch 7: train_acc 0.721743, train_loss 0.501407, test_acc 0.640712, test_loss: 0.914021\n",
      "Epoch 8: train_acc 0.722329, train_loss 0.500286, test_acc 0.643145, test_loss: 0.918828\n",
      "Epoch 9: train_acc 0.721205, train_loss 0.501649, test_acc 0.638103, test_loss: 0.911791\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 10: train_acc 0.728400, train_loss 0.498663, test_acc 0.633245, test_loss: 0.898391\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Train with DAgger, iter 17\n",
      "Iteration 0\n",
      "rollout 1/3 return=-1.000000\n",
      "Iteration 1\n",
      "rollout 2/3 return=-1.000000\n",
      "Iteration 2\n",
      "rollout 3/3 return=-1.000000\n",
      "Return summary: mean=-1.000000, std=0.000000\n",
      "Train the agent with 18178 training data, batch_size 1024, epochs 10\n",
      "Epoch 1: train_acc 0.703997, train_loss 0.571356, test_acc 0.662760, test_loss: 0.818103\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 2: train_acc 0.713035, train_loss 0.535920, test_acc 0.651338, test_loss: 0.854239\n",
      "Epoch 3: train_acc 0.711532, train_loss 0.526239, test_acc 0.657415, test_loss: 0.879426\n",
      "Epoch 4: train_acc 0.716551, train_loss 0.524696, test_acc 0.655680, test_loss: 0.879798\n",
      "Epoch 5: train_acc 0.715894, train_loss 0.523875, test_acc 0.647766, test_loss: 0.871334\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 6: train_acc 0.714657, train_loss 0.518362, test_acc 0.655462, test_loss: 0.867214\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 7: train_acc 0.713418, train_loss 0.512955, test_acc 0.650451, test_loss: 0.866134\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 8: train_acc 0.723772, train_loss 0.506327, test_acc 0.648209, test_loss: 0.883235\n",
      "Epoch 9: train_acc 0.721685, train_loss 0.503024, test_acc 0.636490, test_loss: 0.900605\n",
      "Epoch 10: train_acc 0.725637, train_loss 0.502430, test_acc 0.646654, test_loss: 0.916721\n",
      "Train with DAgger, iter 18\n",
      "Iteration 0\n",
      "rollout 1/3 return=-1.000000\n",
      "Iteration 1\n",
      "rollout 2/3 return=-1.000000\n",
      "Iteration 2\n",
      "rollout 3/3 return=-1.000000\n",
      "Return summary: mean=-1.000000, std=0.000000\n",
      "Train the agent with 18374 training data, batch_size 1024, epochs 10\n",
      "Epoch 1: train_acc 0.707927, train_loss 0.566989, test_acc 0.675801, test_loss: 0.789350\n",
      "Saving model\n",
      "Model was saved successfully\n",
      "Epoch 2: train_acc 0.710850, train_loss 0.534976, test_acc 0.664497, test_loss: 0.796761\n",
      "Epoch 3: train_acc 0.711521, train_loss 0.528549, test_acc 0.659522, test_loss: 0.803403\n",
      "Epoch 4: train_acc 0.712333, train_loss 0.524831, test_acc 0.655132, test_loss: 0.818617\n",
      "Epoch 5: train_acc 0.721963, train_loss 0.523053, test_acc 0.654077, test_loss: 0.832268\n",
      "Epoch 6: train_acc 0.716790, train_loss 0.524410, test_acc 0.672673, test_loss: 0.838427\n",
      "Epoch 7: train_acc 0.723437, train_loss 0.516539, test_acc 0.657103, test_loss: 0.845553\n",
      "Epoch 8: train_acc 0.721282, train_loss 0.510157, test_acc 0.666892, test_loss: 0.861887\n",
      "Epoch 9: train_acc 0.720985, train_loss 0.510098, test_acc 0.663788, test_loss: 0.883771\n",
      "Epoch 10: train_acc 0.721819, train_loss 0.510145, test_acc 0.656224, test_loss: 0.897994\n",
      "Train with DAgger, iter 19\n",
      "Iteration 0\n",
      "rollout 1/3 return=-1.000000\n",
      "Iteration 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-c3586e6dedcc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Train with DAgger, iter %i\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[1;33m(\u001b[0m\u001b[0mstimulated_env\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstimulator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstimulate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent_dagger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_rollouts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_rollouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstimulator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel_obs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpert\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstimulated_env\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstimulated_env\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-38dab233c57e>\u001b[0m in \u001b[0;36mstimulate\u001b[1;34m(self, agent, num_rollouts, render)\u001b[0m\n\u001b[0;32m     52\u001b[0m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m                 \u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_observations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m                 \u001b[0mall_actions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m                 \u001b[0mall_actions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_agent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pommerman-0.2.0-py3.6.egg\\pommerman\\envs\\v0.py\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[0magents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0magent\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_agents\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent_id\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_agent\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_observations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pommerman-0.2.0-py3.6.egg\\pommerman\\forward_model.py\u001b[0m in \u001b[0;36mact\u001b[1;34m(agents, obs, action_space, is_communicative)\u001b[0m\n\u001b[0;32m     98\u001b[0m                 \u001b[0mret\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mact_with_communication\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m                 \u001b[0mret\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mact_ex_communication\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pommerman-0.2.0-py3.6.egg\\pommerman\\forward_model.py\u001b[0m in \u001b[0;36mact_ex_communication\u001b[1;34m(agent)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mact_ex_communication\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_space\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mconstants\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pommerman-0.2.0-py3.6.egg\\pommerman\\agents\\simple_agent.py\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, obs, action_space)\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mammo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ammo'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mblast_strength\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'blast_strength'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mitems\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_djikstra\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmy_position\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbombs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menemies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;31m# Move if we are in an unsafe place.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pommerman-0.2.0-py3.6.egg\\pommerman\\agents\\simple_agent.py\u001b[0m in \u001b[0;36m_djikstra\u001b[1;34m(board, my_position, bombs, enemies, depth, exclude)\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mposition\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mQ\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mutility\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_is_passable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mposition\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menemies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m                 \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mposition\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                 \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Instantiate the environment\n",
    "config = ffa_v0_env()\n",
    "env = Pomme(**config[\"env_kwargs\"])\n",
    "states = {\n",
    "    \"board\": dict(shape=(BOARD_SIZE, BOARD_SIZE, 3,), type='float'),\n",
    "    \"state\": dict(shape=(3,), type='float')\n",
    "}\n",
    "agent_dagger = Agent(env.action_space.n)\n",
    "# Load Expert\n",
    "expert = Expert(config[\"agent\"](0, config[\"game_type\"]))\n",
    "\n",
    "# Generate training data\n",
    "stimulator = Stimulator(env, config)\n",
    "training_data = stimulator.stimulate(expert, num_rollouts=initial_rollouts, render=False)\n",
    "# Train DAgger Agent\n",
    "obs = training_data[0]\n",
    "labls = training_data[1]\n",
    "for i in range(2, iterations):\n",
    "    print(\"Train with DAgger, iter %i\" % i)\n",
    "    (stimulated_env, _) = stimulator.stimulate(agent_dagger, num_rollouts=num_rollouts, render=False)\n",
    "    labels = stimulator.label_obs(expert, stimulated_env)\n",
    "    obs = np.append(obs, stimulated_env, axis=0)\n",
    "    labls = np.append(labls, labels, axis=0)\n",
    "    agent_dagger.train(obs, labls, batch_size=batch_size, epochs=epochs)\n",
    "agent_dagger.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pommerman.characters import Bomber\n",
    "\n",
    "class TrainedAgent(BaseAgent):\n",
    "    def __init__(self, actions, seed=0, save_path=\"./model/model.cptk\", character=Bomber):\n",
    "        super(TrainedAgent, self).__init__(character=character)\n",
    "        self.save_path = save_path\n",
    "        self.actions = actions\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        \n",
    "        # TODO hardcoded\n",
    "        self.conv_ph = tf.placeholder(shape=[None, BOARD_SIZE, BOARD_SIZE, 3], name='conv_ph', dtype=tf.float32)\n",
    "        self.state_ph = tf.placeholder(shape=[None, 3], name='state_ph', dtype=tf.float32)\n",
    "\n",
    "        network = PommNetwork.create_network({'board': self.conv_ph, 'state': self.state_ph})\n",
    "        logits = tf.layers.dense(network, actions)\n",
    "        self.sampled_action = tf.squeeze(tf.multinomial(logits, 1), axis=[1])        \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.saver = tf.train.Saver()\n",
    "        if os.path.isdir(os.path.dirname(self.save_path)):\n",
    "            try:\n",
    "                print(\"Trying to load model\")\n",
    "                self.saver.restore(self.sess, self.save_path)\n",
    "                print(\"Model was loaded successful\")\n",
    "            except:\n",
    "                print(\"Model load failed\")\n",
    "                raise Exception(\"Model load failed\")    \n",
    "\n",
    "    @staticmethod\n",
    "    def featurize(obs):\n",
    "        def get_matrix(dict, key):\n",
    "            res = dict[key]\n",
    "            return res.reshape(res.shape[0], res.shape[1], 1).astype(np.float32)\n",
    "\n",
    "        board = get_matrix(obs, 'board')\n",
    "        teammate_position = None\n",
    "        teammate = obs[\"teammate\"]\n",
    "        if teammate is not None:\n",
    "            teammate = teammate.value\n",
    "            if teammate > 10 and teammate < 15:\n",
    "                teammate_position = np.argwhere(board == teammate)[0]\n",
    "        else:\n",
    "            teammate = None\n",
    "        # My self - 11\n",
    "        # Team mate - 12\n",
    "        # Enemy - 13\n",
    "\n",
    "        # Everyone enemy\n",
    "        board[(board > 10) & (board < 15)] = 13\n",
    "        # I'm not enemy\n",
    "        my_position = obs['position']\n",
    "        board[my_position[0], my_position[1], 0] = 11\n",
    "        # Set teammate\n",
    "        if teammate_position is not None:\n",
    "            board[teammate_position[0], teammate_position[1], teammate_position[2]] = 12\n",
    "\n",
    "        bomb_blast_strength = get_matrix(obs, 'bomb_blast_strength')\n",
    "        bomb_life = get_matrix(obs, 'bomb_life')\n",
    "        conv_inp = np.concatenate([board, bomb_blast_strength, bomb_life], axis=2)\n",
    "        state = np.array([obs[\"ammo\"], obs[\"blast_strength\"], obs[\"can_kick\"]]).astype(np.float32)\n",
    "        return dict(board=conv_inp, state=state)\n",
    "    \n",
    "    def act(self, obs):\n",
    "        obs = self.featurize(obs)\n",
    "        board = np.expand_dims(obs['board'], axis=0)\n",
    "        state = np.expand_dims(obs['state'], axis=0)\n",
    "        res = self.sess.run(self.sampled_action, feed_dict={self.conv_ph: board, self.state_ph: state})\n",
    "        return res\n",
    "\n",
    "    def close(self):\n",
    "        self.sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the environment\n",
    "config = ffa_v0_env()\n",
    "env = Pomme(**config[\"env_kwargs\"])\n",
    "env.seed(0)\n",
    "\n",
    "# Add 3 random agents\n",
    "agents = []\n",
    "for agent_id in range(3):\n",
    "    agents.append(SimpleAgent(config[\"agent\"](agent_id, config[\"game_type\"])))\n",
    "\n",
    "# Add TensorforceAgent\n",
    "agent_id += 1\n",
    "agents.append(TrainedAgent(env.action_space.n, character=config[\"agent\"](agent_id, config[\"game_type\"])))\n",
    "env.set_agents(agents)\n",
    "env.set_init_game_state(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed and reset the environment\n",
    "env.seed(0)\n",
    "obs = env.reset()\n",
    "\n",
    "# Run the agents until we're done\n",
    "done = False\n",
    "while not done:\n",
    "    env.render()\n",
    "    actions = env.act(obs)\n",
    "    obs, reward, done, info = env.step(actions)\n",
    "env.render(close=True)\n",
    "env.close()\n",
    "\n",
    "# Print the result\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
