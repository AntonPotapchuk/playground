{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pommerman\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from pommerman.agents import BaseAgent, SimpleAgent\n",
    "from pommerman.configs import ffa_v0_env\n",
    "from pommerman.constants import BOARD_SIZE\n",
    "from pommerman.envs.v0 import Pomme\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "# Immitation Learning: learn a mapping from observations to actions.\n",
    "from pomm_network import PommNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "epochs = 10\n",
    "num_rollouts = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, actions, seed=0, save_path=\"./model/model.cptk\", log_path='./logs/', save_best_model=True, learning_rate=1e-3):\n",
    "        self.save_path = save_path\n",
    "        self.actions = actions\n",
    "        self.save_best_model = save_best_model\n",
    "        self.seed = seed\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.rewards = []\n",
    "        if os.path.isdir(log_path):\n",
    "            try:\n",
    "                shutil.rmtree(log_path, ignore_errors=True)\n",
    "            except:\n",
    "                print(\"Cant delete log folder\")\n",
    "\n",
    "        # TODO hardcoded\n",
    "        self.conv_ph = tf.placeholder(shape=[None, BOARD_SIZE, BOARD_SIZE, 3], name='conv_ph', dtype=tf.float32)\n",
    "        self.state_ph = tf.placeholder(shape=[None, 3], name='state_ph', dtype=tf.float32)\n",
    "        self.logits_ph = tf.placeholder(shape=[None, actions], name='logits_ph', dtype=tf.int32)\n",
    "\n",
    "        network = PommNetwork.create_network({'board': self.conv_ph, 'state': self.state_ph})\n",
    "        logits = tf.layers.dense(network, actions)\n",
    "        self.sampled_action = tf.squeeze(tf.multinomial(logits, 1), axis=[1])\n",
    "        sy_logprob_n = tf.nn.softmax_cross_entropy_with_logits(labels=self.logits_ph, logits=logits)\n",
    "        self.loss = tf.reduce_mean(sy_logprob_n)  # Loss function that we'll differentiate to get the policy gradient.\n",
    "        self.train_step = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(self.loss)\n",
    "        self.train_writer = tf.summary.FileWriter(log_path + \"train\", self.sess.graph)\n",
    "        self.test_writer = tf.summary.FileWriter(log_path + \"test\")\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.saver = tf.train.Saver()\n",
    "        if os.path.isdir(os.path.dirname(self.save_path)):\n",
    "            try:\n",
    "                print(\"Trying to load model\")\n",
    "                self.saver.restore(self.sess, self.save_path)\n",
    "                print(\"Model was loaded successful\")\n",
    "            except:\n",
    "                print(\"Model load failed\")\n",
    "\n",
    "    def __run_batch(self, X, y, batch_size, training=True):\n",
    "        accuracies = []\n",
    "        losses = []\n",
    "        size = X.shape[-1]\n",
    "        batches = int(np.ceil(size / batch_size))\n",
    "        for i in range(batches):\n",
    "            X_batch, y_batch = None, None\n",
    "            if i == batches - 1:\n",
    "                X_batch = X[i * batch_size:]\n",
    "                y_batch = y[i * batch_size:]\n",
    "            else:\n",
    "                X_batch = X[i * batch_size: (i + 1) * batch_size]\n",
    "                y_batch = y[i * batch_size: (i + 1) * batch_size]\n",
    "            board = []\n",
    "            state = []\n",
    "            for i in range(X_batch.shape[-1]):\n",
    "                val = self.featurize(X_batch[i])\n",
    "                board.append(val['board'])\n",
    "                state.append(val['state'])\n",
    "            y_batch = np.array(y_batch).reshape(-1)\n",
    "            feed_dict = {self.conv_ph: board, self.state_ph: state, self.logits_ph: to_categorical(y_batch, self.actions)}\n",
    "            if training:\n",
    "                _, loss, actions = self.sess.run([self.train_step, self.loss, self.sampled_action], feed_dict=feed_dict)\n",
    "            else:\n",
    "                loss, actions = self.sess.run([self.loss, self.sampled_action], feed_dict=feed_dict)\n",
    "            accuracies.append(accuracy_score(actions, y_batch))\n",
    "            losses.append(loss)\n",
    "        return np.mean(accuracies), np.mean(losses)\n",
    "\n",
    "    def train(self, obs, labels, batch_size, epochs):\n",
    "        print(\"Train the agent with %i training data, batch_size %i, epochs %i\" % (obs.shape[0], batch_size, epochs))\n",
    "        train_obs, val_obs, train_labels, val_labels = train_test_split(obs, labels, test_size=0.2, random_state=self.seed)\n",
    "        prev_loss = np.inf\n",
    "        for i in range(epochs):\n",
    "            train_acc, train_loss = self.__run_batch(train_obs, train_labels, batch_size)\n",
    "            val_acc, val_loss = self.__run_batch(val_obs, val_labels, batch_size, training=False)\n",
    "            print(\"Epoch %d: train_acc %f, train_loss %f, test_acc %f, test_loss: %f\" % (i, train_acc, train_loss, val_acc, val_loss))\n",
    "\n",
    "            summary = tf.summary.Summary()\n",
    "            summary.value.add(tag=\"Train acc\", simple_value=train_acc)\n",
    "            summary.value.add(tag=\"Train loss\", simple_value=train_loss)\n",
    "            self.train_writer.add_summary(summary, i)\n",
    "            summary.value.add(tag=\"Val acc\", simple_value=val_acc)\n",
    "            summary.value.add(tag=\"Val loss\", simple_value=val_loss)\n",
    "            self.test_writer.add_summary(summary, i)\n",
    "            try:\n",
    "                if self.save_best_model:\n",
    "                    if val_loss < prev_loss:\n",
    "                        print(\"Saving model\")\n",
    "                        self.saver.save(self.sess, self.save_path)\n",
    "                        print(\"Model was saved successfully\")\n",
    "                else:\n",
    "                    print(\"Saving model\")\n",
    "                    self.saver.save(self.sess, self.save_path)\n",
    "                    print(\"Model was saved successfully\")\n",
    "            except:\n",
    "                print(\"Failed save model\")\n",
    "            prev_loss = val_loss\n",
    "\n",
    "    @staticmethod\n",
    "    def featurize(obs):\n",
    "        def get_matrix(dict, key):\n",
    "            res = dict[key]\n",
    "            return res.reshape(res.shape[0], res.shape[1], 1).astype(np.float32)\n",
    "\n",
    "        board = get_matrix(obs, 'board')\n",
    "        teammate_position = None\n",
    "        teammate = obs[\"teammate\"]\n",
    "        if teammate is not None:\n",
    "            teammate = teammate.value\n",
    "            if teammate > 10 and teammate < 15:\n",
    "                teammate_position = np.argwhere(board == teammate)[0]\n",
    "        else:\n",
    "            teammate = None\n",
    "        # My self - 11\n",
    "        # Team mate - 12\n",
    "        # Enemy - 13\n",
    "\n",
    "        # Everyone enemy\n",
    "        board[(board > 10) & (board < 15)] = 13\n",
    "        # I'm not enemy\n",
    "        my_position = obs['position']\n",
    "        board[my_position[0], my_position[1], 0] = 11\n",
    "        # Set teammate\n",
    "        if teammate_position is not None:\n",
    "            board[teammate_position[0], teammate_position[1], teammate_position[2]] = 12\n",
    "\n",
    "        bomb_blast_strength = get_matrix(obs, 'bomb_blast_strength')\n",
    "        bomb_life = get_matrix(obs, 'bomb_life')\n",
    "        conv_inp = np.concatenate([board, bomb_blast_strength, bomb_life], axis=2)\n",
    "        state = np.array([obs[\"ammo\"], obs[\"blast_strength\"], obs[\"can_kick\"]]).astype(np.float32)\n",
    "        return dict(board=conv_inp, state=state)\n",
    "\n",
    "    def clear_training_history(self):\n",
    "        self.history = []\n",
    "\n",
    "    def act(self, obs):\n",
    "        obs = self.featurize(obs)\n",
    "        board = np.expand_dims(obs['board'], axis=0)\n",
    "        state = np.expand_dims(obs['state'], axis=0)\n",
    "        res = self.sess.run(self.sampled_action, feed_dict={self.conv_ph: board, self.state_ph: state})\n",
    "        return res\n",
    "\n",
    "    def record_reward(self, reward):\n",
    "        self.rewards.append(np.mean(reward))\n",
    "\n",
    "    def close(self):\n",
    "        self.sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple wrapper around policy function to have an act function\n",
    "class Expert:\n",
    "    def __init__(self, config):\n",
    "        self.__agent = SimpleAgent(config)\n",
    "\n",
    "    def act(self, obs):\n",
    "        return self.__agent.act(obs, None)\n",
    "\n",
    "    def record_reward(self, reward):\n",
    "        pass\n",
    "\n",
    "\n",
    "class TensorforceAgent(BaseAgent):\n",
    "    def act(self, obs, action_space):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Environment wrapper\n",
    "class Stimulator:\n",
    "    def __init__(self, env, config):\n",
    "        self.env = env\n",
    "        self.init(config)\n",
    "\n",
    "    def init(self, config):\n",
    "        self.env.seed(0)\n",
    "        # Add 3 random agents\n",
    "        agents = []\n",
    "        for agent_id in range(3):\n",
    "            agents.append(SimpleAgent(config[\"agent\"](agent_id, config[\"game_type\"])))\n",
    "\n",
    "        # Add TensorforceAgent\n",
    "        agent_id += 1\n",
    "        agents.append(TensorforceAgent(config[\"agent\"](agent_id, config[\"game_type\"])))\n",
    "        self.env.set_agents(agents)\n",
    "        self.env.set_training_agent(agents[-1].agent_id)\n",
    "        self.env.set_init_game_state(None)\n",
    "\n",
    "    def stimulate(self, agent, num_rollouts, render):\n",
    "        returns = []\n",
    "        observations = []\n",
    "        actions = []\n",
    "        for i in range(num_rollouts):\n",
    "            print('Iteration', i)\n",
    "            obs = self.env.reset()[self.env.training_agent]\n",
    "            done = False\n",
    "            totalr = 0.\n",
    "            steps = 0\n",
    "            while not done:\n",
    "                if render:\n",
    "                    self.env.render()\n",
    "\n",
    "                action = agent.act(obs)\n",
    "                observations.append(obs)\n",
    "                actions.append(action)\n",
    "\n",
    "                obs = self.env.get_observations()\n",
    "                all_actions = self.env.act(obs)\n",
    "                all_actions.insert(self.env.training_agent, action)\n",
    "                state, reward, done, _ = self.env.step(all_actions)\n",
    "\n",
    "                obs = state[self.env.training_agent]\n",
    "                r = reward[self.env.training_agent]\n",
    "                totalr += r\n",
    "                steps += 1\n",
    "\n",
    "            print('rollout %i/%i return=%f' % (i + 1, num_rollouts, totalr))\n",
    "            returns.append(totalr)\n",
    "        print('Return summary: mean=%f, std=%f' % (np.mean(returns), np.std(returns)))\n",
    "        agent.record_reward(returns)\n",
    "        return (np.array(observations), np.array(actions))\n",
    "\n",
    "    def label_obs(self, expert, obs):\n",
    "        actions = []\n",
    "        for o in obs:\n",
    "            actions.append(expert.act(o))\n",
    "        return np.array(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to load model\n",
      "INFO:tensorflow:Restoring parameters from ./model/model.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-03 00:14:47,774] Restoring parameters from ./model/model.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was loaded successful\n",
      "Iteration 0\n",
      "rollout 1/3 return=1.000000\n",
      "Iteration 1\n",
      "rollout 2/3 return=-1.000000\n",
      "Iteration 2\n",
      "rollout 3/3 return=1.000000\n",
      "Return summary: mean=0.333333, std=0.942809\n",
      "Train with DAgger, iter 2\n",
      "Iteration 0\n",
      "rollout 1/3 return=-1.000000\n",
      "Iteration 1\n",
      "rollout 2/3 return=-1.000000\n",
      "Iteration 2\n",
      "rollout 3/3 return=-1.000000\n",
      "Return summary: mean=-1.000000, std=0.000000\n",
      "Train the agent with 2570 training data, batch_size 1024, epochs 10\n",
      "Epoch 0: train_acc 0.290365, train_loss 2.694820, test_acc 0.295720, test_loss: 2.564937\n",
      "Epoch 1: train_acc 0.535156, train_loss 1.768964, test_acc 0.252918, test_loss: 2.264899\n",
      "Epoch 2: train_acc 0.354492, train_loss 1.515953, test_acc 0.268482, test_loss: 1.857912\n",
      "Epoch 3: train_acc 0.407552, train_loss 1.293827, test_acc 0.295720, test_loss: 1.667269\n",
      "Epoch 4: train_acc 0.421224, train_loss 1.180040, test_acc 0.322957, test_loss: 1.556439\n",
      "Epoch 5: train_acc 0.359701, train_loss 1.126234, test_acc 0.311284, test_loss: 1.499944\n",
      "Failed save model\n",
      "Epoch 6: train_acc 0.449870, train_loss 1.095609, test_acc 0.336576, test_loss: 1.440589\n",
      "Epoch 7: train_acc 0.427734, train_loss 1.062717, test_acc 0.321012, test_loss: 1.415391\n",
      "Epoch 8: train_acc 0.476562, train_loss 1.035632, test_acc 0.321012, test_loss: 1.414773\n",
      "Epoch 9: train_acc 0.477214, train_loss 1.015721, test_acc 0.330739, test_loss: 1.404316\n",
      "Train with DAgger, iter 3\n",
      "Iteration 0\n",
      "rollout 1/3 return=-1.000000\n",
      "Iteration 1\n",
      "rollout 2/3 return=-1.000000\n",
      "Iteration 2\n",
      "rollout 3/3 return=-1.000000\n",
      "Return summary: mean=-1.000000, std=0.000000\n",
      "Train the agent with 2995 training data, batch_size 1024, epochs 10\n",
      "Epoch 0: train_acc 0.366847, train_loss 1.365644, test_acc 0.370618, test_loss: 1.362843\n",
      "Epoch 1: train_acc 0.378985, train_loss 1.318347, test_acc 0.353923, test_loss: 1.336840\n",
      "Epoch 2: train_acc 0.371322, train_loss 1.270870, test_acc 0.368948, test_loss: 1.302226\n",
      "Failed save model\n",
      "Epoch 3: train_acc 0.392275, train_loss 1.224315, test_acc 0.342237, test_loss: 1.277846\n",
      "Epoch 4: train_acc 0.423020, train_loss 1.190480, test_acc 0.380634, test_loss: 1.255555\n",
      "Epoch 5: train_acc 0.409554, train_loss 1.155755, test_acc 0.388982, test_loss: 1.237470\n",
      "Failed save model\n",
      "Epoch 6: train_acc 0.415413, train_loss 1.125207, test_acc 0.412354, test_loss: 1.219968\n",
      "Epoch 7: train_acc 0.449406, train_loss 1.096251, test_acc 0.400668, test_loss: 1.206781\n",
      "Epoch 8: train_acc 0.448987, train_loss 1.070975, test_acc 0.412354, test_loss: 1.194542\n",
      "Failed save model\n",
      "Epoch 9: train_acc 0.465996, train_loss 1.046726, test_acc 0.404007, test_loss: 1.184585\n",
      "Train with DAgger, iter 4\n",
      "Iteration 0\n",
      "rollout 1/3 return=-1.000000\n",
      "Iteration 1\n",
      "rollout 2/3 return=-1.000000\n",
      "Iteration 2\n",
      "rollout 3/3 return=-1.000000\n",
      "Return summary: mean=-1.000000, std=0.000000\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-21ad8a1e1928>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Train with DAgger, iter %i\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[0mstimulated_env\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstimulator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstimulate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent_dagger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_rollouts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_rollouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstimulator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel_obs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpert\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstimulated_env\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstimulated_env\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mlabls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-f83946a8e165>\u001b[0m in \u001b[0;36mlabel_obs\u001b[1;34m(self, expert, obs)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m             \u001b[0mactions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpert\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-f83946a8e165>\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrecord_reward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pommerman-0.2.0-py3.6.egg\\pommerman\\agents\\simple_agent.py\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, obs, action_space)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;31m# Move towards an enemy if there is one in exactly three reachable spaces.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0mdirection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_near_enemy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmy_position\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menemies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdirection\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prev_direction\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mdirection\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prev_direction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdirection\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pommerman-0.2.0-py3.6.egg\\pommerman\\agents\\simple_agent.py\u001b[0m in \u001b[0;36m_near_enemy\u001b[1;34m(cls, my_position, items, dist, prev, enemies, radius)\u001b[0m\n\u001b[0;32m    347\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_near_enemy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmy_position\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menemies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mradius\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m         \u001b[0mnearest_enemy_position\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nearest_position\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menemies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mradius\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_direction_towards_position\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmy_position\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnearest_enemy_position\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pommerman-0.2.0-py3.6.egg\\pommerman\\agents\\simple_agent.py\u001b[0m in \u001b[0;36m_get_direction_towards_position\u001b[1;34m(my_position, position, prev)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m         \u001b[0mnext_position\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mposition\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m         \u001b[1;32mwhile\u001b[0m \u001b[0mprev\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnext_position\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mmy_position\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    342\u001b[0m             \u001b[0mnext_position\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprev\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnext_position\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: None"
     ]
    }
   ],
   "source": [
    "# Instantiate the environment\n",
    "config = ffa_v0_env()\n",
    "env = Pomme(**config[\"env_kwargs\"])\n",
    "states = {\n",
    "    \"board\": dict(shape=(BOARD_SIZE, BOARD_SIZE, 3,), type='float'),\n",
    "    \"state\": dict(shape=(3,), type='float')\n",
    "}\n",
    "agent_dagger = Agent(env.action_space.n)\n",
    "# Load Expert\n",
    "expert = Expert(config[\"agent\"](0, config[\"game_type\"]))\n",
    "\n",
    "# Generate training data\n",
    "stimulator = Stimulator(env, config)\n",
    "training_data = stimulator.stimulate(expert, num_rollouts=num_rollouts, render=False)\n",
    "# Train DAgger Agent\n",
    "obs = training_data[0]\n",
    "labls = training_data[1]\n",
    "for i in range(2, 15):\n",
    "    print(\"Train with DAgger, iter %i\" % i)\n",
    "    (stimulated_env, _) = stimulator.stimulate(agent_dagger, num_rollouts=num_rollouts, render=False)\n",
    "    labels = stimulator.label_obs(expert, stimulated_env)\n",
    "    obs = np.append(obs, stimulated_env, axis=0)\n",
    "    labls = np.append(labls, labels, axis=0)\n",
    "    agent_dagger.train(obs, labls, batch_size=batch_size, epochs=epochs)\n",
    "agent_dagger.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
