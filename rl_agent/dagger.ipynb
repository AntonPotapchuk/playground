{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pommerman\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from pommerman.agents import BaseAgent, SimpleAgent\n",
    "from pommerman.configs import ffa_v0_env\n",
    "from pommerman.constants import BOARD_SIZE\n",
    "from pommerman.envs.v0 import Pomme\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "# Immitation Learning: learn a mapping from observations to actions.\n",
    "from pomm_network import PommNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "epochs = 10\n",
    "num_rollouts = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, actions, seed=0, save_path=\"./model/model.cptk\", log_path='./logs/', save_best_model=True, learning_rate=1e-3):\n",
    "        self.save_path = save_path\n",
    "        self.actions = actions\n",
    "        self.save_best_model = save_best_model\n",
    "        self.seed = seed\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.rewards = []\n",
    "        if os.path.isdir(log_path):\n",
    "            shutil.rmtree(log_path)\n",
    "\n",
    "        # TODO hardcoded\n",
    "        self.conv_ph = tf.placeholder(shape=[None, BOARD_SIZE, BOARD_SIZE, 3], name='conv_ph', dtype=tf.float32)\n",
    "        self.state_ph = tf.placeholder(shape=[None, 3], name='state_ph', dtype=tf.float32)\n",
    "        self.logits_ph = tf.placeholder(shape=[None, actions], name='logits_ph', dtype=tf.int32)\n",
    "\n",
    "        network = PommNetwork.create_network({'board': self.conv_ph, 'state': self.state_ph})\n",
    "        logits = tf.layers.dense(network, actions)\n",
    "        self.sampled_action = tf.squeeze(tf.multinomial(logits, 1), axis=[1])\n",
    "        sy_logprob_n = tf.nn.softmax_cross_entropy_with_logits(labels=self.logits_ph, logits=logits)\n",
    "        self.loss = tf.reduce_mean(sy_logprob_n)  # Loss function that we'll differentiate to get the policy gradient.\n",
    "        self.train_step = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(self.loss)\n",
    "        self.train_writer = tf.summary.FileWriter(log_path + \"train\", self.sess.graph)\n",
    "        self.test_writer = tf.summary.FileWriter(log_path + \"test\")\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.saver = tf.train.Saver()\n",
    "        if os.path.isdir(os.path.dirname(self.save_path)):\n",
    "            try:\n",
    "                print(\"Trying to load model\")\n",
    "                self.saver.restore(self.sess, self.save_path)\n",
    "                print(\"Model was loaded successful\")\n",
    "            except:\n",
    "                print(\"Model load failed\")\n",
    "\n",
    "    def __run_batch(self, X, y, batch_size, training=True):\n",
    "        accuracies = []\n",
    "        losses = []\n",
    "        size = X.shape[-1]\n",
    "        batches = int(np.ceil(size / batch_size))\n",
    "        for i in range(batches):\n",
    "            X_batch, y_batch = None, None\n",
    "            if i == batches - 1:\n",
    "                X_batch = X[i * batch_size:]\n",
    "                y_batch = y[i * batch_size:]\n",
    "            else:\n",
    "                X_batch = X[i * batch_size: (i + 1) * batch_size]\n",
    "                y_batch = y[i * batch_size: (i + 1) * batch_size]\n",
    "            board = []\n",
    "            state = []\n",
    "            for i in range(X_batch.shape[-1]):\n",
    "                val = self.featurize(X_batch[i])\n",
    "                board.append(val['board'])\n",
    "                state.append(val['state'])\n",
    "            y_batch = np.array(y_batch).reshape(-1)\n",
    "            feed_dict = {self.conv_ph: board, self.state_ph: state, self.logits_ph: to_categorical(y_batch, self.actions)}\n",
    "            if training:\n",
    "                _, loss, actions = self.sess.run([self.train_step, self.loss, self.sampled_action], feed_dict=feed_dict)\n",
    "            else:\n",
    "                loss, actions = self.sess.run([self.loss, self.sampled_action], feed_dict=feed_dict)\n",
    "            accuracies.append(accuracy_score(actions, y_batch))\n",
    "            losses.append(loss)\n",
    "        return np.mean(accuracies), np.mean(losses)\n",
    "\n",
    "    def train(self, obs, labels, batch_size, epochs):\n",
    "        print(\"Train the agent with %i training data, batch_size %i, epochs %i\" % (obs.shape[0], batch_size, epochs))\n",
    "        train_obs, val_obs, train_labels, val_labels = train_test_split(obs, labels, test_size=0.2, random_state=self.seed)\n",
    "        prev_loss = np.inf\n",
    "        for i in range(epochs):\n",
    "            train_acc, train_loss = self.__run_batch(train_obs, train_labels, batch_size)\n",
    "            val_acc, val_loss = self.__run_batch(val_obs, val_labels, batch_size, training=False)\n",
    "            print(\"Epoch %d: train_acc %f, train_loss %f, test_acc %f, test_loss: %f\" % (i, train_acc, train_loss, val_acc, val_loss))\n",
    "\n",
    "            summary = tf.summary.Summary()\n",
    "            summary.value.add(tag=\"Train acc\", simple_value=train_acc)\n",
    "            summary.value.add(tag=\"Train loss\", simple_value=train_loss)\n",
    "            self.train_writer.add_summary(summary, i)\n",
    "            summary.value.add(tag=\"Val acc\", simple_value=val_acc)\n",
    "            summary.value.add(tag=\"Val loss\", simple_value=val_loss)\n",
    "            self.test_writer.add_summary(summary, i)\n",
    "            try:\n",
    "                if self.save_best_model:\n",
    "                    if val_loss < prev_loss:\n",
    "                        self.saver.save(self.sess, self.save_path)\n",
    "                else:\n",
    "                    self.saver.save(self.sess, self.save_path)\n",
    "            except:\n",
    "                print(\"Failed save model\")\n",
    "            prev_loss = val_loss\n",
    "\n",
    "    @staticmethod\n",
    "    def featurize(obs):\n",
    "        def get_matrix(dict, key):\n",
    "            res = dict[key]\n",
    "            return res.reshape(res.shape[0], res.shape[1], 1).astype(np.float32)\n",
    "\n",
    "        board = get_matrix(obs, 'board')\n",
    "        teammate_position = None\n",
    "        teammate = obs[\"teammate\"]\n",
    "        if teammate is not None:\n",
    "            teammate = teammate.value\n",
    "            if teammate > 10 and teammate < 15:\n",
    "                teammate_position = np.argwhere(board == teammate)[0]\n",
    "        else:\n",
    "            teammate = None\n",
    "        # My self - 11\n",
    "        # Team mate - 12\n",
    "        # Enemy - 13\n",
    "\n",
    "        # Everyone enemy\n",
    "        board[(board > 10) & (board < 15)] = 13\n",
    "        # I'm not enemy\n",
    "        my_position = obs['position']\n",
    "        board[my_position[0], my_position[1], 0] = 11\n",
    "        # Set teammate\n",
    "        if teammate_position is not None:\n",
    "            board[teammate_position[0], teammate_position[1], teammate_position[2]] = 12\n",
    "\n",
    "        bomb_blast_strength = get_matrix(obs, 'bomb_blast_strength')\n",
    "        bomb_life = get_matrix(obs, 'bomb_life')\n",
    "        conv_inp = np.concatenate([board, bomb_blast_strength, bomb_life], axis=2)\n",
    "        state = np.array([obs[\"ammo\"], obs[\"blast_strength\"], obs[\"can_kick\"]]).astype(np.float32)\n",
    "        return dict(board=conv_inp, state=state)\n",
    "\n",
    "    def clear_training_history(self):\n",
    "        self.history = []\n",
    "\n",
    "    def act(self, obs):\n",
    "        obs = self.featurize(obs)\n",
    "        board = np.expand_dims(obs['board'], axis=0)\n",
    "        state = np.expand_dims(obs['state'], axis=0)\n",
    "        res = self.sess.run(self.sampled_action, feed_dict={self.conv_ph: board, self.state_ph: state})\n",
    "        return res\n",
    "\n",
    "    def record_reward(self, reward):\n",
    "        self.rewards.append(np.mean(reward))\n",
    "\n",
    "    def close(self):\n",
    "        self.sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple wrapper around policy function to have an act function\n",
    "class Expert:\n",
    "    def __init__(self, config):\n",
    "        self.__agent = SimpleAgent(config)\n",
    "\n",
    "    def act(self, obs):\n",
    "        return self.__agent.act(obs, None)\n",
    "\n",
    "    def record_reward(self, reward):\n",
    "        pass\n",
    "\n",
    "\n",
    "class TensorforceAgent(BaseAgent):\n",
    "    def act(self, obs, action_space):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Environment wrapper\n",
    "class Stimulator:\n",
    "    def __init__(self, env, config):\n",
    "        self.env = env\n",
    "        self.init(config)\n",
    "\n",
    "    def init(self, config):\n",
    "        self.env.seed(0)\n",
    "        # Add 3 random agents\n",
    "        agents = []\n",
    "        for agent_id in range(3):\n",
    "            agents.append(SimpleAgent(config[\"agent\"](agent_id, config[\"game_type\"])))\n",
    "\n",
    "        # Add TensorforceAgent\n",
    "        agent_id += 1\n",
    "        agents.append(TensorforceAgent(config[\"agent\"](agent_id, config[\"game_type\"])))\n",
    "        self.env.set_agents(agents)\n",
    "        self.env.set_training_agent(agents[-1].agent_id)\n",
    "        self.env.set_init_game_state(None)\n",
    "\n",
    "    def stimulate(self, agent, num_rollouts, render):\n",
    "        returns = []\n",
    "        observations = []\n",
    "        actions = []\n",
    "        for i in range(num_rollouts):\n",
    "            print('Iteration', i)\n",
    "            obs = self.env.reset()[self.env.training_agent]\n",
    "            done = False\n",
    "            totalr = 0.\n",
    "            steps = 0\n",
    "            while not done:\n",
    "                if render:\n",
    "                    self.env.render()\n",
    "\n",
    "                action = agent.act(obs)\n",
    "                observations.append(obs)\n",
    "                actions.append(action)\n",
    "\n",
    "                obs = self.env.get_observations()\n",
    "                all_actions = self.env.act(obs)\n",
    "                all_actions.insert(self.env.training_agent, action)\n",
    "                state, reward, done, _ = self.env.step(all_actions)\n",
    "\n",
    "                obs = state[self.env.training_agent]\n",
    "                r = reward[self.env.training_agent]\n",
    "                totalr += r\n",
    "                steps += 1\n",
    "\n",
    "            print('rollout %i/%i return=%f' % (i + 1, num_rollouts, totalr))\n",
    "            returns.append(totalr)\n",
    "        print('Return summary: mean=%f, std=%f' % (np.mean(returns), np.std(returns)))\n",
    "        agent.record_reward(returns)\n",
    "        return (np.array(observations), np.array(actions))\n",
    "\n",
    "    def label_obs(self, expert, obs):\n",
    "        actions = []\n",
    "        for o in obs:\n",
    "            actions.append(expert.act(o))\n",
    "        return np.array(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to load model\n",
      "INFO:tensorflow:Restoring parameters from ./model/model.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-03 00:14:47,774] Restoring parameters from ./model/model.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was loaded successful\n",
      "Iteration 0\n",
      "rollout 1/3 return=1.000000\n",
      "Iteration 1\n",
      "rollout 2/3 return=-1.000000\n",
      "Iteration 2\n",
      "rollout 3/3 return=1.000000\n",
      "Return summary: mean=0.333333, std=0.942809\n",
      "Train with DAgger, iter 2\n",
      "Iteration 0\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the environment\n",
    "config = ffa_v0_env()\n",
    "env = Pomme(**config[\"env_kwargs\"])\n",
    "states = {\n",
    "    \"board\": dict(shape=(BOARD_SIZE, BOARD_SIZE, 3,), type='float'),\n",
    "    \"state\": dict(shape=(3,), type='float')\n",
    "}\n",
    "agent_dagger = Agent(env.action_space.n)\n",
    "# Load Expert\n",
    "expert = Expert(config[\"agent\"](0, config[\"game_type\"]))\n",
    "\n",
    "# Generate training data\n",
    "stimulator = Stimulator(env, config)\n",
    "training_data = stimulator.stimulate(expert, num_rollouts=num_rollouts, render=False)\n",
    "# Train DAgger Agent\n",
    "obs = training_data[0]\n",
    "labls = training_data[1]\n",
    "for i in range(2, 15):\n",
    "    print(\"Train with DAgger, iter %i\" % i)\n",
    "    (stimulated_env, _) = stimulator.stimulate(agent_dagger, num_rollouts=num_rollouts, render=False)\n",
    "    labels = stimulator.label_obs(expert, stimulated_env)\n",
    "    obs = np.append(obs, stimulated_env, axis=0)\n",
    "    labls = np.append(labls, labels, axis=0)\n",
    "    agent_dagger.train(obs, labls, batch_size=batch_size, epochs=epochs)\n",
    "agent_dagger.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
