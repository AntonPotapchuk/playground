{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/hpchome/anton95/.conda/envs/pommerman/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/gpfs/hpchome/anton95/.conda/envs/pommerman/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import error! You will not be able to render --> Cannot connect to \"None\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "\n",
    "from keras.layers import Dense, Concatenate, Input, Convolution2D, Flatten, Activation, BatchNormalization, Add\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "from pommerman.configs import ffa_competition_env\n",
    "from pommerman.envs.v0 import Pomme\n",
    "from pommerman.agents import SimpleAgent, BaseAgent\n",
    "from pommerman.constants import BOARD_SIZE\n",
    "from rl.agents import DDPGAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Env, Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint, Callback\n",
    "from rl.random import OrnsteinUhlenbeckProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_training_steps = 500000\n",
    "log_interval = 10000\n",
    "file_log_path = './dqn/rl_logs/ddgp_go_3blocks/log.txt'\n",
    "tensorboard_path = './dqn/logs/ddgp_go_3blocks/'\n",
    "model_path = './dqn/model/ddgp_go_3blocks/model{step}.h4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(os.path.dirname(file_log_path)):\n",
    "    os.makedirs(os.path.dirname(file_log_path))\n",
    "if not os.path.isdir(os.path.dirname(model_path)):\n",
    "    os.makedirs(os.path.dirname(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorforceAgent(BaseAgent):\n",
    "    def act(self, obs, action_space):\n",
    "        pass\n",
    "\n",
    "\n",
    "class TensorboardLogger(Callback):\n",
    "    \"\"\"Logging in tensorboard without tensorflow ops.\"\"\"\n",
    "    def __init__(self, log_dir):\n",
    "        # Some algorithms compute multiple episodes at once since they are multi-threaded.\n",
    "        # We therefore use a dictionary that is indexed by the episode to separate episodes\n",
    "        # from each other.\n",
    "        self.observations = {}\n",
    "        self.rewards = {}\n",
    "        self.actions = {}\n",
    "        self.metrics = {}\n",
    "        self.step = 0\n",
    "        \"\"\"Creates a summary writer logging to log_dir.\"\"\"\n",
    "        self.writer = tf.summary.FileWriter(log_dir)\n",
    "\n",
    "    def log_scalar(self, tag, value, step):\n",
    "        \"\"\"Log a scalar variable.\n",
    "        Parameter\n",
    "        ----------\n",
    "        tag : basestring\n",
    "            Name of the scalar\n",
    "        value\n",
    "        step : int\n",
    "            training iteration\n",
    "        \"\"\"\n",
    "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])\n",
    "        self.writer.add_summary(summary, step)\n",
    "\n",
    "    def on_train_begin(self, logs):\n",
    "        self.metrics_names = self.model.metrics_names\n",
    "\n",
    "    def on_episode_begin(self, episode, logs):\n",
    "        self.observations[episode] = []\n",
    "        self.rewards[episode] = []\n",
    "        self.actions[episode] = []\n",
    "        self.metrics[episode] = []\n",
    "\n",
    "    def on_episode_end(self, episode, logs):\n",
    "        episode_steps = len(self.observations[episode])\n",
    "        variables = {\n",
    "            'step': self.step,\n",
    "            'episode_steps': episode_steps,\n",
    "            'episode_reward': np.sum(self.rewards[episode]),\n",
    "            'action_mean': np.mean(np.argmax(self.actions[episode], axis=1)),\n",
    "            'action_min': np.min(np.argmax(self.actions[episode], axis=1)),\n",
    "            'action_max': np.max(np.argmax(self.actions[episode], axis=1)),\n",
    "        }\n",
    "\n",
    "        # Format all metrics.\n",
    "        metrics = np.array(self.metrics[episode])\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings('error')\n",
    "            for idx, name in enumerate(self.metrics_names):\n",
    "                try:\n",
    "                    value = np.nanmean(metrics[:, idx])\n",
    "                except Warning:\n",
    "                    value = -1\n",
    "                variables[name] = value\n",
    "        for key, value in variables.items():\n",
    "            self.log_scalar(key, value, episode + 1)\n",
    "\n",
    "        # Free up resources.\n",
    "        del self.observations[episode]\n",
    "        del self.rewards[episode]\n",
    "        del self.actions[episode]\n",
    "        del self.metrics[episode]\n",
    "\n",
    "    def on_step_end(self, step, logs):\n",
    "        episode = logs['episode']\n",
    "        self.observations[episode].append(logs['observation'])\n",
    "        self.rewards[episode].append(logs['reward'])\n",
    "        self.actions[episode].append(logs['action'])\n",
    "        self.metrics[episode].append(logs['metrics'])\n",
    "        self.step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the environment\n",
    "config = ffa_competition_env()\n",
    "env = Pomme(**config[\"env_kwargs\"])\n",
    "np.random.seed(0)\n",
    "env.seed(0)\n",
    "\n",
    "env.set_init_game_state(None)\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "def get_res_block(input):\n",
    "    # Res block 1        \n",
    "    x = Convolution2D(256, 3, padding='same')(input)\n",
    "    x = Activation('elu')(x)\n",
    "    x = Convolution2D(256, 3, padding='same')(x)\n",
    "    x = Add()([input, x])\n",
    "    x = Activation('elu')(x)\n",
    "    return x\n",
    "\n",
    "def create_actor(actions, input_shape=(BOARD_SIZE, BOARD_SIZE, 17,)):\n",
    "    inp = Input(input_shape)\n",
    "    x = Convolution2D(256, 3, padding='same')(inp)\n",
    "    x = Activation('elu')(x)\n",
    "\n",
    "    # Ten residual blocks\n",
    "    for i in range(3):\n",
    "        x = get_res_block(x)\n",
    "\n",
    "    # Output block\n",
    "    # Should be 2 filters\n",
    "    x = Convolution2D(4, 1, padding='same')(x)\n",
    "    x = Activation('elu')(x)\n",
    "    x = Flatten()(x)\n",
    "    out = Dense(actions, activation='softmax')(x)\n",
    "    model = Model(inputs = inp, outputs=out)\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_critic(actions, input_shape=(BOARD_SIZE, BOARD_SIZE, 17,)):\n",
    "    action_input = Input(shape=(actions,), name='action_input')\n",
    "    \n",
    "    obs_inp = Input(input_shape)\n",
    "    x = Convolution2D(256, 3, padding='same')(obs_inp)\n",
    "    x = Activation('elu')(x)\n",
    "\n",
    "    # Ten residual blocks\n",
    "    for i in range(3):\n",
    "        x = get_res_block(x)\n",
    "\n",
    "    # Output block\n",
    "    # Should be 2 filters\n",
    "    x = Convolution2D(4, 1, padding='same')(x)\n",
    "    x = Activation('elu')(x)\n",
    "    x = Flatten()(x)\n",
    "    out = Dense(actions, activation='softmax')(x)\n",
    "    model = Model(inputs = obs_inp, outputs=out)\n",
    "    \n",
    "    x = Concatenate()([action_input, x])\n",
    "    x = Dense(128, activation='elu')(x)\n",
    "    out = Dense(1)(x)\n",
    "    \n",
    "    model = Model(inputs=[action_input, obs_inp], outputs=out)\n",
    "    return action_input, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvWrapper(Env):\n",
    "    \"\"\"The abstract environment class that is used by all agents. This class has the exact\n",
    "        same API that OpenAI Gym uses so that integrating with it is trivial. In contrast to the\n",
    "        OpenAI Gym implementation, this class only defines the abstract methods without any actual\n",
    "        implementation.\n",
    "        To implement your own environment, you need to define the following methods:\n",
    "        - `step`\n",
    "        - `reset`\n",
    "        - `render`\n",
    "        - `close`\n",
    "        Refer to the [Gym documentation](https://gym.openai.com/docs/#environments).\n",
    "        \"\"\"\n",
    "    reward_range = (-1, 1)\n",
    "    action_space = None\n",
    "    observation_space = None\n",
    "\n",
    "    def __init__(self, gym, board_size):\n",
    "        self.gym = gym\n",
    "        self.action_space = gym.action_space\n",
    "        self.observation_space = gym.observation_space\n",
    "        self.reward_range = gym.reward_range\n",
    "        self.board_size = board_size\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Run one timestep of the environment's dynamics.\n",
    "        Accepts an action and returns a tuple (observation, reward, done, info).\n",
    "        # Arguments\n",
    "            action (object): An action provided by the environment.\n",
    "        # Returns\n",
    "            observation (object): Agent's observation of the current environment.\n",
    "            reward (float) : Amount of reward returned after previous action.\n",
    "            done (boolean): Whether the episode has ended, in which case further step() calls will return undefined results.\n",
    "            info (dict): Contains auxiliary diagnostic information (helpful for debugging, and sometimes learning).\n",
    "        \"\"\"\n",
    "        action = np.argmax(action)\n",
    "        obs = self.gym.get_observations()\n",
    "        all_actions = self.gym.act(obs)\n",
    "        all_actions.insert(self.gym.training_agent, action)\n",
    "        state, reward, terminal, info = self.gym.step(all_actions)\n",
    "        agent_state = self.featurize(state[self.gym.training_agent])\n",
    "        agent_reward = reward[self.gym.training_agent]\n",
    "        return agent_state, agent_reward, terminal, info\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the state of the environment and returns an initial observation.\n",
    "        # Returns\n",
    "            observation (object): The initial observation of the space. Initial reward is assumed to be 0.\n",
    "        \"\"\"\n",
    "        # Add 3 random agents\n",
    "        train_agent_pos = np.random.randint(0, 4)\n",
    "        agents = []\n",
    "        for agent_id in range(4):\n",
    "            if agent_id == train_agent_pos:\n",
    "                agents.append(TensorforceAgent(config[\"agent\"](agent_id, config[\"game_type\"])))\n",
    "            else:\n",
    "                agents.append(SimpleAgent(config[\"agent\"](agent_id, config[\"game_type\"])))\n",
    "        self.gym.set_agents(agents)\n",
    "        self.gym.set_training_agent(agents[train_agent_pos].agent_id)\n",
    "        \n",
    "        obs = self.gym.reset()\n",
    "        agent_obs = self.featurize(obs[self.gym.training_agent])\n",
    "        return agent_obs\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        \"\"\"Renders the environment.\n",
    "        The set of supported modes varies per environment. (And some\n",
    "        environments do not support rendering at all.)\n",
    "        # Arguments\n",
    "            mode (str): The mode to render with.\n",
    "            close (bool): Close all open renderings.\n",
    "        \"\"\"\n",
    "        self.gym.render(mode=mode, close=close)\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Override in your subclass to perform any necessary cleanup.\n",
    "        Environments will automatically close() themselves when\n",
    "        garbage collected or when the program exits.\n",
    "        \"\"\"\n",
    "        self.gym.close()\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        \"\"\"Sets the seed for this env's random number generator(s).\n",
    "        # Returns\n",
    "            Returns the list of seeds used in this env's random number generators\n",
    "        \"\"\"\n",
    "        raise self.gym.seed(seed)\n",
    "\n",
    "    def featurize(self, obs):\n",
    "        shape = (BOARD_SIZE, BOARD_SIZE, 1)\n",
    "\n",
    "        def get_matrix(dict, key):\n",
    "            res = dict[key]\n",
    "            return res.reshape(shape).astype(np.float32)\n",
    "\n",
    "        def get_map(board, item):\n",
    "            map = np.zeros(shape)\n",
    "            map[board == item] = 1\n",
    "            return map\n",
    "\n",
    "        board = get_matrix(obs, 'board')\n",
    "\n",
    "        # TODO: probably not needed Passage = 0\n",
    "        rigid_map = get_map(board, 1)               # Rigid = 1\n",
    "        wood_map = get_map(board, 2)                # Wood = 2\n",
    "        bomb_map = get_map(board, 3)                # Bomb = 3\n",
    "        flames_map = get_map(board, 4)              # Flames = 4\n",
    "        fog_map = get_map(board, 5)                 # TODO: not used for first two stages Fog = 5\n",
    "        extra_bomb_map = get_map(board, 6)          # ExtraBomb = 6\n",
    "        incr_range_map = get_map(board, 7)          # IncrRange = 7\n",
    "        kick_map = get_map(board, 8)                # Kick = 8\n",
    "        skull_map = get_map(board, 9)               # Skull = 9\n",
    "\n",
    "        position = obs[\"position\"]\n",
    "        my_position = np.zeros(shape)\n",
    "        my_position[position[0], position[1], 0] = 1\n",
    "\n",
    "        team_mates = get_map(board, obs[\"teammate\"].value) # TODO during documentation it should be an array\n",
    "\n",
    "        enemies = np.zeros(shape)\n",
    "        for enemy in obs[\"enemies\"]:\n",
    "            enemies[board == enemy.value] = 1\n",
    "\n",
    "        bomb_blast_strength = get_matrix(obs, 'bomb_blast_strength')\n",
    "        bomb_life = get_matrix(obs, 'bomb_life')\n",
    "\n",
    "        ammo = np.full((BOARD_SIZE, BOARD_SIZE, 1), obs[\"ammo\"])\n",
    "        blast_strength = np.full((BOARD_SIZE, BOARD_SIZE, 1), obs[\"blast_strength\"])\n",
    "        can_kick = np.full((BOARD_SIZE, BOARD_SIZE, 1), int(obs[\"can_kick\"]))\n",
    "\n",
    "        obs = np.concatenate([my_position, enemies, team_mates, rigid_map,\n",
    "                              wood_map, bomb_map, flames_map,\n",
    "                              fog_map, extra_bomb_map, incr_range_map,\n",
    "                              kick_map, skull_map, bomb_blast_strength,\n",
    "                              bomb_life, ammo, blast_strength, can_kick], axis=2)\n",
    "        return obs \n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "    def __str__(self):\n",
    "        return '<{} instance>'.format(type(self).__name__)\n",
    "\n",
    "\n",
    "class CustomProcessor(Processor):\n",
    "    def process_state_batch(self, batch):\n",
    "        \"\"\"Processes an entire batch of states and returns it.\n",
    "        # Arguments\n",
    "            batch (list): List of states\n",
    "        # Returns\n",
    "            Processed list of states\n",
    "        \"\"\"\n",
    "        batch = np.squeeze(batch, axis=1).astype('float')\n",
    "        return batch\n",
    "\n",
    "    def process_info(self, info):\n",
    "        \"\"\"Processes the info as obtained from the environment for use in an agent and\n",
    "        returns it.\n",
    "        \"\"\"\n",
    "        info['result'] = info['result'].value\n",
    "        return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.reset_default_graph()\n",
    "env_wrapper = EnvWrapper(env, BOARD_SIZE)\n",
    "processor = CustomProcessor()\n",
    "\n",
    "actor = create_actor(nb_actions)\n",
    "action_input, critic = create_critic(nb_actions)\n",
    "\n",
    "memory = SequentialMemory(limit=100000, window_length=1)\n",
    "random_process = OrnsteinUhlenbeckProcess(size=nb_actions, theta=.15, mu=0., sigma=.3)\n",
    "agent = DDPGAgent(nb_actions=nb_actions, actor=actor, critic=critic, critic_action_input=action_input,\n",
    "                  memory=memory, nb_steps_warmup_critic=1024, nb_steps_warmup_actor=1024,\n",
    "                  random_process=random_process, gamma=.99, target_model_update=1e-3,\n",
    "                  batch_size=1024, processor=processor)\n",
    "agent.compile(Adam(lr=0.0001, clipnorm=1.), metrics=['mae'])\n",
    "\n",
    "\n",
    "file_logger = FileLogger(file_log_path, interval=log_interval)\n",
    "checkpoint = ModelIntervalCheckpoint(model_path, interval=log_interval)\n",
    "tensorboard = TensorboardLogger(tensorboard_path)\n",
    "callbacks=[file_logger, checkpoint, tensorboard]\n",
    "if os.path.isfile(model_path):\n",
    "    agent.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 500000 steps ...\n",
      "    104/500000: episode: 1, duration: 2.738s, episode steps: 104, steps per second: 38, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 0.098 [-0.424, 0.549], mean observation: 0.213 [0.000, 9.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    143/500000: episode: 2, duration: 0.609s, episode steps: 39, steps per second: 64, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 0.193 [-0.076, 0.425], mean observation: 0.196 [0.000, 9.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    153/500000: episode: 3, duration: 0.140s, episode steps: 10, steps per second: 72, episode reward: -1.000, mean reward: -0.100 [-1.000, 0.000], mean action: 0.164 [-0.036, 0.418], mean observation: 0.174 [0.000, 9.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    173/500000: episode: 4, duration: 0.276s, episode steps: 20, steps per second: 73, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 0.123 [-0.167, 0.316], mean observation: 0.174 [0.000, 9.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    263/500000: episode: 5, duration: 0.932s, episode steps: 90, steps per second: 97, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 0.224 [-0.332, 0.848], mean observation: 0.217 [0.000, 9.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    283/500000: episode: 6, duration: 0.307s, episode steps: 20, steps per second: 65, episode reward: -1.000, mean reward: -0.050 [-1.000, 0.000], mean action: 0.203 [-0.019, 0.545], mean observation: 0.173 [0.000, 9.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    535/500000: episode: 7, duration: 2.146s, episode steps: 252, steps per second: 117, episode reward: -1.000, mean reward: -0.004 [-1.000, 0.000], mean action: 0.147 [-0.470, 0.685], mean observation: 0.207 [0.000, 9.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    654/500000: episode: 8, duration: 1.675s, episode steps: 119, steps per second: 71, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 0.090 [-0.533, 0.576], mean observation: 0.216 [0.000, 9.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    684/500000: episode: 9, duration: 0.428s, episode steps: 30, steps per second: 70, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 0.099 [-0.104, 0.369], mean observation: 0.205 [0.000, 9.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    734/500000: episode: 10, duration: 0.679s, episode steps: 50, steps per second: 74, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 0.130 [-0.237, 0.526], mean observation: 0.202 [0.000, 9.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    895/500000: episode: 11, duration: 1.580s, episode steps: 161, steps per second: 102, episode reward: -1.000, mean reward: -0.006 [-1.000, 0.000], mean action: 0.215 [-0.100, 0.620], mean observation: 0.213 [0.000, 9.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    938/500000: episode: 12, duration: 0.602s, episode steps: 43, steps per second: 71, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 0.099 [-0.149, 0.321], mean observation: 0.199 [0.000, 9.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    959/500000: episode: 13, duration: 0.377s, episode steps: 21, steps per second: 56, episode reward: -1.000, mean reward: -0.048 [-1.000, 0.000], mean action: 0.173 [-0.067, 0.335], mean observation: 0.195 [0.000, 9.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    975/500000: episode: 14, duration: 0.265s, episode steps: 16, steps per second: 60, episode reward: -1.000, mean reward: -0.062 [-1.000, 0.000], mean action: 0.175 [-0.161, 0.486], mean observation: 0.189 [0.000, 9.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   1113/500000: episode: 15, duration: 62.213s, episode steps: 138, steps per second: 2, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 0.186 [-0.508, 1.359], mean observation: 0.215 [0.000, 9.000], loss: 0.212922, mean_absolute_error: 0.371405, mean_q: 1.006433\n",
      "   1199/500000: episode: 16, duration: 57.650s, episode steps: 86, steps per second: 1, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 0.160 [-0.261, 1.064], mean observation: 0.218 [0.000, 9.000], loss: 0.027026, mean_absolute_error: 0.147516, mean_q: 1.279811\n",
      "   1492/500000: episode: 17, duration: 195.487s, episode steps: 293, steps per second: 1, episode reward: -1.000, mean reward: -0.003 [-1.000, 0.000], mean action: 0.043 [-0.767, 1.570], mean observation: 0.209 [0.000, 9.000], loss: 0.011454, mean_absolute_error: 0.092301, mean_q: 1.520801\n",
      "   1862/500000: episode: 18, duration: 247.057s, episode steps: 370, steps per second: 1, episode reward: -1.000, mean reward: -0.003 [-1.000, 0.000], mean action: 0.114 [-0.631, 1.007], mean observation: 0.204 [0.000, 9.000], loss: 0.003873, mean_absolute_error: 0.063015, mean_q: 1.578391\n",
      "   1914/500000: episode: 19, duration: 35.353s, episode steps: 52, steps per second: 1, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 0.051 [-0.516, 1.038], mean observation: 0.220 [0.000, 9.000], loss: 0.001758, mean_absolute_error: 0.037156, mean_q: 1.551094\n",
      "   2009/500000: episode: 20, duration: 63.805s, episode steps: 95, steps per second: 1, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 0.132 [-0.374, 1.189], mean observation: 0.219 [0.000, 9.000], loss: 0.002256, mean_absolute_error: 0.041365, mean_q: 1.554197\n",
      "   2212/500000: episode: 22, duration: 76.595s, episode steps: 114, steps per second: 1, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 0.051 [-0.606, 1.115], mean observation: 0.214 [0.000, 9.000], loss: 0.002052, mean_absolute_error: 0.040512, mean_q: 1.547665\n",
      "   3012/500000: episode: 23, duration: 534.173s, episode steps: 800, steps per second: 1, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.249 [-0.924, 1.399], mean observation: 0.206 [0.000, 9.000], loss: 0.001623, mean_absolute_error: 0.038336, mean_q: 1.553299\n",
      "   3221/500000: episode: 24, duration: 138.889s, episode steps: 209, steps per second: 2, episode reward: -1.000, mean reward: -0.005 [-1.000, 0.000], mean action: 0.384 [-0.113, 1.272], mean observation: 0.212 [0.000, 9.000], loss: 0.000613, mean_absolute_error: 0.022196, mean_q: 1.546781\n",
      "   4021/500000: episode: 25, duration: 530.545s, episode steps: 800, steps per second: 2, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.310 [-0.793, 1.516], mean observation: 0.204 [0.000, 9.000], loss: 0.000744, mean_absolute_error: 0.025070, mean_q: 1.542947\n",
      "   4168/500000: episode: 26, duration: 98.256s, episode steps: 147, steps per second: 1, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 0.119 [-0.581, 1.442], mean observation: 0.212 [0.000, 9.000], loss: 0.001151, mean_absolute_error: 0.026467, mean_q: 1.541761\n",
      "   4344/500000: episode: 27, duration: 118.004s, episode steps: 176, steps per second: 1, episode reward: -1.000, mean reward: -0.006 [-1.000, 0.000], mean action: 0.216 [-0.549, 1.469], mean observation: 0.212 [0.000, 9.000], loss: 0.001616, mean_absolute_error: 0.030426, mean_q: 1.538309\n",
      "   4421/500000: episode: 28, duration: 51.912s, episode steps: 77, steps per second: 1, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 0.056 [-0.683, 1.126], mean observation: 0.219 [0.000, 9.000], loss: 0.002898, mean_absolute_error: 0.039947, mean_q: 1.543996\n",
      "   4682/500000: episode: 29, duration: 173.938s, episode steps: 261, steps per second: 2, episode reward: -1.000, mean reward: -0.004 [-1.000, 0.000], mean action: 0.238 [-0.460, 1.442], mean observation: 0.207 [0.000, 9.000], loss: 0.001914, mean_absolute_error: 0.034706, mean_q: 1.543893\n",
      "   4798/500000: episode: 30, duration: 77.582s, episode steps: 116, steps per second: 1, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 0.201 [-0.417, 1.740], mean observation: 0.217 [0.000, 9.000], loss: 0.001828, mean_absolute_error: 0.033417, mean_q: 1.533230\n",
      "   4915/500000: episode: 31, duration: 78.839s, episode steps: 117, steps per second: 1, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 0.032 [-0.634, 1.177], mean observation: 0.216 [0.000, 9.000], loss: 0.001776, mean_absolute_error: 0.032274, mean_q: 1.525356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   5000/500000: episode: 32, duration: 57.293s, episode steps: 85, steps per second: 1, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 0.243 [-0.204, 1.332], mean observation: 0.217 [0.000, 9.000], loss: 0.002001, mean_absolute_error: 0.038458, mean_q: 1.518223\n",
      "   5074/500000: episode: 33, duration: 49.532s, episode steps: 74, steps per second: 1, episode reward: -1.000, mean reward: -0.014 [-1.000, 0.000], mean action: 0.131 [-0.575, 1.349], mean observation: 0.217 [0.000, 9.000], loss: 0.002004, mean_absolute_error: 0.037872, mean_q: 1.517834\n",
      "   5291/500000: episode: 34, duration: 145.200s, episode steps: 217, steps per second: 1, episode reward: -1.000, mean reward: -0.005 [-1.000, 0.000], mean action: 0.260 [-0.723, 1.225], mean observation: 0.211 [0.000, 9.000], loss: 0.001447, mean_absolute_error: 0.030355, mean_q: 1.507884\n",
      "   5528/500000: episode: 35, duration: 157.772s, episode steps: 237, steps per second: 2, episode reward: -1.000, mean reward: -0.004 [-1.000, 0.000], mean action: 0.245 [-0.529, 1.805], mean observation: 0.208 [0.000, 9.000], loss: 0.001231, mean_absolute_error: 0.028280, mean_q: 1.493593\n",
      "   5645/500000: episode: 36, duration: 78.455s, episode steps: 117, steps per second: 1, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 0.089 [-0.424, 1.086], mean observation: 0.216 [0.000, 9.000], loss: 0.001765, mean_absolute_error: 0.033131, mean_q: 1.491012\n",
      "   6445/500000: episode: 37, duration: 527.317s, episode steps: 800, steps per second: 2, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.036 [-1.346, 2.143], mean observation: 0.202 [0.000, 9.000], loss: 0.001216, mean_absolute_error: 0.027158, mean_q: 1.482792\n",
      "   6615/500000: episode: 38, duration: 113.163s, episode steps: 170, steps per second: 2, episode reward: -1.000, mean reward: -0.006 [-1.000, 0.000], mean action: 0.061 [-0.534, 0.998], mean observation: 0.213 [0.000, 9.000], loss: 0.000913, mean_absolute_error: 0.024405, mean_q: 1.468785\n",
      "   6696/500000: episode: 39, duration: 54.513s, episode steps: 81, steps per second: 1, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 0.266 [-0.404, 1.404], mean observation: 0.219 [0.000, 9.000], loss: 0.001473, mean_absolute_error: 0.028318, mean_q: 1.468251\n",
      "   7082/500000: episode: 40, duration: 256.828s, episode steps: 386, steps per second: 2, episode reward: -1.000, mean reward: -0.003 [-1.000, 0.000], mean action: 0.343 [-0.518, 1.281], mean observation: 0.204 [0.000, 9.000], loss: 0.001210, mean_absolute_error: 0.024940, mean_q: 1.464742\n",
      "   7401/500000: episode: 41, duration: 210.381s, episode steps: 319, steps per second: 2, episode reward: -1.000, mean reward: -0.003 [-1.000, 0.000], mean action: 0.311 [-0.870, 1.585], mean observation: 0.209 [0.000, 9.000], loss: 0.001601, mean_absolute_error: 0.028638, mean_q: 1.455144\n",
      "   7529/500000: episode: 42, duration: 85.693s, episode steps: 128, steps per second: 1, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 0.152 [-0.421, 1.056], mean observation: 0.215 [0.000, 9.000], loss: 0.001367, mean_absolute_error: 0.026751, mean_q: 1.449525\n",
      "   7987/500000: episode: 43, duration: 303.749s, episode steps: 458, steps per second: 2, episode reward: 1.000, mean reward: 0.002 [0.000, 1.000], mean action: 0.218 [-0.733, 1.612], mean observation: 0.208 [0.000, 9.000], loss: 0.001161, mean_absolute_error: 0.024026, mean_q: 1.448090\n",
      "   8294/500000: episode: 44, duration: 205.222s, episode steps: 307, steps per second: 1, episode reward: -1.000, mean reward: -0.003 [-1.000, 0.000], mean action: 0.134 [-0.527, 1.126], mean observation: 0.210 [0.000, 9.000], loss: 0.001027, mean_absolute_error: 0.021639, mean_q: 1.440812\n",
      "   8614/500000: episode: 45, duration: 213.194s, episode steps: 320, steps per second: 2, episode reward: -1.000, mean reward: -0.003 [-1.000, 0.000], mean action: 0.224 [-0.711, 1.208], mean observation: 0.206 [0.000, 9.000], loss: 0.001365, mean_absolute_error: 0.028036, mean_q: 1.428792\n",
      "   8696/500000: episode: 46, duration: 55.429s, episode steps: 82, steps per second: 1, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 0.229 [-0.491, 1.054], mean observation: 0.208 [0.000, 9.000], loss: 0.001359, mean_absolute_error: 0.026534, mean_q: 1.417807\n",
      "   8823/500000: episode: 47, duration: 85.360s, episode steps: 127, steps per second: 1, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 0.241 [-0.163, 1.144], mean observation: 0.214 [0.000, 9.000], loss: 0.001360, mean_absolute_error: 0.026516, mean_q: 1.415991\n",
      "   9623/500000: episode: 48, duration: 530.743s, episode steps: 800, steps per second: 2, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.132 [-1.348, 1.564], mean observation: 0.209 [0.000, 9.000], loss: 0.001077, mean_absolute_error: 0.022607, mean_q: 1.416634\n",
      "   9747/500000: episode: 49, duration: 83.030s, episode steps: 124, steps per second: 1, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 0.199 [-0.731, 1.188], mean observation: 0.216 [0.000, 9.000], loss: 0.001179, mean_absolute_error: 0.023511, mean_q: 1.418307\n",
      "  10547/500000: episode: 50, duration: 531.899s, episode steps: 800, steps per second: 2, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.063 [-1.377, 1.759], mean observation: 0.205 [0.000, 9.000], loss: 0.001045, mean_absolute_error: 0.021928, mean_q: 1.404119\n",
      "  10719/500000: episode: 51, duration: 114.633s, episode steps: 172, steps per second: 2, episode reward: 1.000, mean reward: 0.006 [0.000, 1.000], mean action: 0.145 [-0.731, 1.254], mean observation: 0.209 [0.000, 9.000], loss: 0.001163, mean_absolute_error: 0.022220, mean_q: 1.392194\n",
      "  10838/500000: episode: 52, duration: 80.061s, episode steps: 119, steps per second: 1, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 0.250 [-0.341, 1.210], mean observation: 0.213 [0.000, 9.000], loss: 0.001083, mean_absolute_error: 0.022131, mean_q: 1.391032\n",
      "  11638/500000: episode: 53, duration: 530.742s, episode steps: 800, steps per second: 2, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.134 [-0.692, 1.626], mean observation: 0.207 [0.000, 9.000], loss: 0.001071, mean_absolute_error: 0.021034, mean_q: 1.374771\n",
      "  12286/500000: episode: 54, duration: 428.429s, episode steps: 648, steps per second: 2, episode reward: -1.000, mean reward: -0.002 [-1.000, 0.000], mean action: 0.083 [-0.930, 1.766], mean observation: 0.204 [0.000, 9.000], loss: 0.001176, mean_absolute_error: 0.021522, mean_q: 1.346592\n",
      "  12447/500000: episode: 55, duration: 107.955s, episode steps: 161, steps per second: 1, episode reward: -1.000, mean reward: -0.006 [-1.000, 0.000], mean action: 0.130 [-0.434, 1.189], mean observation: 0.213 [0.000, 9.000], loss: 0.001220, mean_absolute_error: 0.023935, mean_q: 1.327489\n",
      "  12677/500000: episode: 56, duration: 154.086s, episode steps: 230, steps per second: 1, episode reward: -1.000, mean reward: -0.004 [-1.000, 0.000], mean action: 0.276 [-0.368, 1.487], mean observation: 0.210 [0.000, 9.000], loss: 0.001162, mean_absolute_error: 0.022091, mean_q: 1.322151\n",
      "  12905/500000: episode: 57, duration: 150.940s, episode steps: 228, steps per second: 2, episode reward: -1.000, mean reward: -0.004 [-1.000, 0.000], mean action: 0.177 [-0.665, 1.322], mean observation: 0.209 [0.000, 9.000], loss: 0.001146, mean_absolute_error: 0.023200, mean_q: 1.316051\n",
      "  13066/500000: episode: 58, duration: 107.299s, episode steps: 161, steps per second: 2, episode reward: -1.000, mean reward: -0.006 [-1.000, 0.000], mean action: 0.085 [-0.768, 1.141], mean observation: 0.212 [0.000, 9.000], loss: 0.001139, mean_absolute_error: 0.021674, mean_q: 1.309154\n",
      "  13127/500000: episode: 59, duration: 41.437s, episode steps: 61, steps per second: 1, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 0.209 [-0.070, 0.977], mean observation: 0.219 [0.000, 9.000], loss: 0.001027, mean_absolute_error: 0.021546, mean_q: 1.303891\n",
      "  13206/500000: episode: 60, duration: 53.403s, episode steps: 79, steps per second: 1, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 0.202 [-0.162, 0.983], mean observation: 0.211 [0.000, 9.000], loss: 0.001423, mean_absolute_error: 0.025809, mean_q: 1.302923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  14006/500000: episode: 61, duration: 527.952s, episode steps: 800, steps per second: 2, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.187 [-1.325, 1.980], mean observation: 0.202 [0.000, 9.000], loss: 0.000969, mean_absolute_error: 0.020660, mean_q: 1.292278\n",
      "  14165/500000: episode: 62, duration: 106.954s, episode steps: 159, steps per second: 1, episode reward: -1.000, mean reward: -0.006 [-1.000, 0.000], mean action: 0.008 [-0.759, 1.060], mean observation: 0.218 [0.000, 9.000], loss: 0.001047, mean_absolute_error: 0.020665, mean_q: 1.280582\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-bdc9017f8d32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m history = agent.fit(env_wrapper, nb_steps=number_of_training_steps, visualize=False, verbose=2,\n\u001b[0;32m----> 2\u001b[0;31m         nb_max_episode_steps=env._max_steps, callbacks=callbacks)\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/pommerman/lib/python3.6/site-packages/rl/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, env, nb_steps, action_repetition, callbacks, verbose, visualize, nb_max_start_steps, start_step_policy, log_interval, nb_max_episode_steps)\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_repetition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_action_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m                     \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m                     \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-48221b2b7bd7>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_observations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mall_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mall_actions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pommerman/lib/python3.6/site-packages/pommerman-0.2.0-py3.6.egg/pommerman/envs/v0.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m    131\u001b[0m         agents = [agent for agent in self._agents \\\n\u001b[1;32m    132\u001b[0m                   if agent.agent_id != self.training_agent]\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_observations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pommerman/lib/python3.6/site-packages/pommerman-0.2.0-py3.6.egg/pommerman/forward_model.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(agents, obs, action_space, is_communicative)\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_with_communication\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_ex_communication\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pommerman/lib/python3.6/site-packages/pommerman-0.2.0-py3.6.egg/pommerman/forward_model.py\u001b[0m in \u001b[0;36mact_ex_communication\u001b[0;34m(agent)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mact_ex_communication\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_space\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pommerman/lib/python3.6/site-packages/pommerman-0.2.0-py3.6.egg/pommerman/agents/simple_agent.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, obs, action_space)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;31m# Move towards an enemy if there is one in exactly three reachable spaces.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mdirection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_near_enemy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_position\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menemies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         if direction is not None and (self._prev_direction != direction or\n\u001b[1;32m     63\u001b[0m                                       random.random() < .5):\n",
      "\u001b[0;32m~/.conda/envs/pommerman/lib/python3.6/site-packages/pommerman-0.2.0-py3.6.egg/pommerman/agents/simple_agent.py\u001b[0m in \u001b[0;36m_near_enemy\u001b[0;34m(cls, my_position, items, dist, prev, enemies, radius)\u001b[0m\n\u001b[1;32m    377\u001b[0m                                                        radius)\n\u001b[1;32m    378\u001b[0m         return cls._get_direction_towards_position(my_position,\n\u001b[0;32m--> 379\u001b[0;31m                                                    nearest_enemy_position, prev)\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pommerman/lib/python3.6/site-packages/pommerman-0.2.0-py3.6.egg/pommerman/agents/simple_agent.py\u001b[0m in \u001b[0;36m_get_direction_towards_position\u001b[0;34m(my_position, position, prev)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0mnext_position\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0;32mwhile\u001b[0m \u001b[0mprev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_position\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mmy_position\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m             \u001b[0mnext_position\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_position\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: None"
     ]
    }
   ],
   "source": [
    "history = agent.fit(env_wrapper, nb_steps=number_of_training_steps, visualize=False, verbose=2,\n",
    "        nb_max_episode_steps=env._max_steps, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save_weights(model_path, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([123,1,2,4]).dtype"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
