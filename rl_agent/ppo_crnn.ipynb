{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from pommerman.envs.v0 import Pomme\n",
    "from pommerman.agents import SimpleAgent, BaseAgent\n",
    "from pommerman.configs import ffa_v0_env\n",
    "from pommerman.constants import BOARD_SIZE, GameType\n",
    "from tensorforce.agents import PPOAgent\n",
    "from tensorforce.execution import Runner\n",
    "from tensorforce.contrib.openai_gym import OpenAIGym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 30000\n",
    "batching_capacity = 1000\n",
    "save_seconds = 300\n",
    "main_dir = './ppoPcrnn/'\n",
    "log_path = main_dir + 'logs/'\n",
    "model_path = main_dir + 'model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(main_dir):\n",
    "    os.mkdir(main_dir)\n",
    "if os.path.isdir(log_path):\n",
    "    shutil.rmtree(log_path, ignore_errors=True)\n",
    "os.mkdir(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "ename": "TensorForceError",
     "evalue": "Error: object PommNetwork not found in predefined objects: []",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTensorForceError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-9e4f4002c21e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     ),\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0msaver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m )\n",
      "\u001b[0;32m~/.conda/envs/pommerman/lib/python3.6/site-packages/tensorforce/agents/ppo_agent.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, states, actions, network, batched_observe, batching_capacity, scope, device, saver, summarizer, execution, variable_noise, states_preprocessing, actions_exploration, reward_preprocessing, update_mode, memory, discount, distributions, entropy_regularization, baseline_mode, baseline, baseline_optimizer, gae_lambda, likelihood_ratio_clipping, step_optimizer, subsampling_fraction, optimization_steps)\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0mnetwork\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0mdistributions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0mentropy_regularization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mentropy_regularization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         )\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pommerman/lib/python3.6/site-packages/tensorforce/agents/learning_agent.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, states, actions, network, update_mode, memory, optimizer, batched_observe, batching_capacity, scope, device, saver, summarizer, execution, variable_noise, states_preprocessing, actions_exploration, reward_preprocessing, discount, distributions, entropy_regularization)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mactions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mbatched_observe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatched_observe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0mbatching_capacity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatching_capacity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         )\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pommerman/lib/python3.6/site-packages/tensorforce/agents/agent.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, states, actions, batched_observe, batching_capacity)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pommerman/lib/python3.6/site-packages/tensorforce/agents/ppo_agent.py\u001b[0m in \u001b[0;36minitialize_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mbaseline_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbaseline_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mgae_lambda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgae_lambda\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m             \u001b[0mlikelihood_ratio_clipping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlikelihood_ratio_clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m         )\n",
      "\u001b[0;32m~/.conda/envs/pommerman/lib/python3.6/site-packages/tensorforce/models/pg_prob_ratio_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, states, actions, scope, device, saver, summarizer, execution, batching_capacity, variable_noise, states_preprocessing, actions_exploration, reward_preprocessing, update_mode, memory, optimizer, discount, network, distributions, entropy_regularization, baseline_mode, baseline, baseline_optimizer, gae_lambda, likelihood_ratio_clipping)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mbaseline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbaseline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mbaseline_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbaseline_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mgae_lambda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgae_lambda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         )\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pommerman/lib/python3.6/site-packages/tensorforce/models/pg_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, states, actions, scope, device, saver, summarizer, execution, batching_capacity, variable_noise, states_preprocessing, actions_exploration, reward_preprocessing, update_mode, memory, optimizer, discount, network, distributions, entropy_regularization, baseline_mode, baseline, baseline_optimizer, gae_lambda)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mdistributions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mentropy_regularization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mentropy_regularization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0mrequires_deterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         )\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pommerman/lib/python3.6/site-packages/tensorforce/models/distribution_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, states, actions, scope, device, saver, summarizer, execution, batching_capacity, variable_noise, states_preprocessing, actions_exploration, reward_preprocessing, update_mode, memory, optimizer, discount, network, distributions, entropy_regularization, requires_deterministic)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mmemory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0mdiscount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdiscount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         )\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pommerman/lib/python3.6/site-packages/tensorforce/models/memory_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, states, actions, scope, device, saver, summarizer, execution, batching_capacity, variable_noise, states_preprocessing, actions_exploration, reward_preprocessing, update_mode, memory, optimizer, discount)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mstates_preprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstates_preprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mactions_exploration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mactions_exploration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mreward_preprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreward_preprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         )\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pommerman/lib/python3.6/site-packages/tensorforce/models/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, states, actions, scope, device, saver, summarizer, execution, batching_capacity, variable_noise, states_preprocessing, actions_exploration, reward_preprocessing, tf_session_dump_dir)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;31m# Setup Model (create and build graph (local and global if distributed), server, session, etc..).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pommerman/lib/python3.6/site-packages/tensorforce/models/model.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m                 \u001b[0;31m# Create model's \"external\" components.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m                 \u001b[0;31m# Create tensorflow functions from \"tf_\"-methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_components_and_tf_funcs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                 \u001b[0;31m# Create core variables (timestep, episode counters, buffers for states/actions/internals).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pommerman/lib/python3.6/site-packages/tensorforce/models/pg_model.py\u001b[0m in \u001b[0;36msetup_components_and_tf_funcs\u001b[0;34m(self, custom_getter)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msetup_components_and_tf_funcs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mcustom_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPGModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_components_and_tf_funcs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# Baseline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pommerman/lib/python3.6/site-packages/tensorforce/models/distribution_model.py\u001b[0m in \u001b[0;36msetup_components_and_tf_funcs\u001b[0;34m(self, custom_getter)\u001b[0m\n\u001b[1;32m     99\u001b[0m         self.network = Network.from_spec(\n\u001b[1;32m    100\u001b[0m             \u001b[0mspec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         )\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pommerman/lib/python3.6/site-packages/tensorforce/core/networks/network.py\u001b[0m in \u001b[0;36mfrom_spec\u001b[0;34m(spec, kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0mdefault_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLayeredNetwork\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             )\n\u001b[1;32m    182\u001b[0m         \u001b[0;31m# If neither format, invalid spec and will fail on assert\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pommerman/lib/python3.6/site-packages/tensorforce/util.py\u001b[0m in \u001b[0;36mget_object\u001b[0;34m(obj, predefined_objects, default_object, kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m             raise TensorForceError(\"Error: object {} not found in predefined objects: {}\".format(\n\u001b[1;32m    180\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredefined_objects\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m             ))\n\u001b[1;32m    183\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTensorForceError\u001b[0m: Error: object PommNetwork not found in predefined objects: []"
     ]
    }
   ],
   "source": [
    "# Instantiate the environment\n",
    "config = ffa_v0_env()\n",
    "env = Pomme(**config[\"env_kwargs\"])\n",
    "env.seed(0)\n",
    "\n",
    "# Create a Proximal Policy Optimization agent\n",
    "network = dict(type='pomm_network.PommNetwork')\n",
    "#TODO size\n",
    "states = {\n",
    "    \"board\": dict(shape=(25, 25, 3, ), type='float'),\n",
    "    \"state\": dict(shape=(3,), type='float')\n",
    "}\n",
    "saver = {\n",
    "    \"directory\": model_path,\n",
    "    \"seconds\": save_seconds,\n",
    "    \"load\": os.path.isdir(model_path)\n",
    "}\n",
    "agent = PPOAgent(\n",
    "    states=states,\n",
    "    actions=dict(type='int', num_actions=env.action_space.n),\n",
    "    network=network,\n",
    "    batching_capacity=batching_capacity,\n",
    "    step_optimizer=dict(\n",
    "        type='adam',\n",
    "        learning_rate=1e-4\n",
    "    ),\n",
    "    saver=saver\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorforceAgent(BaseAgent):\n",
    "    def act(self, obs, action_space):\n",
    "        pass\n",
    "# Add 3 random agents\n",
    "agents = []\n",
    "for agent_id in range(3):\n",
    "    agents.append(SimpleAgent(config[\"agent\"](agent_id, config[\"game_type\"])))\n",
    "\n",
    "# Add TensorforceAgent\n",
    "agent_id += 1\n",
    "agents.append(TensorforceAgent(config[\"agent\"](agent_id, config[\"game_type\"])))\n",
    "env.set_agents(agents)\n",
    "env.set_training_agent(agents[-1].agent_id)\n",
    "env.set_init_game_state(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedEnv(OpenAIGym):\n",
    "    def __init__(self, gym, visualize=False):\n",
    "        self.gym = gym\n",
    "        self.visualize = visualize\n",
    "\n",
    "    def execute(self, actions):\n",
    "        if self.visualize:\n",
    "            self.gym.render()\n",
    "\n",
    "        obs = self.gym.get_observations()\n",
    "        all_actions = self.gym.act(obs)\n",
    "        all_actions.insert(self.gym.training_agent, actions)\n",
    "        state, reward, terminal, _ = self.gym.step(all_actions)\n",
    "        agent_state = WrappedEnv.featurize(state[self.gym.training_agent])\n",
    "        agent_reward = reward[self.gym.training_agent]\n",
    "        # If nobody die, use some \"smart\" reward\n",
    "        if agent_reward == 0:\n",
    "            agent_reward = self.gym.train_reward\n",
    "        return agent_state, terminal, agent_reward\n",
    "\n",
    "    def reset(self):\n",
    "        obs = self.gym.reset()\n",
    "        agent_obs = WrappedEnv.featurize(obs[3])\n",
    "        return agent_obs\n",
    "    \n",
    "    @staticmethod\n",
    "    def center_view(board, mypos, viewSize=25):\n",
    "        # make sure of odd viewSize\n",
    "        viewSize = viewSize + 1 if viewSize % 2 == 0 else viewSize \n",
    "\n",
    "        # assumed board's odd shape, dimensions must be odd!        \n",
    "        wmax, hmax = board.shape[1]*2+1, board.shape[0]*2+1 \n",
    "        agentView = np.ones((wmax,hmax,1)) # agent centric full-world coverage\n",
    "        center = (agentView.shape[0]//2+1, agentView.shape[1]//2+1)\n",
    "\n",
    "        # copy board to the new view\n",
    "        offset_y = center[0]-mypos[0]-1\n",
    "        offset_x = center[1]-mypos[1]-1\n",
    "        agentView[offset_y:offset_y+13, offset_x:offset_x+13] = board\n",
    "        #np.savetxt('board.txt', agentView, fmt=\"%2.i\") # save to file for debug\n",
    "\n",
    "        # finalize view size\n",
    "        r = viewSize // 2\n",
    "        start, end = center[0]-r-1, center[0]+r\n",
    "        agentView = agentView[start:end, start:end] \n",
    "        #np.savetxt('board_cut.txt', agentView, fmt=\"%2.i\") # save to file for debug\n",
    "\n",
    "        return np.array(agentView, dtype=np.float32)\n",
    "\n",
    "    @staticmethod\n",
    "    def featurize(obs):\n",
    "        def get_matrix(dict, key):\n",
    "            res = dict[key]\n",
    "            return res.reshape(res.shape[0], res.shape[1], 1).astype(np.float32)\n",
    "\n",
    "        board = get_matrix(obs, 'board')\n",
    "        teammate_position = None\n",
    "        teammate = obs[\"teammate\"]\n",
    "        if teammate is not None:\n",
    "            teammate = teammate.value\n",
    "            if teammate > 10 and teammate < 15:\n",
    "                teammate_position = np.argwhere(board == teammate)[0]\n",
    "        else:\n",
    "            teammate = None\n",
    "        # My self - 11\n",
    "        # Team mate - 12\n",
    "        # Enemy - 13\n",
    "\n",
    "        # Everyone enemy\n",
    "        board[(board > 10) & (board < 15)] = 13\n",
    "        # I'm not enemy\n",
    "        my_position = obs['position']\n",
    "        board[my_position[0], my_position[1], 0] = 11\n",
    "        # Set teammate\n",
    "        if teammate_position is not None:\n",
    "            board[teammate_position[0], teammate_position[1], teammate_position[2]] = 12\n",
    "\n",
    "        bomb_blast_strength = get_matrix(obs, 'bomb_blast_strength')\n",
    "        bomb_life = get_matrix(obs, 'bomb_life')\n",
    "        \n",
    "        board = WrappedEnv.center_view(board, my_position)\n",
    "        bomb_blast_strength = WrappedEnv.center_view(bomb_blast_strength, my_position)\n",
    "        bomb_life = WrappedEnv.center_view(bomb_life, my_position)\n",
    "        \n",
    "        conv_inp = np.concatenate([board, bomb_blast_strength, bomb_life], axis=2)\n",
    "        state = np.array([obs[\"ammo\"], obs[\"blast_strength\"], obs[\"can_kick\"]]).astype(np.float32)        \n",
    "        return dict(board=conv_inp, state=state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode_finished(r):\n",
    "    if r.episode % 10 == 0:\n",
    "        print(\"Finished episode {ep} after {ts} timesteps\".format(ep=r.episode + 1, ts = r.timestep + 1))\n",
    "        print(\"Episode reward: {}\".format(r.episode_rewards[-1]))\n",
    "        print(\"Average of last 10 rewards: {}\".format(np.mean(r.episode_rewards[10:])))\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 11 after 500 timesteps\n",
      "Episode reward: -1.1320000000000001\n",
      "Average of last 10 rewards: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/hpchome/anton95/.conda/envs/pommerman/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/gpfs/hpchome/anton95/.conda/envs/pommerman/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 21 after 1030 timesteps\n",
      "Episode reward: -1.177\n",
      "Average of last 10 rewards: -1.1018999999999999\n",
      "Finished episode 31 after 1684 timesteps\n",
      "Episode reward: -1.051\n",
      "Average of last 10 rewards: -1.0785\n",
      "Finished episode 41 after 2511 timesteps\n",
      "Episode reward: -0.6439999999999999\n",
      "Average of last 10 rewards: -1.0694333333333332\n",
      "Finished episode 51 after 3041 timesteps\n",
      "Episode reward: -1.032\n",
      "Average of last 10 rewards: -1.0682500000000001\n",
      "Finished episode 61 after 3608 timesteps\n",
      "Episode reward: -1.2750000000000001\n",
      "Average of last 10 rewards: -1.0659\n",
      "Finished episode 71 after 4113 timesteps\n",
      "Episode reward: -1.039\n",
      "Average of last 10 rewards: -1.0701666666666667\n",
      "Finished episode 81 after 4545 timesteps\n",
      "Episode reward: -1.027\n",
      "Average of last 10 rewards: -1.0708571428571425\n",
      "Finished episode 91 after 4942 timesteps\n",
      "Episode reward: -1.103\n",
      "Average of last 10 rewards: -1.0714875\n",
      "Finished episode 101 after 5541 timesteps\n",
      "Episode reward: -1.2460000000000002\n",
      "Average of last 10 rewards: -1.0665111111111112\n",
      "Finished episode 111 after 6239 timesteps\n",
      "Episode reward: -1.219\n",
      "Average of last 10 rewards: -1.0666699999999998\n",
      "Finished episode 121 after 6775 timesteps\n",
      "Episode reward: -0.7150000000000001\n",
      "Average of last 10 rewards: -1.0656454545454546\n",
      "Finished episode 131 after 7404 timesteps\n",
      "Episode reward: -1.03\n",
      "Average of last 10 rewards: -1.069475\n",
      "Finished episode 141 after 8274 timesteps\n",
      "Episode reward: -1.104\n",
      "Average of last 10 rewards: -1.070753846153846\n",
      "INFO:tensorflow:Saving checkpoints for 8692 into ./ppo/model/model.ckpt.\n",
      "Finished episode 151 after 8892 timesteps\n",
      "Episode reward: -1.2550000000000001\n",
      "Average of last 10 rewards: -1.0727642857142858\n",
      "Finished episode 161 after 9424 timesteps\n",
      "Episode reward: -1.05\n",
      "Average of last 10 rewards: -1.0739733333333334\n",
      "Finished episode 171 after 9983 timesteps\n",
      "Episode reward: -1.143\n",
      "Average of last 10 rewards: -1.071725\n",
      "Finished episode 181 after 10498 timesteps\n",
      "Episode reward: -1.089\n",
      "Average of last 10 rewards: -1.072541176470588\n",
      "Finished episode 191 after 11214 timesteps\n",
      "Episode reward: -1.1800000000000002\n",
      "Average of last 10 rewards: -1.066027777777778\n",
      "Finished episode 201 after 11931 timesteps\n",
      "Episode reward: -1.05\n",
      "Average of last 10 rewards: -1.069221052631579\n",
      "Finished episode 211 after 12644 timesteps\n",
      "Episode reward: -1.143\n",
      "Average of last 10 rewards: -1.07171\n",
      "Finished episode 221 after 13213 timesteps\n",
      "Episode reward: -1.1920000000000002\n",
      "Average of last 10 rewards: -1.0718857142857143\n",
      "Finished episode 231 after 13623 timesteps\n",
      "Episode reward: -1.207\n",
      "Average of last 10 rewards: -1.070490909090909\n",
      "Finished episode 241 after 14254 timesteps\n",
      "Episode reward: -1.11\n",
      "Average of last 10 rewards: -1.0718260869565217\n",
      "Finished episode 251 after 15011 timesteps\n",
      "Episode reward: -1.105\n",
      "Average of last 10 rewards: -1.07405\n",
      "Finished episode 261 after 15461 timesteps\n",
      "Episode reward: -1.07\n",
      "Average of last 10 rewards: -1.072608\n",
      "Finished episode 271 after 16052 timesteps\n",
      "Episode reward: -1.036\n",
      "Average of last 10 rewards: -1.0736346153846152\n",
      "Finished episode 281 after 16390 timesteps\n",
      "Episode reward: -1.067\n",
      "Average of last 10 rewards: -1.0719777777777777\n",
      "Finished episode 291 after 16991 timesteps\n",
      "Episode reward: -1.069\n",
      "Average of last 10 rewards: -1.0715035714285714\n",
      "INFO:tensorflow:Saving checkpoints for 17167 into ./ppo/model/model.ckpt.\n",
      "Finished episode 301 after 17626 timesteps\n",
      "Episode reward: -1.052\n",
      "Average of last 10 rewards: -1.0696586206896552\n",
      "Finished episode 311 after 18172 timesteps\n",
      "Episode reward: -1.059\n",
      "Average of last 10 rewards: -1.0709733333333333\n",
      "Finished episode 321 after 19012 timesteps\n",
      "Episode reward: -1.039\n",
      "Average of last 10 rewards: -1.0691290322580647\n",
      "Finished episode 331 after 19395 timesteps\n",
      "Episode reward: -1.041\n",
      "Average of last 10 rewards: -1.0686812499999998\n",
      "Finished episode 341 after 20720 timesteps\n",
      "Episode reward: -0.9850000000000005\n",
      "Average of last 10 rewards: -1.0699151515151515\n",
      "Finished episode 351 after 21442 timesteps\n",
      "Episode reward: -1.051\n",
      "Average of last 10 rewards: -1.0657558823529412\n",
      "Finished episode 361 after 22104 timesteps\n",
      "Episode reward: -1.069\n",
      "Average of last 10 rewards: -1.0673371428571428\n",
      "Finished episode 371 after 23288 timesteps\n",
      "Episode reward: -1.099\n",
      "Average of last 10 rewards: -1.056452777777778\n",
      "Finished episode 381 after 24027 timesteps\n",
      "Episode reward: -1.103\n",
      "Average of last 10 rewards: -1.0551162162162162\n",
      "Finished episode 391 after 24850 timesteps\n",
      "Episode reward: -1.071\n",
      "Average of last 10 rewards: -1.0569973684210527\n",
      "Finished episode 401 after 25541 timesteps\n",
      "Episode reward: -1.048\n",
      "Average of last 10 rewards: -1.0561102564102565\n",
      "INFO:tensorflow:Saving checkpoints for 26033 into ./ppo/model/model.ckpt.\n",
      "Finished episode 411 after 26034 timesteps\n",
      "Episode reward: -1.06\n",
      "Average of last 10 rewards: -1.05702\n",
      "Finished episode 421 after 26455 timesteps\n",
      "Episode reward: -1.064\n",
      "Average of last 10 rewards: -1.054439024390244\n",
      "Finished episode 431 after 26980 timesteps\n",
      "Episode reward: -1.074\n",
      "Average of last 10 rewards: -1.0548071428571428\n",
      "Finished episode 441 after 27708 timesteps\n",
      "Episode reward: -1.169\n",
      "Average of last 10 rewards: -1.0561162790697673\n",
      "Finished episode 451 after 28145 timesteps\n",
      "Episode reward: -1.145\n",
      "Average of last 10 rewards: -1.0556590909090908\n",
      "Finished episode 461 after 28897 timesteps\n",
      "Episode reward: -1.099\n",
      "Average of last 10 rewards: -1.0566200000000001\n",
      "Finished episode 471 after 29417 timesteps\n",
      "Episode reward: -1.09\n",
      "Average of last 10 rewards: -1.0571717391304347\n",
      "Finished episode 481 after 30146 timesteps\n",
      "Episode reward: -1.101\n",
      "Average of last 10 rewards: -1.0583106382978724\n",
      "Finished episode 491 after 30893 timesteps\n",
      "Episode reward: -1.254\n",
      "Average of last 10 rewards: -1.0589583333333334\n",
      "Finished episode 501 after 31633 timesteps\n",
      "Episode reward: -1.155\n",
      "Average of last 10 rewards: -1.0580020408163266\n",
      "Finished episode 511 after 32279 timesteps\n",
      "Episode reward: -1.043\n",
      "Average of last 10 rewards: -1.0588899999999999\n",
      "Finished episode 521 after 33128 timesteps\n",
      "Episode reward: -1.2460000000000002\n",
      "Average of last 10 rewards: -1.0589176470588235\n",
      "Finished episode 531 after 34139 timesteps\n",
      "Episode reward: -1.045\n",
      "Average of last 10 rewards: -1.0593557692307694\n",
      "INFO:tensorflow:Saving checkpoints for 34650 into ./ppo/model/model.ckpt.\n",
      "Finished episode 541 after 34651 timesteps\n",
      "Episode reward: -1.086\n",
      "Average of last 10 rewards: -1.06008679245283\n",
      "Finished episode 551 after 35296 timesteps\n",
      "Episode reward: -1.1620000000000001\n",
      "Average of last 10 rewards: -1.0613722222222224\n",
      "Finished episode 561 after 35886 timesteps\n",
      "Episode reward: -1.098\n",
      "Average of last 10 rewards: -1.0606654545454546\n",
      "Finished episode 571 after 36608 timesteps\n",
      "Episode reward: -1.042\n",
      "Average of last 10 rewards: -1.0588875000000002\n",
      "Finished episode 581 after 37309 timesteps\n",
      "Episode reward: -1.045\n",
      "Average of last 10 rewards: -1.0585122807017546\n",
      "Finished episode 591 after 37854 timesteps\n",
      "Episode reward: -1.057\n",
      "Average of last 10 rewards: -1.0591810344827586\n",
      "Finished episode 601 after 38410 timesteps\n",
      "Episode reward: -1.04\n",
      "Average of last 10 rewards: -1.0597559322033898\n",
      "Finished episode 611 after 38847 timesteps\n",
      "Episode reward: -1.076\n",
      "Average of last 10 rewards: -1.0593716666666666\n",
      "Finished episode 621 after 39319 timesteps\n",
      "Episode reward: -1.1320000000000001\n",
      "Average of last 10 rewards: -1.058922950819672\n",
      "Finished episode 631 after 39981 timesteps\n",
      "Episode reward: -1.071\n",
      "Average of last 10 rewards: -1.059058064516129\n",
      "Finished episode 641 after 40465 timesteps\n",
      "Episode reward: -0.762\n",
      "Average of last 10 rewards: -1.0589063492063493\n",
      "Finished episode 651 after 41160 timesteps\n",
      "Episode reward: -1.1620000000000001\n",
      "Average of last 10 rewards: -1.0594734375\n",
      "Finished episode 661 after 41752 timesteps\n",
      "Episode reward: -1.055\n",
      "Average of last 10 rewards: -1.059990769230769\n",
      "Finished episode 671 after 42766 timesteps\n",
      "Episode reward: -1.1400000000000001\n",
      "Average of last 10 rewards: -1.0588727272727272\n",
      "INFO:tensorflow:Saving checkpoints for 43173 into ./ppo/model/model.ckpt.\n",
      "Finished episode 681 after 43174 timesteps\n",
      "Episode reward: -1.036\n",
      "Average of last 10 rewards: -1.0579731343283583\n",
      "Finished episode 691 after 43660 timesteps\n",
      "Episode reward: -1.068\n",
      "Average of last 10 rewards: -1.0585411764705883\n",
      "Finished episode 701 after 44150 timesteps\n",
      "Episode reward: -1.037\n",
      "Average of last 10 rewards: -1.0572782608695652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 711 after 45020 timesteps\n",
      "Episode reward: -1.199\n",
      "Average of last 10 rewards: -1.0586442857142857\n",
      "Finished episode 721 after 45635 timesteps\n",
      "Episode reward: -1.034\n",
      "Average of last 10 rewards: -1.058723943661972\n",
      "Finished episode 731 after 46145 timesteps\n",
      "Episode reward: -1.058\n",
      "Average of last 10 rewards: -1.0585597222222223\n",
      "Finished episode 741 after 46526 timesteps\n",
      "Episode reward: -1.04\n",
      "Average of last 10 rewards: -1.058146575342466\n",
      "Finished episode 751 after 47273 timesteps\n",
      "Episode reward: -1.059\n",
      "Average of last 10 rewards: -1.0588297297297298\n",
      "Finished episode 761 after 48000 timesteps\n",
      "Episode reward: -0.8660000000000001\n",
      "Average of last 10 rewards: -1.0591439999999999\n",
      "Finished episode 771 after 48591 timesteps\n",
      "Episode reward: -1.074\n",
      "Average of last 10 rewards: -1.059742105263158\n",
      "Finished episode 781 after 49242 timesteps\n",
      "Episode reward: -1.247\n",
      "Average of last 10 rewards: -1.0592623376623378\n",
      "Finished episode 791 after 49746 timesteps\n",
      "Episode reward: -0.6719999999999999\n",
      "Average of last 10 rewards: -1.0591602564102565\n",
      "Finished episode 801 after 50193 timesteps\n",
      "Episode reward: -0.7050000000000001\n",
      "Average of last 10 rewards: -1.0583974683544304\n",
      "Finished episode 811 after 50778 timesteps\n",
      "Episode reward: -0.402\n",
      "Average of last 10 rewards: -1.05748125\n",
      "INFO:tensorflow:Saving checkpoints for 50839 into ./ppo/model/model.ckpt.\n",
      "Finished episode 821 after 51558 timesteps\n",
      "Episode reward: -1.4600000000000004\n",
      "Average of last 10 rewards: -1.057004938271605\n",
      "Finished episode 831 after 52218 timesteps\n",
      "Episode reward: -1.2380000000000002\n",
      "Average of last 10 rewards: -1.0578914634146341\n",
      "Finished episode 841 after 52936 timesteps\n",
      "Episode reward: -1.036\n",
      "Average of last 10 rewards: -1.0570361445783134\n",
      "Finished episode 851 after 53613 timesteps\n",
      "Episode reward: -1.037\n",
      "Average of last 10 rewards: -1.0569797619047618\n",
      "Finished episode 861 after 54040 timesteps\n",
      "Episode reward: -1.137\n",
      "Average of last 10 rewards: -1.0571870588235293\n",
      "Finished episode 871 after 54561 timesteps\n",
      "Episode reward: -1.045\n",
      "Average of last 10 rewards: -1.0574616279069768\n",
      "Finished episode 881 after 55046 timesteps\n",
      "Episode reward: -1.111\n",
      "Average of last 10 rewards: -1.0577310344827586\n",
      "Finished episode 891 after 55483 timesteps\n",
      "Episode reward: -1.059\n",
      "Average of last 10 rewards: -1.0565727272727272\n",
      "Finished episode 901 after 56325 timesteps\n",
      "Episode reward: -1.043\n",
      "Average of last 10 rewards: -1.0571977528089886\n",
      "Finished episode 911 after 56870 timesteps\n",
      "Episode reward: -1.107\n",
      "Average of last 10 rewards: -1.0575\n",
      "Finished episode 921 after 57423 timesteps\n",
      "Episode reward: -1.03\n",
      "Average of last 10 rewards: -1.0573615384615385\n",
      "Finished episode 931 after 58010 timesteps\n",
      "Episode reward: -1.1800000000000002\n",
      "Average of last 10 rewards: -1.0573445652173914\n",
      "INFO:tensorflow:Saving checkpoints for 58777 into ./ppo/model/model.ckpt.\n",
      "Finished episode 941 after 58778 timesteps\n",
      "Episode reward: -0.6950000000000001\n",
      "Average of last 10 rewards: -1.0569612903225807\n",
      "Finished episode 951 after 59362 timesteps\n",
      "Episode reward: -1.123\n",
      "Average of last 10 rewards: -1.057453191489362\n",
      "Finished episode 961 after 59855 timesteps\n",
      "Episode reward: -0.774\n",
      "Average of last 10 rewards: -1.0570936842105263\n",
      "Finished episode 971 after 60953 timesteps\n",
      "Episode reward: -1.055\n",
      "Average of last 10 rewards: -1.0581447916666666\n",
      "Finished episode 981 after 61689 timesteps\n",
      "Episode reward: -1.074\n",
      "Average of last 10 rewards: -1.0584628865979382\n",
      "Finished episode 991 after 62207 timesteps\n",
      "Episode reward: -1.053\n",
      "Average of last 10 rewards: -1.0584224489795917\n",
      "Finished episode 1001 after 62687 timesteps\n",
      "Episode reward: -1.051\n",
      "Average of last 10 rewards: -1.0582343434343435\n",
      "Finished episode 1011 after 63122 timesteps\n",
      "Episode reward: -1.076\n",
      "Average of last 10 rewards: -1.058046\n",
      "Finished episode 1021 after 63620 timesteps\n",
      "Episode reward: -1.117\n",
      "Average of last 10 rewards: -1.058079207920792\n",
      "Finished episode 1031 after 64090 timesteps\n",
      "Episode reward: -1.087\n",
      "Average of last 10 rewards: -1.057890196078431\n",
      "Finished episode 1041 after 64494 timesteps\n",
      "Episode reward: -1.059\n",
      "Average of last 10 rewards: -1.0580475728155343\n",
      "Finished episode 1051 after 65022 timesteps\n",
      "Episode reward: -1.041\n",
      "Average of last 10 rewards: -1.0584250000000002\n",
      "Finished episode 1061 after 65818 timesteps\n",
      "Episode reward: -1.1720000000000002\n",
      "Average of last 10 rewards: -1.059052380952381\n",
      "INFO:tensorflow:Saving checkpoints for 66646 into ./ppo/model/model.ckpt.\n",
      "Finished episode 1071 after 66647 timesteps\n",
      "Episode reward: -1.02\n",
      "Average of last 10 rewards: -1.0596481132075473\n",
      "Finished episode 1081 after 67074 timesteps\n",
      "Episode reward: -1.054\n",
      "Average of last 10 rewards: -1.0598635514018693\n",
      "Finished episode 1091 after 67617 timesteps\n",
      "Episode reward: -1.149\n",
      "Average of last 10 rewards: -1.0602194444444444\n",
      "Finished episode 1101 after 68595 timesteps\n",
      "Episode reward: -1.066\n",
      "Average of last 10 rewards: -1.0611954128440368\n",
      "Finished episode 1111 after 69119 timesteps\n",
      "Episode reward: -1.072\n",
      "Average of last 10 rewards: -1.060798181818182\n",
      "Finished episode 1121 after 69802 timesteps\n",
      "Episode reward: -1.045\n",
      "Average of last 10 rewards: -1.0613963963963964\n",
      "Finished episode 1131 after 70477 timesteps\n",
      "Episode reward: -1.013\n",
      "Average of last 10 rewards: -1.0614571428571429\n",
      "Finished episode 1141 after 71180 timesteps\n",
      "Episode reward: -1.032\n",
      "Average of last 10 rewards: -1.0620982300884956\n",
      "Finished episode 1151 after 71909 timesteps\n",
      "Episode reward: -1.3470000000000002\n",
      "Average of last 10 rewards: -1.0627061403508773\n",
      "Finished episode 1161 after 72355 timesteps\n",
      "Episode reward: -1.068\n",
      "Average of last 10 rewards: -1.0624973913043478\n",
      "Finished episode 1171 after 72893 timesteps\n",
      "Episode reward: -1.139\n",
      "Average of last 10 rewards: -1.0625422413793104\n",
      "Finished episode 1181 after 73739 timesteps\n",
      "Episode reward: -1.2730000000000001\n",
      "Average of last 10 rewards: -1.0629017094017095\n",
      "INFO:tensorflow:Saving checkpoints for 74411 into ./ppo/model/model.ckpt.\n",
      "Finished episode 1191 after 74412 timesteps\n",
      "Episode reward: -1.088\n",
      "Average of last 10 rewards: -1.0627813559322032\n",
      "Finished episode 1201 after 75142 timesteps\n",
      "Episode reward: -1.079\n",
      "Average of last 10 rewards: -1.0634613445378152\n",
      "Finished episode 1211 after 75712 timesteps\n",
      "Episode reward: -1.1900000000000002\n",
      "Average of last 10 rewards: -1.0633650000000001\n",
      "Finished episode 1221 after 76266 timesteps\n",
      "Episode reward: -1.036\n",
      "Average of last 10 rewards: -1.0633487603305785\n",
      "Finished episode 1231 after 77190 timesteps\n",
      "Episode reward: -1.2140000000000002\n",
      "Average of last 10 rewards: -1.0637483606557379\n",
      "Finished episode 1241 after 77685 timesteps\n",
      "Episode reward: -1.3490000000000002\n",
      "Average of last 10 rewards: -1.0634341463414634\n",
      "Finished episode 1251 after 78159 timesteps\n",
      "Episode reward: -1.131\n",
      "Average of last 10 rewards: -1.0636\n",
      "Finished episode 1261 after 78885 timesteps\n",
      "Episode reward: -1.1620000000000001\n",
      "Average of last 10 rewards: -1.0641808000000001\n",
      "Finished episode 1271 after 79353 timesteps\n",
      "Episode reward: -1.1500000000000001\n",
      "Average of last 10 rewards: -1.0640650793650794\n",
      "Finished episode 1281 after 80070 timesteps\n",
      "Episode reward: -0.8560000000000001\n",
      "Average of last 10 rewards: -1.0642913385826773\n",
      "Finished episode 1291 after 80700 timesteps\n",
      "Episode reward: -1.038\n",
      "Average of last 10 rewards: -1.0646015625\n",
      "Finished episode 1301 after 81323 timesteps\n",
      "Episode reward: -1.112\n",
      "Average of last 10 rewards: -1.064020930232558\n",
      "INFO:tensorflow:Saving checkpoints for 82019 into ./ppo/model/model.ckpt.\n",
      "Finished episode 1311 after 82169 timesteps\n",
      "Episode reward: -1.2570000000000001\n",
      "Average of last 10 rewards: -1.0646838461538461\n",
      "Finished episode 1321 after 83145 timesteps\n",
      "Episode reward: -0.9050000000000002\n",
      "Average of last 10 rewards: -1.063667175572519\n",
      "Finished episode 1331 after 83740 timesteps\n",
      "Episode reward: -1.041\n",
      "Average of last 10 rewards: -1.0632757575757574\n",
      "Finished episode 1341 after 84127 timesteps\n",
      "Episode reward: -1.145\n",
      "Average of last 10 rewards: -1.0633857142857142\n",
      "Finished episode 1351 after 85008 timesteps\n",
      "Episode reward: -1.067\n",
      "Average of last 10 rewards: -1.0636425373134328\n",
      "Finished episode 1361 after 85539 timesteps\n",
      "Episode reward: -1.115\n",
      "Average of last 10 rewards: -1.0632688888888888\n",
      "Finished episode 1371 after 86439 timesteps\n",
      "Episode reward: -1.046\n",
      "Average of last 10 rewards: -1.0636764705882353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 1381 after 86847 timesteps\n",
      "Episode reward: -1.092\n",
      "Average of last 10 rewards: -1.0637992700729926\n",
      "Finished episode 1391 after 87329 timesteps\n",
      "Episode reward: -1.052\n",
      "Average of last 10 rewards: -1.0636949275362317\n",
      "Finished episode 1401 after 87873 timesteps\n",
      "Episode reward: -0.6839999999999999\n",
      "Average of last 10 rewards: -1.0635352517985612\n",
      "Finished episode 1411 after 88342 timesteps\n",
      "Episode reward: -1.047\n",
      "Average of last 10 rewards: -1.0633771428571428\n",
      "INFO:tensorflow:Saving checkpoints for 88963 into ./ppo/model/model.ckpt.\n",
      "Finished episode 1421 after 88964 timesteps\n",
      "Episode reward: -1.057\n",
      "Average of last 10 rewards: -1.0636234042553192\n",
      "Finished episode 1431 after 89775 timesteps\n",
      "Episode reward: -1.065\n",
      "Average of last 10 rewards: -1.0639\n",
      "Finished episode 1441 after 90308 timesteps\n",
      "Episode reward: -1.052\n",
      "Average of last 10 rewards: -1.0637335664335665\n",
      "Finished episode 1451 after 90756 timesteps\n",
      "Episode reward: -1.034\n",
      "Average of last 10 rewards: -1.0637340277777778\n",
      "Finished episode 1461 after 91567 timesteps\n",
      "Episode reward: -1.055\n",
      "Average of last 10 rewards: -1.0637634482758622\n",
      "Finished episode 1471 after 92243 timesteps\n",
      "Episode reward: -1.042\n",
      "Average of last 10 rewards: -1.0637849315068495\n",
      "Finished episode 1481 after 92777 timesteps\n",
      "Episode reward: -0.7670000000000001\n",
      "Average of last 10 rewards: -1.0637061224489797\n",
      "Finished episode 1491 after 93533 timesteps\n",
      "Episode reward: -1.1980000000000002\n",
      "Average of last 10 rewards: -1.0634925675675675\n",
      "Finished episode 1501 after 94313 timesteps\n",
      "Episode reward: 1.408\n",
      "Average of last 10 rewards: -1.0617724832214765\n",
      "Finished episode 1511 after 94854 timesteps\n",
      "Episode reward: -1.1560000000000001\n",
      "Average of last 10 rewards: -1.061998\n",
      "Finished episode 1521 after 95719 timesteps\n",
      "Episode reward: -1.029\n",
      "Average of last 10 rewards: -1.061958940397351\n",
      "Finished episode 1531 after 96199 timesteps\n",
      "Episode reward: -0.6699999999999999\n",
      "Average of last 10 rewards: -1.061363157894737\n",
      "INFO:tensorflow:Saving checkpoints for 96625 into ./ppo/model/model.ckpt.\n",
      "Finished episode 1541 after 96626 timesteps\n",
      "Episode reward: -1.045\n",
      "Average of last 10 rewards: -1.061561437908497\n",
      "Finished episode 1551 after 97307 timesteps\n",
      "Episode reward: -1.4080000000000004\n",
      "Average of last 10 rewards: -1.0619077922077924\n",
      "Finished episode 1561 after 98062 timesteps\n",
      "Episode reward: -1.061\n",
      "Average of last 10 rewards: -1.0623767741935486\n",
      "Finished episode 1571 after 98528 timesteps\n",
      "Episode reward: -1.064\n",
      "Average of last 10 rewards: -1.062302564102564\n",
      "Finished episode 1581 after 99077 timesteps\n",
      "Episode reward: -1.141\n",
      "Average of last 10 rewards: -1.0624324840764332\n",
      "Finished episode 1591 after 99492 timesteps\n",
      "Episode reward: -1.171\n",
      "Average of last 10 rewards: -1.0624987341772152\n",
      "Finished episode 1601 after 99894 timesteps\n",
      "Episode reward: -1.092\n",
      "Average of last 10 rewards: -1.0625232704402514\n",
      "Finished episode 1611 after 100490 timesteps\n",
      "Episode reward: -1.058\n",
      "Average of last 10 rewards: -1.062834375\n",
      "Finished episode 1621 after 101053 timesteps\n",
      "Episode reward: -1.052\n",
      "Average of last 10 rewards: -1.0631527950310558\n",
      "Finished episode 1631 after 101471 timesteps\n",
      "Episode reward: -1.06\n",
      "Average of last 10 rewards: -1.063279012345679\n",
      "Finished episode 1641 after 102048 timesteps\n",
      "Episode reward: -0.7430000000000001\n",
      "Average of last 10 rewards: -1.0622625766871165\n",
      "Finished episode 1651 after 102525 timesteps\n",
      "Episode reward: -1.04\n",
      "Average of last 10 rewards: -1.062407317073171\n",
      "Finished episode 1661 after 102979 timesteps\n",
      "Episode reward: -1.12\n",
      "Average of last 10 rewards: -1.0624666666666667\n",
      "Finished episode 1671 after 103358 timesteps\n",
      "Episode reward: -1.069\n",
      "Average of last 10 rewards: -1.0624801204819279\n",
      "Finished episode 1681 after 104013 timesteps\n",
      "Episode reward: -1.104\n",
      "Average of last 10 rewards: -1.062460479041916\n",
      "INFO:tensorflow:Saving checkpoints for 104607 into ./ppo/model/model.ckpt.\n",
      "Finished episode 1691 after 104819 timesteps\n",
      "Episode reward: -1.1720000000000002\n",
      "Average of last 10 rewards: -1.0627964285714286\n",
      "Finished episode 1701 after 105309 timesteps\n",
      "Episode reward: -1.074\n",
      "Average of last 10 rewards: -1.0626254437869822\n",
      "Finished episode 1711 after 105818 timesteps\n",
      "Episode reward: -1.049\n",
      "Average of last 10 rewards: -1.062485294117647\n",
      "Finished episode 1721 after 106506 timesteps\n",
      "Episode reward: -1.3110000000000002\n",
      "Average of last 10 rewards: -1.0625888888888888\n",
      "Finished episode 1731 after 107349 timesteps\n",
      "Episode reward: -0.656\n",
      "Average of last 10 rewards: -1.061963372093023\n",
      "Finished episode 1741 after 107908 timesteps\n",
      "Episode reward: -1.2280000000000002\n",
      "Average of last 10 rewards: -1.0621410404624276\n",
      "Finished episode 1751 after 108234 timesteps\n",
      "Episode reward: -1.11\n",
      "Average of last 10 rewards: -1.061907471264368\n",
      "Finished episode 1761 after 109043 timesteps\n",
      "Episode reward: -1.047\n",
      "Average of last 10 rewards: -1.0621074285714287\n",
      "Finished episode 1771 after 109516 timesteps\n",
      "Episode reward: -1.046\n",
      "Average of last 10 rewards: -1.061990340909091\n",
      "Finished episode 1781 after 110078 timesteps\n",
      "Episode reward: -1.165\n",
      "Average of last 10 rewards: -1.0621847457627118\n",
      "Finished episode 1791 after 110436 timesteps\n",
      "Episode reward: -1.092\n",
      "Average of last 10 rewards: -1.0622511235955057\n",
      "Finished episode 1801 after 111002 timesteps\n",
      "Episode reward: -1.023\n",
      "Average of last 10 rewards: -1.0622022346368716\n",
      "Finished episode 1811 after 111501 timesteps\n",
      "Episode reward: -1.05\n",
      "Average of last 10 rewards: -1.0623144444444446\n",
      "Finished episode 1821 after 112157 timesteps\n",
      "Episode reward: -1.131\n",
      "Average of last 10 rewards: -1.0623370165745858\n",
      "INFO:tensorflow:Saving checkpoints for 112506 into ./ppo/model/model.ckpt.\n",
      "Finished episode 1831 after 112653 timesteps\n",
      "Episode reward: -1.07\n",
      "Average of last 10 rewards: -1.0622082417582417\n",
      "Finished episode 1841 after 113118 timesteps\n",
      "Episode reward: -1.1680000000000001\n",
      "Average of last 10 rewards: -1.0623814207650273\n",
      "Finished episode 1851 after 113903 timesteps\n",
      "Episode reward: -1.077\n",
      "Average of last 10 rewards: -1.0623048913043478\n",
      "Finished episode 1861 after 114494 timesteps\n",
      "Episode reward: -1.075\n",
      "Average of last 10 rewards: -1.062292972972973\n",
      "Finished episode 1871 after 114930 timesteps\n",
      "Episode reward: -1.1420000000000001\n",
      "Average of last 10 rewards: -1.0621827956989247\n",
      "Finished episode 1881 after 115305 timesteps\n",
      "Episode reward: -1.007\n",
      "Average of last 10 rewards: -1.0621673796791444\n",
      "Finished episode 1891 after 115952 timesteps\n",
      "Episode reward: -1.2220000000000002\n",
      "Average of last 10 rewards: -1.061961170212766\n",
      "Finished episode 1901 after 116541 timesteps\n",
      "Episode reward: -1.05\n",
      "Average of last 10 rewards: -1.0617285714285714\n",
      "Finished episode 1911 after 117299 timesteps\n",
      "Episode reward: -1.091\n",
      "Average of last 10 rewards: -1.061681052631579\n",
      "Finished episode 1921 after 118107 timesteps\n",
      "Episode reward: -1.2590000000000001\n",
      "Average of last 10 rewards: -1.0618842931937174\n",
      "Finished episode 1931 after 118482 timesteps\n",
      "Episode reward: -1.039\n",
      "Average of last 10 rewards: -1.06195\n",
      "Finished episode 1941 after 119303 timesteps\n",
      "Episode reward: -1.119\n",
      "Average of last 10 rewards: -1.0621212435233163\n",
      "Finished episode 1951 after 119913 timesteps\n",
      "Episode reward: -1.083\n",
      "Average of last 10 rewards: -1.0621252577319589\n",
      "INFO:tensorflow:Saving checkpoints for 119981 into ./ppo/model/model.ckpt.\n",
      "Finished episode 1961 after 120623 timesteps\n",
      "Episode reward: -1.1980000000000002\n",
      "Average of last 10 rewards: -1.062397948717949\n",
      "Finished episode 1971 after 121069 timesteps\n",
      "Episode reward: -1.052\n",
      "Average of last 10 rewards: -1.0624714285714285\n",
      "Finished episode 1981 after 121557 timesteps\n",
      "Episode reward: -1.044\n",
      "Average of last 10 rewards: -1.062415736040609\n",
      "Finished episode 1991 after 122454 timesteps\n",
      "Episode reward: -1.122\n",
      "Average of last 10 rewards: -1.0625348484848485\n",
      "Finished episode 2001 after 122894 timesteps\n",
      "Episode reward: -1.071\n",
      "Average of last 10 rewards: -1.0625929648241208\n",
      "Finished episode 2011 after 123710 timesteps\n",
      "Episode reward: -1.1580000000000001\n",
      "Average of last 10 rewards: -1.0630505000000001\n",
      "Finished episode 2021 after 124220 timesteps\n",
      "Episode reward: -1.038\n",
      "Average of last 10 rewards: -1.0630567164179103\n",
      "Finished episode 2031 after 124700 timesteps\n",
      "Episode reward: -1.033\n",
      "Average of last 10 rewards: -1.063209405940594\n",
      "Finished episode 2041 after 125213 timesteps\n",
      "Episode reward: -1.131\n",
      "Average of last 10 rewards: -1.0631960591133005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 2051 after 125877 timesteps\n",
      "Episode reward: -1.074\n",
      "Average of last 10 rewards: -1.063444607843137\n",
      "Finished episode 2061 after 126565 timesteps\n",
      "Episode reward: -1.062\n",
      "Average of last 10 rewards: -1.063361951219512\n",
      "Finished episode 2071 after 127214 timesteps\n",
      "Episode reward: -1.079\n",
      "Average of last 10 rewards: -1.0630757281553398\n",
      "INFO:tensorflow:Saving checkpoints for 127677 into ./ppo/model/model.ckpt.\n",
      "Finished episode 2081 after 127971 timesteps\n",
      "Episode reward: -1.043\n",
      "Average of last 10 rewards: -1.062919806763285\n",
      "Finished episode 2091 after 128584 timesteps\n",
      "Episode reward: -1.3860000000000003\n",
      "Average of last 10 rewards: -1.0627783653846155\n",
      "Finished episode 2151 after 131846 timesteps\n",
      "Episode reward: -1.039\n",
      "Average of last 10 rewards: -1.0633509345794392\n",
      "Finished episode 2161 after 132601 timesteps\n",
      "Episode reward: -1.041\n",
      "Average of last 10 rewards: -1.0635972093023256\n",
      "Finished episode 2171 after 133087 timesteps\n",
      "Episode reward: -1.137\n",
      "Average of last 10 rewards: -1.0632180555555557\n",
      "Finished episode 2181 after 133481 timesteps\n",
      "Episode reward: -1.04\n",
      "Average of last 10 rewards: -1.063093548387097\n",
      "Finished episode 2191 after 134069 timesteps\n",
      "Episode reward: -1.187\n",
      "Average of last 10 rewards: -1.0631302752293577\n",
      "Finished episode 2201 after 134860 timesteps\n",
      "Episode reward: -1.159\n",
      "Average of last 10 rewards: -1.0633657534246577\n",
      "INFO:tensorflow:Saving checkpoints for 135220 into ./ppo/model/model.ckpt.\n",
      "Finished episode 2211 after 135510 timesteps\n",
      "Episode reward: -1.062\n",
      "Average of last 10 rewards: -1.063049090909091\n",
      "Finished episode 2221 after 136148 timesteps\n",
      "Episode reward: -1.071\n",
      "Average of last 10 rewards: -1.0630117647058825\n",
      "Finished episode 2231 after 136948 timesteps\n",
      "Episode reward: -1.098\n",
      "Average of last 10 rewards: -1.063200900900901\n",
      "Finished episode 2241 after 137585 timesteps\n",
      "Episode reward: -1.3860000000000001\n",
      "Average of last 10 rewards: -1.0634636771300447\n",
      "Finished episode 2251 after 138308 timesteps\n",
      "Episode reward: -1.104\n",
      "Average of last 10 rewards: -1.0631973214285715\n",
      "Finished episode 2261 after 138701 timesteps\n",
      "Episode reward: -1.079\n",
      "Average of last 10 rewards: -1.0628964444444444\n",
      "Finished episode 2271 after 139165 timesteps\n",
      "Episode reward: -1.066\n",
      "Average of last 10 rewards: -1.0630123893805312\n",
      "Finished episode 2281 after 139763 timesteps\n",
      "Episode reward: -1.139\n",
      "Average of last 10 rewards: -1.0632784140969163\n",
      "Finished episode 2291 after 140357 timesteps\n",
      "Episode reward: -1.074\n",
      "Average of last 10 rewards: -1.06315\n",
      "Finished episode 2301 after 141088 timesteps\n",
      "Episode reward: -1.2460000000000002\n",
      "Average of last 10 rewards: -1.063110480349345\n",
      "Finished episode 2311 after 141615 timesteps\n",
      "Episode reward: -1.108\n",
      "Average of last 10 rewards: -1.0632495652173914\n",
      "Finished episode 2321 after 142530 timesteps\n",
      "Episode reward: -1.12\n",
      "Average of last 10 rewards: -1.0632926406926408\n",
      "INFO:tensorflow:Saving checkpoints for 142881 into ./ppo/model/model.ckpt.\n",
      "Finished episode 2331 after 143374 timesteps\n",
      "Episode reward: -1.159\n",
      "Average of last 10 rewards: -1.0635043103448276\n",
      "Finished episode 2341 after 143738 timesteps\n",
      "Episode reward: -1.018\n",
      "Average of last 10 rewards: -1.063466094420601\n",
      "Finished episode 2351 after 144369 timesteps\n",
      "Episode reward: -1.062\n",
      "Average of last 10 rewards: -1.0633880341880342\n",
      "Finished episode 2361 after 145116 timesteps\n",
      "Episode reward: -1.177\n",
      "Average of last 10 rewards: -1.063451489361702\n",
      "Finished episode 2371 after 145738 timesteps\n",
      "Episode reward: -1.098\n",
      "Average of last 10 rewards: -1.063439406779661\n",
      "Finished episode 2381 after 146463 timesteps\n",
      "Episode reward: -0.6910000000000001\n",
      "Average of last 10 rewards: -1.0634957805907173\n",
      "Finished episode 2391 after 146797 timesteps\n",
      "Episode reward: -1.052\n",
      "Average of last 10 rewards: -1.0632970588235293\n",
      "Finished episode 2401 after 147376 timesteps\n",
      "Episode reward: -1.057\n",
      "Average of last 10 rewards: -1.0635242677824268\n",
      "Finished episode 2411 after 147965 timesteps\n",
      "Episode reward: -1.066\n",
      "Average of last 10 rewards: -1.0633983333333332\n",
      "Finished episode 2421 after 148698 timesteps\n",
      "Episode reward: -1.079\n",
      "Average of last 10 rewards: -1.063794190871369\n",
      "Finished episode 2431 after 149312 timesteps\n",
      "Episode reward: -1.063\n",
      "Average of last 10 rewards: -1.0640272727272726\n",
      "Finished episode 2441 after 149717 timesteps\n",
      "Episode reward: -1.072\n",
      "Average of last 10 rewards: -1.0641255144032922\n",
      "Finished episode 2451 after 150474 timesteps\n",
      "Episode reward: -1.092\n",
      "Average of last 10 rewards: -1.0642426229508197\n",
      "INFO:tensorflow:Saving checkpoints for 150652 into ./ppo/model/model.ckpt.\n",
      "Finished episode 2461 after 151140 timesteps\n",
      "Episode reward: -1.237\n",
      "Average of last 10 rewards: -1.0643220408163265\n",
      "Finished episode 2471 after 151749 timesteps\n",
      "Episode reward: -1.098\n",
      "Average of last 10 rewards: -1.0642621951219513\n",
      "Finished episode 2481 after 152728 timesteps\n",
      "Episode reward: -1.1400000000000001\n",
      "Average of last 10 rewards: -1.0646194331983807\n",
      "Finished episode 2491 after 153446 timesteps\n",
      "Episode reward: -1.067\n",
      "Average of last 10 rewards: -1.064840322580645\n",
      "Finished episode 2501 after 154238 timesteps\n",
      "Episode reward: -1.053\n",
      "Average of last 10 rewards: -1.065237751004016\n",
      "Finished episode 2511 after 154967 timesteps\n",
      "Episode reward: -1.046\n",
      "Average of last 10 rewards: -1.0654408\n",
      "Finished episode 2521 after 155630 timesteps\n",
      "Episode reward: -1.063\n",
      "Average of last 10 rewards: -1.065396015936255\n",
      "Finished episode 2531 after 156479 timesteps\n",
      "Episode reward: -1.06\n",
      "Average of last 10 rewards: -1.065822619047619\n",
      "Finished episode 2541 after 157384 timesteps\n",
      "Episode reward: -1.1680000000000001\n",
      "Average of last 10 rewards: -1.065306324110672\n",
      "Finished episode 2551 after 157795 timesteps\n",
      "Episode reward: -1.065\n",
      "Average of last 10 rewards: -1.0653956692913387\n",
      "INFO:tensorflow:Saving checkpoints for 158617 into ./ppo/model/model.ckpt.\n",
      "Finished episode 2561 after 158618 timesteps\n",
      "Episode reward: -1.057\n",
      "Average of last 10 rewards: -1.0656588235294118\n",
      "Finished episode 2571 after 159074 timesteps\n",
      "Episode reward: -0.696\n",
      "Average of last 10 rewards: -1.0656246093750001\n",
      "Finished episode 2581 after 160011 timesteps\n",
      "Episode reward: -1.113\n",
      "Average of last 10 rewards: -1.065857587548638\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and run the environment for 5 episodes.\n",
    "wrapped_env = WrappedEnv(env, False)\n",
    "runner = Runner(agent=agent, environment=wrapped_env)\n",
    "runner.run(num_episodes=num_episodes, episode_finished=episode_finished, max_episode_timesteps=env._max_steps)\n",
    "print(\"Stats: \", runner.episode_rewards, runner.episode_timesteps, runner.episode_times)\n",
    "\n",
    "try:\n",
    "    runner.close()\n",
    "except AttributeError as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
