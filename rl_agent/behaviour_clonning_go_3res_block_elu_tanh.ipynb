{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/hpchome/anton95/.conda/envs/pommerman/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/gpfs/hpchome/anton95/.conda/envs/pommerman/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import error! You will not be able to render --> Cannot connect to \"None\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger, TensorBoard\n",
    "from keras.layers import Input, Dense, Flatten, Convolution2D, BatchNormalization, Activation, Add\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "from pommerman.constants import BOARD_SIZE\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2048\n",
    "epochs = 150\n",
    "early_stopping = 5\n",
    "action_space = 6\n",
    "\n",
    "log_path = './supervised_learning/logs/go_3res_block_elu_tanh/'\n",
    "model_path = './supervised_learning/model/go_3res_block_elu_tanh/model.h4'\n",
    "\n",
    "train_data_path    = './dataset/'\n",
    "train_data_labels  = os.path.join(train_data_path, 'labels.npy')\n",
    "train_data_reward  = os.path.join(train_data_path, 'reward.npy')\n",
    "train_data_obs_map = os.path.join(train_data_path, 'obs_map.npy')\n",
    "\n",
    "if not os.path.isdir(train_data_path):\n",
    "    os.makedirs(train_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, actions, save_path, log_path, save_best_only=True, seed=0):\n",
    "        K.clear_session()\n",
    "        self.log_path = log_path\n",
    "        self.save_path = save_path\n",
    "        self.actions = actions\n",
    "        self.save_best_only = save_best_only\n",
    "        \n",
    "        # Create model\n",
    "        self.model = self.create_model(actions)\n",
    "        # Load model if exists\n",
    "        if not os.path.isdir(os.path.dirname(save_path)):\n",
    "            os.makedirs(os.path.dirname(save_path))            \n",
    "        if os.path.isfile(self.save_path):\n",
    "            try:\n",
    "                print(\"Trying to load model\")\n",
    "                self.model.load_weights(self.save_path)\n",
    "                print(\"Model was loaded successful\")\n",
    "            except:\n",
    "                print(\"Model load failed\")\n",
    "        \n",
    "    def get_res_block(self, input):\n",
    "        # Res block 1        \n",
    "        x = Convolution2D(256, 3, padding='same')(input)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('elu')(x)\n",
    "        x = Convolution2D(256, 3, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Add()([input, x])\n",
    "        x = Activation('elu')(x)\n",
    "        return x\n",
    "        \n",
    "    def create_model(self, actions, input_shape=(11, 11, 18,)):\n",
    "        inp = Input(input_shape)\n",
    "        x = Convolution2D(256, 3, padding='same')(inp)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('elu')(x)\n",
    "        \n",
    "        # 3 residual blocks\n",
    "        for i in range(3):\n",
    "            x = self.get_res_block(x)\n",
    "        \n",
    "        # Output block\n",
    "        # Should be 2 filters\n",
    "        x = Convolution2D(2, 1, padding='same')(x)\n",
    "        x = BatchNormalization()(x)   \n",
    "        x = Activation('elu')(x)\n",
    "        x = Flatten()(x)  \n",
    "        \n",
    "        probs  = Dense(actions, activation='softmax', name='actions')(x)\n",
    "        reward = Dense(1, activation='tanh', name='reward')(x)\n",
    "        \n",
    "        model = Model(inputs = inp, outputs=[probs, reward])\n",
    "        model.compile(optimizer='adam', loss=['categorical_crossentropy', 'mae'], metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def train(self, obs, actions, rewards, batch_size=16384, epochs=100,\n",
    "              early_stopping = 10, class_weight=None, initial_epoch=0):\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='loss', patience=early_stopping)\n",
    "        checkpoint     = ModelCheckpoint(self.save_path, monitor='loss', save_best_only=self.save_best_only)\n",
    "        reduce_lr      = ReduceLROnPlateau(monitor='loss', patience=2, factor=0.1)\n",
    "        logger         = CSVLogger(self.log_path + 'log.csv', append=True)\n",
    "        tensorboard    = TensorBoard(self.log_path, batch_size=batch_size)\n",
    "        \n",
    "        history = self.model.fit(x=obs, y=[actions, rewards], batch_size=batch_size, epochs=epochs, verbose=1,\n",
    "                       callbacks=[early_stopping, checkpoint, reduce_lr, logger, tensorboard],\n",
    "                       validation_split=0.15, shuffle=True, class_weight=class_weight, initial_epoch=initial_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels       = np.load(train_data_labels)\n",
    "observations = np.load(train_data_obs_map)\n",
    "rewards      = np.load(train_data_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = to_categorical(labels, num_classes=action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((553065, 6), (553065, 11, 11, 18), (553065,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape, observations.shape, rewards.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.15941164, 0.20397964, 0.190945  , 0.20009764, 0.20339382,\n",
       "       0.04217226], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(labels, axis=0) / np.sum(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.04551126, 0.81707501, 0.87285166, 0.83292671, 0.81942839,\n",
       "       3.9520451 ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights = compute_class_weight('balanced', np.unique(np.argmax(labels, axis=1)), np.argmax(labels, axis=1))\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(action_space, model_path, log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 11, 11, 18)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 11, 11, 256)  41728       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 11, 11, 256)  1024        conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 11, 11, 256)  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 11, 11, 256)  590080      activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 11, 11, 256)  1024        conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 11, 11, 256)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 11, 11, 256)  590080      activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 11, 11, 256)  1024        conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 11, 11, 256)  0           activation_1[0][0]               \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 11, 11, 256)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 11, 11, 256)  590080      activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 11, 11, 256)  1024        conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 11, 11, 256)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 11, 11, 256)  590080      activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 11, 11, 256)  1024        conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 11, 11, 256)  0           activation_3[0][0]               \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 11, 11, 256)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 11, 11, 256)  590080      activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 11, 11, 256)  1024        conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 11, 11, 256)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 11, 11, 256)  590080      activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 11, 11, 256)  1024        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 11, 11, 256)  0           activation_5[0][0]               \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 11, 11, 256)  0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 11, 11, 2)    514         activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 11, 11, 2)    8           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 11, 11, 2)    0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 242)          0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "actions (Dense)                 (None, 6)            1458        flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reward (Dense)                  (None, 1)            243         flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,591,599\n",
      "Trainable params: 3,588,011\n",
      "Non-trainable params: 3,588\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "trainer.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 470105 samples, validate on 82960 samples\n",
      "Epoch 1/150\n",
      "470105/470105 [==============================] - 131s 279us/step - loss: 2.0955 - actions_loss: 1.6011 - reward_loss: 0.4944 - actions_acc: 0.2877 - reward_acc: 0.7519 - val_loss: 2.0442 - val_actions_loss: 1.5383 - val_reward_loss: 0.5059 - val_actions_acc: 0.3420 - val_reward_acc: 0.7480\n",
      "Epoch 2/150\n",
      "470105/470105 [==============================] - 127s 271us/step - loss: 1.8446 - actions_loss: 1.3532 - reward_loss: 0.4914 - actions_acc: 0.3999 - reward_acc: 0.7544 - val_loss: 1.9442 - val_actions_loss: 1.4394 - val_reward_loss: 0.5048 - val_actions_acc: 0.3779 - val_reward_acc: 0.7480\n",
      "Epoch 3/150\n",
      "470105/470105 [==============================] - 127s 271us/step - loss: 1.5806 - actions_loss: 1.0892 - reward_loss: 0.4914 - actions_acc: 0.5145 - reward_acc: 0.7544 - val_loss: 1.5341 - val_actions_loss: 1.0291 - val_reward_loss: 0.5050 - val_actions_acc: 0.5546 - val_reward_acc: 0.7480\n",
      "Epoch 4/150\n",
      "470105/470105 [==============================] - 127s 271us/step - loss: 1.4065 - actions_loss: 0.9152 - reward_loss: 0.4913 - actions_acc: 0.5864 - reward_acc: 0.7544 - val_loss: 1.4391 - val_actions_loss: 0.9349 - val_reward_loss: 0.5042 - val_actions_acc: 0.5729 - val_reward_acc: 0.7480\n",
      "Epoch 5/150\n",
      "470105/470105 [==============================] - 127s 271us/step - loss: 1.3253 - actions_loss: 0.8340 - reward_loss: 0.4913 - actions_acc: 0.6119 - reward_acc: 0.7544 - val_loss: 1.3499 - val_actions_loss: 0.8458 - val_reward_loss: 0.5042 - val_actions_acc: 0.6126 - val_reward_acc: 0.7480\n",
      "Epoch 6/150\n",
      "470105/470105 [==============================] - 127s 270us/step - loss: 1.2902 - actions_loss: 0.7989 - reward_loss: 0.4913 - actions_acc: 0.6233 - reward_acc: 0.7544 - val_loss: 1.3296 - val_actions_loss: 0.8256 - val_reward_loss: 0.5041 - val_actions_acc: 0.6240 - val_reward_acc: 0.7480\n",
      "Epoch 7/150\n",
      "470105/470105 [==============================] - 127s 271us/step - loss: 1.2670 - actions_loss: 0.7758 - reward_loss: 0.4912 - actions_acc: 0.6313 - reward_acc: 0.7544 - val_loss: 1.2829 - val_actions_loss: 0.7789 - val_reward_loss: 0.5041 - val_actions_acc: 0.6291 - val_reward_acc: 0.7480\n",
      "Epoch 8/150\n",
      "470105/470105 [==============================] - 127s 271us/step - loss: 1.2521 - actions_loss: 0.7609 - reward_loss: 0.4912 - actions_acc: 0.6366 - reward_acc: 0.7544 - val_loss: 1.2956 - val_actions_loss: 0.7915 - val_reward_loss: 0.5041 - val_actions_acc: 0.6272 - val_reward_acc: 0.7480\n",
      "Epoch 9/150\n",
      "470105/470105 [==============================] - 127s 271us/step - loss: 1.2411 - actions_loss: 0.7498 - reward_loss: 0.4912 - actions_acc: 0.6405 - reward_acc: 0.7544 - val_loss: 1.2922 - val_actions_loss: 0.7881 - val_reward_loss: 0.5041 - val_actions_acc: 0.6243 - val_reward_acc: 0.7480\n",
      "Epoch 10/150\n",
      "470105/470105 [==============================] - 127s 271us/step - loss: 1.2305 - actions_loss: 0.7393 - reward_loss: 0.4912 - actions_acc: 0.6453 - reward_acc: 0.7544 - val_loss: 1.2887 - val_actions_loss: 0.7847 - val_reward_loss: 0.5040 - val_actions_acc: 0.6309 - val_reward_acc: 0.7480\n",
      "Epoch 11/150\n",
      "470105/470105 [==============================] - 127s 271us/step - loss: 1.2225 - actions_loss: 0.7312 - reward_loss: 0.4912 - actions_acc: 0.6498 - reward_acc: 0.7544 - val_loss: 1.2808 - val_actions_loss: 0.7767 - val_reward_loss: 0.5041 - val_actions_acc: 0.6274 - val_reward_acc: 0.7480\n",
      "Epoch 12/150\n",
      "470105/470105 [==============================] - 127s 271us/step - loss: 1.2130 - actions_loss: 0.7218 - reward_loss: 0.4912 - actions_acc: 0.6533 - reward_acc: 0.7544 - val_loss: 1.2683 - val_actions_loss: 0.7642 - val_reward_loss: 0.5041 - val_actions_acc: 0.6314 - val_reward_acc: 0.7480\n",
      "Epoch 13/150\n",
      "470105/470105 [==============================] - 127s 270us/step - loss: 1.2048 - actions_loss: 0.7136 - reward_loss: 0.4912 - actions_acc: 0.6583 - reward_acc: 0.7544 - val_loss: 1.2621 - val_actions_loss: 0.7581 - val_reward_loss: 0.5041 - val_actions_acc: 0.6404 - val_reward_acc: 0.7480\n",
      "Epoch 14/150\n",
      "470105/470105 [==============================] - 127s 271us/step - loss: 1.1977 - actions_loss: 0.7065 - reward_loss: 0.4912 - actions_acc: 0.6609 - reward_acc: 0.7544 - val_loss: 1.2607 - val_actions_loss: 0.7566 - val_reward_loss: 0.5040 - val_actions_acc: 0.6385 - val_reward_acc: 0.7480\n",
      "Epoch 15/150\n",
      "470105/470105 [==============================] - 127s 271us/step - loss: 1.1914 - actions_loss: 0.7002 - reward_loss: 0.4912 - actions_acc: 0.6637 - reward_acc: 0.7544 - val_loss: 1.2592 - val_actions_loss: 0.7551 - val_reward_loss: 0.5041 - val_actions_acc: 0.6367 - val_reward_acc: 0.7480\n",
      "Epoch 16/150\n",
      "470105/470105 [==============================] - 127s 271us/step - loss: 1.1851 - actions_loss: 0.6939 - reward_loss: 0.4912 - actions_acc: 0.6676 - reward_acc: 0.7544 - val_loss: 1.2475 - val_actions_loss: 0.7433 - val_reward_loss: 0.5041 - val_actions_acc: 0.6429 - val_reward_acc: 0.7480\n",
      "Epoch 17/150\n",
      "470105/470105 [==============================] - 127s 271us/step - loss: 1.1766 - actions_loss: 0.6854 - reward_loss: 0.4912 - actions_acc: 0.6727 - reward_acc: 0.7544 - val_loss: 1.2609 - val_actions_loss: 0.7568 - val_reward_loss: 0.5040 - val_actions_acc: 0.6381 - val_reward_acc: 0.7480\n",
      "Epoch 18/150\n",
      "470105/470105 [==============================] - 127s 271us/step - loss: 1.1718 - actions_loss: 0.6806 - reward_loss: 0.4912 - actions_acc: 0.6760 - reward_acc: 0.7544 - val_loss: 1.2901 - val_actions_loss: 0.7860 - val_reward_loss: 0.5041 - val_actions_acc: 0.6297 - val_reward_acc: 0.7480\n",
      "Epoch 19/150\n",
      "470105/470105 [==============================] - 127s 271us/step - loss: 1.1647 - actions_loss: 0.6735 - reward_loss: 0.4912 - actions_acc: 0.6805 - reward_acc: 0.7544 - val_loss: 1.2821 - val_actions_loss: 0.7780 - val_reward_loss: 0.5041 - val_actions_acc: 0.6318 - val_reward_acc: 0.7480\n",
      "Epoch 20/150\n",
      "470105/470105 [==============================] - 127s 271us/step - loss: 1.1578 - actions_loss: 0.6666 - reward_loss: 0.4912 - actions_acc: 0.6849 - reward_acc: 0.7544 - val_loss: 1.2688 - val_actions_loss: 0.7647 - val_reward_loss: 0.5041 - val_actions_acc: 0.6348 - val_reward_acc: 0.7480\n",
      "Epoch 21/150\n",
      "470105/470105 [==============================] - 127s 271us/step - loss: 1.1490 - actions_loss: 0.6577 - reward_loss: 0.4912 - actions_acc: 0.6904 - reward_acc: 0.7544 - val_loss: 1.2995 - val_actions_loss: 0.7954 - val_reward_loss: 0.5041 - val_actions_acc: 0.6328 - val_reward_acc: 0.7480\n",
      "Epoch 22/150\n",
      "470105/470105 [==============================] - 127s 271us/step - loss: 1.1420 - actions_loss: 0.6508 - reward_loss: 0.4912 - actions_acc: 0.6952 - reward_acc: 0.7544 - val_loss: 1.2938 - val_actions_loss: 0.7898 - val_reward_loss: 0.5040 - val_actions_acc: 0.6363 - val_reward_acc: 0.7480\n",
      "Epoch 23/150\n",
      "470105/470105 [==============================] - 127s 271us/step - loss: 1.1317 - actions_loss: 0.6405 - reward_loss: 0.4912 - actions_acc: 0.7002 - reward_acc: 0.7544 - val_loss: 1.3022 - val_actions_loss: 0.7982 - val_reward_loss: 0.5040 - val_actions_acc: 0.6363 - val_reward_acc: 0.7480\n",
      "Epoch 24/150\n",
      "470105/470105 [==============================] - 127s 271us/step - loss: 1.1230 - actions_loss: 0.6318 - reward_loss: 0.4912 - actions_acc: 0.7063 - reward_acc: 0.7544 - val_loss: 1.3083 - val_actions_loss: 0.8042 - val_reward_loss: 0.5041 - val_actions_acc: 0.6319 - val_reward_acc: 0.7480\n",
      "Epoch 25/150\n",
      "470105/470105 [==============================] - 127s 271us/step - loss: 1.1120 - actions_loss: 0.6208 - reward_loss: 0.4912 - actions_acc: 0.7124 - reward_acc: 0.7544 - val_loss: 1.2925 - val_actions_loss: 0.7884 - val_reward_loss: 0.5041 - val_actions_acc: 0.6368 - val_reward_acc: 0.7480\n",
      "Epoch 26/150\n",
      "470105/470105 [==============================] - 127s 271us/step - loss: 1.1008 - actions_loss: 0.6096 - reward_loss: 0.4912 - actions_acc: 0.7188 - reward_acc: 0.7544 - val_loss: 1.3474 - val_actions_loss: 0.8433 - val_reward_loss: 0.5041 - val_actions_acc: 0.6298 - val_reward_acc: 0.7480\n",
      "Epoch 27/150\n",
      "470105/470105 [==============================] - 127s 271us/step - loss: 1.0901 - actions_loss: 0.5989 - reward_loss: 0.4912 - actions_acc: 0.7249 - reward_acc: 0.7544 - val_loss: 1.3691 - val_actions_loss: 0.8651 - val_reward_loss: 0.5041 - val_actions_acc: 0.6300 - val_reward_acc: 0.7480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/150\n",
      "470105/470105 [==============================] - 127s 271us/step - loss: 1.0768 - actions_loss: 0.5856 - reward_loss: 0.4912 - actions_acc: 0.7333 - reward_acc: 0.7544 - val_loss: 1.3446 - val_actions_loss: 0.8405 - val_reward_loss: 0.5041 - val_actions_acc: 0.6279 - val_reward_acc: 0.7480\n",
      "Epoch 29/150\n",
      "470105/470105 [==============================] - 127s 271us/step - loss: 1.0634 - actions_loss: 0.5722 - reward_loss: 0.4912 - actions_acc: 0.7408 - reward_acc: 0.7544 - val_loss: 1.3883 - val_actions_loss: 0.8842 - val_reward_loss: 0.5041 - val_actions_acc: 0.6316 - val_reward_acc: 0.7479\n",
      "Epoch 30/150\n",
      "470105/470105 [==============================] - 127s 271us/step - loss: 1.0521 - actions_loss: 0.5609 - reward_loss: 0.4911 - actions_acc: 0.7463 - reward_acc: 0.7544 - val_loss: 1.4147 - val_actions_loss: 0.9107 - val_reward_loss: 0.5041 - val_actions_acc: 0.6233 - val_reward_acc: 0.7480\n",
      "Epoch 31/150\n",
      "470105/470105 [==============================] - 127s 271us/step - loss: 1.0361 - actions_loss: 0.5449 - reward_loss: 0.4911 - actions_acc: 0.7557 - reward_acc: 0.7544 - val_loss: 1.3952 - val_actions_loss: 0.8911 - val_reward_loss: 0.5041 - val_actions_acc: 0.6327 - val_reward_acc: 0.7480\n",
      "Epoch 32/150\n",
      "307200/470105 [==================>...........] - ETA: 41s - loss: 1.0157 - actions_loss: 0.5244 - reward_loss: 0.4912 - actions_acc: 0.7664 - reward_acc: 0.7544"
     ]
    }
   ],
   "source": [
    "trainer.train(observations, labels, rewards, batch_size=batch_size, \n",
    "              epochs=epochs, early_stopping=early_stopping, class_weight=[class_weights, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    }
   ],
   "source": [
    "print(\"finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
