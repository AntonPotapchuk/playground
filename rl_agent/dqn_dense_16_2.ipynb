{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/hpchome/anton95/.conda/envs/pommerman/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/gpfs/hpchome/anton95/.conda/envs/pommerman/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Flatten, Convolution2D, Input\n",
    "from keras.optimizers import Adam\n",
    "from pommerman.configs import ffa_v0_env\n",
    "from pommerman.envs.v0 import Pomme\n",
    "from pommerman.agents import SimpleAgent, BaseAgent\n",
    "from pommerman.constants import BOARD_SIZE\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Env, Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint, Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_training_steps = 5000000\n",
    "log_interval = 1000\n",
    "file_log_path = './dqn/rl_logs/dense_16_2/log.txt'\n",
    "tensorboard_path = './dqn/logs/dense_16_2/'\n",
    "model_path = './dqn/model/dense_16_2/model{step}.h4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(os.path.dirname(file_log_path)):\n",
    "    os.makedirs(os.path.dirname(file_log_path))\n",
    "if not os.path.isdir(os.path.dirname(model_path)):\n",
    "    os.makedirs(os.path.dirname(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorforceAgent(BaseAgent):\n",
    "    def act(self, obs, action_space):\n",
    "        pass\n",
    "\n",
    "\n",
    "class TensorboardLogger(Callback):\n",
    "    \"\"\"Logging in tensorboard without tensorflow ops.\"\"\"\n",
    "    def __init__(self, log_dir):\n",
    "        # Some algorithms compute multiple episodes at once since they are multi-threaded.\n",
    "        # We therefore use a dictionary that is indexed by the episode to separate episodes\n",
    "        # from each other.\n",
    "        self.observations = {}\n",
    "        self.rewards = {}\n",
    "        self.actions = {}\n",
    "        self.metrics = {}\n",
    "        self.step = 0\n",
    "        \"\"\"Creates a summary writer logging to log_dir.\"\"\"\n",
    "        self.writer = tf.summary.FileWriter(log_dir)\n",
    "\n",
    "    def log_scalar(self, tag, value, step):\n",
    "        \"\"\"Log a scalar variable.\n",
    "        Parameter\n",
    "        ----------\n",
    "        tag : basestring\n",
    "            Name of the scalar\n",
    "        value\n",
    "        step : int\n",
    "            training iteration\n",
    "        \"\"\"\n",
    "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])\n",
    "        self.writer.add_summary(summary, step)\n",
    "\n",
    "    def on_train_begin(self, logs):\n",
    "        self.metrics_names = self.model.metrics_names\n",
    "\n",
    "    def on_episode_begin(self, episode, logs):\n",
    "        self.observations[episode] = []\n",
    "        self.rewards[episode] = []\n",
    "        self.actions[episode] = []\n",
    "        self.metrics[episode] = []\n",
    "\n",
    "    def on_episode_end(self, episode, logs):\n",
    "        episode_steps = len(self.observations[episode])\n",
    "        variables = {\n",
    "            'step': self.step,\n",
    "            'nb_steps': self.params['nb_steps'],\n",
    "            'episode_steps': episode_steps,\n",
    "            'episode_reward': np.sum(self.rewards[episode]),\n",
    "            'reward_mean': np.mean(self.rewards[episode]),\n",
    "            'reward_min': np.min(self.rewards[episode]),\n",
    "            'reward_max': np.max(self.rewards[episode]),\n",
    "            'action_mean': np.mean(self.actions[episode]),\n",
    "            'action_min': np.min(self.actions[episode]),\n",
    "            'action_max': np.max(self.actions[episode]),\n",
    "            'obs_mean': np.mean(self.observations[episode]),\n",
    "            'obs_min': np.min(self.observations[episode]),\n",
    "            'obs_max': np.max(self.observations[episode]),\n",
    "        }\n",
    "\n",
    "        # Format all metrics.\n",
    "        metrics = np.array(self.metrics[episode])\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings('error')\n",
    "            for idx, name in enumerate(self.metrics_names):\n",
    "                try:\n",
    "                    value = np.nanmean(metrics[:, idx])\n",
    "                except Warning:\n",
    "                    value = -1\n",
    "                variables[name] = value\n",
    "        for key, value in variables.items():\n",
    "            self.log_scalar(key, value, episode + 1)\n",
    "\n",
    "        # Free up resources.\n",
    "        del self.observations[episode]\n",
    "        del self.rewards[episode]\n",
    "        del self.actions[episode]\n",
    "        del self.metrics[episode]\n",
    "\n",
    "    def on_step_end(self, step, logs):\n",
    "        episode = logs['episode']\n",
    "        self.observations[episode].append(logs['observation'])\n",
    "        self.rewards[episode].append(logs['reward'])\n",
    "        self.actions[episode].append(logs['action'])\n",
    "        self.metrics[episode].append(logs['metrics'])\n",
    "        self.step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 2369)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                37920     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 102       \n",
      "=================================================================\n",
      "Total params: 38,294\n",
      "Trainable params: 38,294\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the environment\n",
    "config = ffa_v0_env()\n",
    "env = Pomme(**config[\"env_kwargs\"])\n",
    "np.random.seed(0)\n",
    "env.seed(0)\n",
    "# Add 3 random agents\n",
    "agents = []\n",
    "for agent_id in range(3):\n",
    "    agents.append(SimpleAgent(config[\"agent\"](agent_id, config[\"game_type\"])))\n",
    "\n",
    "# Add TensorforceAgent\n",
    "agent_id += 1\n",
    "agents.append(TensorforceAgent(config[\"agent\"](agent_id, config[\"game_type\"])))\n",
    "env.set_agents(agents)\n",
    "env.set_training_agent(agents[-1].agent_id)\n",
    "env.set_init_game_state(None)\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "\n",
    "def create_model(actions, input_shape=(2369,)):\n",
    "    inp = Input(input_shape)        \n",
    "    x = Dense(16, activation='relu')(inp)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    out = Dense(actions)(x)\n",
    "    model = Model(inputs = inp, outputs=out)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Next, we build a very simple model regardless of the dueling architecture\n",
    "# if you enable dueling network in DQN , DQN will build a dueling network base on your model automatically\n",
    "# Also, you can build a dueling network by yourself and turn off the dueling network in DQN.\n",
    "model = create_model(nb_actions)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvWrapper(Env):\n",
    "    \"\"\"The abstract environment class that is used by all agents. This class has the exact\n",
    "        same API that OpenAI Gym uses so that integrating with it is trivial. In contrast to the\n",
    "        OpenAI Gym implementation, this class only defines the abstract methods without any actual\n",
    "        implementation.\n",
    "        To implement your own environment, you need to define the following methods:\n",
    "        - `step`\n",
    "        - `reset`\n",
    "        - `render`\n",
    "        - `close`\n",
    "        Refer to the [Gym documentation](https://gym.openai.com/docs/#environments).\n",
    "        \"\"\"\n",
    "    reward_range = (-1, 1)\n",
    "    action_space = None\n",
    "    observation_space = None\n",
    "\n",
    "    def __init__(self, gym, board_size):\n",
    "        self.gym = gym\n",
    "        self.action_space = gym.action_space\n",
    "        self.observation_space = gym.observation_space\n",
    "        self.reward_range = gym.reward_range\n",
    "        self.board_size = board_size\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Run one timestep of the environment's dynamics.\n",
    "        Accepts an action and returns a tuple (observation, reward, done, info).\n",
    "        # Arguments\n",
    "            action (object): An action provided by the environment.\n",
    "        # Returns\n",
    "            observation (object): Agent's observation of the current environment.\n",
    "            reward (float) : Amount of reward returned after previous action.\n",
    "            done (boolean): Whether the episode has ended, in which case further step() calls will return undefined results.\n",
    "            info (dict): Contains auxiliary diagnostic information (helpful for debugging, and sometimes learning).\n",
    "        \"\"\"\n",
    "        obs = self.gym.get_observations()\n",
    "        all_actions = self.gym.act(obs)\n",
    "        all_actions.insert(self.gym.training_agent, action)\n",
    "        state, reward, terminal, info = self.gym.step(all_actions)\n",
    "        agent_state = self.featurize(state[self.gym.training_agent])\n",
    "        agent_reward = reward[self.gym.training_agent]\n",
    "        return agent_state, agent_reward, terminal, info\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the state of the environment and returns an initial observation.\n",
    "        # Returns\n",
    "            observation (object): The initial observation of the space. Initial reward is assumed to be 0.\n",
    "        \"\"\"\n",
    "        obs = self.gym.reset()\n",
    "        agent_obs = self.featurize(obs[self.gym.training_agent])\n",
    "        return agent_obs\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        \"\"\"Renders the environment.\n",
    "        The set of supported modes varies per environment. (And some\n",
    "        environments do not support rendering at all.)\n",
    "        # Arguments\n",
    "            mode (str): The mode to render with.\n",
    "            close (bool): Close all open renderings.\n",
    "        \"\"\"\n",
    "        self.gym.render(mode=mode, close=close)\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Override in your subclass to perform any necessary cleanup.\n",
    "        Environments will automatically close() themselves when\n",
    "        garbage collected or when the program exits.\n",
    "        \"\"\"\n",
    "        self.gym.close()\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        \"\"\"Sets the seed for this env's random number generator(s).\n",
    "        # Returns\n",
    "            Returns the list of seeds used in this env's random number generators\n",
    "        \"\"\"\n",
    "        raise self.gym.seed(seed)\n",
    "\n",
    "    def configure(self, *args, **kwargs):\n",
    "        \"\"\"Provides runtime configuration to the environment.\n",
    "        This configuration should consist of data that tells your\n",
    "        environment how to run (such as an address of a remote server,\n",
    "        or path to your ImageNet data). It should not affect the\n",
    "        semantics of the environment.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def featurize(self, obs):\n",
    "        shape = (self.board_size, self.board_size, 1)\n",
    "\n",
    "        def get_matrix(dict, key):\n",
    "            res = dict[key]\n",
    "            return res.reshape(shape).astype(np.float32)\n",
    "\n",
    "        def get_map(board, item):\n",
    "            map = np.zeros(shape)\n",
    "            map[board == item] = 1\n",
    "            return map\n",
    "\n",
    "        board = get_matrix(obs, 'board')\n",
    "\n",
    "        # TODO: probably not needed Passage = 0\n",
    "        rigid_map = get_map(board, 1)               # Rigid = 1\n",
    "        wood_map = get_map(board, 2)                # Wood = 2\n",
    "        bomb_map = get_map(board, 3)                # Bomb = 3\n",
    "        flames_map = get_map(board, 4)              # Flames = 4\n",
    "        fog_map = get_map(board, 5)                 # TODO: not used for first two stages Fog = 5\n",
    "        extra_bomb_map = get_map(board, 6)          # ExtraBomb = 6\n",
    "        incr_range_map = get_map(board, 7)          # IncrRange = 7\n",
    "        kick_map = get_map(board, 8)                # Kick = 8\n",
    "        skull_map = get_map(board, 9)               # Skull = 9\n",
    "\n",
    "        position = obs[\"position\"]\n",
    "        my_position = np.zeros(shape)\n",
    "        my_position[position[0], position[1], 0] = 1\n",
    "\n",
    "        team_mates = get_map(board, obs[\"teammate\"].value) # TODO during documentation it should be an array\n",
    "\n",
    "        enemies = np.zeros(shape)\n",
    "        for enemy in obs[\"enemies\"]:\n",
    "            enemies[board == enemy.value] = 1\n",
    "\n",
    "        bomb_blast_strength = get_matrix(obs, 'bomb_blast_strength')\n",
    "        bomb_life = get_matrix(obs, 'bomb_life')\n",
    "\n",
    "        ammo = obs[\"ammo\"]\n",
    "        blast_strength = obs[\"blast_strength\"]\n",
    "        can_kick = int(obs[\"can_kick\"])\n",
    "\n",
    "        obs = np.concatenate([my_position, enemies, team_mates, rigid_map,\n",
    "                              wood_map, bomb_map, flames_map,\n",
    "                              fog_map, extra_bomb_map, incr_range_map,\n",
    "                              kick_map, skull_map, bomb_blast_strength,\n",
    "                              bomb_life], axis=2).flatten()\n",
    "        obs = np.append(obs, [ammo, blast_strength, can_kick])\n",
    "        return obs\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "    def __str__(self):\n",
    "        return '<{} instance>'.format(type(self).__name__)\n",
    "\n",
    "\n",
    "class CustomProcessor(Processor):\n",
    "    def process_state_batch(self, batch):\n",
    "        \"\"\"Processes an entire batch of states and returns it.\n",
    "        # Arguments\n",
    "            batch (list): List of states\n",
    "        # Returns\n",
    "            Processed list of states\n",
    "        \"\"\"\n",
    "        batch = np.squeeze(batch, axis=1)\n",
    "        return batch\n",
    "\n",
    "    def process_info(self, info):\n",
    "        \"\"\"Processes the info as obtained from the environment for use in an agent and\n",
    "        returns it.\n",
    "        \"\"\"\n",
    "        info['result'] = info['result'].value\n",
    "        return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_wrapper = EnvWrapper(env, BOARD_SIZE)\n",
    "processor = CustomProcessor()\n",
    "\n",
    "# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
    "# even the metrics!\n",
    "memory = SequentialMemory(limit=500000, window_length=1)\n",
    "policy = BoltzmannQPolicy()\n",
    "file_logger = FileLogger(file_log_path, interval=log_interval)\n",
    "checkpoint = ModelIntervalCheckpoint(model_path, interval=log_interval)\n",
    "tensorboard = TensorboardLogger(tensorboard_path)\n",
    "callbacks=[file_logger, checkpoint, tensorboard]\n",
    "# enable the dueling network\n",
    "# you can specify the dueling_type to one of {'avg','max','naive'}\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=512,\n",
    "               enable_dueling_network=True, dueling_type='avg', target_model_update=1e-2, policy=policy,\n",
    "               processor=processor, batch_size=512)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "if os.path.isfile(model_path):\n",
    "    dqn.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000000 steps ...\n",
      "      30/5000000: episode: 1, duration: 1.437s, episode steps: 30, steps per second: 21, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.633 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: --, mean_absolute_error: --, mean_q: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/hpchome/anton95/.conda/envs/pommerman/lib/python3.6/site-packages/rl/memory.py:29: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      90/5000000: episode: 2, duration: 3.764s, episode steps: 60, steps per second: 16, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.467 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 0.052536, mean_absolute_error: 0.245707, mean_q: 0.190787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/hpchome/anton95/.conda/envs/pommerman/lib/python3.6/site-packages/rl/memory.py:29: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     121/5000000: episode: 3, duration: 1.363s, episode steps: 31, steps per second: 23, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 2.516 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 0.044316, mean_absolute_error: 0.250631, mean_q: 0.099055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/hpchome/anton95/.conda/envs/pommerman/lib/python3.6/site-packages/rl/memory.py:29: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     207/5000000: episode: 4, duration: 3.743s, episode steps: 86, steps per second: 23, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.430 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 0.022808, mean_absolute_error: 0.273776, mean_q: 0.069738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/hpchome/anton95/.conda/envs/pommerman/lib/python3.6/site-packages/rl/memory.py:29: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     312/5000000: episode: 5, duration: 4.450s, episode steps: 105, steps per second: 24, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.095 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 0.006251, mean_absolute_error: 0.252224, mean_q: 0.150801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/hpchome/anton95/.conda/envs/pommerman/lib/python3.6/site-packages/rl/memory.py:29: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     338/5000000: episode: 6, duration: 1.292s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.500 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 0.006272, mean_absolute_error: 0.280301, mean_q: 0.253209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/hpchome/anton95/.conda/envs/pommerman/lib/python3.6/site-packages/rl/memory.py:29: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     477/5000000: episode: 7, duration: 6.596s, episode steps: 139, steps per second: 21, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 2.827 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 0.006375, mean_absolute_error: 0.284122, mean_q: 0.279719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/hpchome/anton95/.conda/envs/pommerman/lib/python3.6/site-packages/rl/memory.py:29: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     594/5000000: episode: 8, duration: 5.506s, episode steps: 117, steps per second: 21, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.410 [0.000, 5.000], mean observation: 0.058 [0.000, 24.000], loss: 0.004469, mean_absolute_error: 0.273400, mean_q: 0.297819\n",
      "     620/5000000: episode: 9, duration: 1.288s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.269 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 0.004038, mean_absolute_error: 0.282602, mean_q: 0.322059\n",
      "     703/5000000: episode: 10, duration: 4.042s, episode steps: 83, steps per second: 21, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.843 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 0.003680, mean_absolute_error: 0.290938, mean_q: 0.348442\n",
      "     729/5000000: episode: 11, duration: 1.330s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.846 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 0.004876, mean_absolute_error: 0.299003, mean_q: 0.366333\n",
      "     795/5000000: episode: 12, duration: 3.070s, episode steps: 66, steps per second: 21, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 2.500 [0.000, 5.000], mean observation: 0.074 [0.000, 24.000], loss: 0.004277, mean_absolute_error: 0.309311, mean_q: 0.378649\n",
      "     848/5000000: episode: 13, duration: 2.447s, episode steps: 53, steps per second: 22, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.887 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 0.004390, mean_absolute_error: 0.329107, mean_q: 0.389743\n",
      "     876/5000000: episode: 14, duration: 1.317s, episode steps: 28, steps per second: 21, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.857 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 0.004308, mean_absolute_error: 0.342246, mean_q: 0.404498\n",
      "     913/5000000: episode: 15, duration: 1.780s, episode steps: 37, steps per second: 21, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 2.216 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 0.004014, mean_absolute_error: 0.357679, mean_q: 0.422618\n",
      "     973/5000000: episode: 16, duration: 2.853s, episode steps: 60, steps per second: 21, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.433 [0.000, 5.000], mean observation: 0.062 [0.000, 24.000], loss: 0.003965, mean_absolute_error: 0.376343, mean_q: 0.445317\n",
      "    1111/5000000: episode: 17, duration: 6.449s, episode steps: 138, steps per second: 21, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 2.406 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 0.004188, mean_absolute_error: 0.414493, mean_q: 0.492863\n",
      "    1252/5000000: episode: 18, duration: 6.696s, episode steps: 141, steps per second: 21, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 2.489 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 0.004259, mean_absolute_error: 0.461440, mean_q: 0.552957\n",
      "    1282/5000000: episode: 19, duration: 1.415s, episode steps: 30, steps per second: 21, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.200 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 0.003572, mean_absolute_error: 0.490525, mean_q: 0.587380\n",
      "    1324/5000000: episode: 20, duration: 1.934s, episode steps: 42, steps per second: 22, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 2.143 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 0.003786, mean_absolute_error: 0.500051, mean_q: 0.597201\n",
      "    1359/5000000: episode: 21, duration: 1.703s, episode steps: 35, steps per second: 21, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.314 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 0.003756, mean_absolute_error: 0.512489, mean_q: 0.608183\n",
      "    1463/5000000: episode: 22, duration: 4.789s, episode steps: 104, steps per second: 22, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.538 [0.000, 5.000], mean observation: 0.062 [0.000, 24.000], loss: 0.003589, mean_absolute_error: 0.533991, mean_q: 0.633306\n",
      "    1489/5000000: episode: 23, duration: 1.255s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.654 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 0.003883, mean_absolute_error: 0.553957, mean_q: 0.640568\n",
      "    1570/5000000: episode: 24, duration: 3.837s, episode steps: 81, steps per second: 21, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.617 [0.000, 5.000], mean observation: 0.074 [0.000, 24.000], loss: 0.004685, mean_absolute_error: 0.573057, mean_q: 0.668256\n",
      "    1596/5000000: episode: 25, duration: 1.295s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.269 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 0.004788, mean_absolute_error: 0.590099, mean_q: 0.689018\n",
      "    1690/5000000: episode: 26, duration: 4.181s, episode steps: 94, steps per second: 22, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.596 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 0.004980, mean_absolute_error: 0.609966, mean_q: 0.693080\n",
      "    1721/5000000: episode: 27, duration: 1.445s, episode steps: 31, steps per second: 21, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 2.548 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 0.004438, mean_absolute_error: 0.632225, mean_q: 0.695588\n",
      "    1747/5000000: episode: 28, duration: 1.235s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.462 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 0.004574, mean_absolute_error: 0.633568, mean_q: 0.698046\n",
      "    1773/5000000: episode: 29, duration: 1.214s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.731 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 0.004821, mean_absolute_error: 0.638135, mean_q: 0.710012\n",
      "    1798/5000000: episode: 30, duration: 1.115s, episode steps: 25, steps per second: 22, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.640 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 0.005244, mean_absolute_error: 0.641033, mean_q: 0.710278\n",
      "    1865/5000000: episode: 31, duration: 3.349s, episode steps: 67, steps per second: 20, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 2.134 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 0.005818, mean_absolute_error: 0.659202, mean_q: 0.723474\n",
      "    1892/5000000: episode: 32, duration: 1.331s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.481 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 0.006677, mean_absolute_error: 0.663437, mean_q: 0.734689\n",
      "    1933/5000000: episode: 33, duration: 2.063s, episode steps: 41, steps per second: 20, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 2.415 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 0.005767, mean_absolute_error: 0.665531, mean_q: 0.737393\n",
      "    1959/5000000: episode: 34, duration: 1.235s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.038 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 0.006113, mean_absolute_error: 0.672327, mean_q: 0.748167\n",
      "    1991/5000000: episode: 35, duration: 1.631s, episode steps: 32, steps per second: 20, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 2.625 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 0.006335, mean_absolute_error: 0.685134, mean_q: 0.760458\n",
      "    2020/5000000: episode: 36, duration: 3.118s, episode steps: 29, steps per second: 9, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.897 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 0.005444, mean_absolute_error: 0.689770, mean_q: 0.767651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    2046/5000000: episode: 37, duration: 1.283s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.154 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 0.006809, mean_absolute_error: 0.703314, mean_q: 0.783511\n",
      "    2072/5000000: episode: 38, duration: 1.251s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.385 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 0.006496, mean_absolute_error: 0.705944, mean_q: 0.780088\n",
      "    2152/5000000: episode: 39, duration: 3.702s, episode steps: 80, steps per second: 22, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 2.663 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 0.006736, mean_absolute_error: 0.715262, mean_q: 0.799860\n",
      "    2443/5000000: episode: 40, duration: 14.236s, episode steps: 291, steps per second: 20, episode reward: -1.000, mean reward: -0.003 [-1.000, 0.000], mean action: 2.488 [0.000, 5.000], mean observation: 0.059 [0.000, 24.000], loss: 0.007794, mean_absolute_error: 0.756629, mean_q: 0.836049\n",
      "    2494/5000000: episode: 41, duration: 2.462s, episode steps: 51, steps per second: 21, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 2.882 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 0.008575, mean_absolute_error: 0.791193, mean_q: 0.854903\n",
      "    2520/5000000: episode: 42, duration: 1.307s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.923 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 0.008567, mean_absolute_error: 0.792764, mean_q: 0.848539\n",
      "    2766/5000000: episode: 43, duration: 10.726s, episode steps: 246, steps per second: 23, episode reward: -1.000, mean reward: -0.004 [-1.000, 0.000], mean action: 2.488 [0.000, 5.000], mean observation: 0.060 [0.000, 24.000], loss: 0.008756, mean_absolute_error: 0.824661, mean_q: 0.888166\n",
      "    2817/5000000: episode: 44, duration: 2.306s, episode steps: 51, steps per second: 22, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 2.216 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 0.009126, mean_absolute_error: 0.844636, mean_q: 0.918094\n",
      "    2849/5000000: episode: 45, duration: 1.548s, episode steps: 32, steps per second: 21, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 2.156 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 0.009902, mean_absolute_error: 0.854805, mean_q: 0.934725\n",
      "    2903/5000000: episode: 46, duration: 2.479s, episode steps: 54, steps per second: 22, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.111 [0.000, 5.000], mean observation: 0.062 [0.000, 24.000], loss: 0.009623, mean_absolute_error: 0.863336, mean_q: 0.949289\n",
      "    2929/5000000: episode: 47, duration: 1.292s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.385 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 0.010254, mean_absolute_error: 0.871816, mean_q: 0.965004\n",
      "    3015/5000000: episode: 48, duration: 4.209s, episode steps: 86, steps per second: 20, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.384 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 0.011160, mean_absolute_error: 0.889104, mean_q: 0.991424\n",
      "    3042/5000000: episode: 49, duration: 1.272s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.741 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 0.011496, mean_absolute_error: 0.905487, mean_q: 1.013394\n",
      "    3067/5000000: episode: 50, duration: 1.037s, episode steps: 25, steps per second: 24, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.240 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 0.014570, mean_absolute_error: 0.910281, mean_q: 1.013651\n",
      "    3119/5000000: episode: 51, duration: 2.456s, episode steps: 52, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.635 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 0.013325, mean_absolute_error: 0.929280, mean_q: 1.045887\n",
      "    3215/5000000: episode: 52, duration: 4.613s, episode steps: 96, steps per second: 21, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.396 [0.000, 5.000], mean observation: 0.062 [0.000, 24.000], loss: 0.013664, mean_absolute_error: 0.942081, mean_q: 1.057884\n",
      "    3242/5000000: episode: 53, duration: 1.256s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.630 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 0.013855, mean_absolute_error: 0.955892, mean_q: 1.074529\n",
      "    3270/5000000: episode: 54, duration: 1.233s, episode steps: 28, steps per second: 23, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.321 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 0.012775, mean_absolute_error: 0.952632, mean_q: 1.073963\n",
      "    3301/5000000: episode: 55, duration: 1.507s, episode steps: 31, steps per second: 21, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 2.871 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 0.015079, mean_absolute_error: 0.964406, mean_q: 1.085223\n",
      "    3386/5000000: episode: 56, duration: 3.923s, episode steps: 85, steps per second: 22, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.247 [0.000, 5.000], mean observation: 0.077 [0.000, 24.000], loss: 0.016595, mean_absolute_error: 0.983785, mean_q: 1.114063\n",
      "    3447/5000000: episode: 57, duration: 3.076s, episode steps: 61, steps per second: 20, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 2.918 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 0.017118, mean_absolute_error: 1.011927, mean_q: 1.149818\n",
      "    3473/5000000: episode: 58, duration: 1.308s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.654 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 0.019933, mean_absolute_error: 1.012184, mean_q: 1.158619\n",
      "    3511/5000000: episode: 59, duration: 1.818s, episode steps: 38, steps per second: 21, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 2.395 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 0.018648, mean_absolute_error: 1.026337, mean_q: 1.176701\n",
      "    3552/5000000: episode: 60, duration: 2.016s, episode steps: 41, steps per second: 20, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 2.268 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 0.018698, mean_absolute_error: 1.051454, mean_q: 1.205954\n",
      "    3577/5000000: episode: 61, duration: 1.259s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 1.920 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 0.021806, mean_absolute_error: 1.082953, mean_q: 1.244011\n",
      "    3602/5000000: episode: 62, duration: 1.224s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.160 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 0.023758, mean_absolute_error: 1.080111, mean_q: 1.235029\n",
      "    3627/5000000: episode: 63, duration: 1.309s, episode steps: 25, steps per second: 19, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.320 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 0.025291, mean_absolute_error: 1.089315, mean_q: 1.250876\n",
      "    3682/5000000: episode: 64, duration: 2.687s, episode steps: 55, steps per second: 20, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.745 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 0.024537, mean_absolute_error: 1.106533, mean_q: 1.274636\n",
      "    3707/5000000: episode: 65, duration: 1.269s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.760 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 0.028333, mean_absolute_error: 1.131308, mean_q: 1.314829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    3734/5000000: episode: 66, duration: 1.344s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.926 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 0.028921, mean_absolute_error: 1.129578, mean_q: 1.314826\n",
      "    3777/5000000: episode: 67, duration: 2.028s, episode steps: 43, steps per second: 21, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 1.953 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 0.035122, mean_absolute_error: 1.148795, mean_q: 1.339238\n",
      "    3805/5000000: episode: 68, duration: 1.357s, episode steps: 28, steps per second: 21, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.286 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 0.038109, mean_absolute_error: 1.172363, mean_q: 1.368114\n",
      "    3900/5000000: episode: 69, duration: 4.595s, episode steps: 95, steps per second: 21, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.358 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 0.046462, mean_absolute_error: 1.191027, mean_q: 1.397027\n",
      "    3928/5000000: episode: 70, duration: 1.410s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.536 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 0.059734, mean_absolute_error: 1.206841, mean_q: 1.435624\n",
      "    3988/5000000: episode: 71, duration: 2.888s, episode steps: 60, steps per second: 21, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.283 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 0.056500, mean_absolute_error: 1.246516, mean_q: 1.485800\n",
      "    4013/5000000: episode: 72, duration: 1.223s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.680 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 0.062349, mean_absolute_error: 1.302080, mean_q: 1.555627\n",
      "    4084/5000000: episode: 73, duration: 3.243s, episode steps: 71, steps per second: 22, episode reward: -1.000, mean reward: -0.014 [-1.000, 0.000], mean action: 2.577 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 0.061765, mean_absolute_error: 1.300713, mean_q: 1.557572\n",
      "    4164/5000000: episode: 74, duration: 4.045s, episode steps: 80, steps per second: 20, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 2.475 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 0.064904, mean_absolute_error: 1.311761, mean_q: 1.580848\n",
      "    4193/5000000: episode: 75, duration: 1.379s, episode steps: 29, steps per second: 21, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.345 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 0.070228, mean_absolute_error: 1.394750, mean_q: 1.684976\n",
      "    4252/5000000: episode: 76, duration: 2.737s, episode steps: 59, steps per second: 22, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.051 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 0.068838, mean_absolute_error: 1.404016, mean_q: 1.695879\n",
      "    4277/5000000: episode: 77, duration: 1.163s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.720 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 0.070127, mean_absolute_error: 1.416808, mean_q: 1.709698\n",
      "    4305/5000000: episode: 78, duration: 1.321s, episode steps: 28, steps per second: 21, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.679 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 0.063116, mean_absolute_error: 1.384114, mean_q: 1.674442\n",
      "    4407/5000000: episode: 79, duration: 4.853s, episode steps: 102, steps per second: 21, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.216 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 0.089801, mean_absolute_error: 1.440245, mean_q: 1.737804\n",
      "    4433/5000000: episode: 80, duration: 1.324s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.654 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 0.105506, mean_absolute_error: 1.475756, mean_q: 1.784121\n",
      "    4519/5000000: episode: 81, duration: 3.828s, episode steps: 86, steps per second: 22, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.779 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 0.131462, mean_absolute_error: 1.495643, mean_q: 1.829373\n",
      "    4551/5000000: episode: 82, duration: 1.379s, episode steps: 32, steps per second: 23, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 2.312 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 0.093796, mean_absolute_error: 1.561028, mean_q: 1.918407\n",
      "    4585/5000000: episode: 83, duration: 1.672s, episode steps: 34, steps per second: 20, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.471 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 0.165121, mean_absolute_error: 1.596547, mean_q: 1.962059\n",
      "    4615/5000000: episode: 84, duration: 1.494s, episode steps: 30, steps per second: 20, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.667 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 0.161816, mean_absolute_error: 1.579674, mean_q: 1.964624\n",
      "    4650/5000000: episode: 85, duration: 1.731s, episode steps: 35, steps per second: 20, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.200 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 0.174032, mean_absolute_error: 1.603762, mean_q: 2.012285\n",
      "    4681/5000000: episode: 86, duration: 1.381s, episode steps: 31, steps per second: 22, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 2.806 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 0.155840, mean_absolute_error: 1.619451, mean_q: 2.054790\n",
      "    4706/5000000: episode: 87, duration: 1.225s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.280 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 0.180697, mean_absolute_error: 1.686445, mean_q: 2.133481\n",
      "    4733/5000000: episode: 88, duration: 1.269s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.667 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 0.156506, mean_absolute_error: 1.614191, mean_q: 2.056677\n",
      "    4762/5000000: episode: 89, duration: 1.431s, episode steps: 29, steps per second: 20, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.897 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 0.155823, mean_absolute_error: 1.709605, mean_q: 2.169913\n",
      "    4789/5000000: episode: 90, duration: 1.451s, episode steps: 27, steps per second: 19, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.259 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 0.166005, mean_absolute_error: 1.676364, mean_q: 2.131512\n",
      "    4846/5000000: episode: 91, duration: 2.692s, episode steps: 57, steps per second: 21, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.316 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 0.164160, mean_absolute_error: 1.746099, mean_q: 2.219203\n",
      "    4873/5000000: episode: 92, duration: 1.308s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.333 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 0.256517, mean_absolute_error: 1.786190, mean_q: 2.268486\n",
      "    4928/5000000: episode: 93, duration: 2.685s, episode steps: 55, steps per second: 20, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.564 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 0.199771, mean_absolute_error: 1.817184, mean_q: 2.314912\n",
      "    5129/5000000: episode: 94, duration: 9.685s, episode steps: 201, steps per second: 21, episode reward: -1.000, mean reward: -0.005 [-1.000, 0.000], mean action: 2.403 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 0.237506, mean_absolute_error: 1.914531, mean_q: 2.437587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    5175/5000000: episode: 95, duration: 2.185s, episode steps: 46, steps per second: 21, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 2.652 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 0.239084, mean_absolute_error: 2.031946, mean_q: 2.607229\n",
      "    5443/5000000: episode: 96, duration: 12.190s, episode steps: 268, steps per second: 22, episode reward: -1.000, mean reward: -0.004 [-1.000, 0.000], mean action: 2.619 [0.000, 5.000], mean observation: 0.060 [0.000, 24.000], loss: 0.288806, mean_absolute_error: 2.128265, mean_q: 2.717347\n",
      "    5533/5000000: episode: 97, duration: 4.439s, episode steps: 90, steps per second: 20, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 3.078 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 0.381317, mean_absolute_error: 2.282888, mean_q: 2.915964\n",
      "    5558/5000000: episode: 98, duration: 1.272s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.520 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 0.456147, mean_absolute_error: 2.371160, mean_q: 3.033595\n",
      "    5617/5000000: episode: 99, duration: 2.812s, episode steps: 59, steps per second: 21, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.051 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 0.455989, mean_absolute_error: 2.406039, mean_q: 3.083138\n",
      "    5642/5000000: episode: 100, duration: 1.152s, episode steps: 25, steps per second: 22, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.400 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 0.506930, mean_absolute_error: 2.465093, mean_q: 3.156700\n",
      "    5669/5000000: episode: 101, duration: 1.272s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.444 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 0.412045, mean_absolute_error: 2.484183, mean_q: 3.181575\n",
      "    5698/5000000: episode: 102, duration: 1.380s, episode steps: 29, steps per second: 21, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.724 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 0.426428, mean_absolute_error: 2.548190, mean_q: 3.261879\n",
      "    5758/5000000: episode: 103, duration: 2.879s, episode steps: 60, steps per second: 21, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.667 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 0.397152, mean_absolute_error: 2.537837, mean_q: 3.248157\n",
      "    5798/5000000: episode: 104, duration: 1.978s, episode steps: 40, steps per second: 20, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 2.075 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 0.377684, mean_absolute_error: 2.559095, mean_q: 3.262233\n",
      "    5851/5000000: episode: 105, duration: 2.620s, episode steps: 53, steps per second: 20, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.528 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 0.492325, mean_absolute_error: 2.643144, mean_q: 3.375888\n",
      "    5933/5000000: episode: 106, duration: 4.000s, episode steps: 82, steps per second: 20, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.451 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 0.526180, mean_absolute_error: 2.713215, mean_q: 3.477932\n",
      "    5959/5000000: episode: 107, duration: 1.261s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.692 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 0.473012, mean_absolute_error: 2.803718, mean_q: 3.604714\n",
      "    6000/5000000: episode: 108, duration: 1.963s, episode steps: 41, steps per second: 21, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 2.122 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 0.523633, mean_absolute_error: 2.826553, mean_q: 3.632161\n",
      "    6026/5000000: episode: 109, duration: 1.309s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.846 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 0.413672, mean_absolute_error: 2.792632, mean_q: 3.575252\n",
      "    6051/5000000: episode: 110, duration: 1.246s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.520 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 0.442246, mean_absolute_error: 2.871477, mean_q: 3.670254\n",
      "    6081/5000000: episode: 111, duration: 1.375s, episode steps: 30, steps per second: 22, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 3.000 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 0.538482, mean_absolute_error: 2.850144, mean_q: 3.637254\n",
      "    6107/5000000: episode: 112, duration: 1.230s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.385 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 0.581381, mean_absolute_error: 2.882639, mean_q: 3.695830\n",
      "    6229/5000000: episode: 113, duration: 5.694s, episode steps: 122, steps per second: 21, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.680 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 0.663597, mean_absolute_error: 3.079130, mean_q: 3.939182\n",
      "    6255/5000000: episode: 114, duration: 1.193s, episode steps: 26, steps per second: 22, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.577 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1.125383, mean_absolute_error: 3.245354, mean_q: 4.164669\n",
      "    6283/5000000: episode: 115, duration: 1.270s, episode steps: 28, steps per second: 22, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.893 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 0.985078, mean_absolute_error: 3.304440, mean_q: 4.257004\n",
      "    6314/5000000: episode: 116, duration: 1.482s, episode steps: 31, steps per second: 21, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 2.065 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 0.883543, mean_absolute_error: 3.373352, mean_q: 4.362918\n",
      "    6430/5000000: episode: 117, duration: 5.610s, episode steps: 116, steps per second: 21, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.405 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 0.861269, mean_absolute_error: 3.491059, mean_q: 4.504135\n",
      "    6528/5000000: episode: 118, duration: 4.777s, episode steps: 98, steps per second: 21, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.418 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 0.778146, mean_absolute_error: 3.667842, mean_q: 4.741955\n",
      "    6590/5000000: episode: 119, duration: 2.921s, episode steps: 62, steps per second: 21, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 2.645 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 0.971062, mean_absolute_error: 3.907679, mean_q: 5.018046\n",
      "    6634/5000000: episode: 120, duration: 2.214s, episode steps: 44, steps per second: 20, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 2.227 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 0.999474, mean_absolute_error: 3.943652, mean_q: 5.069030\n",
      "    6661/5000000: episode: 121, duration: 1.361s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.370 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1.141836, mean_absolute_error: 4.044084, mean_q: 5.174137\n",
      "    6686/5000000: episode: 122, duration: 1.158s, episode steps: 25, steps per second: 22, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.800 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 0.878165, mean_absolute_error: 4.078727, mean_q: 5.237539\n",
      "    6713/5000000: episode: 123, duration: 1.336s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.481 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 0.907026, mean_absolute_error: 4.098627, mean_q: 5.229302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    6739/5000000: episode: 124, duration: 1.290s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.308 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1.092139, mean_absolute_error: 4.118625, mean_q: 5.281473\n",
      "    6764/5000000: episode: 125, duration: 1.200s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.640 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 0.980119, mean_absolute_error: 4.301497, mean_q: 5.524764\n",
      "    6790/5000000: episode: 126, duration: 1.186s, episode steps: 26, steps per second: 22, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.846 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 0.891630, mean_absolute_error: 4.245378, mean_q: 5.452711\n",
      "    6820/5000000: episode: 127, duration: 1.465s, episode steps: 30, steps per second: 20, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.700 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 1.032462, mean_absolute_error: 4.270795, mean_q: 5.460810\n",
      "    6857/5000000: episode: 128, duration: 1.755s, episode steps: 37, steps per second: 21, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 2.541 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 0.960775, mean_absolute_error: 4.494948, mean_q: 5.749660\n",
      "    6884/5000000: episode: 129, duration: 1.346s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.444 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 0.964842, mean_absolute_error: 4.386959, mean_q: 5.588360\n",
      "    6912/5000000: episode: 130, duration: 1.274s, episode steps: 28, steps per second: 22, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.679 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1.014099, mean_absolute_error: 4.263532, mean_q: 5.438679\n",
      "    6937/5000000: episode: 131, duration: 1.193s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.480 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 0.964649, mean_absolute_error: 4.645417, mean_q: 5.960120\n",
      "    6991/5000000: episode: 132, duration: 2.420s, episode steps: 54, steps per second: 22, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.481 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 1.019389, mean_absolute_error: 4.559699, mean_q: 5.828453\n",
      "    7252/5000000: episode: 133, duration: 12.800s, episode steps: 261, steps per second: 20, episode reward: -1.000, mean reward: -0.004 [-1.000, 0.000], mean action: 2.245 [0.000, 5.000], mean observation: 0.060 [0.000, 24.000], loss: 1.059018, mean_absolute_error: 4.923862, mean_q: 6.296930\n",
      "    7277/5000000: episode: 134, duration: 1.203s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.360 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 1.190374, mean_absolute_error: 5.289126, mean_q: 6.745463\n",
      "    7337/5000000: episode: 135, duration: 2.906s, episode steps: 60, steps per second: 21, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.350 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 1.065241, mean_absolute_error: 5.310184, mean_q: 6.767042\n",
      "    7372/5000000: episode: 136, duration: 1.771s, episode steps: 35, steps per second: 20, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.171 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 1.167667, mean_absolute_error: 5.447424, mean_q: 6.948785\n",
      "    7407/5000000: episode: 137, duration: 1.591s, episode steps: 35, steps per second: 22, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.571 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 1.004351, mean_absolute_error: 5.515579, mean_q: 7.066215\n",
      "    7608/5000000: episode: 138, duration: 9.177s, episode steps: 201, steps per second: 22, episode reward: -1.000, mean reward: -0.005 [-1.000, 0.000], mean action: 2.388 [0.000, 5.000], mean observation: 0.061 [0.000, 24.000], loss: 1.196320, mean_absolute_error: 5.764242, mean_q: 7.336332\n",
      "    7778/5000000: episode: 139, duration: 8.298s, episode steps: 170, steps per second: 20, episode reward: -1.000, mean reward: -0.006 [-1.000, 0.000], mean action: 2.435 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 1.426103, mean_absolute_error: 6.244573, mean_q: 7.934540\n",
      "    7859/5000000: episode: 140, duration: 3.755s, episode steps: 81, steps per second: 22, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.346 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 1.534229, mean_absolute_error: 6.670318, mean_q: 8.492262\n",
      "    7884/5000000: episode: 141, duration: 1.184s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.720 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 1.519376, mean_absolute_error: 6.777936, mean_q: 8.578084\n",
      "    8078/5000000: episode: 142, duration: 9.243s, episode steps: 194, steps per second: 21, episode reward: -1.000, mean reward: -0.005 [-1.000, 0.000], mean action: 2.309 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 1.902220, mean_absolute_error: 7.000997, mean_q: 8.885707\n",
      "    8179/5000000: episode: 143, duration: 4.732s, episode steps: 101, steps per second: 21, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.069 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 2.521807, mean_absolute_error: 7.495230, mean_q: 9.525777\n",
      "    8210/5000000: episode: 144, duration: 1.360s, episode steps: 31, steps per second: 23, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 2.484 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 2.720388, mean_absolute_error: 7.685903, mean_q: 9.757914\n",
      "    8236/5000000: episode: 145, duration: 1.228s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.962 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 2.695296, mean_absolute_error: 7.581326, mean_q: 9.677056\n",
      "    8262/5000000: episode: 146, duration: 1.284s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.385 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 2.637108, mean_absolute_error: 7.933809, mean_q: 10.088976\n",
      "    8289/5000000: episode: 147, duration: 1.265s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.185 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 2.860152, mean_absolute_error: 8.229630, mean_q: 10.421357\n",
      "    8317/5000000: episode: 148, duration: 1.293s, episode steps: 28, steps per second: 22, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.607 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 2.624003, mean_absolute_error: 8.123994, mean_q: 10.289148\n",
      "    8343/5000000: episode: 149, duration: 1.293s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.462 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 2.686124, mean_absolute_error: 8.346019, mean_q: 10.621258\n",
      "    8422/5000000: episode: 150, duration: 3.586s, episode steps: 79, steps per second: 22, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 2.646 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 2.766270, mean_absolute_error: 8.430062, mean_q: 10.660479\n",
      "    8528/5000000: episode: 151, duration: 5.076s, episode steps: 106, steps per second: 21, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.236 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 2.961227, mean_absolute_error: 8.707204, mean_q: 11.046166\n",
      "    8553/5000000: episode: 152, duration: 1.234s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.360 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 2.821014, mean_absolute_error: 8.879296, mean_q: 11.286446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    8630/5000000: episode: 153, duration: 3.640s, episode steps: 77, steps per second: 21, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 2.026 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 3.213791, mean_absolute_error: 8.915653, mean_q: 11.322460\n",
      "    8728/5000000: episode: 154, duration: 4.806s, episode steps: 98, steps per second: 20, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.684 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 3.232484, mean_absolute_error: 9.353251, mean_q: 11.856326\n",
      "    8835/5000000: episode: 155, duration: 4.980s, episode steps: 107, steps per second: 21, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.607 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 3.247223, mean_absolute_error: 9.744396, mean_q: 12.294409\n",
      "    9033/5000000: episode: 156, duration: 9.107s, episode steps: 198, steps per second: 22, episode reward: -1.000, mean reward: -0.005 [-1.000, 0.000], mean action: 2.485 [0.000, 5.000], mean observation: 0.062 [0.000, 24.000], loss: 3.881174, mean_absolute_error: 10.235502, mean_q: 12.958715\n",
      "    9182/5000000: episode: 157, duration: 7.006s, episode steps: 149, steps per second: 21, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 2.544 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 4.534046, mean_absolute_error: 10.988663, mean_q: 13.912935\n",
      "    9213/5000000: episode: 158, duration: 1.496s, episode steps: 31, steps per second: 21, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 2.194 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 4.341225, mean_absolute_error: 10.848701, mean_q: 13.699092\n",
      "    9239/5000000: episode: 159, duration: 1.279s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.269 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 5.267292, mean_absolute_error: 11.404445, mean_q: 14.307436\n",
      "    9411/5000000: episode: 160, duration: 7.919s, episode steps: 172, steps per second: 22, episode reward: -1.000, mean reward: -0.006 [-1.000, 0.000], mean action: 2.541 [0.000, 5.000], mean observation: 0.061 [0.000, 24.000], loss: 5.306365, mean_absolute_error: 11.844449, mean_q: 14.990231\n",
      "    9450/5000000: episode: 161, duration: 2.034s, episode steps: 39, steps per second: 19, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 2.026 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 5.180471, mean_absolute_error: 12.519176, mean_q: 15.834775\n",
      "    9477/5000000: episode: 162, duration: 1.304s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.259 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 5.662405, mean_absolute_error: 12.214424, mean_q: 15.343687\n",
      "    9507/5000000: episode: 163, duration: 1.509s, episode steps: 30, steps per second: 20, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.233 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 5.813910, mean_absolute_error: 12.515573, mean_q: 15.803863\n",
      "    9533/5000000: episode: 164, duration: 1.249s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.538 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 5.572112, mean_absolute_error: 13.112981, mean_q: 16.551085\n",
      "    9629/5000000: episode: 165, duration: 4.689s, episode steps: 96, steps per second: 20, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.271 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 6.054934, mean_absolute_error: 12.824519, mean_q: 16.222355\n",
      "    9656/5000000: episode: 166, duration: 1.286s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.444 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 7.891851, mean_absolute_error: 13.224952, mean_q: 16.762646\n",
      "    9681/5000000: episode: 167, duration: 1.217s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.680 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 7.209488, mean_absolute_error: 13.730136, mean_q: 17.355637\n",
      "    9707/5000000: episode: 168, duration: 1.282s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.154 [1.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 7.475373, mean_absolute_error: 13.118525, mean_q: 16.667049\n",
      "    9737/5000000: episode: 169, duration: 1.494s, episode steps: 30, steps per second: 20, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.000 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 8.550046, mean_absolute_error: 13.711387, mean_q: 17.197815\n",
      "   10011/5000000: episode: 170, duration: 12.816s, episode steps: 274, steps per second: 21, episode reward: -1.000, mean reward: -0.004 [-1.000, 0.000], mean action: 2.547 [0.000, 5.000], mean observation: 0.058 [0.000, 24.000], loss: 8.421202, mean_absolute_error: 14.548652, mean_q: 18.393494\n",
      "   10037/5000000: episode: 171, duration: 1.249s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.077 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 8.390054, mean_absolute_error: 15.101976, mean_q: 19.034809\n",
      "   10062/5000000: episode: 172, duration: 1.236s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.760 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 9.540465, mean_absolute_error: 15.984959, mean_q: 20.188614\n",
      "   10200/5000000: episode: 173, duration: 6.704s, episode steps: 138, steps per second: 21, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 2.420 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 10.273876, mean_absolute_error: 16.073816, mean_q: 20.362669\n",
      "   10253/5000000: episode: 174, duration: 2.520s, episode steps: 53, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.528 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 10.587190, mean_absolute_error: 16.478992, mean_q: 20.944294\n",
      "   10307/5000000: episode: 175, duration: 2.524s, episode steps: 54, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.000 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 11.016990, mean_absolute_error: 17.013998, mean_q: 21.709236\n",
      "   10340/5000000: episode: 176, duration: 1.759s, episode steps: 33, steps per second: 19, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 1.970 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 11.910156, mean_absolute_error: 16.782894, mean_q: 21.265337\n",
      "   10421/5000000: episode: 177, duration: 3.993s, episode steps: 81, steps per second: 20, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.815 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 11.678699, mean_absolute_error: 17.147390, mean_q: 21.876431\n",
      "   10475/5000000: episode: 178, duration: 2.697s, episode steps: 54, steps per second: 20, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.815 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 13.113996, mean_absolute_error: 18.491865, mean_q: 23.426846\n",
      "   10517/5000000: episode: 179, duration: 1.998s, episode steps: 42, steps per second: 21, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 1.619 [0.000, 5.000], mean observation: 0.062 [0.000, 24.000], loss: 15.495081, mean_absolute_error: 18.821430, mean_q: 23.998432\n",
      "   10542/5000000: episode: 180, duration: 1.186s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.560 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 16.095545, mean_absolute_error: 18.969885, mean_q: 24.199654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   10644/5000000: episode: 181, duration: 4.908s, episode steps: 102, steps per second: 21, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.167 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 16.072033, mean_absolute_error: 19.882431, mean_q: 25.361235\n",
      "   10876/5000000: episode: 182, duration: 11.361s, episode steps: 232, steps per second: 20, episode reward: -1.000, mean reward: -0.004 [-1.000, 0.000], mean action: 2.082 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 18.621986, mean_absolute_error: 20.983665, mean_q: 26.698381\n",
      "   10903/5000000: episode: 183, duration: 1.418s, episode steps: 27, steps per second: 19, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.000 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 20.114305, mean_absolute_error: 23.333277, mean_q: 29.761124\n",
      "   10943/5000000: episode: 184, duration: 1.934s, episode steps: 40, steps per second: 21, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 2.150 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 19.016785, mean_absolute_error: 23.138952, mean_q: 29.506901\n",
      "   10969/5000000: episode: 185, duration: 1.333s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.308 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 24.877686, mean_absolute_error: 23.476336, mean_q: 29.906490\n",
      "   10995/5000000: episode: 186, duration: 1.195s, episode steps: 26, steps per second: 22, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 1.923 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 19.262447, mean_absolute_error: 23.052113, mean_q: 29.318644\n",
      "   11151/5000000: episode: 187, duration: 7.451s, episode steps: 156, steps per second: 21, episode reward: -1.000, mean reward: -0.006 [-1.000, 0.000], mean action: 2.064 [0.000, 5.000], mean observation: 0.061 [0.000, 24.000], loss: 19.020687, mean_absolute_error: 23.736900, mean_q: 30.159506\n",
      "   11177/5000000: episode: 188, duration: 1.250s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.615 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 18.265347, mean_absolute_error: 24.761814, mean_q: 31.418861\n",
      "   11206/5000000: episode: 189, duration: 1.342s, episode steps: 29, steps per second: 22, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.724 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 25.742121, mean_absolute_error: 24.595034, mean_q: 31.197016\n",
      "   11300/5000000: episode: 190, duration: 4.150s, episode steps: 94, steps per second: 23, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.553 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 21.943136, mean_absolute_error: 25.087296, mean_q: 31.649643\n",
      "   11325/5000000: episode: 191, duration: 1.135s, episode steps: 25, steps per second: 22, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 4.040 [2.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 21.768536, mean_absolute_error: 25.920471, mean_q: 32.744026\n",
      "   11385/5000000: episode: 192, duration: 2.750s, episode steps: 60, steps per second: 22, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.467 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 25.091530, mean_absolute_error: 26.583935, mean_q: 33.608101\n",
      "   11501/5000000: episode: 193, duration: 5.454s, episode steps: 116, steps per second: 21, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.216 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 24.925014, mean_absolute_error: 27.103977, mean_q: 34.205803\n",
      "   11665/5000000: episode: 194, duration: 7.695s, episode steps: 164, steps per second: 21, episode reward: -1.000, mean reward: -0.006 [-1.000, 0.000], mean action: 1.945 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 24.628063, mean_absolute_error: 27.548260, mean_q: 34.712704\n",
      "   11702/5000000: episode: 195, duration: 1.807s, episode steps: 37, steps per second: 20, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 2.270 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 29.001385, mean_absolute_error: 28.674448, mean_q: 36.145271\n",
      "   11791/5000000: episode: 196, duration: 4.232s, episode steps: 89, steps per second: 21, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.326 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 32.164635, mean_absolute_error: 28.890156, mean_q: 36.539478\n",
      "   11819/5000000: episode: 197, duration: 3.179s, episode steps: 28, steps per second: 9, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.250 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 27.411469, mean_absolute_error: 30.554577, mean_q: 38.489994\n",
      "   11846/5000000: episode: 198, duration: 1.314s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.815 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 35.576778, mean_absolute_error: 30.212698, mean_q: 38.079765\n",
      "   11937/5000000: episode: 199, duration: 4.647s, episode steps: 91, steps per second: 20, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.220 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 36.680092, mean_absolute_error: 31.584883, mean_q: 39.822754\n",
      "   11990/5000000: episode: 200, duration: 2.375s, episode steps: 53, steps per second: 22, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.868 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 26.123415, mean_absolute_error: 30.506233, mean_q: 38.409672\n",
      "   12052/5000000: episode: 201, duration: 2.862s, episode steps: 62, steps per second: 22, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 2.194 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 33.246796, mean_absolute_error: 32.118313, mean_q: 40.434216\n",
      "   12077/5000000: episode: 202, duration: 1.277s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.320 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 35.283794, mean_absolute_error: 30.279739, mean_q: 37.933231\n",
      "   12165/5000000: episode: 203, duration: 4.294s, episode steps: 88, steps per second: 20, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.761 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 35.630737, mean_absolute_error: 32.645786, mean_q: 40.910881\n",
      "   12223/5000000: episode: 204, duration: 2.824s, episode steps: 58, steps per second: 21, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.379 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 33.891964, mean_absolute_error: 31.683910, mean_q: 39.747066\n",
      "   12257/5000000: episode: 205, duration: 1.572s, episode steps: 34, steps per second: 22, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.471 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 34.299583, mean_absolute_error: 33.635303, mean_q: 42.043667\n",
      "   12393/5000000: episode: 206, duration: 6.680s, episode steps: 136, steps per second: 20, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 2.434 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 41.186993, mean_absolute_error: 33.953590, mean_q: 42.580833\n",
      "   12418/5000000: episode: 207, duration: 1.218s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.320 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 38.083267, mean_absolute_error: 33.519203, mean_q: 41.954140\n",
      "   12587/5000000: episode: 208, duration: 8.218s, episode steps: 169, steps per second: 21, episode reward: -1.000, mean reward: -0.006 [-1.000, 0.000], mean action: 2.183 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 62.251884, mean_absolute_error: 35.892860, mean_q: 44.935883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   12700/5000000: episode: 209, duration: 5.376s, episode steps: 113, steps per second: 21, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.044 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 91.446739, mean_absolute_error: 37.652191, mean_q: 47.241055\n",
      "   12973/5000000: episode: 210, duration: 13.377s, episode steps: 273, steps per second: 20, episode reward: -1.000, mean reward: -0.004 [-1.000, 0.000], mean action: 2.374 [0.000, 5.000], mean observation: 0.061 [0.000, 24.000], loss: 81.529160, mean_absolute_error: 38.376240, mean_q: 48.115513\n",
      "   13004/5000000: episode: 211, duration: 1.477s, episode steps: 31, steps per second: 21, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 2.000 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 81.049133, mean_absolute_error: 40.062618, mean_q: 50.468708\n",
      "   13034/5000000: episode: 212, duration: 1.382s, episode steps: 30, steps per second: 22, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.367 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 84.121147, mean_absolute_error: 41.796158, mean_q: 52.612839\n",
      "   13069/5000000: episode: 213, duration: 1.652s, episode steps: 35, steps per second: 21, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.343 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 89.154869, mean_absolute_error: 40.505348, mean_q: 50.432365\n",
      "   13101/5000000: episode: 214, duration: 1.447s, episode steps: 32, steps per second: 22, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 2.000 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 70.656441, mean_absolute_error: 40.679008, mean_q: 50.885101\n",
      "   13156/5000000: episode: 215, duration: 2.640s, episode steps: 55, steps per second: 21, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.073 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 75.107208, mean_absolute_error: 41.496788, mean_q: 51.994362\n",
      "   13181/5000000: episode: 216, duration: 1.189s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 1.840 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 94.158211, mean_absolute_error: 44.478882, mean_q: 55.699043\n",
      "   13206/5000000: episode: 217, duration: 1.231s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.040 [2.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 70.911575, mean_absolute_error: 40.033337, mean_q: 50.027245\n",
      "   13272/5000000: episode: 218, duration: 3.082s, episode steps: 66, steps per second: 21, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 2.076 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 82.567192, mean_absolute_error: 40.458656, mean_q: 50.641815\n",
      "   13350/5000000: episode: 219, duration: 3.595s, episode steps: 78, steps per second: 22, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 2.269 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 75.665901, mean_absolute_error: 42.566658, mean_q: 53.241367\n",
      "   13539/5000000: episode: 220, duration: 9.247s, episode steps: 189, steps per second: 20, episode reward: -1.000, mean reward: -0.005 [-1.000, 0.000], mean action: 2.026 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 91.959946, mean_absolute_error: 44.257622, mean_q: 55.512947\n",
      "   13621/5000000: episode: 221, duration: 3.937s, episode steps: 82, steps per second: 21, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.171 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 100.031563, mean_absolute_error: 44.860683, mean_q: 56.201897\n",
      "   13693/5000000: episode: 222, duration: 3.423s, episode steps: 72, steps per second: 21, episode reward: -1.000, mean reward: -0.014 [-1.000, 0.000], mean action: 2.153 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 94.632782, mean_absolute_error: 46.088856, mean_q: 57.801350\n",
      "   13756/5000000: episode: 223, duration: 3.015s, episode steps: 63, steps per second: 21, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 2.349 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 68.100349, mean_absolute_error: 46.392151, mean_q: 58.026234\n",
      "   13784/5000000: episode: 224, duration: 1.402s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.250 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 80.084023, mean_absolute_error: 47.309612, mean_q: 59.306026\n",
      "   13914/5000000: episode: 225, duration: 6.589s, episode steps: 130, steps per second: 20, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.362 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 128.871277, mean_absolute_error: 47.500408, mean_q: 59.493076\n",
      "   14115/5000000: episode: 226, duration: 9.624s, episode steps: 201, steps per second: 21, episode reward: -1.000, mean reward: -0.005 [-1.000, 0.000], mean action: 2.184 [0.000, 5.000], mean observation: 0.060 [0.000, 24.000], loss: 109.674362, mean_absolute_error: 49.083237, mean_q: 61.424950\n",
      "   14142/5000000: episode: 227, duration: 1.410s, episode steps: 27, steps per second: 19, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.963 [1.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 104.827950, mean_absolute_error: 49.274250, mean_q: 61.665672\n",
      "   14169/5000000: episode: 228, duration: 1.293s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 3.037 [1.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 173.629623, mean_absolute_error: 49.013546, mean_q: 61.384277\n",
      "   14195/5000000: episode: 229, duration: 1.231s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.500 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 73.190498, mean_absolute_error: 49.201817, mean_q: 61.497147\n",
      "   14225/5000000: episode: 230, duration: 1.473s, episode steps: 30, steps per second: 20, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.233 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 118.443306, mean_absolute_error: 50.334717, mean_q: 63.052292\n",
      "   14252/5000000: episode: 231, duration: 1.263s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 1.852 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 108.710579, mean_absolute_error: 50.458908, mean_q: 63.026928\n",
      "   14337/5000000: episode: 232, duration: 4.089s, episode steps: 85, steps per second: 21, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.812 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 115.729179, mean_absolute_error: 52.021679, mean_q: 65.106110\n",
      "   14377/5000000: episode: 233, duration: 1.879s, episode steps: 40, steps per second: 21, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 2.200 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 143.729950, mean_absolute_error: 53.037060, mean_q: 66.317726\n",
      "   14411/5000000: episode: 234, duration: 1.653s, episode steps: 34, steps per second: 21, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.706 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 152.494339, mean_absolute_error: 51.897251, mean_q: 64.686356\n",
      "   14438/5000000: episode: 235, duration: 1.284s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.889 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 114.227737, mean_absolute_error: 54.498161, mean_q: 67.948715\n",
      "   14518/5000000: episode: 236, duration: 3.619s, episode steps: 80, steps per second: 22, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 2.525 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 88.382347, mean_absolute_error: 52.547291, mean_q: 65.504333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   14554/5000000: episode: 237, duration: 1.724s, episode steps: 36, steps per second: 21, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 1.917 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 133.405197, mean_absolute_error: 52.948315, mean_q: 66.106720\n",
      "   14579/5000000: episode: 238, duration: 1.238s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.440 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 127.329727, mean_absolute_error: 55.293713, mean_q: 68.971863\n",
      "   14755/5000000: episode: 239, duration: 8.562s, episode steps: 176, steps per second: 21, episode reward: -1.000, mean reward: -0.006 [-1.000, 0.000], mean action: 2.324 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 170.925232, mean_absolute_error: 53.298195, mean_q: 66.383644\n",
      "   14783/5000000: episode: 240, duration: 1.405s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.964 [1.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 133.964005, mean_absolute_error: 56.245689, mean_q: 70.088821\n",
      "   14808/5000000: episode: 241, duration: 1.248s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 1.880 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 151.295227, mean_absolute_error: 55.025047, mean_q: 68.873039\n",
      "   14839/5000000: episode: 242, duration: 1.450s, episode steps: 31, steps per second: 21, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 2.323 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 209.731445, mean_absolute_error: 54.309414, mean_q: 67.843994\n",
      "   14989/5000000: episode: 243, duration: 6.687s, episode steps: 150, steps per second: 22, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 2.547 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 173.185013, mean_absolute_error: 55.827122, mean_q: 69.824554\n",
      "   15044/5000000: episode: 244, duration: 2.675s, episode steps: 55, steps per second: 21, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 3.164 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 194.891312, mean_absolute_error: 55.865341, mean_q: 69.940231\n",
      "   15069/5000000: episode: 245, duration: 1.215s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.640 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 154.577271, mean_absolute_error: 53.927158, mean_q: 67.323616\n",
      "   15151/5000000: episode: 246, duration: 3.932s, episode steps: 82, steps per second: 21, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.793 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 200.722687, mean_absolute_error: 56.960915, mean_q: 71.183311\n",
      "   15176/5000000: episode: 247, duration: 1.268s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.600 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 160.733749, mean_absolute_error: 61.972519, mean_q: 77.618279\n",
      "   15201/5000000: episode: 248, duration: 1.251s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.880 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 631.496277, mean_absolute_error: 61.348507, mean_q: 77.039154\n",
      "   15287/5000000: episode: 249, duration: 4.157s, episode steps: 86, steps per second: 21, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 4.058 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 315.402863, mean_absolute_error: 60.894264, mean_q: 76.216232\n",
      "   15391/5000000: episode: 250, duration: 4.914s, episode steps: 104, steps per second: 21, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.971 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 383.809570, mean_absolute_error: 62.141960, mean_q: 77.625961\n",
      "   15446/5000000: episode: 251, duration: 2.586s, episode steps: 55, steps per second: 21, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.473 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 344.591949, mean_absolute_error: 61.763229, mean_q: 77.126228\n",
      "   15520/5000000: episode: 252, duration: 3.505s, episode steps: 74, steps per second: 21, episode reward: -1.000, mean reward: -0.014 [-1.000, 0.000], mean action: 2.865 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 305.546631, mean_absolute_error: 63.596001, mean_q: 79.571953\n",
      "   15574/5000000: episode: 253, duration: 2.621s, episode steps: 54, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.500 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 420.562225, mean_absolute_error: 63.248001, mean_q: 79.075844\n",
      "   15599/5000000: episode: 254, duration: 1.209s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.320 [1.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 305.163147, mean_absolute_error: 64.944504, mean_q: 81.412453\n",
      "   15624/5000000: episode: 255, duration: 1.264s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.240 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 331.305664, mean_absolute_error: 66.914154, mean_q: 83.193001\n",
      "   15652/5000000: episode: 256, duration: 1.434s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 4.000 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 435.480042, mean_absolute_error: 65.659035, mean_q: 82.363853\n",
      "   15677/5000000: episode: 257, duration: 1.132s, episode steps: 25, steps per second: 22, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.960 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 275.533356, mean_absolute_error: 61.829292, mean_q: 77.161652\n",
      "   15702/5000000: episode: 258, duration: 1.207s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.680 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 435.887115, mean_absolute_error: 65.570915, mean_q: 82.016487\n",
      "   15743/5000000: episode: 259, duration: 1.949s, episode steps: 41, steps per second: 21, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 3.049 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 498.654297, mean_absolute_error: 65.865028, mean_q: 82.359016\n",
      "   15772/5000000: episode: 260, duration: 1.418s, episode steps: 29, steps per second: 20, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 3.379 [1.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 411.641418, mean_absolute_error: 63.329556, mean_q: 79.404739\n",
      "   15827/5000000: episode: 261, duration: 2.530s, episode steps: 55, steps per second: 22, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 3.145 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 318.694092, mean_absolute_error: 66.606812, mean_q: 83.505653\n",
      "   16075/5000000: episode: 262, duration: 11.548s, episode steps: 248, steps per second: 21, episode reward: -1.000, mean reward: -0.004 [-1.000, 0.000], mean action: 2.843 [0.000, 5.000], mean observation: 0.061 [0.000, 24.000], loss: 267.171112, mean_absolute_error: 67.320595, mean_q: 84.181313\n",
      "   16127/5000000: episode: 263, duration: 2.572s, episode steps: 52, steps per second: 20, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.750 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 273.838928, mean_absolute_error: 69.486458, mean_q: 86.788048\n",
      "   16165/5000000: episode: 264, duration: 1.745s, episode steps: 38, steps per second: 22, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 2.632 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 433.433502, mean_absolute_error: 70.468971, mean_q: 88.060150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   16194/5000000: episode: 265, duration: 1.387s, episode steps: 29, steps per second: 21, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.448 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 260.890228, mean_absolute_error: 72.457680, mean_q: 90.670601\n",
      "   16292/5000000: episode: 266, duration: 4.824s, episode steps: 98, steps per second: 20, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.796 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 486.012787, mean_absolute_error: 68.800682, mean_q: 86.245789\n",
      "   16317/5000000: episode: 267, duration: 1.192s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.600 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 332.956085, mean_absolute_error: 68.277466, mean_q: 85.763771\n",
      "   16345/5000000: episode: 268, duration: 1.362s, episode steps: 28, steps per second: 21, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 3.357 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 400.674042, mean_absolute_error: 72.793533, mean_q: 91.721977\n",
      "   16373/5000000: episode: 269, duration: 1.415s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 3.393 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 528.320068, mean_absolute_error: 68.183922, mean_q: 85.460045\n",
      "   16399/5000000: episode: 270, duration: 1.268s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.038 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 326.996124, mean_absolute_error: 72.724808, mean_q: 91.268143\n",
      "   16479/5000000: episode: 271, duration: 3.939s, episode steps: 80, steps per second: 20, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 3.150 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 384.232788, mean_absolute_error: 72.631859, mean_q: 90.855721\n",
      "   16504/5000000: episode: 272, duration: 1.171s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.120 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 238.619476, mean_absolute_error: 77.466728, mean_q: 97.118919\n",
      "   16557/5000000: episode: 273, duration: 2.609s, episode steps: 53, steps per second: 20, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 3.377 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 453.532349, mean_absolute_error: 75.765854, mean_q: 95.238243\n",
      "   16584/5000000: episode: 274, duration: 1.340s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 3.111 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 483.266174, mean_absolute_error: 71.085899, mean_q: 89.138824\n",
      "   16634/5000000: episode: 275, duration: 2.429s, episode steps: 50, steps per second: 21, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 3.900 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 326.447418, mean_absolute_error: 74.036903, mean_q: 92.970398\n",
      "   16661/5000000: episode: 276, duration: 1.333s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 3.481 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 340.141571, mean_absolute_error: 78.115776, mean_q: 98.079185\n",
      "   16712/5000000: episode: 277, duration: 2.366s, episode steps: 51, steps per second: 22, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 3.373 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 218.094223, mean_absolute_error: 76.314255, mean_q: 96.091896\n",
      "   16739/5000000: episode: 278, duration: 1.305s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 3.185 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 288.293396, mean_absolute_error: 76.552727, mean_q: 95.815620\n",
      "   16765/5000000: episode: 279, duration: 1.370s, episode steps: 26, steps per second: 19, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.846 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 297.658569, mean_absolute_error: 73.360832, mean_q: 92.081345\n",
      "   16790/5000000: episode: 280, duration: 1.204s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.520 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 505.920746, mean_absolute_error: 78.080246, mean_q: 98.245911\n",
      "   16841/5000000: episode: 281, duration: 2.310s, episode steps: 51, steps per second: 22, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 3.235 [0.000, 5.000], mean observation: 0.074 [0.000, 24.000], loss: 333.653595, mean_absolute_error: 77.017570, mean_q: 96.615387\n",
      "   16921/5000000: episode: 282, duration: 3.651s, episode steps: 80, steps per second: 22, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 3.850 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 336.167908, mean_absolute_error: 79.038498, mean_q: 99.188133\n",
      "   16971/5000000: episode: 283, duration: 2.332s, episode steps: 50, steps per second: 21, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 3.520 [0.000, 5.000], mean observation: 0.077 [0.000, 24.000], loss: 270.173584, mean_absolute_error: 76.828384, mean_q: 96.158546\n",
      "   16997/5000000: episode: 284, duration: 1.172s, episode steps: 26, steps per second: 22, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.923 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 641.876038, mean_absolute_error: 80.755974, mean_q: 101.400337\n",
      "   17023/5000000: episode: 285, duration: 1.231s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.577 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 411.731018, mean_absolute_error: 77.762001, mean_q: 97.235924\n",
      "   17055/5000000: episode: 286, duration: 1.517s, episode steps: 32, steps per second: 21, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 3.500 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 283.275024, mean_absolute_error: 79.615692, mean_q: 99.959778\n",
      "   17080/5000000: episode: 287, duration: 1.110s, episode steps: 25, steps per second: 23, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.520 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 543.131653, mean_absolute_error: 81.445076, mean_q: 102.241554\n",
      "   17105/5000000: episode: 288, duration: 1.285s, episode steps: 25, steps per second: 19, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.360 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 283.390381, mean_absolute_error: 81.261536, mean_q: 102.355049\n",
      "   17161/5000000: episode: 289, duration: 2.783s, episode steps: 56, steps per second: 20, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 3.232 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 436.455414, mean_absolute_error: 81.375717, mean_q: 102.123222\n",
      "   17188/5000000: episode: 290, duration: 1.321s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 3.704 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 324.035553, mean_absolute_error: 79.436264, mean_q: 99.407379\n",
      "   17241/5000000: episode: 291, duration: 2.544s, episode steps: 53, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 3.623 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 291.685699, mean_absolute_error: 81.722740, mean_q: 102.408867\n",
      "   17266/5000000: episode: 292, duration: 1.136s, episode steps: 25, steps per second: 22, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.480 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 365.572784, mean_absolute_error: 80.315369, mean_q: 100.616638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   17292/5000000: episode: 293, duration: 1.240s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 4.192 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 246.246689, mean_absolute_error: 84.632614, mean_q: 106.097977\n",
      "   17317/5000000: episode: 294, duration: 1.201s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.400 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 285.380951, mean_absolute_error: 86.251610, mean_q: 108.734924\n",
      "   17344/5000000: episode: 295, duration: 1.219s, episode steps: 27, steps per second: 22, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 4.259 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 203.348328, mean_absolute_error: 84.412338, mean_q: 106.109039\n",
      "   17369/5000000: episode: 296, duration: 1.213s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.920 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 303.880127, mean_absolute_error: 79.878807, mean_q: 99.902611\n",
      "   17394/5000000: episode: 297, duration: 1.268s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.680 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 202.624420, mean_absolute_error: 81.060577, mean_q: 101.778671\n",
      "   17444/5000000: episode: 298, duration: 2.268s, episode steps: 50, steps per second: 22, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 3.320 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 278.926666, mean_absolute_error: 84.004036, mean_q: 105.474701\n",
      "   17470/5000000: episode: 299, duration: 1.271s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.692 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 437.056915, mean_absolute_error: 84.521935, mean_q: 105.984894\n",
      "   17495/5000000: episode: 300, duration: 1.163s, episode steps: 25, steps per second: 22, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 4.160 [1.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 297.777100, mean_absolute_error: 83.949463, mean_q: 105.186493\n",
      "   17595/5000000: episode: 301, duration: 4.699s, episode steps: 100, steps per second: 21, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 3.840 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 412.653046, mean_absolute_error: 84.897507, mean_q: 106.359993\n",
      "   17689/5000000: episode: 302, duration: 4.556s, episode steps: 94, steps per second: 21, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 3.287 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 366.873810, mean_absolute_error: 85.016235, mean_q: 106.188667\n",
      "   17715/5000000: episode: 303, duration: 1.295s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 4.115 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 614.678772, mean_absolute_error: 88.683640, mean_q: 110.551224\n",
      "   17740/5000000: episode: 304, duration: 1.300s, episode steps: 25, steps per second: 19, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.960 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 183.470795, mean_absolute_error: 87.602913, mean_q: 109.245804\n",
      "   17765/5000000: episode: 305, duration: 1.227s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.640 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 555.413513, mean_absolute_error: 88.858757, mean_q: 110.723763\n",
      "   17796/5000000: episode: 306, duration: 1.397s, episode steps: 31, steps per second: 22, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 3.194 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 351.934296, mean_absolute_error: 90.520317, mean_q: 113.297295\n",
      "   17821/5000000: episode: 307, duration: 1.142s, episode steps: 25, steps per second: 22, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.800 [1.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 317.024872, mean_absolute_error: 87.123665, mean_q: 108.617569\n",
      "   17954/5000000: episode: 308, duration: 6.372s, episode steps: 133, steps per second: 21, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 3.045 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 366.288757, mean_absolute_error: 89.046539, mean_q: 110.748749\n",
      "   17981/5000000: episode: 309, duration: 1.374s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.852 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 439.640411, mean_absolute_error: 88.478912, mean_q: 110.036308\n",
      "   18008/5000000: episode: 310, duration: 1.308s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.778 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 550.553711, mean_absolute_error: 92.018791, mean_q: 115.288177\n",
      "   18060/5000000: episode: 311, duration: 2.464s, episode steps: 52, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.904 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 445.842712, mean_absolute_error: 91.630440, mean_q: 114.473091\n",
      "   18216/5000000: episode: 312, duration: 7.407s, episode steps: 156, steps per second: 21, episode reward: -1.000, mean reward: -0.006 [-1.000, 0.000], mean action: 3.096 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 394.370605, mean_absolute_error: 89.277626, mean_q: 111.556190\n",
      "   18268/5000000: episode: 313, duration: 2.411s, episode steps: 52, steps per second: 22, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 3.077 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 359.719391, mean_absolute_error: 88.953377, mean_q: 110.882332\n",
      "   18295/5000000: episode: 314, duration: 1.325s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.074 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 611.753357, mean_absolute_error: 86.626534, mean_q: 107.546371\n",
      "   18345/5000000: episode: 315, duration: 2.300s, episode steps: 50, steps per second: 22, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 3.440 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 488.929840, mean_absolute_error: 94.421272, mean_q: 117.834862\n",
      "   18370/5000000: episode: 316, duration: 1.137s, episode steps: 25, steps per second: 22, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.160 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 618.406433, mean_absolute_error: 99.444611, mean_q: 124.184875\n",
      "   18398/5000000: episode: 317, duration: 1.318s, episode steps: 28, steps per second: 21, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 3.929 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 632.893250, mean_absolute_error: 96.193916, mean_q: 119.942520\n",
      "   18535/5000000: episode: 318, duration: 6.654s, episode steps: 137, steps per second: 21, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 3.219 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 389.138794, mean_absolute_error: 97.204407, mean_q: 121.345871\n",
      "   18561/5000000: episode: 319, duration: 1.354s, episode steps: 26, steps per second: 19, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.385 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 483.051208, mean_absolute_error: 97.936455, mean_q: 122.167427\n",
      "   18587/5000000: episode: 320, duration: 1.226s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.577 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 252.031647, mean_absolute_error: 96.253494, mean_q: 119.685295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   18615/5000000: episode: 321, duration: 1.413s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 3.286 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 643.804688, mean_absolute_error: 97.442680, mean_q: 121.311447\n",
      "   18640/5000000: episode: 322, duration: 1.192s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.000 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 232.376511, mean_absolute_error: 91.983147, mean_q: 114.683914\n",
      "   18668/5000000: episode: 323, duration: 1.354s, episode steps: 28, steps per second: 21, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 4.214 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 652.274231, mean_absolute_error: 96.667419, mean_q: 120.498001\n",
      "   18693/5000000: episode: 324, duration: 1.195s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.680 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 430.283325, mean_absolute_error: 96.928352, mean_q: 120.629677\n",
      "   18719/5000000: episode: 325, duration: 1.243s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.115 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 537.378845, mean_absolute_error: 92.617416, mean_q: 115.046028\n",
      "   18747/5000000: episode: 326, duration: 1.323s, episode steps: 28, steps per second: 21, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 3.071 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 462.066315, mean_absolute_error: 95.151749, mean_q: 118.463272\n",
      "   18807/5000000: episode: 327, duration: 2.835s, episode steps: 60, steps per second: 21, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.833 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 704.976013, mean_absolute_error: 99.011597, mean_q: 123.293823\n",
      "   18850/5000000: episode: 328, duration: 2.062s, episode steps: 43, steps per second: 21, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 2.512 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 518.639404, mean_absolute_error: 100.645920, mean_q: 125.149414\n",
      "   18902/5000000: episode: 329, duration: 2.559s, episode steps: 52, steps per second: 20, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 3.096 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 589.261841, mean_absolute_error: 100.163696, mean_q: 124.330902\n",
      "   18936/5000000: episode: 330, duration: 1.647s, episode steps: 34, steps per second: 21, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.794 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 819.966919, mean_absolute_error: 102.782509, mean_q: 127.754684\n",
      "   19139/5000000: episode: 331, duration: 9.540s, episode steps: 203, steps per second: 21, episode reward: -1.000, mean reward: -0.005 [-1.000, 0.000], mean action: 3.108 [0.000, 5.000], mean observation: 0.061 [0.000, 24.000], loss: 595.446716, mean_absolute_error: 99.542076, mean_q: 123.870552\n",
      "   19193/5000000: episode: 332, duration: 2.604s, episode steps: 54, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.815 [0.000, 5.000], mean observation: 0.073 [0.000, 24.000], loss: 657.458252, mean_absolute_error: 99.470314, mean_q: 123.915665\n",
      "   19247/5000000: episode: 333, duration: 2.625s, episode steps: 54, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.685 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 584.918274, mean_absolute_error: 101.798691, mean_q: 126.656075\n",
      "   19273/5000000: episode: 334, duration: 1.333s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.077 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 490.630798, mean_absolute_error: 104.642700, mean_q: 129.886353\n",
      "   19331/5000000: episode: 335, duration: 2.726s, episode steps: 58, steps per second: 21, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 3.103 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 509.748016, mean_absolute_error: 106.064720, mean_q: 131.976044\n",
      "   19411/5000000: episode: 336, duration: 3.910s, episode steps: 80, steps per second: 20, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 3.550 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 624.327271, mean_absolute_error: 106.687378, mean_q: 133.012222\n",
      "   19436/5000000: episode: 337, duration: 1.258s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.760 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 665.174194, mean_absolute_error: 108.110519, mean_q: 134.844757\n",
      "   19727/5000000: episode: 338, duration: 14.210s, episode steps: 291, steps per second: 20, episode reward: -1.000, mean reward: -0.003 [-1.000, 0.000], mean action: 3.357 [0.000, 5.000], mean observation: 0.057 [0.000, 24.000], loss: 565.462708, mean_absolute_error: 105.384384, mean_q: 131.140289\n",
      "   19777/5000000: episode: 339, duration: 2.392s, episode steps: 50, steps per second: 21, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 3.420 [0.000, 5.000], mean observation: 0.076 [0.000, 24.000], loss: 392.970795, mean_absolute_error: 105.839920, mean_q: 132.128723\n",
      "   19803/5000000: episode: 340, duration: 1.173s, episode steps: 26, steps per second: 22, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.615 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1140.444336, mean_absolute_error: 106.113640, mean_q: 131.964951\n",
      "   19830/5000000: episode: 341, duration: 1.371s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 3.778 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 655.174133, mean_absolute_error: 103.941635, mean_q: 129.660233\n",
      "   20000/5000000: episode: 342, duration: 8.127s, episode steps: 170, steps per second: 21, episode reward: -1.000, mean reward: -0.006 [-1.000, 0.000], mean action: 3.594 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 717.775696, mean_absolute_error: 112.225113, mean_q: 139.678421\n",
      "   20032/5000000: episode: 343, duration: 1.567s, episode steps: 32, steps per second: 20, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 4.656 [2.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 478.291077, mean_absolute_error: 108.613686, mean_q: 135.748901\n",
      "   20059/5000000: episode: 344, duration: 1.386s, episode steps: 27, steps per second: 19, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 3.593 [2.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1045.184570, mean_absolute_error: 115.016983, mean_q: 143.291031\n",
      "   20140/5000000: episode: 345, duration: 3.832s, episode steps: 81, steps per second: 21, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 3.556 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 678.806213, mean_absolute_error: 117.437111, mean_q: 146.211807\n",
      "   20166/5000000: episode: 346, duration: 1.237s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.615 [2.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 986.825073, mean_absolute_error: 112.197350, mean_q: 140.614212\n",
      "   20191/5000000: episode: 347, duration: 1.267s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.200 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 826.349060, mean_absolute_error: 115.591072, mean_q: 143.870438\n",
      "   20250/5000000: episode: 348, duration: 2.812s, episode steps: 59, steps per second: 21, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 3.508 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 475.560394, mean_absolute_error: 113.962898, mean_q: 142.231888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   20302/5000000: episode: 349, duration: 2.508s, episode steps: 52, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 3.615 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 636.441101, mean_absolute_error: 115.360748, mean_q: 144.252792\n",
      "   20327/5000000: episode: 350, duration: 1.201s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.200 [2.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 585.786621, mean_absolute_error: 113.541229, mean_q: 141.963806\n",
      "   20353/5000000: episode: 351, duration: 1.375s, episode steps: 26, steps per second: 19, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.615 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 350.845276, mean_absolute_error: 118.375748, mean_q: 148.134018\n",
      "   20378/5000000: episode: 352, duration: 1.254s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.720 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 753.926270, mean_absolute_error: 120.263893, mean_q: 150.483032\n",
      "   20404/5000000: episode: 353, duration: 1.309s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.885 [1.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 557.853516, mean_absolute_error: 113.819687, mean_q: 142.637680\n",
      "   20429/5000000: episode: 354, duration: 1.274s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.720 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 758.462830, mean_absolute_error: 116.912422, mean_q: 146.627640\n",
      "   20454/5000000: episode: 355, duration: 1.226s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 4.360 [2.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 691.892517, mean_absolute_error: 120.410736, mean_q: 150.153442\n",
      "   20479/5000000: episode: 356, duration: 1.228s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 4.880 [2.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 328.674805, mean_absolute_error: 115.047829, mean_q: 143.865280\n",
      "   20531/5000000: episode: 357, duration: 2.504s, episode steps: 52, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 3.115 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 794.128174, mean_absolute_error: 121.335358, mean_q: 151.586411\n",
      "   20558/5000000: episode: 358, duration: 1.289s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 4.593 [1.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1094.702759, mean_absolute_error: 128.137726, mean_q: 160.128799\n",
      "   20608/5000000: episode: 359, duration: 2.341s, episode steps: 50, steps per second: 21, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 3.880 [1.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 1077.387451, mean_absolute_error: 120.214714, mean_q: 150.009674\n",
      "   20660/5000000: episode: 360, duration: 2.423s, episode steps: 52, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 3.827 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 769.573303, mean_absolute_error: 121.972694, mean_q: 152.743378\n",
      "   20687/5000000: episode: 361, duration: 1.303s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 3.667 [2.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 617.903809, mean_absolute_error: 116.167381, mean_q: 144.945145\n",
      "   20713/5000000: episode: 362, duration: 1.304s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.769 [1.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 707.720947, mean_absolute_error: 126.034576, mean_q: 157.091797\n",
      "   20740/5000000: episode: 363, duration: 1.322s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 4.037 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 739.518738, mean_absolute_error: 123.681053, mean_q: 154.558884\n",
      "   20765/5000000: episode: 364, duration: 1.269s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.680 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 663.720703, mean_absolute_error: 125.498924, mean_q: 156.568024\n",
      "   20791/5000000: episode: 365, duration: 1.322s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.385 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 1089.421753, mean_absolute_error: 124.507828, mean_q: 155.685165\n",
      "   20841/5000000: episode: 366, duration: 2.453s, episode steps: 50, steps per second: 20, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 4.040 [1.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1171.744019, mean_absolute_error: 130.269516, mean_q: 162.727310\n",
      "   20866/5000000: episode: 367, duration: 1.231s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.600 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 1120.578857, mean_absolute_error: 129.111725, mean_q: 161.737854\n",
      "   20917/5000000: episode: 368, duration: 2.517s, episode steps: 51, steps per second: 20, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 3.706 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 721.298767, mean_absolute_error: 130.490936, mean_q: 162.840057\n",
      "   20949/5000000: episode: 369, duration: 1.551s, episode steps: 32, steps per second: 21, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 2.719 [1.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 777.358337, mean_absolute_error: 130.599258, mean_q: 162.426117\n",
      "   20977/5000000: episode: 370, duration: 1.456s, episode steps: 28, steps per second: 19, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 3.500 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 539.010010, mean_absolute_error: 129.498795, mean_q: 161.135864\n",
      "   21005/5000000: episode: 371, duration: 1.398s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 3.643 [1.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 732.627380, mean_absolute_error: 132.697479, mean_q: 165.749969\n",
      "   21030/5000000: episode: 372, duration: 1.267s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.600 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 484.031433, mean_absolute_error: 125.334015, mean_q: 155.670181\n",
      "   21056/5000000: episode: 373, duration: 1.297s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.692 [1.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1003.173157, mean_absolute_error: 122.615082, mean_q: 152.603897\n",
      "   21083/5000000: episode: 374, duration: 1.365s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 3.370 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 635.528748, mean_absolute_error: 124.702621, mean_q: 155.630234\n",
      "   21164/5000000: episode: 375, duration: 4.053s, episode steps: 81, steps per second: 20, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 3.753 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 910.829285, mean_absolute_error: 129.784302, mean_q: 161.609268\n",
      "   21222/5000000: episode: 376, duration: 2.753s, episode steps: 58, steps per second: 21, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 3.603 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 716.705872, mean_absolute_error: 128.879440, mean_q: 160.347122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   21252/5000000: episode: 377, duration: 1.411s, episode steps: 30, steps per second: 21, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 4.200 [2.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1596.133057, mean_absolute_error: 130.428925, mean_q: 163.677383\n",
      "   21277/5000000: episode: 378, duration: 1.298s, episode steps: 25, steps per second: 19, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.480 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 1069.050903, mean_absolute_error: 123.204010, mean_q: 154.040558\n",
      "   21305/5000000: episode: 379, duration: 1.338s, episode steps: 28, steps per second: 21, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 3.786 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 840.945129, mean_absolute_error: 126.421303, mean_q: 157.762299\n",
      "   21384/5000000: episode: 380, duration: 4.029s, episode steps: 79, steps per second: 20, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 3.709 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 942.957886, mean_absolute_error: 131.198792, mean_q: 162.955368\n",
      "   21416/5000000: episode: 381, duration: 1.705s, episode steps: 32, steps per second: 19, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 3.500 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 447.081055, mean_absolute_error: 128.481812, mean_q: 159.500671\n",
      "   21442/5000000: episode: 382, duration: 1.314s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.846 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 803.041016, mean_absolute_error: 130.480484, mean_q: 161.679535\n",
      "   21467/5000000: episode: 383, duration: 1.183s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.840 [1.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 428.455444, mean_absolute_error: 128.406891, mean_q: 159.527924\n",
      "   21496/5000000: episode: 384, duration: 1.486s, episode steps: 29, steps per second: 20, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 3.621 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 1224.812988, mean_absolute_error: 127.475327, mean_q: 158.730301\n",
      "   21525/5000000: episode: 385, duration: 1.463s, episode steps: 29, steps per second: 20, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 3.483 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 607.387146, mean_absolute_error: 136.141876, mean_q: 169.991272\n",
      "   21550/5000000: episode: 386, duration: 1.353s, episode steps: 25, steps per second: 18, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.120 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 860.758545, mean_absolute_error: 133.563553, mean_q: 166.261520\n",
      "   21575/5000000: episode: 387, duration: 1.233s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.720 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 949.360596, mean_absolute_error: 140.657516, mean_q: 174.679214\n",
      "   21630/5000000: episode: 388, duration: 2.665s, episode steps: 55, steps per second: 21, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.891 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 771.762756, mean_absolute_error: 131.094360, mean_q: 162.831360\n",
      "   21681/5000000: episode: 389, duration: 2.452s, episode steps: 51, steps per second: 21, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 3.843 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 971.960876, mean_absolute_error: 130.198929, mean_q: 161.800125\n",
      "   21709/5000000: episode: 390, duration: 1.388s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 3.214 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1155.597412, mean_absolute_error: 130.123672, mean_q: 161.258804\n",
      "   21734/5000000: episode: 391, duration: 1.195s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 572.067078, mean_absolute_error: 132.358643, mean_q: 164.683685\n",
      "   21761/5000000: episode: 392, duration: 1.353s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 3.667 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 1658.736694, mean_absolute_error: 133.420181, mean_q: 165.604507\n",
      "   21786/5000000: episode: 393, duration: 1.221s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.800 [1.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 1180.060059, mean_absolute_error: 132.085709, mean_q: 163.754105\n",
      "   21865/5000000: episode: 394, duration: 3.763s, episode steps: 79, steps per second: 21, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 3.747 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 1277.697754, mean_absolute_error: 136.750351, mean_q: 169.796906\n",
      "   21890/5000000: episode: 395, duration: 1.292s, episode steps: 25, steps per second: 19, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.360 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 682.536804, mean_absolute_error: 139.733337, mean_q: 173.448746\n",
      "   21966/5000000: episode: 396, duration: 3.851s, episode steps: 76, steps per second: 20, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 3.763 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 1063.609497, mean_absolute_error: 134.022659, mean_q: 166.334152\n",
      "   21992/5000000: episode: 397, duration: 1.210s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.731 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 922.308350, mean_absolute_error: 138.474716, mean_q: 172.110916\n",
      "   22045/5000000: episode: 398, duration: 2.626s, episode steps: 53, steps per second: 20, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 3.887 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 692.285583, mean_absolute_error: 137.529617, mean_q: 170.862503\n",
      "   22070/5000000: episode: 399, duration: 1.278s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.480 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 1016.704529, mean_absolute_error: 143.968323, mean_q: 178.968704\n",
      "   22095/5000000: episode: 400, duration: 1.273s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.360 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 1812.830322, mean_absolute_error: 138.842285, mean_q: 172.676575\n",
      "   22120/5000000: episode: 401, duration: 1.315s, episode steps: 25, steps per second: 19, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.640 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 1019.849792, mean_absolute_error: 145.186783, mean_q: 180.513947\n",
      "   22146/5000000: episode: 402, duration: 1.242s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.615 [1.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 769.046204, mean_absolute_error: 133.667465, mean_q: 166.312408\n",
      "   22173/5000000: episode: 403, duration: 1.347s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 4.296 [3.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 708.833679, mean_absolute_error: 146.845978, mean_q: 182.222458\n",
      "   22344/5000000: episode: 404, duration: 8.790s, episode steps: 171, steps per second: 19, episode reward: -1.000, mean reward: -0.006 [-1.000, 0.000], mean action: 3.216 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 965.802795, mean_absolute_error: 144.912521, mean_q: 179.092819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   22371/5000000: episode: 405, duration: 1.274s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 3.667 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 617.786804, mean_absolute_error: 143.897583, mean_q: 177.665100\n",
      "   22398/5000000: episode: 406, duration: 1.401s, episode steps: 27, steps per second: 19, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 3.074 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 1747.225830, mean_absolute_error: 153.143173, mean_q: 189.461807\n",
      "   22450/5000000: episode: 407, duration: 2.500s, episode steps: 52, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 3.250 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 822.323242, mean_absolute_error: 137.908005, mean_q: 170.540634\n",
      "   22484/5000000: episode: 408, duration: 1.641s, episode steps: 34, steps per second: 21, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 3.029 [1.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 1319.216309, mean_absolute_error: 145.245056, mean_q: 179.270660\n",
      "   22510/5000000: episode: 409, duration: 1.401s, episode steps: 26, steps per second: 19, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.154 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 2242.251953, mean_absolute_error: 142.693771, mean_q: 176.569275\n",
      "   22550/5000000: episode: 410, duration: 1.898s, episode steps: 40, steps per second: 21, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 2.900 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 1131.247437, mean_absolute_error: 145.574707, mean_q: 179.090485\n",
      "   22576/5000000: episode: 411, duration: 1.325s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.269 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 1180.861450, mean_absolute_error: 146.008362, mean_q: 179.699875\n",
      "   22614/5000000: episode: 412, duration: 1.901s, episode steps: 38, steps per second: 20, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 2.553 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 1446.625977, mean_absolute_error: 140.371979, mean_q: 172.786285\n",
      "   22666/5000000: episode: 413, duration: 2.614s, episode steps: 52, steps per second: 20, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 3.173 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1181.485352, mean_absolute_error: 145.823761, mean_q: 179.426590\n",
      "   22744/5000000: episode: 414, duration: 3.747s, episode steps: 78, steps per second: 21, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 2.859 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 782.401062, mean_absolute_error: 149.624344, mean_q: 184.459946\n",
      "   22771/5000000: episode: 415, duration: 1.430s, episode steps: 27, steps per second: 19, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 3.370 [1.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1361.802490, mean_absolute_error: 159.211044, mean_q: 196.009018\n",
      "   22903/5000000: episode: 416, duration: 6.613s, episode steps: 132, steps per second: 20, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 3.030 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1028.874512, mean_absolute_error: 149.542175, mean_q: 184.189896\n",
      "   22931/5000000: episode: 417, duration: 1.252s, episode steps: 28, steps per second: 22, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.821 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 803.137634, mean_absolute_error: 155.397537, mean_q: 190.295685\n",
      "   23037/5000000: episode: 418, duration: 5.174s, episode steps: 106, steps per second: 20, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 3.387 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 1005.734253, mean_absolute_error: 151.167999, mean_q: 186.171005\n",
      "   23066/5000000: episode: 419, duration: 1.470s, episode steps: 29, steps per second: 20, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 3.138 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 1259.100708, mean_absolute_error: 149.770615, mean_q: 184.417999\n",
      "   23093/5000000: episode: 420, duration: 1.274s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 3.704 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 992.834717, mean_absolute_error: 155.843689, mean_q: 190.607285\n",
      "   23121/5000000: episode: 421, duration: 1.283s, episode steps: 28, steps per second: 22, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 3.821 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1504.475342, mean_absolute_error: 157.007843, mean_q: 193.688080\n",
      "   23173/5000000: episode: 422, duration: 2.401s, episode steps: 52, steps per second: 22, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 3.231 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1055.135986, mean_absolute_error: 154.317139, mean_q: 190.133453\n",
      "   23235/5000000: episode: 423, duration: 2.765s, episode steps: 62, steps per second: 22, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 3.129 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1161.184326, mean_absolute_error: 154.374420, mean_q: 190.478745\n",
      "   23352/5000000: episode: 424, duration: 5.667s, episode steps: 117, steps per second: 21, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 3.137 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 1195.898315, mean_absolute_error: 154.066589, mean_q: 189.899567\n",
      "   23564/5000000: episode: 425, duration: 9.543s, episode steps: 212, steps per second: 22, episode reward: -1.000, mean reward: -0.005 [-1.000, 0.000], mean action: 2.915 [0.000, 5.000], mean observation: 0.062 [0.000, 24.000], loss: 947.922241, mean_absolute_error: 158.406586, mean_q: 194.699402\n",
      "   23703/5000000: episode: 426, duration: 6.401s, episode steps: 139, steps per second: 22, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 3.058 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 963.611755, mean_absolute_error: 160.010101, mean_q: 196.914047\n",
      "   23737/5000000: episode: 427, duration: 1.549s, episode steps: 34, steps per second: 22, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 3.441 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 894.231140, mean_absolute_error: 154.201065, mean_q: 189.419647\n",
      "   23762/5000000: episode: 428, duration: 1.181s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.840 [1.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 1481.336060, mean_absolute_error: 150.258545, mean_q: 185.128159\n",
      "   23788/5000000: episode: 429, duration: 1.402s, episode steps: 26, steps per second: 19, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.462 [1.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 1305.171875, mean_absolute_error: 161.206573, mean_q: 197.911865\n",
      "   23825/5000000: episode: 430, duration: 1.777s, episode steps: 37, steps per second: 21, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 3.405 [1.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 843.754578, mean_absolute_error: 166.049683, mean_q: 203.729401\n",
      "   23862/5000000: episode: 431, duration: 1.759s, episode steps: 37, steps per second: 21, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 2.865 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 908.936157, mean_absolute_error: 165.383560, mean_q: 203.499725\n",
      "   23887/5000000: episode: 432, duration: 1.199s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.200 [1.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 1440.652344, mean_absolute_error: 164.809555, mean_q: 202.406799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   24007/5000000: episode: 433, duration: 5.756s, episode steps: 120, steps per second: 21, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 3.267 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 1112.160156, mean_absolute_error: 165.623703, mean_q: 203.672684\n",
      "   24081/5000000: episode: 434, duration: 3.438s, episode steps: 74, steps per second: 22, episode reward: -1.000, mean reward: -0.014 [-1.000, 0.000], mean action: 3.041 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 1020.163208, mean_absolute_error: 160.404449, mean_q: 197.025177\n",
      "   24146/5000000: episode: 435, duration: 3.013s, episode steps: 65, steps per second: 22, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 3.154 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 1515.223267, mean_absolute_error: 162.066788, mean_q: 199.004593\n",
      "   24203/5000000: episode: 436, duration: 2.792s, episode steps: 57, steps per second: 20, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.947 [1.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1365.055542, mean_absolute_error: 162.953873, mean_q: 200.191574\n",
      "   24230/5000000: episode: 437, duration: 1.269s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 3.296 [1.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 1005.497131, mean_absolute_error: 166.669907, mean_q: 204.853043\n",
      "   24286/5000000: episode: 438, duration: 2.565s, episode steps: 56, steps per second: 22, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 3.589 [1.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 1131.180908, mean_absolute_error: 161.871689, mean_q: 198.768021\n",
      "   24349/5000000: episode: 439, duration: 3.019s, episode steps: 63, steps per second: 21, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 3.238 [1.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1127.790405, mean_absolute_error: 157.539322, mean_q: 194.041138\n",
      "   24378/5000000: episode: 440, duration: 1.417s, episode steps: 29, steps per second: 20, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 3.276 [1.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1322.160400, mean_absolute_error: 166.325867, mean_q: 203.782516\n",
      "   24413/5000000: episode: 441, duration: 1.650s, episode steps: 35, steps per second: 21, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 3.029 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 1118.499023, mean_absolute_error: 161.262817, mean_q: 198.293472\n",
      "   24599/5000000: episode: 442, duration: 9.039s, episode steps: 186, steps per second: 21, episode reward: -1.000, mean reward: -0.005 [-1.000, 0.000], mean action: 3.237 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 1101.126343, mean_absolute_error: 163.880783, mean_q: 201.069489\n",
      "   24635/5000000: episode: 443, duration: 1.615s, episode steps: 36, steps per second: 22, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 3.306 [1.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 986.896912, mean_absolute_error: 169.783997, mean_q: 208.651627\n",
      "   24680/5000000: episode: 444, duration: 2.134s, episode steps: 45, steps per second: 21, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 2.822 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 1194.133667, mean_absolute_error: 158.363205, mean_q: 194.798523\n",
      "   24937/5000000: episode: 445, duration: 12.507s, episode steps: 257, steps per second: 21, episode reward: -1.000, mean reward: -0.004 [-1.000, 0.000], mean action: 3.082 [0.000, 5.000], mean observation: 0.059 [0.000, 24.000], loss: 1394.458618, mean_absolute_error: 170.391006, mean_q: 209.081955\n",
      "   25225/5000000: episode: 446, duration: 13.295s, episode steps: 288, steps per second: 22, episode reward: -1.000, mean reward: -0.003 [-1.000, 0.000], mean action: 2.819 [0.000, 5.000], mean observation: 0.059 [0.000, 24.000], loss: 1678.231445, mean_absolute_error: 173.830078, mean_q: 213.801514\n",
      "   25265/5000000: episode: 447, duration: 1.938s, episode steps: 40, steps per second: 21, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 2.250 [1.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 2003.482422, mean_absolute_error: 181.082916, mean_q: 222.385696\n",
      "   25531/5000000: episode: 448, duration: 12.448s, episode steps: 266, steps per second: 21, episode reward: -1.000, mean reward: -0.004 [-1.000, 0.000], mean action: 2.846 [0.000, 5.000], mean observation: 0.062 [0.000, 24.000], loss: 1675.112793, mean_absolute_error: 178.698044, mean_q: 220.716400\n",
      "   25628/5000000: episode: 449, duration: 4.640s, episode steps: 97, steps per second: 21, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.814 [1.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 1698.530151, mean_absolute_error: 183.546692, mean_q: 226.609482\n",
      "   25693/5000000: episode: 450, duration: 2.910s, episode steps: 65, steps per second: 22, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 2.862 [1.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 2053.072754, mean_absolute_error: 178.589966, mean_q: 221.252762\n",
      "   25775/5000000: episode: 451, duration: 3.988s, episode steps: 82, steps per second: 21, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.598 [1.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 1555.148438, mean_absolute_error: 187.881271, mean_q: 232.931717\n",
      "   26199/5000000: episode: 452, duration: 21.028s, episode steps: 424, steps per second: 20, episode reward: -1.000, mean reward: -0.002 [-1.000, 0.000], mean action: 2.929 [1.000, 5.000], mean observation: 0.052 [0.000, 24.000], loss: 1743.160645, mean_absolute_error: 186.618927, mean_q: 232.390686\n",
      "   26246/5000000: episode: 453, duration: 2.262s, episode steps: 47, steps per second: 21, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 3.021 [1.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 1961.695801, mean_absolute_error: 193.640091, mean_q: 240.530975\n",
      "   26276/5000000: episode: 454, duration: 1.449s, episode steps: 30, steps per second: 21, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.900 [2.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 2792.950439, mean_absolute_error: 199.335312, mean_q: 249.250534\n",
      "   26334/5000000: episode: 455, duration: 2.711s, episode steps: 58, steps per second: 21, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 3.224 [1.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 1909.345825, mean_absolute_error: 193.402191, mean_q: 242.002060\n",
      "   26587/5000000: episode: 456, duration: 12.322s, episode steps: 253, steps per second: 21, episode reward: -1.000, mean reward: -0.004 [-1.000, 0.000], mean action: 2.964 [1.000, 5.000], mean observation: 0.055 [0.000, 24.000], loss: 1598.512085, mean_absolute_error: 198.554321, mean_q: 248.108444\n",
      "   26704/5000000: episode: 457, duration: 5.502s, episode steps: 117, steps per second: 21, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.991 [1.000, 5.000], mean observation: 0.059 [0.000, 24.000], loss: 2080.464600, mean_absolute_error: 198.702377, mean_q: 248.707306\n",
      "   26759/5000000: episode: 458, duration: 2.829s, episode steps: 55, steps per second: 19, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.982 [1.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 1754.455078, mean_absolute_error: 208.235580, mean_q: 260.242340\n",
      "   26793/5000000: episode: 459, duration: 1.674s, episode steps: 34, steps per second: 20, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.500 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 1630.108887, mean_absolute_error: 210.347916, mean_q: 263.365723\n",
      "   26835/5000000: episode: 460, duration: 1.994s, episode steps: 42, steps per second: 21, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 2.810 [1.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1684.551147, mean_absolute_error: 212.805740, mean_q: 266.034180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   26884/5000000: episode: 461, duration: 2.402s, episode steps: 49, steps per second: 20, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 3.000 [1.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 1573.771851, mean_absolute_error: 209.784119, mean_q: 262.462799\n",
      "   26928/5000000: episode: 462, duration: 2.140s, episode steps: 44, steps per second: 21, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 2.682 [1.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 1363.203247, mean_absolute_error: 204.600296, mean_q: 256.191132\n",
      "   26971/5000000: episode: 463, duration: 2.033s, episode steps: 43, steps per second: 21, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 3.140 [1.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 2178.035889, mean_absolute_error: 223.022888, mean_q: 277.506927\n",
      "   27067/5000000: episode: 464, duration: 4.576s, episode steps: 96, steps per second: 21, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.542 [1.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 1880.088501, mean_absolute_error: 214.230148, mean_q: 266.897339\n",
      "   27124/5000000: episode: 465, duration: 2.798s, episode steps: 57, steps per second: 20, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.807 [1.000, 5.000], mean observation: 0.060 [0.000, 24.000], loss: 1513.767090, mean_absolute_error: 219.467072, mean_q: 274.294434\n",
      "   27188/5000000: episode: 466, duration: 3.172s, episode steps: 64, steps per second: 20, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 2.562 [1.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 1765.571533, mean_absolute_error: 223.358047, mean_q: 278.253784\n",
      "   27250/5000000: episode: 467, duration: 3.031s, episode steps: 62, steps per second: 20, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 2.839 [1.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 1835.507324, mean_absolute_error: 215.269974, mean_q: 267.607452\n",
      "   27292/5000000: episode: 468, duration: 1.932s, episode steps: 42, steps per second: 22, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 2.810 [1.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 1530.529419, mean_absolute_error: 221.540787, mean_q: 276.118744\n",
      "   27391/5000000: episode: 469, duration: 4.867s, episode steps: 99, steps per second: 20, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.616 [1.000, 5.000], mean observation: 0.062 [0.000, 24.000], loss: 1737.290405, mean_absolute_error: 215.111740, mean_q: 267.695740\n",
      "   27440/5000000: episode: 470, duration: 2.380s, episode steps: 49, steps per second: 21, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 2.776 [1.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 1732.589600, mean_absolute_error: 224.188324, mean_q: 279.017883\n",
      "   27468/5000000: episode: 471, duration: 1.407s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.179 [2.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1195.737549, mean_absolute_error: 224.091400, mean_q: 277.994812\n",
      "   27533/5000000: episode: 472, duration: 3.305s, episode steps: 65, steps per second: 20, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 2.477 [1.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 1431.199219, mean_absolute_error: 214.490601, mean_q: 266.280426\n",
      "   27580/5000000: episode: 473, duration: 2.342s, episode steps: 47, steps per second: 20, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 2.277 [1.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1521.559692, mean_absolute_error: 232.374069, mean_q: 288.271393\n",
      "   27608/5000000: episode: 474, duration: 1.411s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.143 [1.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 1335.455811, mean_absolute_error: 228.264252, mean_q: 281.058228\n",
      "   27660/5000000: episode: 475, duration: 2.489s, episode steps: 52, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.423 [1.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 1800.072021, mean_absolute_error: 226.928574, mean_q: 281.672882\n",
      "   27754/5000000: episode: 476, duration: 4.523s, episode steps: 94, steps per second: 21, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.660 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 1545.997314, mean_absolute_error: 228.548294, mean_q: 283.676056\n",
      "   27782/5000000: episode: 477, duration: 1.391s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.179 [1.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 2198.697021, mean_absolute_error: 235.431885, mean_q: 292.144318\n",
      "   27897/5000000: episode: 478, duration: 5.496s, episode steps: 115, steps per second: 21, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.513 [0.000, 5.000], mean observation: 0.062 [0.000, 24.000], loss: 1871.104858, mean_absolute_error: 230.141037, mean_q: 285.702911\n",
      "   27957/5000000: episode: 479, duration: 2.904s, episode steps: 60, steps per second: 21, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.150 [1.000, 5.000], mean observation: 0.061 [0.000, 24.000], loss: 1467.496460, mean_absolute_error: 237.726166, mean_q: 294.179382\n",
      "   28006/5000000: episode: 480, duration: 2.271s, episode steps: 49, steps per second: 22, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 2.306 [1.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 1730.673950, mean_absolute_error: 224.616364, mean_q: 278.874847\n",
      "   28045/5000000: episode: 481, duration: 1.868s, episode steps: 39, steps per second: 21, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 2.026 [1.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 1473.660156, mean_absolute_error: 221.123001, mean_q: 273.524750\n",
      "   28273/5000000: episode: 482, duration: 11.129s, episode steps: 228, steps per second: 20, episode reward: -1.000, mean reward: -0.004 [-1.000, 0.000], mean action: 1.961 [1.000, 5.000], mean observation: 0.059 [0.000, 24.000], loss: 1730.614258, mean_absolute_error: 233.132751, mean_q: 288.611023\n",
      "   28382/5000000: episode: 483, duration: 5.092s, episode steps: 109, steps per second: 21, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.248 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 1658.497803, mean_absolute_error: 235.680634, mean_q: 292.168213\n",
      "   28459/5000000: episode: 484, duration: 3.726s, episode steps: 77, steps per second: 21, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 1.948 [1.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 1936.527222, mean_absolute_error: 240.403687, mean_q: 297.874939\n",
      "   28542/5000000: episode: 485, duration: 3.831s, episode steps: 83, steps per second: 22, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 1.819 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 1857.748657, mean_absolute_error: 242.743927, mean_q: 301.155182\n",
      "   28569/5000000: episode: 486, duration: 1.245s, episode steps: 27, steps per second: 22, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 3.074 [1.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 2126.134766, mean_absolute_error: 243.726837, mean_q: 301.454132\n",
      "   28745/5000000: episode: 487, duration: 8.422s, episode steps: 176, steps per second: 21, episode reward: -1.000, mean reward: -0.006 [-1.000, 0.000], mean action: 1.790 [1.000, 5.000], mean observation: 0.061 [0.000, 24.000], loss: 1567.643066, mean_absolute_error: 247.915421, mean_q: 307.271942\n",
      "   28957/5000000: episode: 488, duration: 9.996s, episode steps: 212, steps per second: 21, episode reward: -1.000, mean reward: -0.005 [-1.000, 0.000], mean action: 2.358 [1.000, 5.000], mean observation: 0.057 [0.000, 24.000], loss: 1693.364136, mean_absolute_error: 244.181274, mean_q: 302.366425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   29003/5000000: episode: 489, duration: 2.142s, episode steps: 46, steps per second: 21, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 2.196 [1.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 1644.252563, mean_absolute_error: 253.298599, mean_q: 313.223602\n",
      "   29042/5000000: episode: 490, duration: 1.863s, episode steps: 39, steps per second: 21, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 3.077 [1.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1629.859253, mean_absolute_error: 256.956146, mean_q: 318.221497\n",
      "   29091/5000000: episode: 491, duration: 2.456s, episode steps: 49, steps per second: 20, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 2.265 [1.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 1703.216187, mean_absolute_error: 264.351471, mean_q: 327.443359\n",
      "   29172/5000000: episode: 492, duration: 4.014s, episode steps: 81, steps per second: 20, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.247 [1.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1874.630615, mean_absolute_error: 254.674149, mean_q: 315.275970\n",
      "   29232/5000000: episode: 493, duration: 2.821s, episode steps: 60, steps per second: 21, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.683 [1.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 1872.056030, mean_absolute_error: 249.419464, mean_q: 308.858215\n",
      "   29257/5000000: episode: 494, duration: 1.157s, episode steps: 25, steps per second: 22, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.640 [1.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 2058.692627, mean_absolute_error: 248.745255, mean_q: 307.931335\n",
      "   29387/5000000: episode: 495, duration: 6.240s, episode steps: 130, steps per second: 21, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 1.969 [1.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 1874.000000, mean_absolute_error: 252.192963, mean_q: 311.670654\n",
      "   29523/5000000: episode: 496, duration: 6.614s, episode steps: 136, steps per second: 21, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 2.368 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 1881.109375, mean_absolute_error: 252.786758, mean_q: 312.887756\n",
      "   29600/5000000: episode: 497, duration: 3.570s, episode steps: 77, steps per second: 22, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 3.416 [1.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 2032.461060, mean_absolute_error: 273.833313, mean_q: 338.276276\n",
      "   29648/5000000: episode: 498, duration: 2.354s, episode steps: 48, steps per second: 20, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 2.979 [1.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 1819.859985, mean_absolute_error: 257.545319, mean_q: 318.074371\n",
      "   29676/5000000: episode: 499, duration: 1.323s, episode steps: 28, steps per second: 21, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.071 [1.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1927.396606, mean_absolute_error: 260.684296, mean_q: 322.268982\n",
      "   29717/5000000: episode: 500, duration: 1.955s, episode steps: 41, steps per second: 21, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 2.683 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 2150.261719, mean_absolute_error: 272.529663, mean_q: 336.388458\n",
      "   29825/5000000: episode: 501, duration: 5.489s, episode steps: 108, steps per second: 20, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.611 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 1812.871216, mean_absolute_error: 264.061249, mean_q: 326.564484\n",
      "   29990/5000000: episode: 502, duration: 7.939s, episode steps: 165, steps per second: 21, episode reward: -1.000, mean reward: -0.006 [-1.000, 0.000], mean action: 2.333 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 2029.507568, mean_absolute_error: 261.997833, mean_q: 323.486298\n",
      "   30045/5000000: episode: 503, duration: 2.402s, episode steps: 55, steps per second: 23, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.855 [1.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 2602.954346, mean_absolute_error: 274.372437, mean_q: 339.002594\n",
      "   30070/5000000: episode: 504, duration: 1.131s, episode steps: 25, steps per second: 22, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.840 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 2033.740723, mean_absolute_error: 269.075592, mean_q: 332.836395\n",
      "   30102/5000000: episode: 505, duration: 1.539s, episode steps: 32, steps per second: 21, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 3.750 [2.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 2163.304688, mean_absolute_error: 279.067017, mean_q: 346.012939\n",
      "   30132/5000000: episode: 506, duration: 1.546s, episode steps: 30, steps per second: 19, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 3.267 [1.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 2027.303589, mean_absolute_error: 258.246979, mean_q: 320.020660\n",
      "   30174/5000000: episode: 507, duration: 1.976s, episode steps: 42, steps per second: 21, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 2.262 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 2496.502930, mean_absolute_error: 268.451324, mean_q: 332.619598\n",
      "   30226/5000000: episode: 508, duration: 2.465s, episode steps: 52, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.846 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 2670.902100, mean_absolute_error: 271.440308, mean_q: 335.129089\n",
      "   30251/5000000: episode: 509, duration: 1.122s, episode steps: 25, steps per second: 22, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.240 [1.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 2360.952881, mean_absolute_error: 279.852539, mean_q: 345.536438\n",
      "   30279/5000000: episode: 510, duration: 1.369s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 4.357 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 2440.749512, mean_absolute_error: 276.677826, mean_q: 342.040070\n",
      "   30339/5000000: episode: 511, duration: 2.895s, episode steps: 60, steps per second: 21, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.833 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 2392.395752, mean_absolute_error: 274.272888, mean_q: 339.090302\n",
      "   30364/5000000: episode: 512, duration: 1.275s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.520 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 1990.911621, mean_absolute_error: 291.990479, mean_q: 361.405823\n",
      "   30416/5000000: episode: 513, duration: 2.375s, episode steps: 52, steps per second: 22, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 3.154 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 2478.116943, mean_absolute_error: 263.204590, mean_q: 326.223114\n",
      "   30441/5000000: episode: 514, duration: 1.273s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.000 [1.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 3138.494629, mean_absolute_error: 269.988861, mean_q: 333.698212\n",
      "   30621/5000000: episode: 515, duration: 8.713s, episode steps: 180, steps per second: 21, episode reward: -1.000, mean reward: -0.006 [-1.000, 0.000], mean action: 2.422 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 3158.944336, mean_absolute_error: 282.463531, mean_q: 349.719604\n",
      "   30679/5000000: episode: 516, duration: 2.745s, episode steps: 58, steps per second: 21, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.793 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 3414.663574, mean_absolute_error: 282.279175, mean_q: 349.188232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   30751/5000000: episode: 517, duration: 3.550s, episode steps: 72, steps per second: 20, episode reward: -1.000, mean reward: -0.014 [-1.000, 0.000], mean action: 2.764 [0.000, 5.000], mean observation: 0.061 [0.000, 24.000], loss: 3684.555664, mean_absolute_error: 274.959900, mean_q: 340.382202\n",
      "   30807/5000000: episode: 518, duration: 2.676s, episode steps: 56, steps per second: 21, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 3.000 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 4200.188477, mean_absolute_error: 294.019318, mean_q: 364.547455\n",
      "   31039/5000000: episode: 519, duration: 11.284s, episode steps: 232, steps per second: 21, episode reward: -1.000, mean reward: -0.004 [-1.000, 0.000], mean action: 2.603 [0.000, 5.000], mean observation: 0.057 [0.000, 24.000], loss: 3214.854980, mean_absolute_error: 298.390167, mean_q: 369.252411\n",
      "   31075/5000000: episode: 520, duration: 1.773s, episode steps: 36, steps per second: 20, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 2.889 [2.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 4195.730469, mean_absolute_error: 322.426849, mean_q: 400.159302\n",
      "   31104/5000000: episode: 521, duration: 1.444s, episode steps: 29, steps per second: 20, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 3.759 [1.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 2935.887451, mean_absolute_error: 301.036041, mean_q: 373.576080\n",
      "   31130/5000000: episode: 522, duration: 1.153s, episode steps: 26, steps per second: 23, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 4.769 [1.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 3568.527588, mean_absolute_error: 296.847809, mean_q: 367.156067\n",
      "   31204/5000000: episode: 523, duration: 3.609s, episode steps: 74, steps per second: 21, episode reward: -1.000, mean reward: -0.014 [-1.000, 0.000], mean action: 2.581 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 3047.437256, mean_absolute_error: 304.923065, mean_q: 378.063171\n",
      "   31233/5000000: episode: 524, duration: 1.430s, episode steps: 29, steps per second: 20, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 4.379 [2.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 2520.245605, mean_absolute_error: 301.495514, mean_q: 374.752930\n",
      "   31308/5000000: episode: 525, duration: 3.650s, episode steps: 75, steps per second: 21, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 2.760 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 3437.742188, mean_absolute_error: 301.803619, mean_q: 374.540253\n",
      "   31335/5000000: episode: 526, duration: 1.238s, episode steps: 27, steps per second: 22, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 3.815 [2.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 3157.883789, mean_absolute_error: 321.928467, mean_q: 398.488098\n",
      "   31375/5000000: episode: 527, duration: 1.902s, episode steps: 40, steps per second: 21, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 2.850 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 3251.100342, mean_absolute_error: 302.236267, mean_q: 375.362091\n",
      "   31493/5000000: episode: 528, duration: 5.411s, episode steps: 118, steps per second: 22, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.873 [0.000, 5.000], mean observation: 0.060 [0.000, 24.000], loss: 3222.018555, mean_absolute_error: 304.202698, mean_q: 377.532135\n",
      "   31521/5000000: episode: 529, duration: 1.324s, episode steps: 28, steps per second: 21, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.714 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 3970.312744, mean_absolute_error: 304.377930, mean_q: 377.894714\n",
      "   31601/5000000: episode: 530, duration: 3.768s, episode steps: 80, steps per second: 21, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 2.850 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 3502.976562, mean_absolute_error: 314.516205, mean_q: 390.121826\n",
      "   31886/5000000: episode: 531, duration: 14.248s, episode steps: 285, steps per second: 20, episode reward: -1.000, mean reward: -0.004 [-1.000, 0.000], mean action: 2.825 [0.000, 5.000], mean observation: 0.052 [0.000, 24.000], loss: 3349.118896, mean_absolute_error: 318.402893, mean_q: 394.882141\n",
      "   31914/5000000: episode: 532, duration: 1.265s, episode steps: 28, steps per second: 22, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 3.071 [2.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 3466.139404, mean_absolute_error: 328.588959, mean_q: 408.376526\n",
      "   31942/5000000: episode: 533, duration: 1.419s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 3.500 [2.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 3334.321777, mean_absolute_error: 335.787323, mean_q: 417.699219\n",
      "   31971/5000000: episode: 534, duration: 1.351s, episode steps: 29, steps per second: 21, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 3.241 [2.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 3388.887207, mean_absolute_error: 339.245453, mean_q: 420.209229\n",
      "   32035/5000000: episode: 535, duration: 2.983s, episode steps: 64, steps per second: 21, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 3.062 [2.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 3062.120605, mean_absolute_error: 326.178833, mean_q: 404.555145\n",
      "   32141/5000000: episode: 536, duration: 5.121s, episode steps: 106, steps per second: 21, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.783 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 3625.102295, mean_absolute_error: 331.795837, mean_q: 411.247620\n",
      "   32170/5000000: episode: 537, duration: 1.382s, episode steps: 29, steps per second: 21, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.862 [2.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 3419.258545, mean_absolute_error: 348.712708, mean_q: 431.373505\n",
      "   32197/5000000: episode: 538, duration: 1.156s, episode steps: 27, steps per second: 23, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 3.000 [2.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 3013.827637, mean_absolute_error: 326.094604, mean_q: 403.915009\n",
      "   32305/5000000: episode: 539, duration: 4.940s, episode steps: 108, steps per second: 22, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.620 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 3718.157715, mean_absolute_error: 340.913879, mean_q: 423.206390\n",
      "   32331/5000000: episode: 540, duration: 1.334s, episode steps: 26, steps per second: 19, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 4.154 [2.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 3497.158691, mean_absolute_error: 333.032532, mean_q: 413.610046\n",
      "   32381/5000000: episode: 541, duration: 2.253s, episode steps: 50, steps per second: 22, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 2.960 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 4410.586426, mean_absolute_error: 335.997467, mean_q: 416.857574\n",
      "   32425/5000000: episode: 542, duration: 2.094s, episode steps: 44, steps per second: 21, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 2.886 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 3617.471191, mean_absolute_error: 346.266449, mean_q: 429.197571\n",
      "   32697/5000000: episode: 543, duration: 12.412s, episode steps: 272, steps per second: 22, episode reward: 1.000, mean reward: 0.004 [0.000, 1.000], mean action: 2.963 [0.000, 4.000], mean observation: 0.057 [0.000, 24.000], loss: 4582.315430, mean_absolute_error: 352.268768, mean_q: 437.185516\n",
      "   32731/5000000: episode: 544, duration: 1.754s, episode steps: 34, steps per second: 19, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.676 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 4748.140137, mean_absolute_error: 348.717438, mean_q: 432.954742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   32777/5000000: episode: 545, duration: 2.282s, episode steps: 46, steps per second: 20, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 3.065 [3.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 4794.031250, mean_absolute_error: 349.426727, mean_q: 433.326172\n",
      "   32810/5000000: episode: 546, duration: 1.485s, episode steps: 33, steps per second: 22, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 3.606 [2.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 4302.381348, mean_absolute_error: 348.797058, mean_q: 433.444611\n",
      "   32837/5000000: episode: 547, duration: 1.412s, episode steps: 27, steps per second: 19, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 3.000 [2.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 3856.890869, mean_absolute_error: 357.137299, mean_q: 443.054657\n",
      "   32868/5000000: episode: 548, duration: 1.493s, episode steps: 31, steps per second: 21, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 2.226 [1.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 5104.282227, mean_absolute_error: 366.226715, mean_q: 454.850800\n",
      "   32898/5000000: episode: 549, duration: 1.491s, episode steps: 30, steps per second: 20, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.367 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 4457.307129, mean_absolute_error: 370.948395, mean_q: 459.545776\n",
      "   32924/5000000: episode: 550, duration: 1.234s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 4.154 [3.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 4528.652832, mean_absolute_error: 381.967010, mean_q: 474.447327\n",
      "   32952/5000000: episode: 551, duration: 1.355s, episode steps: 28, steps per second: 21, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 4.286 [3.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 4361.469238, mean_absolute_error: 351.838287, mean_q: 436.645416\n",
      "   33031/5000000: episode: 552, duration: 13.778s, episode steps: 79, steps per second: 6, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 3.177 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 3921.808105, mean_absolute_error: 344.942047, mean_q: 427.978455\n",
      "   33060/5000000: episode: 553, duration: 1.339s, episode steps: 29, steps per second: 22, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 3.759 [2.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 4550.726074, mean_absolute_error: 368.084320, mean_q: 457.041382\n",
      "   33092/5000000: episode: 554, duration: 1.599s, episode steps: 32, steps per second: 20, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 3.031 [2.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 3914.429443, mean_absolute_error: 353.172546, mean_q: 439.517670\n",
      "   33124/5000000: episode: 555, duration: 1.573s, episode steps: 32, steps per second: 20, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 2.750 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 4333.254883, mean_absolute_error: 372.164917, mean_q: 462.492126\n",
      "   33215/5000000: episode: 556, duration: 4.377s, episode steps: 91, steps per second: 21, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.681 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 5622.544434, mean_absolute_error: 374.132782, mean_q: 464.149689\n",
      "   33242/5000000: episode: 557, duration: 1.318s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 3.111 [3.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 6081.780273, mean_absolute_error: 389.402283, mean_q: 482.885925\n",
      "   33269/5000000: episode: 558, duration: 1.358s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 3.148 [3.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 5186.825684, mean_absolute_error: 377.104156, mean_q: 467.312439\n",
      "   33325/5000000: episode: 559, duration: 2.624s, episode steps: 56, steps per second: 21, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 3.089 [2.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 5385.100586, mean_absolute_error: 385.842010, mean_q: 478.773590\n",
      "   33350/5000000: episode: 560, duration: 1.205s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.600 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 6358.580566, mean_absolute_error: 409.106964, mean_q: 508.649445\n",
      "   33404/5000000: episode: 561, duration: 2.591s, episode steps: 54, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 3.407 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 5390.006836, mean_absolute_error: 378.559357, mean_q: 469.945251\n",
      "   33433/5000000: episode: 562, duration: 1.447s, episode steps: 29, steps per second: 20, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 3.103 [3.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 5966.045898, mean_absolute_error: 383.169952, mean_q: 475.749237\n",
      "   33460/5000000: episode: 563, duration: 1.351s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 4.407 [2.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 7551.403809, mean_absolute_error: 393.290771, mean_q: 487.227814\n",
      "   33488/5000000: episode: 564, duration: 1.382s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 3.786 [1.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 5241.020508, mean_absolute_error: 397.390198, mean_q: 493.016510\n",
      "   33516/5000000: episode: 565, duration: 1.492s, episode steps: 28, steps per second: 19, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 4.464 [3.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 7003.090820, mean_absolute_error: 378.293640, mean_q: 468.330597\n",
      "   33547/5000000: episode: 566, duration: 1.430s, episode steps: 31, steps per second: 22, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 3.129 [3.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 7116.637207, mean_absolute_error: 394.725159, mean_q: 490.330017\n",
      "   33575/5000000: episode: 567, duration: 1.324s, episode steps: 28, steps per second: 21, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 3.500 [1.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 6267.512695, mean_absolute_error: 398.714294, mean_q: 494.383636\n",
      "   33601/5000000: episode: 568, duration: 1.273s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.077 [3.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 5823.013184, mean_absolute_error: 388.572083, mean_q: 480.576874\n",
      "   33658/5000000: episode: 569, duration: 2.744s, episode steps: 57, steps per second: 21, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 3.456 [2.000, 5.000], mean observation: 0.073 [0.000, 24.000], loss: 6153.922852, mean_absolute_error: 407.727966, mean_q: 506.314972\n",
      "   33685/5000000: episode: 570, duration: 1.357s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 3.370 [3.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 6289.104980, mean_absolute_error: 409.772095, mean_q: 507.533417\n",
      "   33711/5000000: episode: 571, duration: 1.251s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.692 [3.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 5935.423828, mean_absolute_error: 386.083801, mean_q: 478.689667\n",
      "   33739/5000000: episode: 572, duration: 1.444s, episode steps: 28, steps per second: 19, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 3.071 [3.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 5724.919434, mean_absolute_error: 415.858093, mean_q: 513.822815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   33829/5000000: episode: 573, duration: 4.329s, episode steps: 90, steps per second: 21, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 3.044 [3.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 5696.390625, mean_absolute_error: 385.186981, mean_q: 477.171753\n",
      "   33900/5000000: episode: 574, duration: 3.305s, episode steps: 71, steps per second: 21, episode reward: -1.000, mean reward: -0.014 [-1.000, 0.000], mean action: 3.352 [1.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 5359.159668, mean_absolute_error: 405.280457, mean_q: 501.617126\n",
      "   33929/5000000: episode: 575, duration: 1.390s, episode steps: 29, steps per second: 21, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 4.552 [2.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 5906.999023, mean_absolute_error: 430.046204, mean_q: 532.078979\n",
      "   33954/5000000: episode: 576, duration: 1.229s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.280 [1.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 4764.312500, mean_absolute_error: 401.571716, mean_q: 497.172729\n",
      "   34035/5000000: episode: 577, duration: 4.075s, episode steps: 81, steps per second: 20, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.358 [1.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 6646.938477, mean_absolute_error: 404.591003, mean_q: 501.082764\n",
      "   34061/5000000: episode: 578, duration: 1.265s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.115 [3.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 6927.727539, mean_absolute_error: 437.386719, mean_q: 541.759155\n",
      "   34092/5000000: episode: 579, duration: 1.491s, episode steps: 31, steps per second: 21, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 2.903 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 5705.037598, mean_absolute_error: 415.503723, mean_q: 514.584595\n",
      "   34120/5000000: episode: 580, duration: 1.295s, episode steps: 28, steps per second: 22, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 3.500 [3.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 5234.290527, mean_absolute_error: 428.933289, mean_q: 531.604858\n",
      "   34219/5000000: episode: 581, duration: 4.747s, episode steps: 99, steps per second: 21, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.758 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 6581.689941, mean_absolute_error: 428.851837, mean_q: 531.536438\n",
      "   34254/5000000: episode: 582, duration: 1.667s, episode steps: 35, steps per second: 21, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 3.171 [3.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 6701.313477, mean_absolute_error: 420.378754, mean_q: 520.388062\n",
      "   34319/5000000: episode: 583, duration: 3.291s, episode steps: 65, steps per second: 20, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 3.646 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 6212.037109, mean_absolute_error: 405.249023, mean_q: 501.710724\n",
      "   34345/5000000: episode: 584, duration: 1.280s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.423 [1.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 6579.041504, mean_absolute_error: 420.848175, mean_q: 521.096252\n",
      "   34371/5000000: episode: 585, duration: 1.195s, episode steps: 26, steps per second: 22, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.962 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 6776.223145, mean_absolute_error: 421.447235, mean_q: 521.722107\n",
      "   34404/5000000: episode: 586, duration: 1.601s, episode steps: 33, steps per second: 21, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 3.303 [3.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 8224.432617, mean_absolute_error: 416.385468, mean_q: 515.201050\n",
      "   34627/5000000: episode: 587, duration: 10.417s, episode steps: 223, steps per second: 21, episode reward: -1.000, mean reward: -0.004 [-1.000, 0.000], mean action: 3.004 [0.000, 5.000], mean observation: 0.060 [0.000, 24.000], loss: 6788.368164, mean_absolute_error: 434.418030, mean_q: 538.017883\n",
      "   34658/5000000: episode: 588, duration: 1.488s, episode steps: 31, steps per second: 21, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 4.355 [1.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 7668.364258, mean_absolute_error: 389.229797, mean_q: 480.892853\n",
      "   34684/5000000: episode: 589, duration: 1.217s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.154 [3.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 8145.890625, mean_absolute_error: 439.702118, mean_q: 543.211060\n",
      "   34714/5000000: episode: 590, duration: 1.441s, episode steps: 30, steps per second: 21, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.967 [1.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 7792.164062, mean_absolute_error: 423.080627, mean_q: 524.131104\n",
      "   34741/5000000: episode: 591, duration: 1.289s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 3.519 [3.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 8085.901855, mean_absolute_error: 426.156708, mean_q: 525.755249\n",
      "   34768/5000000: episode: 592, duration: 1.254s, episode steps: 27, steps per second: 22, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 3.630 [2.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 8104.104980, mean_absolute_error: 458.216980, mean_q: 568.791565\n",
      "   34793/5000000: episode: 593, duration: 1.237s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.160 [3.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 6319.433105, mean_absolute_error: 438.070068, mean_q: 543.328308\n",
      "   34822/5000000: episode: 594, duration: 1.312s, episode steps: 29, steps per second: 22, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 3.069 [3.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 8355.361328, mean_absolute_error: 440.481201, mean_q: 546.872559\n",
      "   34855/5000000: episode: 595, duration: 1.484s, episode steps: 33, steps per second: 22, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 3.455 [3.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 6187.436035, mean_absolute_error: 440.945862, mean_q: 544.911316\n",
      "   34880/5000000: episode: 596, duration: 1.136s, episode steps: 25, steps per second: 22, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.440 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 8004.034180, mean_absolute_error: 494.880951, mean_q: 612.530945\n",
      "   34912/5000000: episode: 597, duration: 1.642s, episode steps: 32, steps per second: 19, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 3.062 [2.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 6295.765625, mean_absolute_error: 454.457275, mean_q: 561.759644\n",
      "   34988/5000000: episode: 598, duration: 3.664s, episode steps: 76, steps per second: 21, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 3.092 [3.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 6021.261719, mean_absolute_error: 434.448853, mean_q: 537.434021\n",
      "   35094/5000000: episode: 599, duration: 4.927s, episode steps: 106, steps per second: 22, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 3.104 [3.000, 5.000], mean observation: 0.058 [0.000, 24.000], loss: 7018.118164, mean_absolute_error: 451.067444, mean_q: 557.362366\n",
      "   35120/5000000: episode: 600, duration: 1.235s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.692 [1.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 7456.226562, mean_absolute_error: 434.167511, mean_q: 536.311218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   35145/5000000: episode: 601, duration: 1.224s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.080 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 6949.111328, mean_absolute_error: 444.594147, mean_q: 549.210022\n",
      "   35290/5000000: episode: 602, duration: 6.977s, episode steps: 145, steps per second: 21, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 3.076 [3.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 7898.523438, mean_absolute_error: 448.815033, mean_q: 554.024353\n",
      "   35343/5000000: episode: 603, duration: 2.517s, episode steps: 53, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 3.151 [1.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 8270.734375, mean_absolute_error: 452.706024, mean_q: 558.946594\n",
      "   35369/5000000: episode: 604, duration: 1.223s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.462 [3.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 7887.276855, mean_absolute_error: 455.592804, mean_q: 561.809509\n",
      "   35395/5000000: episode: 605, duration: 1.284s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.731 [3.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 10797.233398, mean_absolute_error: 446.639435, mean_q: 550.900635\n",
      "   35423/5000000: episode: 606, duration: 1.370s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 3.071 [3.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 9278.434570, mean_absolute_error: 451.473541, mean_q: 557.173035\n",
      "   35643/5000000: episode: 607, duration: 10.403s, episode steps: 220, steps per second: 21, episode reward: -1.000, mean reward: -0.005 [-1.000, 0.000], mean action: 3.005 [0.000, 5.000], mean observation: 0.055 [0.000, 24.000], loss: 8351.627930, mean_absolute_error: 468.873505, mean_q: 578.766602\n",
      "   35672/5000000: episode: 608, duration: 1.365s, episode steps: 29, steps per second: 21, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 3.103 [3.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 9549.501953, mean_absolute_error: 450.805939, mean_q: 556.556580\n",
      "   35700/5000000: episode: 609, duration: 1.281s, episode steps: 28, steps per second: 22, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 3.357 [3.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 7619.108398, mean_absolute_error: 469.976959, mean_q: 580.898071\n",
      "   35755/5000000: episode: 610, duration: 2.587s, episode steps: 55, steps per second: 21, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.673 [1.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 7166.294922, mean_absolute_error: 477.079468, mean_q: 589.083984\n",
      "   35781/5000000: episode: 611, duration: 1.306s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.385 [1.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 9627.204102, mean_absolute_error: 477.201843, mean_q: 589.453796\n",
      "   35834/5000000: episode: 612, duration: 2.376s, episode steps: 53, steps per second: 22, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 3.113 [2.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 7241.211914, mean_absolute_error: 476.576508, mean_q: 588.806335\n",
      "   35881/5000000: episode: 613, duration: 2.194s, episode steps: 47, steps per second: 21, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 3.170 [1.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 9729.090820, mean_absolute_error: 473.922089, mean_q: 585.596863\n",
      "   36178/5000000: episode: 614, duration: 13.168s, episode steps: 297, steps per second: 23, episode reward: -1.000, mean reward: -0.003 [-1.000, 0.000], mean action: 3.013 [1.000, 5.000], mean observation: 0.053 [0.000, 24.000], loss: 9696.799805, mean_absolute_error: 485.065826, mean_q: 598.423096\n",
      "   36593/5000000: episode: 615, duration: 19.348s, episode steps: 415, steps per second: 21, episode reward: -1.000, mean reward: -0.002 [-1.000, 0.000], mean action: 2.995 [0.000, 5.000], mean observation: 0.050 [0.000, 24.000], loss: 11921.783203, mean_absolute_error: 498.989960, mean_q: 615.330933\n",
      "   36622/5000000: episode: 616, duration: 1.413s, episode steps: 29, steps per second: 21, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 3.034 [1.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 17166.455078, mean_absolute_error: 509.864685, mean_q: 629.669434\n",
      "   36655/5000000: episode: 617, duration: 1.618s, episode steps: 33, steps per second: 20, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 3.212 [3.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 10292.358398, mean_absolute_error: 487.301636, mean_q: 601.689453\n",
      "   36687/5000000: episode: 618, duration: 1.493s, episode steps: 32, steps per second: 21, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 3.844 [1.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 15833.836914, mean_absolute_error: 517.963257, mean_q: 639.333130\n",
      "   36717/5000000: episode: 619, duration: 1.382s, episode steps: 30, steps per second: 22, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.933 [1.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 11154.705078, mean_absolute_error: 527.169312, mean_q: 652.609192\n",
      "   36801/5000000: episode: 620, duration: 4.008s, episode steps: 84, steps per second: 21, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.631 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 10137.879883, mean_absolute_error: 520.132996, mean_q: 640.591064\n",
      "   36850/5000000: episode: 621, duration: 2.367s, episode steps: 49, steps per second: 21, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 3.224 [3.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 12449.587891, mean_absolute_error: 509.417938, mean_q: 627.222229\n",
      "   36951/5000000: episode: 622, duration: 5.010s, episode steps: 101, steps per second: 20, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.802 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 19960.742188, mean_absolute_error: 506.857056, mean_q: 623.842285\n",
      "   37006/5000000: episode: 623, duration: 2.567s, episode steps: 55, steps per second: 21, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 3.345 [2.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 14138.942383, mean_absolute_error: 517.618774, mean_q: 637.266968\n",
      "   37033/5000000: episode: 624, duration: 1.269s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 3.185 [2.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 17805.751953, mean_absolute_error: 527.237732, mean_q: 649.120850\n",
      "   37059/5000000: episode: 625, duration: 1.200s, episode steps: 26, steps per second: 22, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 4.038 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 12098.499023, mean_absolute_error: 541.440369, mean_q: 666.448364\n",
      "   37088/5000000: episode: 626, duration: 1.428s, episode steps: 29, steps per second: 20, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 4.069 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 13586.173828, mean_absolute_error: 518.115967, mean_q: 637.591187\n",
      "   37117/5000000: episode: 627, duration: 1.470s, episode steps: 29, steps per second: 20, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 4.207 [1.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 14661.386719, mean_absolute_error: 515.818848, mean_q: 634.872864\n",
      "   37175/5000000: episode: 628, duration: 2.845s, episode steps: 58, steps per second: 20, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 3.017 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 15208.627930, mean_absolute_error: 526.739868, mean_q: 648.260803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   37200/5000000: episode: 629, duration: 1.225s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.920 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 12137.572266, mean_absolute_error: 516.760986, mean_q: 637.207336\n",
      "   37227/5000000: episode: 630, duration: 1.363s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 3.222 [3.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 21896.914062, mean_absolute_error: 530.698486, mean_q: 653.696472\n",
      "   37253/5000000: episode: 631, duration: 1.383s, episode steps: 26, steps per second: 19, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 4.769 [3.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 19563.595703, mean_absolute_error: 509.041138, mean_q: 626.117004\n",
      "   37278/5000000: episode: 632, duration: 1.186s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.400 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 21885.398438, mean_absolute_error: 525.820068, mean_q: 647.170105\n",
      "   37375/5000000: episode: 633, duration: 4.394s, episode steps: 97, steps per second: 22, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 3.031 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 21555.857422, mean_absolute_error: 535.525146, mean_q: 659.750916\n",
      "   37404/5000000: episode: 634, duration: 1.540s, episode steps: 29, steps per second: 19, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 3.379 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 18885.566406, mean_absolute_error: 528.381165, mean_q: 651.005920\n",
      "   37451/5000000: episode: 635, duration: 2.204s, episode steps: 47, steps per second: 21, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 3.766 [2.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 14862.927734, mean_absolute_error: 554.016724, mean_q: 682.480225\n",
      "   37485/5000000: episode: 636, duration: 1.642s, episode steps: 34, steps per second: 21, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.735 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 15634.797852, mean_absolute_error: 549.163086, mean_q: 675.926025\n",
      "   37513/5000000: episode: 637, duration: 1.324s, episode steps: 28, steps per second: 21, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 3.536 [3.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 16296.208008, mean_absolute_error: 534.172974, mean_q: 656.670410\n",
      "   37639/5000000: episode: 638, duration: 5.960s, episode steps: 126, steps per second: 21, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 3.087 [2.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 15918.586914, mean_absolute_error: 545.142639, mean_q: 671.343811\n",
      "   37664/5000000: episode: 639, duration: 1.247s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.360 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 20894.294922, mean_absolute_error: 543.825684, mean_q: 671.522705\n",
      "   37716/5000000: episode: 640, duration: 2.589s, episode steps: 52, steps per second: 20, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 3.365 [1.000, 5.000], mean observation: 0.073 [0.000, 24.000], loss: 14579.943359, mean_absolute_error: 550.679016, mean_q: 680.125610\n",
      "   37742/5000000: episode: 641, duration: 1.257s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.231 [1.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 18684.767578, mean_absolute_error: 540.157410, mean_q: 664.075867\n",
      "   37775/5000000: episode: 642, duration: 1.504s, episode steps: 33, steps per second: 22, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 3.333 [1.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 17836.521484, mean_absolute_error: 550.562439, mean_q: 677.675171\n",
      "   37802/5000000: episode: 643, duration: 1.275s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 3.222 [3.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 21058.365234, mean_absolute_error: 545.061096, mean_q: 670.403198\n",
      "   37854/5000000: episode: 644, duration: 2.591s, episode steps: 52, steps per second: 20, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.558 [0.000, 5.000], mean observation: 0.073 [0.000, 24.000], loss: 14545.783203, mean_absolute_error: 546.491882, mean_q: 673.172852\n",
      "   37882/5000000: episode: 645, duration: 1.336s, episode steps: 28, steps per second: 21, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 3.643 [1.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 24262.150391, mean_absolute_error: 521.898987, mean_q: 639.928711\n",
      "   37908/5000000: episode: 646, duration: 1.144s, episode steps: 26, steps per second: 23, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.077 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 18416.738281, mean_absolute_error: 553.574585, mean_q: 679.657959\n",
      "   37963/5000000: episode: 647, duration: 2.591s, episode steps: 55, steps per second: 21, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 3.600 [3.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 29938.960938, mean_absolute_error: 543.713928, mean_q: 666.931335\n",
      "   37989/5000000: episode: 648, duration: 1.228s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.308 [2.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 20695.175781, mean_absolute_error: 569.488831, mean_q: 698.945923\n",
      "   38024/5000000: episode: 649, duration: 1.633s, episode steps: 35, steps per second: 21, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 3.143 [3.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 17677.421875, mean_absolute_error: 574.149597, mean_q: 705.853210\n",
      "   38127/5000000: episode: 650, duration: 4.913s, episode steps: 103, steps per second: 21, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 3.019 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 22061.466797, mean_absolute_error: 569.571167, mean_q: 701.452515\n",
      "   38165/5000000: episode: 651, duration: 1.828s, episode steps: 38, steps per second: 21, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 3.053 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 21733.693359, mean_absolute_error: 588.282593, mean_q: 723.870605\n",
      "   38193/5000000: episode: 652, duration: 1.227s, episode steps: 28, steps per second: 23, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.571 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 16360.194336, mean_absolute_error: 578.223083, mean_q: 711.617737\n",
      "   38253/5000000: episode: 653, duration: 2.895s, episode steps: 60, steps per second: 21, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 3.367 [1.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 28001.251953, mean_absolute_error: 539.280579, mean_q: 662.696228\n",
      "   38279/5000000: episode: 654, duration: 1.376s, episode steps: 26, steps per second: 19, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.308 [3.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 19146.447266, mean_absolute_error: 599.539001, mean_q: 740.296936\n",
      "   38310/5000000: episode: 655, duration: 1.477s, episode steps: 31, steps per second: 21, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 2.290 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 29888.878906, mean_absolute_error: 568.637634, mean_q: 701.060791\n",
      "   38336/5000000: episode: 656, duration: 1.298s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 4.038 [1.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 22017.031250, mean_absolute_error: 512.315063, mean_q: 628.669617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   38363/5000000: episode: 657, duration: 1.460s, episode steps: 27, steps per second: 18, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 3.370 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 14642.191406, mean_absolute_error: 582.695862, mean_q: 718.832886\n",
      "   38389/5000000: episode: 658, duration: 1.322s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.423 [3.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 30711.886719, mean_absolute_error: 562.821594, mean_q: 692.194458\n",
      "   38501/5000000: episode: 659, duration: 5.340s, episode steps: 112, steps per second: 21, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 3.196 [1.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 28156.140625, mean_absolute_error: 571.039185, mean_q: 703.766968\n",
      "   38549/5000000: episode: 660, duration: 2.385s, episode steps: 48, steps per second: 20, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 3.375 [3.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 21204.113281, mean_absolute_error: 576.299866, mean_q: 709.218445\n",
      "   38586/5000000: episode: 661, duration: 1.750s, episode steps: 37, steps per second: 21, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 3.838 [1.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 46739.460938, mean_absolute_error: 582.679688, mean_q: 718.038208\n",
      "   38617/5000000: episode: 662, duration: 1.485s, episode steps: 31, steps per second: 21, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 3.355 [1.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 27425.746094, mean_absolute_error: 578.761536, mean_q: 713.824768\n",
      "   38692/5000000: episode: 663, duration: 3.443s, episode steps: 75, steps per second: 22, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 2.827 [0.000, 5.000], mean observation: 0.076 [0.000, 24.000], loss: 15958.110352, mean_absolute_error: 564.529968, mean_q: 695.802734\n",
      "   38720/5000000: episode: 664, duration: 1.320s, episode steps: 28, steps per second: 21, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 3.536 [3.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 42685.460938, mean_absolute_error: 575.064026, mean_q: 706.723938\n",
      "   38748/5000000: episode: 665, duration: 1.410s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 3.714 [1.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 19002.136719, mean_absolute_error: 589.508362, mean_q: 724.617737\n",
      "   38804/5000000: episode: 666, duration: 2.755s, episode steps: 56, steps per second: 20, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 3.214 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 30249.250000, mean_absolute_error: 588.935486, mean_q: 723.913086\n",
      "   38830/5000000: episode: 667, duration: 1.270s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.808 [3.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 10995.636719, mean_absolute_error: 585.710876, mean_q: 720.396240\n",
      "   38890/5000000: episode: 668, duration: 2.976s, episode steps: 60, steps per second: 20, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 3.783 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 32860.699219, mean_absolute_error: 595.780823, mean_q: 733.174500\n",
      "   38915/5000000: episode: 669, duration: 1.285s, episode steps: 25, steps per second: 19, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.160 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 54179.675781, mean_absolute_error: 601.777039, mean_q: 738.682251\n",
      "   38940/5000000: episode: 670, duration: 1.274s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.960 [1.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 30905.724609, mean_absolute_error: 593.657410, mean_q: 732.072632\n",
      "   38966/5000000: episode: 671, duration: 1.216s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.846 [3.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 49293.917969, mean_absolute_error: 614.674194, mean_q: 756.505981\n",
      "   38991/5000000: episode: 672, duration: 1.220s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.920 [3.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 21313.466797, mean_absolute_error: 597.852356, mean_q: 735.508179\n",
      "   39016/5000000: episode: 673, duration: 12.989s, episode steps: 25, steps per second: 2, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.640 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 30586.029297, mean_absolute_error: 634.787903, mean_q: 784.290649\n",
      "   39046/5000000: episode: 674, duration: 1.463s, episode steps: 30, steps per second: 21, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 3.500 [1.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 39731.578125, mean_absolute_error: 585.282288, mean_q: 719.642639\n",
      "   39077/5000000: episode: 675, duration: 1.477s, episode steps: 31, steps per second: 21, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 3.290 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 40859.296875, mean_absolute_error: 629.199097, mean_q: 775.532288\n",
      "   39102/5000000: episode: 676, duration: 1.178s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.840 [1.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 23866.562500, mean_absolute_error: 573.286011, mean_q: 706.312744\n",
      "   39131/5000000: episode: 677, duration: 1.431s, episode steps: 29, steps per second: 20, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 3.966 [1.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 48250.824219, mean_absolute_error: 578.936768, mean_q: 711.905701\n",
      "   39157/5000000: episode: 678, duration: 1.298s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.231 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 52454.527344, mean_absolute_error: 592.288696, mean_q: 728.037231\n",
      "   39207/5000000: episode: 679, duration: 2.357s, episode steps: 50, steps per second: 21, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 3.780 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 24631.574219, mean_absolute_error: 587.010498, mean_q: 722.776184\n",
      "   39259/5000000: episode: 680, duration: 2.465s, episode steps: 52, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 3.192 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 45562.347656, mean_absolute_error: 599.035217, mean_q: 737.882812\n",
      "   39284/5000000: episode: 681, duration: 1.128s, episode steps: 25, steps per second: 22, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.400 [1.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 48300.269531, mean_absolute_error: 577.812622, mean_q: 709.623657\n",
      "   39309/5000000: episode: 682, duration: 1.226s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.640 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 43554.500000, mean_absolute_error: 622.427307, mean_q: 767.523254\n",
      "   39363/5000000: episode: 683, duration: 2.531s, episode steps: 54, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 3.796 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 36807.191406, mean_absolute_error: 600.519714, mean_q: 740.577881\n",
      "   39417/5000000: episode: 684, duration: 2.383s, episode steps: 54, steps per second: 23, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 3.741 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 51379.671875, mean_absolute_error: 614.273193, mean_q: 755.865906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   39444/5000000: episode: 685, duration: 1.339s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 3.519 [2.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 35366.019531, mean_absolute_error: 605.181152, mean_q: 745.509521\n",
      "   39469/5000000: episode: 686, duration: 1.190s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.440 [1.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 89438.773438, mean_absolute_error: 624.870300, mean_q: 765.142334\n",
      "   39494/5000000: episode: 687, duration: 1.269s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.920 [1.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 34396.019531, mean_absolute_error: 628.275208, mean_q: 776.410645\n",
      "   39519/5000000: episode: 688, duration: 1.215s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.920 [1.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 41170.023438, mean_absolute_error: 587.001465, mean_q: 723.459900\n",
      "   39573/5000000: episode: 689, duration: 2.579s, episode steps: 54, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 3.278 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 57972.292969, mean_absolute_error: 642.780334, mean_q: 793.126038\n",
      "   39618/5000000: episode: 690, duration: 2.293s, episode steps: 45, steps per second: 20, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 2.867 [1.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 54378.023438, mean_absolute_error: 644.980225, mean_q: 795.859558\n",
      "   39671/5000000: episode: 691, duration: 2.535s, episode steps: 53, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 3.057 [0.000, 5.000], mean observation: 0.073 [0.000, 24.000], loss: 35487.968750, mean_absolute_error: 664.066895, mean_q: 819.836731\n",
      "   39704/5000000: episode: 692, duration: 1.555s, episode steps: 33, steps per second: 21, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 3.485 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 49008.507812, mean_absolute_error: 654.074524, mean_q: 803.522095\n",
      "   39754/5000000: episode: 693, duration: 2.371s, episode steps: 50, steps per second: 21, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 3.720 [1.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 39340.960938, mean_absolute_error: 632.954590, mean_q: 777.899353\n",
      "   39914/5000000: episode: 694, duration: 7.720s, episode steps: 160, steps per second: 21, episode reward: -1.000, mean reward: -0.006 [-1.000, 0.000], mean action: 3.281 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 62678.042969, mean_absolute_error: 638.278076, mean_q: 785.359253\n",
      "   39939/5000000: episode: 695, duration: 1.244s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.360 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 56042.816406, mean_absolute_error: 688.912964, mean_q: 851.027832\n",
      "   39967/5000000: episode: 696, duration: 1.488s, episode steps: 28, steps per second: 19, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 3.786 [1.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 78745.070312, mean_absolute_error: 666.474792, mean_q: 821.448730\n",
      "   40017/5000000: episode: 697, duration: 2.414s, episode steps: 50, steps per second: 21, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 3.760 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 81929.453125, mean_absolute_error: 618.151855, mean_q: 758.277405\n",
      "   40042/5000000: episode: 698, duration: 1.186s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.880 [3.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 62369.406250, mean_absolute_error: 628.639465, mean_q: 774.835083\n",
      "   40094/5000000: episode: 699, duration: 2.439s, episode steps: 52, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 3.654 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 74285.273438, mean_absolute_error: 643.532959, mean_q: 791.954346\n",
      "   40150/5000000: episode: 700, duration: 2.542s, episode steps: 56, steps per second: 22, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 3.839 [1.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 71579.609375, mean_absolute_error: 646.584167, mean_q: 793.831360\n",
      "   40178/5000000: episode: 701, duration: 1.327s, episode steps: 28, steps per second: 21, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 1.929 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 36214.445312, mean_absolute_error: 678.160034, mean_q: 834.372864\n",
      "   40204/5000000: episode: 702, duration: 1.223s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 4.885 [2.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 55462.917969, mean_absolute_error: 614.808960, mean_q: 753.585571\n",
      "   40283/5000000: episode: 703, duration: 3.832s, episode steps: 79, steps per second: 21, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 3.304 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 48063.589844, mean_absolute_error: 638.766418, mean_q: 783.101074\n",
      "   40310/5000000: episode: 704, duration: 1.284s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 3.444 [1.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 42119.597656, mean_absolute_error: 659.294189, mean_q: 809.760132\n",
      "   40335/5000000: episode: 705, duration: 1.173s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 4.280 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 116828.437500, mean_absolute_error: 677.200378, mean_q: 829.035339\n",
      "   40361/5000000: episode: 706, duration: 1.268s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.885 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 74785.742188, mean_absolute_error: 636.896179, mean_q: 779.275085\n",
      "   40414/5000000: episode: 707, duration: 2.481s, episode steps: 53, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.717 [1.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 98699.937500, mean_absolute_error: 638.473694, mean_q: 783.135986\n",
      "   40443/5000000: episode: 708, duration: 1.360s, episode steps: 29, steps per second: 21, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.966 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 50855.699219, mean_absolute_error: 631.136902, mean_q: 774.098145\n",
      "   40501/5000000: episode: 709, duration: 2.830s, episode steps: 58, steps per second: 20, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 3.345 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 113635.921875, mean_absolute_error: 698.291809, mean_q: 857.400635\n",
      "   40527/5000000: episode: 710, duration: 1.277s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.923 [1.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 58845.500000, mean_absolute_error: 679.389954, mean_q: 836.759521\n",
      "   40552/5000000: episode: 711, duration: 1.138s, episode steps: 25, steps per second: 22, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.640 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 126293.062500, mean_absolute_error: 671.151794, mean_q: 822.899048\n",
      "   40594/5000000: episode: 712, duration: 2.074s, episode steps: 42, steps per second: 20, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 3.690 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 86094.531250, mean_absolute_error: 651.057129, mean_q: 799.971558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   40644/5000000: episode: 713, duration: 2.439s, episode steps: 50, steps per second: 21, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 4.640 [1.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 35965.871094, mean_absolute_error: 656.320312, mean_q: 807.772278\n",
      "   40669/5000000: episode: 714, duration: 1.247s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 1.960 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 69498.367188, mean_absolute_error: 668.106628, mean_q: 822.473511\n",
      "   40694/5000000: episode: 715, duration: 1.167s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 4.720 [1.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 48457.398438, mean_absolute_error: 674.313843, mean_q: 829.473145\n",
      "   40719/5000000: episode: 716, duration: 1.120s, episode steps: 25, steps per second: 22, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 4.760 [1.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 85776.039062, mean_absolute_error: 666.419067, mean_q: 820.330444\n",
      "   40771/5000000: episode: 717, duration: 2.612s, episode steps: 52, steps per second: 20, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 3.615 [0.000, 5.000], mean observation: 0.073 [0.000, 24.000], loss: 105789.914062, mean_absolute_error: 658.027588, mean_q: 809.729553\n",
      "   40824/5000000: episode: 718, duration: 2.532s, episode steps: 53, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 3.887 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 41809.386719, mean_absolute_error: 688.152954, mean_q: 845.669678\n",
      "   40849/5000000: episode: 719, duration: 1.279s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.480 [1.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 110189.921875, mean_absolute_error: 713.558838, mean_q: 877.952515\n",
      "   40899/5000000: episode: 720, duration: 2.476s, episode steps: 50, steps per second: 20, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 4.680 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 59797.503906, mean_absolute_error: 668.981262, mean_q: 822.878296\n",
      "   40925/5000000: episode: 721, duration: 1.334s, episode steps: 26, steps per second: 19, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.346 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 176730.562500, mean_absolute_error: 721.424072, mean_q: 887.281006\n",
      "   40950/5000000: episode: 722, duration: 1.178s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 4.880 [3.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 62194.574219, mean_absolute_error: 693.031433, mean_q: 854.489685\n",
      "   40985/5000000: episode: 723, duration: 1.704s, episode steps: 35, steps per second: 21, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.800 [1.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 83028.085938, mean_absolute_error: 657.021362, mean_q: 806.665466\n",
      "   41012/5000000: episode: 724, duration: 1.341s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 4.074 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 123793.382812, mean_absolute_error: 683.020630, mean_q: 838.538696\n",
      "   41038/5000000: episode: 725, duration: 1.290s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.346 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 73543.945312, mean_absolute_error: 728.666565, mean_q: 895.182556\n",
      "   41063/5000000: episode: 726, duration: 1.165s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.280 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 280882.125000, mean_absolute_error: 728.751404, mean_q: 895.416931\n",
      "   41091/5000000: episode: 727, duration: 1.302s, episode steps: 28, steps per second: 21, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 1.714 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 141989.984375, mean_absolute_error: 687.971313, mean_q: 846.065552\n",
      "   41116/5000000: episode: 728, duration: 1.133s, episode steps: 25, steps per second: 22, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 59905.023438, mean_absolute_error: 635.647827, mean_q: 781.196167\n",
      "   41146/5000000: episode: 729, duration: 1.483s, episode steps: 30, steps per second: 20, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 4.200 [1.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 80588.718750, mean_absolute_error: 689.945007, mean_q: 850.078491\n",
      "   41171/5000000: episode: 730, duration: 1.130s, episode steps: 25, steps per second: 22, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.560 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 111020.046875, mean_absolute_error: 786.553650, mean_q: 969.969849\n",
      "   41196/5000000: episode: 731, duration: 1.219s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.440 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 138354.234375, mean_absolute_error: 698.403687, mean_q: 858.507874\n",
      "   41224/5000000: episode: 732, duration: 1.379s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 4.464 [2.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 42611.656250, mean_absolute_error: 665.730530, mean_q: 819.692017\n",
      "   41253/5000000: episode: 733, duration: 1.537s, episode steps: 29, steps per second: 19, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 4.552 [1.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 66367.203125, mean_absolute_error: 730.830200, mean_q: 903.378845\n",
      "   41304/5000000: episode: 734, duration: 2.334s, episode steps: 51, steps per second: 22, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 3.549 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 123274.437500, mean_absolute_error: 702.493774, mean_q: 865.951050\n",
      "   41329/5000000: episode: 735, duration: 1.252s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 76947.726562, mean_absolute_error: 690.071960, mean_q: 850.381470\n",
      "   41354/5000000: episode: 736, duration: 1.261s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 4.840 [3.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 253706.062500, mean_absolute_error: 696.428467, mean_q: 857.872192\n",
      "   41404/5000000: episode: 737, duration: 2.293s, episode steps: 50, steps per second: 22, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 4.700 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 128407.382812, mean_absolute_error: 706.817810, mean_q: 871.213684\n",
      "   41451/5000000: episode: 738, duration: 2.296s, episode steps: 47, steps per second: 20, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 3.426 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 150909.140625, mean_absolute_error: 745.719055, mean_q: 920.739502\n",
      "   41476/5000000: episode: 739, duration: 1.086s, episode steps: 25, steps per second: 23, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 4.520 [1.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 33086.578125, mean_absolute_error: 675.831116, mean_q: 834.718140\n",
      "   41502/5000000: episode: 740, duration: 1.280s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 4.885 [2.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 49653.828125, mean_absolute_error: 682.595459, mean_q: 843.286194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   41527/5000000: episode: 741, duration: 1.154s, episode steps: 25, steps per second: 22, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 143823.000000, mean_absolute_error: 712.038696, mean_q: 878.545288\n",
      "   41561/5000000: episode: 742, duration: 1.573s, episode steps: 34, steps per second: 22, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.176 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 124176.585938, mean_absolute_error: 738.948975, mean_q: 911.503418\n",
      "   41591/5000000: episode: 743, duration: 1.495s, episode steps: 30, steps per second: 20, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.300 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 54747.636719, mean_absolute_error: 772.908936, mean_q: 950.495239\n",
      "   41631/5000000: episode: 744, duration: 1.777s, episode steps: 40, steps per second: 23, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 3.250 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 251877.921875, mean_absolute_error: 732.308228, mean_q: 901.623413\n",
      "   41656/5000000: episode: 745, duration: 1.225s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 156346.234375, mean_absolute_error: 754.308289, mean_q: 932.227356\n",
      "   41757/5000000: episode: 746, duration: 4.922s, episode steps: 101, steps per second: 21, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 3.307 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 152347.406250, mean_absolute_error: 743.801208, mean_q: 918.857666\n",
      "   41782/5000000: episode: 747, duration: 1.225s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 4.960 [4.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 54536.359375, mean_absolute_error: 786.463257, mean_q: 972.130310\n",
      "   41808/5000000: episode: 748, duration: 1.194s, episode steps: 26, steps per second: 22, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.269 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 117908.015625, mean_absolute_error: 725.279968, mean_q: 894.239563\n",
      "   41854/5000000: episode: 749, duration: 2.248s, episode steps: 46, steps per second: 20, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 2.217 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 156591.859375, mean_absolute_error: 695.375366, mean_q: 858.308838\n",
      "   41880/5000000: episode: 750, duration: 1.281s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 4.846 [1.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 155690.875000, mean_absolute_error: 736.447632, mean_q: 907.450623\n",
      "   41905/5000000: episode: 751, duration: 1.234s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.200 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 95226.250000, mean_absolute_error: 754.777100, mean_q: 934.713745\n",
      "   41931/5000000: episode: 752, duration: 1.220s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 1.577 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 134344.562500, mean_absolute_error: 742.537537, mean_q: 919.303650\n",
      "   41982/5000000: episode: 753, duration: 2.508s, episode steps: 51, steps per second: 20, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 3.765 [0.000, 5.000], mean observation: 0.073 [0.000, 24.000], loss: 156499.046875, mean_absolute_error: 730.970276, mean_q: 902.091919\n",
      "   42032/5000000: episode: 754, duration: 2.422s, episode steps: 50, steps per second: 21, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 3.900 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 169083.718750, mean_absolute_error: 784.901733, mean_q: 961.695374\n",
      "   42060/5000000: episode: 755, duration: 1.308s, episode steps: 28, steps per second: 21, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 4.536 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 98648.164062, mean_absolute_error: 768.323730, mean_q: 949.493164\n",
      "   42111/5000000: episode: 756, duration: 2.433s, episode steps: 51, steps per second: 21, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 3.314 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 217946.375000, mean_absolute_error: 794.591431, mean_q: 975.695618\n",
      "   42136/5000000: episode: 757, duration: 1.200s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 4.920 [3.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 175449.375000, mean_absolute_error: 737.798035, mean_q: 907.640808\n",
      "   42161/5000000: episode: 758, duration: 1.104s, episode steps: 25, steps per second: 23, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.280 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 67788.937500, mean_absolute_error: 729.040222, mean_q: 898.893433\n",
      "   42192/5000000: episode: 759, duration: 1.492s, episode steps: 31, steps per second: 21, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 3.065 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 119550.218750, mean_absolute_error: 741.018616, mean_q: 913.368896\n",
      "   42272/5000000: episode: 760, duration: 3.644s, episode steps: 80, steps per second: 22, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 2.987 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 232392.281250, mean_absolute_error: 797.164490, mean_q: 983.248413\n",
      "   42324/5000000: episode: 761, duration: 2.436s, episode steps: 52, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 4.538 [1.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 191808.171875, mean_absolute_error: 779.540649, mean_q: 961.805298\n",
      "   42350/5000000: episode: 762, duration: 1.292s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 4.769 [1.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 98491.109375, mean_absolute_error: 742.005981, mean_q: 917.713745\n",
      "   42406/5000000: episode: 763, duration: 2.616s, episode steps: 56, steps per second: 21, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.911 [0.000, 5.000], mean observation: 0.062 [0.000, 24.000], loss: 164610.703125, mean_absolute_error: 769.079773, mean_q: 950.943481\n",
      "   42458/5000000: episode: 764, duration: 2.485s, episode steps: 52, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 4.423 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 169519.140625, mean_absolute_error: 765.342529, mean_q: 943.942383\n",
      "   42483/5000000: episode: 765, duration: 1.174s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 4.880 [2.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 125557.000000, mean_absolute_error: 815.076172, mean_q: 1005.507446\n",
      "   42534/5000000: episode: 766, duration: 2.452s, episode steps: 51, steps per second: 21, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 3.843 [1.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 161415.953125, mean_absolute_error: 808.392456, mean_q: 998.281067\n",
      "   42564/5000000: episode: 767, duration: 1.454s, episode steps: 30, steps per second: 21, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.633 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 146643.109375, mean_absolute_error: 780.537231, mean_q: 966.579773\n",
      "   42615/5000000: episode: 768, duration: 2.431s, episode steps: 51, steps per second: 21, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 3.824 [0.000, 5.000], mean observation: 0.074 [0.000, 24.000], loss: 110714.656250, mean_absolute_error: 812.377991, mean_q: 997.749084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   42640/5000000: episode: 769, duration: 1.188s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 4.840 [1.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 120893.820312, mean_absolute_error: 817.658752, mean_q: 1011.090637\n",
      "   42665/5000000: episode: 770, duration: 1.161s, episode steps: 25, steps per second: 22, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 4.600 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 52059.351562, mean_absolute_error: 787.718933, mean_q: 974.420715\n",
      "   42690/5000000: episode: 771, duration: 1.209s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 119876.242188, mean_absolute_error: 805.873779, mean_q: 996.258728\n",
      "   42715/5000000: episode: 772, duration: 1.268s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 393583.406250, mean_absolute_error: 785.447815, mean_q: 967.815857\n",
      "   42741/5000000: episode: 773, duration: 1.231s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 4.962 [4.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 148219.781250, mean_absolute_error: 797.721985, mean_q: 987.330322\n",
      "   42766/5000000: episode: 774, duration: 1.180s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 4.160 [1.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 393046.593750, mean_absolute_error: 864.501160, mean_q: 1068.398682\n",
      "   42792/5000000: episode: 775, duration: 1.264s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 4.846 [3.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 233868.765625, mean_absolute_error: 848.215271, mean_q: 1047.055664\n",
      "   42827/5000000: episode: 776, duration: 1.619s, episode steps: 35, steps per second: 22, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 3.400 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 176710.937500, mean_absolute_error: 826.512512, mean_q: 1022.969727\n",
      "   42852/5000000: episode: 777, duration: 1.301s, episode steps: 25, steps per second: 19, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.840 [3.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 220738.984375, mean_absolute_error: 819.394592, mean_q: 1012.939453\n",
      "   42880/5000000: episode: 778, duration: 1.372s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 1.536 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 185637.671875, mean_absolute_error: 809.291809, mean_q: 1000.944519\n",
      "   42907/5000000: episode: 779, duration: 1.236s, episode steps: 27, steps per second: 22, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 3.926 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 274366.218750, mean_absolute_error: 849.841797, mean_q: 1051.163696\n",
      "   42932/5000000: episode: 780, duration: 1.202s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 4.840 [1.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 64108.121094, mean_absolute_error: 764.013184, mean_q: 943.705139\n",
      "   42957/5000000: episode: 781, duration: 1.175s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.720 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 204439.953125, mean_absolute_error: 793.454041, mean_q: 981.202026\n",
      "   42985/5000000: episode: 782, duration: 1.274s, episode steps: 28, steps per second: 22, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 3.107 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 114947.460938, mean_absolute_error: 815.031616, mean_q: 1004.887878\n",
      "   43039/5000000: episode: 783, duration: 2.658s, episode steps: 54, steps per second: 20, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 3.056 [1.000, 5.000], mean observation: 0.075 [0.000, 24.000], loss: 184762.859375, mean_absolute_error: 844.012573, mean_q: 1045.294434\n",
      "   43155/5000000: episode: 784, duration: 5.505s, episode steps: 116, steps per second: 21, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 3.216 [0.000, 5.000], mean observation: 0.062 [0.000, 24.000], loss: 157184.406250, mean_absolute_error: 829.377747, mean_q: 1027.049683\n",
      "   43231/5000000: episode: 785, duration: 3.539s, episode steps: 76, steps per second: 21, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 3.276 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 136679.375000, mean_absolute_error: 821.269714, mean_q: 1015.539185\n",
      "   43256/5000000: episode: 786, duration: 1.240s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.800 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 451457.718750, mean_absolute_error: 859.960999, mean_q: 1055.346436\n",
      "   43283/5000000: episode: 787, duration: 1.367s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 3.259 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 324596.781250, mean_absolute_error: 845.169006, mean_q: 1043.386963\n",
      "   43430/5000000: episode: 788, duration: 6.571s, episode steps: 147, steps per second: 22, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 2.918 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 238949.218750, mean_absolute_error: 820.163147, mean_q: 1013.679871\n",
      "   43468/5000000: episode: 789, duration: 1.907s, episode steps: 38, steps per second: 20, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 2.579 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 144191.515625, mean_absolute_error: 846.577454, mean_q: 1042.319092\n",
      "   43522/5000000: episode: 790, duration: 2.667s, episode steps: 54, steps per second: 20, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.500 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 119687.585938, mean_absolute_error: 834.745972, mean_q: 1032.359863\n",
      "   43549/5000000: episode: 791, duration: 1.251s, episode steps: 27, steps per second: 22, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.259 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 113965.054688, mean_absolute_error: 840.280457, mean_q: 1041.345581\n",
      "   43604/5000000: episode: 792, duration: 2.552s, episode steps: 55, steps per second: 22, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.982 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 166244.578125, mean_absolute_error: 896.824036, mean_q: 1102.878540\n",
      "   43629/5000000: episode: 793, duration: 1.256s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.000 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 145667.234375, mean_absolute_error: 849.728210, mean_q: 1050.609741\n",
      "   43731/5000000: episode: 794, duration: 4.927s, episode steps: 102, steps per second: 21, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.539 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 187321.000000, mean_absolute_error: 873.610596, mean_q: 1077.611084\n",
      "   43756/5000000: episode: 795, duration: 1.172s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.840 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 436794.468750, mean_absolute_error: 871.578186, mean_q: 1076.162842\n",
      "   43885/5000000: episode: 796, duration: 6.399s, episode steps: 129, steps per second: 20, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 3.116 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 224448.171875, mean_absolute_error: 878.776917, mean_q: 1084.135864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   43912/5000000: episode: 797, duration: 1.352s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 4.037 [1.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 105970.218750, mean_absolute_error: 834.490540, mean_q: 1028.827026\n",
      "   43938/5000000: episode: 798, duration: 1.293s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.962 [1.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 128475.179688, mean_absolute_error: 814.542969, mean_q: 1005.776611\n",
      "   44022/5000000: episode: 799, duration: 4.091s, episode steps: 84, steps per second: 21, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.202 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 222560.953125, mean_absolute_error: 864.139709, mean_q: 1065.555664\n",
      "   44052/5000000: episode: 800, duration: 1.437s, episode steps: 30, steps per second: 21, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.967 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 240076.984375, mean_absolute_error: 904.703552, mean_q: 1116.883545\n",
      "   44131/5000000: episode: 801, duration: 3.921s, episode steps: 79, steps per second: 20, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 2.468 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 188617.843750, mean_absolute_error: 877.817078, mean_q: 1081.171143\n",
      "   44285/5000000: episode: 802, duration: 7.531s, episode steps: 154, steps per second: 20, episode reward: -1.000, mean reward: -0.006 [-1.000, 0.000], mean action: 3.013 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 202287.296875, mean_absolute_error: 909.780457, mean_q: 1123.784424\n",
      "   44318/5000000: episode: 803, duration: 1.616s, episode steps: 33, steps per second: 20, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 3.606 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 364999.156250, mean_absolute_error: 856.951965, mean_q: 1056.390137\n",
      "   44399/5000000: episode: 804, duration: 3.870s, episode steps: 81, steps per second: 21, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.309 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 193540.156250, mean_absolute_error: 912.873840, mean_q: 1124.908569\n",
      "   44424/5000000: episode: 805, duration: 1.196s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.760 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 278295.750000, mean_absolute_error: 858.792419, mean_q: 1055.672974\n",
      "   44549/5000000: episode: 806, duration: 5.973s, episode steps: 125, steps per second: 21, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.912 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 390047.250000, mean_absolute_error: 929.385986, mean_q: 1146.819336\n",
      "   44574/5000000: episode: 807, duration: 1.199s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.760 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 153211.343750, mean_absolute_error: 993.444275, mean_q: 1228.826538\n",
      "   44601/5000000: episode: 808, duration: 1.276s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 3.111 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 225753.890625, mean_absolute_error: 902.477661, mean_q: 1116.290527\n",
      "   44664/5000000: episode: 809, duration: 3.076s, episode steps: 63, steps per second: 20, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 2.524 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 290288.656250, mean_absolute_error: 929.650635, mean_q: 1147.140259\n",
      "   44692/5000000: episode: 810, duration: 1.282s, episode steps: 28, steps per second: 22, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.286 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 299614.500000, mean_absolute_error: 936.478210, mean_q: 1155.441162\n",
      "   44800/5000000: episode: 811, duration: 5.318s, episode steps: 108, steps per second: 20, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.769 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 359505.281250, mean_absolute_error: 972.565979, mean_q: 1199.997803\n",
      "   44833/5000000: episode: 812, duration: 1.544s, episode steps: 33, steps per second: 21, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 2.152 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 242264.375000, mean_absolute_error: 905.313843, mean_q: 1119.419922\n",
      "   44886/5000000: episode: 813, duration: 2.636s, episode steps: 53, steps per second: 20, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.736 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 207262.906250, mean_absolute_error: 973.499878, mean_q: 1201.812622\n",
      "   44912/5000000: episode: 814, duration: 1.291s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.231 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 391295.937500, mean_absolute_error: 1079.553711, mean_q: 1333.162598\n",
      "   44938/5000000: episode: 815, duration: 1.380s, episode steps: 26, steps per second: 19, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.577 [1.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 231707.031250, mean_absolute_error: 970.777405, mean_q: 1194.418701\n",
      "   44965/5000000: episode: 816, duration: 1.364s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.815 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 285498.187500, mean_absolute_error: 984.009033, mean_q: 1217.752319\n",
      "   45016/5000000: episode: 817, duration: 14.656s, episode steps: 51, steps per second: 3, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 2.431 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 162488.140625, mean_absolute_error: 967.818848, mean_q: 1191.637085\n",
      "   45050/5000000: episode: 818, duration: 1.614s, episode steps: 34, steps per second: 21, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.559 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 181629.968750, mean_absolute_error: 965.498535, mean_q: 1189.432007\n",
      "   45077/5000000: episode: 819, duration: 1.350s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.222 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 241905.921875, mean_absolute_error: 990.973572, mean_q: 1222.838989\n",
      "   45103/5000000: episode: 820, duration: 1.397s, episode steps: 26, steps per second: 19, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.462 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 732654.750000, mean_absolute_error: 1097.460205, mean_q: 1347.007690\n",
      "   45128/5000000: episode: 821, duration: 1.239s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.160 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 202602.515625, mean_absolute_error: 1045.574585, mean_q: 1285.085938\n",
      "   45175/5000000: episode: 822, duration: 2.219s, episode steps: 47, steps per second: 21, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 2.489 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 300177.031250, mean_absolute_error: 1003.092834, mean_q: 1239.193115\n",
      "   45234/5000000: episode: 823, duration: 2.936s, episode steps: 59, steps per second: 20, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.051 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 181012.312500, mean_absolute_error: 961.186157, mean_q: 1187.398560\n",
      "   45260/5000000: episode: 824, duration: 1.259s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 1.308 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 414038.875000, mean_absolute_error: 977.164978, mean_q: 1207.229248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   45285/5000000: episode: 825, duration: 1.239s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.760 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 257242.953125, mean_absolute_error: 1102.756836, mean_q: 1363.156860\n",
      "   45368/5000000: episode: 826, duration: 3.848s, episode steps: 83, steps per second: 22, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.663 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 363554.062500, mean_absolute_error: 1028.674805, mean_q: 1270.304077\n",
      "   45524/5000000: episode: 827, duration: 7.398s, episode steps: 156, steps per second: 21, episode reward: -1.000, mean reward: -0.006 [-1.000, 0.000], mean action: 2.654 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 315076.093750, mean_absolute_error: 1014.739258, mean_q: 1253.076416\n",
      "   45549/5000000: episode: 828, duration: 1.239s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.680 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 303126.875000, mean_absolute_error: 1048.108154, mean_q: 1294.503784\n",
      "   45575/5000000: episode: 829, duration: 1.250s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.500 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 178640.562500, mean_absolute_error: 907.170532, mean_q: 1120.883179\n",
      "   45610/5000000: episode: 830, duration: 1.641s, episode steps: 35, steps per second: 21, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.229 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 233324.250000, mean_absolute_error: 1086.447876, mean_q: 1342.823242\n",
      "   45650/5000000: episode: 831, duration: 1.962s, episode steps: 40, steps per second: 20, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 2.425 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 238959.656250, mean_absolute_error: 1037.617920, mean_q: 1285.650757\n",
      "   45680/5000000: episode: 832, duration: 1.345s, episode steps: 30, steps per second: 22, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.667 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 107195.734375, mean_absolute_error: 942.023560, mean_q: 1166.431519\n",
      "   45970/5000000: episode: 833, duration: 13.750s, episode steps: 290, steps per second: 21, episode reward: -1.000, mean reward: -0.003 [-1.000, 0.000], mean action: 2.417 [0.000, 5.000], mean observation: 0.061 [0.000, 24.000], loss: 266706.375000, mean_absolute_error: 1026.069580, mean_q: 1262.918701\n",
      "   46004/5000000: episode: 834, duration: 1.702s, episode steps: 34, steps per second: 20, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.559 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 177241.953125, mean_absolute_error: 1100.170898, mean_q: 1362.897217\n",
      "   46035/5000000: episode: 835, duration: 1.530s, episode steps: 31, steps per second: 20, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 3.226 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 357534.906250, mean_absolute_error: 1094.418457, mean_q: 1347.665649\n",
      "   46121/5000000: episode: 836, duration: 4.258s, episode steps: 86, steps per second: 20, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.453 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 623415.500000, mean_absolute_error: 1095.220703, mean_q: 1347.081177\n",
      "   46150/5000000: episode: 837, duration: 1.495s, episode steps: 29, steps per second: 19, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.276 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 667478.250000, mean_absolute_error: 1117.368286, mean_q: 1373.891968\n",
      "   46186/5000000: episode: 838, duration: 1.821s, episode steps: 36, steps per second: 20, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 2.333 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 611626.125000, mean_absolute_error: 985.052185, mean_q: 1213.504883\n",
      "   46249/5000000: episode: 839, duration: 3.118s, episode steps: 63, steps per second: 20, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 2.429 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 418388.437500, mean_absolute_error: 1142.439453, mean_q: 1410.535767\n",
      "   46279/5000000: episode: 840, duration: 1.400s, episode steps: 30, steps per second: 21, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.533 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 201543.500000, mean_absolute_error: 996.803894, mean_q: 1231.779175\n",
      "   46314/5000000: episode: 841, duration: 1.593s, episode steps: 35, steps per second: 22, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.314 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 263442.593750, mean_absolute_error: 1071.607178, mean_q: 1307.340210\n",
      "   46377/5000000: episode: 842, duration: 3.007s, episode steps: 63, steps per second: 21, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 2.032 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 395139.031250, mean_absolute_error: 1081.338379, mean_q: 1331.366943\n",
      "   46404/5000000: episode: 843, duration: 1.226s, episode steps: 27, steps per second: 22, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 1.444 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 385036.531250, mean_absolute_error: 1094.377563, mean_q: 1336.811768\n",
      "   46542/5000000: episode: 844, duration: 6.633s, episode steps: 138, steps per second: 21, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 2.551 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 321987.656250, mean_absolute_error: 1082.073486, mean_q: 1330.892944\n",
      "   46568/5000000: episode: 845, duration: 1.396s, episode steps: 26, steps per second: 19, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.500 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 425012.843750, mean_absolute_error: 1179.025269, mean_q: 1449.726074\n",
      "   46595/5000000: episode: 846, duration: 1.326s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.444 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 873790.875000, mean_absolute_error: 1242.673218, mean_q: 1526.848389\n",
      "   46748/5000000: episode: 847, duration: 7.248s, episode steps: 153, steps per second: 21, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 2.412 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 455032.093750, mean_absolute_error: 1140.395264, mean_q: 1401.516113\n",
      "   46830/5000000: episode: 848, duration: 3.933s, episode steps: 82, steps per second: 21, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.500 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 257519.421875, mean_absolute_error: 1128.164185, mean_q: 1389.496338\n",
      "   46885/5000000: episode: 849, duration: 2.743s, episode steps: 55, steps per second: 20, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.491 [0.000, 5.000], mean observation: 0.073 [0.000, 24.000], loss: 482683.375000, mean_absolute_error: 1216.096191, mean_q: 1486.944458\n",
      "   46918/5000000: episode: 850, duration: 1.596s, episode steps: 33, steps per second: 21, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 1.545 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 700270.500000, mean_absolute_error: 1135.735962, mean_q: 1394.860962\n",
      "   46945/5000000: episode: 851, duration: 1.409s, episode steps: 27, steps per second: 19, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.704 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 344516.062500, mean_absolute_error: 1216.897583, mean_q: 1492.859497\n",
      "   47012/5000000: episode: 852, duration: 3.306s, episode steps: 67, steps per second: 20, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 2.522 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 404174.875000, mean_absolute_error: 1119.015381, mean_q: 1383.303223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   47039/5000000: episode: 853, duration: 1.371s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.481 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 299974.843750, mean_absolute_error: 1151.706055, mean_q: 1420.728027\n",
      "   47097/5000000: episode: 854, duration: 2.793s, episode steps: 58, steps per second: 21, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 3.121 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 526611.062500, mean_absolute_error: 1167.130493, mean_q: 1431.016846\n",
      "   47157/5000000: episode: 855, duration: 2.948s, episode steps: 60, steps per second: 20, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.383 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 460864.593750, mean_absolute_error: 1190.122681, mean_q: 1461.799316\n",
      "   47223/5000000: episode: 856, duration: 3.273s, episode steps: 66, steps per second: 20, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 2.136 [0.000, 5.000], mean observation: 0.075 [0.000, 24.000], loss: 322201.656250, mean_absolute_error: 1252.765625, mean_q: 1529.808228\n",
      "   47260/5000000: episode: 857, duration: 1.787s, episode steps: 37, steps per second: 21, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 2.919 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 360764.156250, mean_absolute_error: 1118.626953, mean_q: 1377.551025\n",
      "   47317/5000000: episode: 858, duration: 2.560s, episode steps: 57, steps per second: 22, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.544 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 327083.437500, mean_absolute_error: 1213.884155, mean_q: 1483.571411\n",
      "   47347/5000000: episode: 859, duration: 1.521s, episode steps: 30, steps per second: 20, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.967 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 475631.187500, mean_absolute_error: 1350.116455, mean_q: 1645.564575\n",
      "   47407/5000000: episode: 860, duration: 2.865s, episode steps: 60, steps per second: 21, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.317 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 371332.656250, mean_absolute_error: 1160.898193, mean_q: 1428.566162\n",
      "   47433/5000000: episode: 861, duration: 1.330s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.346 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 627068.000000, mean_absolute_error: 1154.660034, mean_q: 1422.221436\n",
      "   47496/5000000: episode: 862, duration: 2.947s, episode steps: 63, steps per second: 21, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 2.508 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 573957.625000, mean_absolute_error: 1182.986206, mean_q: 1458.307983\n",
      "   47521/5000000: episode: 863, duration: 1.240s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.480 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 272771.656250, mean_absolute_error: 1118.444946, mean_q: 1376.532471\n",
      "   47555/5000000: episode: 864, duration: 1.587s, episode steps: 34, steps per second: 21, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.471 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 475605.343750, mean_absolute_error: 1175.023560, mean_q: 1433.184204\n",
      "   47663/5000000: episode: 865, duration: 5.184s, episode steps: 108, steps per second: 21, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.824 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 680787.500000, mean_absolute_error: 1180.846191, mean_q: 1451.675293\n",
      "   47688/5000000: episode: 866, duration: 1.185s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.560 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 733483.250000, mean_absolute_error: 1123.590210, mean_q: 1381.592041\n",
      "   47747/5000000: episode: 867, duration: 2.710s, episode steps: 59, steps per second: 22, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.678 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 717842.875000, mean_absolute_error: 1377.430298, mean_q: 1673.250244\n",
      "   47892/5000000: episode: 868, duration: 6.991s, episode steps: 145, steps per second: 21, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 2.448 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 747822.500000, mean_absolute_error: 1253.834839, mean_q: 1532.315796\n",
      "   48075/5000000: episode: 869, duration: 8.339s, episode steps: 183, steps per second: 22, episode reward: -1.000, mean reward: -0.005 [-1.000, 0.000], mean action: 2.475 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 714947.937500, mean_absolute_error: 1263.975220, mean_q: 1550.448120\n",
      "   48109/5000000: episode: 870, duration: 1.548s, episode steps: 34, steps per second: 22, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.735 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 610892.062500, mean_absolute_error: 1257.875854, mean_q: 1540.041748\n",
      "   48152/5000000: episode: 871, duration: 2.098s, episode steps: 43, steps per second: 20, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 2.279 [0.000, 5.000], mean observation: 0.062 [0.000, 24.000], loss: 752956.375000, mean_absolute_error: 1340.691528, mean_q: 1650.789917\n",
      "   48278/5000000: episode: 872, duration: 5.951s, episode steps: 126, steps per second: 21, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.516 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 526208.187500, mean_absolute_error: 1270.280640, mean_q: 1564.046143\n",
      "   48359/5000000: episode: 873, duration: 3.840s, episode steps: 81, steps per second: 21, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.210 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 612560.062500, mean_absolute_error: 1269.769897, mean_q: 1562.805786\n",
      "   48420/5000000: episode: 874, duration: 2.919s, episode steps: 61, steps per second: 21, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 2.393 [0.000, 5.000], mean observation: 0.073 [0.000, 24.000], loss: 546317.562500, mean_absolute_error: 1324.660278, mean_q: 1628.439453\n",
      "   48472/5000000: episode: 875, duration: 2.464s, episode steps: 52, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.558 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 581279.437500, mean_absolute_error: 1311.079956, mean_q: 1614.940674\n",
      "   48498/5000000: episode: 876, duration: 1.362s, episode steps: 26, steps per second: 19, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.769 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 514793.437500, mean_absolute_error: 1236.188599, mean_q: 1519.532104\n",
      "   48524/5000000: episode: 877, duration: 1.165s, episode steps: 26, steps per second: 22, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.577 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 428945.187500, mean_absolute_error: 1291.923340, mean_q: 1591.197876\n",
      "   48549/5000000: episode: 878, duration: 1.170s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.600 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 770927.187500, mean_absolute_error: 1466.759521, mean_q: 1770.677979\n",
      "   48576/5000000: episode: 879, duration: 1.330s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.630 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 746610.437500, mean_absolute_error: 1218.641724, mean_q: 1499.138428\n",
      "   48662/5000000: episode: 880, duration: 3.939s, episode steps: 86, steps per second: 22, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.430 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 415515.250000, mean_absolute_error: 1344.704346, mean_q: 1653.588623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   48687/5000000: episode: 881, duration: 1.119s, episode steps: 25, steps per second: 22, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.680 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 444471.093750, mean_absolute_error: 1372.189697, mean_q: 1685.195801\n",
      "   48804/5000000: episode: 882, duration: 5.054s, episode steps: 117, steps per second: 23, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.521 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 671530.312500, mean_absolute_error: 1304.497803, mean_q: 1603.978760\n",
      "   48864/5000000: episode: 883, duration: 2.878s, episode steps: 60, steps per second: 21, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.167 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 332317.031250, mean_absolute_error: 1336.498413, mean_q: 1639.502319\n",
      "   48918/5000000: episode: 884, duration: 2.543s, episode steps: 54, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.241 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 922505.187500, mean_absolute_error: 1362.414062, mean_q: 1672.623657\n",
      "   48977/5000000: episode: 885, duration: 2.990s, episode steps: 59, steps per second: 20, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.441 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 755254.625000, mean_absolute_error: 1318.344238, mean_q: 1620.160645\n",
      "   49149/5000000: episode: 886, duration: 8.188s, episode steps: 172, steps per second: 21, episode reward: -1.000, mean reward: -0.006 [-1.000, 0.000], mean action: 2.448 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 709275.812500, mean_absolute_error: 1375.751587, mean_q: 1685.186279\n",
      "   49200/5000000: episode: 887, duration: 2.430s, episode steps: 51, steps per second: 21, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 2.255 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 819846.187500, mean_absolute_error: 1522.004272, mean_q: 1854.064087\n",
      "   49229/5000000: episode: 888, duration: 1.389s, episode steps: 29, steps per second: 21, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.586 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1200090.750000, mean_absolute_error: 1501.994751, mean_q: 1822.029907\n",
      "   49256/5000000: episode: 889, duration: 1.254s, episode steps: 27, steps per second: 22, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.370 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 284271.468750, mean_absolute_error: 1457.041870, mean_q: 1775.589355\n",
      "   49318/5000000: episode: 890, duration: 2.895s, episode steps: 62, steps per second: 21, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 2.581 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 447356.812500, mean_absolute_error: 1457.250488, mean_q: 1790.569580\n",
      "   49353/5000000: episode: 891, duration: 1.626s, episode steps: 35, steps per second: 22, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.600 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 1129412.750000, mean_absolute_error: 1393.171387, mean_q: 1715.045166\n",
      "   49379/5000000: episode: 892, duration: 1.265s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.423 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 713620.062500, mean_absolute_error: 1483.420044, mean_q: 1823.145386\n",
      "   49405/5000000: episode: 893, duration: 1.253s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.000 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1040368.250000, mean_absolute_error: 1543.693481, mean_q: 1887.399292\n",
      "   49475/5000000: episode: 894, duration: 3.339s, episode steps: 70, steps per second: 21, episode reward: -1.000, mean reward: -0.014 [-1.000, 0.000], mean action: 2.314 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 492613.312500, mean_absolute_error: 1458.417969, mean_q: 1788.223389\n",
      "   49539/5000000: episode: 895, duration: 2.969s, episode steps: 64, steps per second: 22, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 2.609 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 443640.250000, mean_absolute_error: 1515.646729, mean_q: 1848.624634\n",
      "   49601/5000000: episode: 896, duration: 2.869s, episode steps: 62, steps per second: 22, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 2.952 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 754082.812500, mean_absolute_error: 1338.005981, mean_q: 1647.659790\n",
      "   49727/5000000: episode: 897, duration: 5.814s, episode steps: 126, steps per second: 22, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.381 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 593150.062500, mean_absolute_error: 1483.014893, mean_q: 1813.943726\n",
      "   49764/5000000: episode: 898, duration: 1.695s, episode steps: 37, steps per second: 22, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 2.000 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 917732.437500, mean_absolute_error: 1563.471680, mean_q: 1906.703369\n",
      "   49823/5000000: episode: 899, duration: 2.806s, episode steps: 59, steps per second: 21, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.441 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 957307.875000, mean_absolute_error: 1467.399048, mean_q: 1797.310791\n",
      "   49854/5000000: episode: 900, duration: 1.474s, episode steps: 31, steps per second: 21, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 2.613 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 403895.937500, mean_absolute_error: 1376.597656, mean_q: 1693.918091\n",
      "   49879/5000000: episode: 901, duration: 1.208s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.520 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 983845.937500, mean_absolute_error: 1440.688110, mean_q: 1762.223389\n",
      "   50156/5000000: episode: 902, duration: 12.370s, episode steps: 277, steps per second: 22, episode reward: -1.000, mean reward: -0.004 [-1.000, 0.000], mean action: 2.440 [0.000, 5.000], mean observation: 0.058 [0.000, 24.000], loss: 993735.500000, mean_absolute_error: 1576.798462, mean_q: 1922.760620\n",
      "   50297/5000000: episode: 903, duration: 6.852s, episode steps: 141, steps per second: 21, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 2.426 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 1273305.000000, mean_absolute_error: 1574.882324, mean_q: 1929.692383\n",
      "   50330/5000000: episode: 904, duration: 1.707s, episode steps: 33, steps per second: 19, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 3.152 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1268443.625000, mean_absolute_error: 1632.061890, mean_q: 1995.691772\n",
      "   50385/5000000: episode: 905, duration: 2.638s, episode steps: 55, steps per second: 21, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.382 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 898752.937500, mean_absolute_error: 1552.770142, mean_q: 1895.783569\n",
      "   50443/5000000: episode: 906, duration: 2.940s, episode steps: 58, steps per second: 20, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.759 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 1091685.000000, mean_absolute_error: 1624.014526, mean_q: 1990.345947\n",
      "   50656/5000000: episode: 907, duration: 10.374s, episode steps: 213, steps per second: 21, episode reward: -1.000, mean reward: -0.005 [-1.000, 0.000], mean action: 2.418 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 1006149.250000, mean_absolute_error: 1612.544800, mean_q: 1964.396362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   50712/5000000: episode: 908, duration: 2.737s, episode steps: 56, steps per second: 20, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.929 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1228422.000000, mean_absolute_error: 1678.000977, mean_q: 2037.649292\n",
      "   50768/5000000: episode: 909, duration: 2.591s, episode steps: 56, steps per second: 22, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.411 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 1416386.625000, mean_absolute_error: 1671.809570, mean_q: 2043.048096\n",
      "   50823/5000000: episode: 910, duration: 2.514s, episode steps: 55, steps per second: 22, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.382 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 608340.000000, mean_absolute_error: 1689.295166, mean_q: 2070.196777\n",
      "   50964/5000000: episode: 911, duration: 6.632s, episode steps: 141, steps per second: 21, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 2.645 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 1256257.875000, mean_absolute_error: 1639.409058, mean_q: 2008.385010\n",
      "   51019/5000000: episode: 912, duration: 17.244s, episode steps: 55, steps per second: 3, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.491 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1249148.500000, mean_absolute_error: 1709.196167, mean_q: 2076.225098\n",
      "   51151/5000000: episode: 913, duration: 6.439s, episode steps: 132, steps per second: 20, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.659 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 662874.437500, mean_absolute_error: 1660.508179, mean_q: 2034.477783\n",
      "   51271/5000000: episode: 914, duration: 5.844s, episode steps: 120, steps per second: 21, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.433 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 887013.312500, mean_absolute_error: 1766.266479, mean_q: 2166.370117\n",
      "   51387/5000000: episode: 915, duration: 5.429s, episode steps: 116, steps per second: 21, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.621 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 1500345.250000, mean_absolute_error: 1684.367065, mean_q: 2060.092285\n",
      "   51505/5000000: episode: 916, duration: 5.672s, episode steps: 118, steps per second: 21, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.559 [0.000, 5.000], mean observation: 0.073 [0.000, 24.000], loss: 671754.687500, mean_absolute_error: 1649.484985, mean_q: 2020.604492\n",
      "   51535/5000000: episode: 917, duration: 1.422s, episode steps: 30, steps per second: 21, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.633 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 1540543.375000, mean_absolute_error: 1690.302490, mean_q: 2071.692871\n",
      "   51636/5000000: episode: 918, duration: 4.850s, episode steps: 101, steps per second: 21, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.426 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1586054.500000, mean_absolute_error: 1768.346802, mean_q: 2157.002686\n",
      "   51663/5000000: episode: 919, duration: 1.407s, episode steps: 27, steps per second: 19, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.926 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1097238.500000, mean_absolute_error: 1785.519775, mean_q: 2194.472168\n",
      "   51694/5000000: episode: 920, duration: 1.606s, episode steps: 31, steps per second: 19, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 2.323 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 1429010.750000, mean_absolute_error: 1799.708008, mean_q: 2210.429199\n",
      "   51808/5000000: episode: 921, duration: 5.474s, episode steps: 114, steps per second: 21, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.561 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 600833.687500, mean_absolute_error: 1771.154297, mean_q: 2146.938232\n",
      "   51955/5000000: episode: 922, duration: 7.293s, episode steps: 147, steps per second: 20, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 2.327 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 1042954.000000, mean_absolute_error: 1726.246826, mean_q: 2115.086670\n",
      "   51982/5000000: episode: 923, duration: 1.294s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 3.222 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1286388.250000, mean_absolute_error: 1792.420288, mean_q: 2159.523193\n",
      "   52047/5000000: episode: 924, duration: 3.080s, episode steps: 65, steps per second: 21, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 2.538 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1499047.875000, mean_absolute_error: 1911.639038, mean_q: 2345.589844\n",
      "   52087/5000000: episode: 925, duration: 1.894s, episode steps: 40, steps per second: 21, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 2.425 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 1459958.750000, mean_absolute_error: 1881.651978, mean_q: 2307.177002\n",
      "   52137/5000000: episode: 926, duration: 2.333s, episode steps: 50, steps per second: 21, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 2.400 [0.000, 5.000], mean observation: 0.076 [0.000, 24.000], loss: 2440526.000000, mean_absolute_error: 1939.316406, mean_q: 2371.788574\n",
      "   52174/5000000: episode: 927, duration: 1.805s, episode steps: 37, steps per second: 21, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 2.486 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1238495.625000, mean_absolute_error: 1707.756958, mean_q: 2089.529541\n",
      "   52245/5000000: episode: 928, duration: 3.460s, episode steps: 71, steps per second: 21, episode reward: -1.000, mean reward: -0.014 [-1.000, 0.000], mean action: 2.268 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 1791700.375000, mean_absolute_error: 1894.931152, mean_q: 2316.939697\n",
      "   52276/5000000: episode: 929, duration: 1.389s, episode steps: 31, steps per second: 22, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 2.548 [0.000, 5.000], mean observation: 0.062 [0.000, 24.000], loss: 1251138.125000, mean_absolute_error: 1836.115723, mean_q: 2257.322266\n",
      "   52306/5000000: episode: 930, duration: 1.451s, episode steps: 30, steps per second: 21, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.367 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 1785746.750000, mean_absolute_error: 2213.728271, mean_q: 2651.167969\n",
      "   52336/5000000: episode: 931, duration: 1.415s, episode steps: 30, steps per second: 21, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.000 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 516763.218750, mean_absolute_error: 1666.350098, mean_q: 2045.175537\n",
      "   52402/5000000: episode: 932, duration: 3.372s, episode steps: 66, steps per second: 20, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 2.606 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 2088599.500000, mean_absolute_error: 1921.356079, mean_q: 2342.689941\n",
      "   52436/5000000: episode: 933, duration: 1.695s, episode steps: 34, steps per second: 20, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.824 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 2480594.750000, mean_absolute_error: 1893.843628, mean_q: 2304.375488\n",
      "   52465/5000000: episode: 934, duration: 1.418s, episode steps: 29, steps per second: 20, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.103 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 787386.937500, mean_absolute_error: 1884.409790, mean_q: 2306.621582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   52590/5000000: episode: 935, duration: 6.127s, episode steps: 125, steps per second: 20, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.664 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 1859690.000000, mean_absolute_error: 1956.981201, mean_q: 2389.634766\n",
      "   52617/5000000: episode: 936, duration: 1.386s, episode steps: 27, steps per second: 19, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.407 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 799163.562500, mean_absolute_error: 1948.598389, mean_q: 2384.162598\n",
      "   52679/5000000: episode: 937, duration: 3.026s, episode steps: 62, steps per second: 20, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 3.145 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 2283334.000000, mean_absolute_error: 1897.678955, mean_q: 2316.779541\n",
      "   52705/5000000: episode: 938, duration: 1.225s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.846 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 858513.750000, mean_absolute_error: 1873.780762, mean_q: 2289.291748\n",
      "   52732/5000000: episode: 939, duration: 1.383s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.074 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 2247398.500000, mean_absolute_error: 1825.885986, mean_q: 2246.872559\n",
      "   52766/5000000: episode: 940, duration: 1.610s, episode steps: 34, steps per second: 21, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.147 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 2449001.250000, mean_absolute_error: 1918.720703, mean_q: 2335.414795\n",
      "   52895/5000000: episode: 941, duration: 5.891s, episode steps: 129, steps per second: 22, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.450 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1168162.625000, mean_absolute_error: 1938.171753, mean_q: 2374.448730\n",
      "   52950/5000000: episode: 942, duration: 2.768s, episode steps: 55, steps per second: 20, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.345 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 1137319.750000, mean_absolute_error: 2077.372559, mean_q: 2541.810303\n",
      "   52975/5000000: episode: 943, duration: 1.303s, episode steps: 25, steps per second: 19, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.720 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 1646486.125000, mean_absolute_error: 2091.691895, mean_q: 2517.249268\n",
      "   53041/5000000: episode: 944, duration: 3.336s, episode steps: 66, steps per second: 20, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 2.000 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 2514742.250000, mean_absolute_error: 1974.927734, mean_q: 2410.661133\n",
      "   53068/5000000: episode: 945, duration: 1.242s, episode steps: 27, steps per second: 22, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 3.111 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 518065.875000, mean_absolute_error: 1963.090088, mean_q: 2413.466553\n",
      "   53159/5000000: episode: 946, duration: 4.449s, episode steps: 91, steps per second: 20, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.648 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 1560586.875000, mean_absolute_error: 1989.493164, mean_q: 2446.490234\n",
      "   53245/5000000: episode: 947, duration: 4.240s, episode steps: 86, steps per second: 20, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.163 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 2180127.500000, mean_absolute_error: 2026.358154, mean_q: 2486.077881\n",
      "   53317/5000000: episode: 948, duration: 3.382s, episode steps: 72, steps per second: 21, episode reward: -1.000, mean reward: -0.014 [-1.000, 0.000], mean action: 2.208 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1881364.000000, mean_absolute_error: 2172.606689, mean_q: 2645.657959\n",
      "   53511/5000000: episode: 949, duration: 9.516s, episode steps: 194, steps per second: 20, episode reward: -1.000, mean reward: -0.005 [-1.000, 0.000], mean action: 2.278 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 1841837.250000, mean_absolute_error: 2062.764404, mean_q: 2527.262451\n",
      "   53566/5000000: episode: 950, duration: 2.690s, episode steps: 55, steps per second: 20, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.345 [0.000, 5.000], mean observation: 0.074 [0.000, 24.000], loss: 2004047.250000, mean_absolute_error: 2198.764893, mean_q: 2664.500732\n",
      "   53593/5000000: episode: 951, duration: 1.287s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.296 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 2374623.250000, mean_absolute_error: 2389.136475, mean_q: 2899.966797\n",
      "   53620/5000000: episode: 952, duration: 1.294s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.519 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 2423321.500000, mean_absolute_error: 2152.006104, mean_q: 2614.802979\n",
      "   53654/5000000: episode: 953, duration: 1.626s, episode steps: 34, steps per second: 21, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.618 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 2942550.000000, mean_absolute_error: 2018.604126, mean_q: 2466.721924\n",
      "   53707/5000000: episode: 954, duration: 2.398s, episode steps: 53, steps per second: 22, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.925 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 1959182.750000, mean_absolute_error: 2090.354004, mean_q: 2552.043701\n",
      "   53732/5000000: episode: 955, duration: 1.170s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.480 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 2884390.000000, mean_absolute_error: 2386.497314, mean_q: 2873.163330\n",
      "   53811/5000000: episode: 956, duration: 3.721s, episode steps: 79, steps per second: 21, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 2.646 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 2735432.750000, mean_absolute_error: 2127.409912, mean_q: 2594.549805\n",
      "   53845/5000000: episode: 957, duration: 1.597s, episode steps: 34, steps per second: 21, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.441 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 1345861.375000, mean_absolute_error: 2295.873535, mean_q: 2798.896484\n",
      "   53881/5000000: episode: 958, duration: 1.664s, episode steps: 36, steps per second: 22, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 2.556 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 1590575.375000, mean_absolute_error: 1955.507568, mean_q: 2398.682373\n",
      "   53908/5000000: episode: 959, duration: 1.305s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.704 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 4681336.500000, mean_absolute_error: 2301.241699, mean_q: 2820.488037\n",
      "   53936/5000000: episode: 960, duration: 1.329s, episode steps: 28, steps per second: 21, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.250 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 715126.500000, mean_absolute_error: 2091.064209, mean_q: 2571.363037\n",
      "   53965/5000000: episode: 961, duration: 1.353s, episode steps: 29, steps per second: 21, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.000 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 2855674.000000, mean_absolute_error: 2266.533203, mean_q: 2775.417236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   54008/5000000: episode: 962, duration: 1.946s, episode steps: 43, steps per second: 22, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 2.326 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 1871924.625000, mean_absolute_error: 2071.746338, mean_q: 2537.257324\n",
      "   54098/5000000: episode: 963, duration: 4.340s, episode steps: 90, steps per second: 21, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.544 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 3629339.750000, mean_absolute_error: 2337.366943, mean_q: 2842.648438\n",
      "   54141/5000000: episode: 964, duration: 2.065s, episode steps: 43, steps per second: 21, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 2.070 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 1147876.750000, mean_absolute_error: 2349.723877, mean_q: 2846.072266\n",
      "   54307/5000000: episode: 965, duration: 8.130s, episode steps: 166, steps per second: 20, episode reward: -1.000, mean reward: -0.006 [-1.000, 0.000], mean action: 2.524 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 3115131.750000, mean_absolute_error: 2278.925537, mean_q: 2785.583496\n",
      "   54393/5000000: episode: 966, duration: 3.870s, episode steps: 86, steps per second: 22, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.500 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 3260185.750000, mean_absolute_error: 2343.612793, mean_q: 2858.253662\n",
      "   54419/5000000: episode: 967, duration: 1.282s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.308 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 748955.875000, mean_absolute_error: 2156.624512, mean_q: 2650.868164\n",
      "   54447/5000000: episode: 968, duration: 1.456s, episode steps: 28, steps per second: 19, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.536 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 2081026.375000, mean_absolute_error: 2711.746582, mean_q: 3300.334229\n",
      "   54744/5000000: episode: 969, duration: 14.583s, episode steps: 297, steps per second: 20, episode reward: -1.000, mean reward: -0.003 [-1.000, 0.000], mean action: 2.367 [0.000, 5.000], mean observation: 0.061 [0.000, 24.000], loss: 2219323.000000, mean_absolute_error: 2399.730225, mean_q: 2927.630371\n",
      "   54777/5000000: episode: 970, duration: 1.548s, episode steps: 33, steps per second: 21, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 2.394 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1389510.250000, mean_absolute_error: 2565.528564, mean_q: 3118.076904\n",
      "   54802/5000000: episode: 971, duration: 1.294s, episode steps: 25, steps per second: 19, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.360 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 2960762.000000, mean_absolute_error: 2373.651367, mean_q: 2884.129395\n",
      "   54897/5000000: episode: 972, duration: 4.660s, episode steps: 95, steps per second: 20, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.611 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 2179385.000000, mean_absolute_error: 2434.948242, mean_q: 2962.354248\n",
      "   54922/5000000: episode: 973, duration: 1.185s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.680 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 3156708.500000, mean_absolute_error: 3142.474365, mean_q: 3759.363525\n",
      "   54947/5000000: episode: 974, duration: 1.254s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.280 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 4026542.000000, mean_absolute_error: 2789.963867, mean_q: 3390.332520\n",
      "   55051/5000000: episode: 975, duration: 5.104s, episode steps: 104, steps per second: 20, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.519 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 2539966.500000, mean_absolute_error: 2441.145508, mean_q: 2976.504150\n",
      "   55080/5000000: episode: 976, duration: 1.383s, episode steps: 29, steps per second: 21, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.000 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 2594130.750000, mean_absolute_error: 2338.800293, mean_q: 2867.638672\n",
      "   55105/5000000: episode: 977, duration: 1.115s, episode steps: 25, steps per second: 22, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.480 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 6183049.000000, mean_absolute_error: 2752.084717, mean_q: 3344.314697\n",
      "   55134/5000000: episode: 978, duration: 1.341s, episode steps: 29, steps per second: 22, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.759 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 4998154.000000, mean_absolute_error: 2586.995605, mean_q: 3150.212646\n",
      "   55189/5000000: episode: 979, duration: 2.598s, episode steps: 55, steps per second: 21, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.564 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 3398283.000000, mean_absolute_error: 2485.600586, mean_q: 3026.225830\n",
      "   55385/5000000: episode: 980, duration: 9.534s, episode steps: 196, steps per second: 21, episode reward: -1.000, mean reward: -0.005 [-1.000, 0.000], mean action: 2.469 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 2532225.750000, mean_absolute_error: 2669.312256, mean_q: 3257.346191\n",
      "   55439/5000000: episode: 981, duration: 2.486s, episode steps: 54, steps per second: 22, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.611 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1575814.875000, mean_absolute_error: 2589.209229, mean_q: 3145.820312\n",
      "   55464/5000000: episode: 982, duration: 1.228s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.200 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 956091.937500, mean_absolute_error: 2416.484375, mean_q: 2946.287598\n",
      "   55493/5000000: episode: 983, duration: 1.324s, episode steps: 29, steps per second: 22, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.517 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 4556776.500000, mean_absolute_error: 2700.727539, mean_q: 3284.774170\n",
      "   55522/5000000: episode: 984, duration: 1.449s, episode steps: 29, steps per second: 20, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.655 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 3967983.000000, mean_absolute_error: 2627.616211, mean_q: 3171.586182\n",
      "   55550/5000000: episode: 985, duration: 1.365s, episode steps: 28, steps per second: 21, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.464 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 3688738.500000, mean_absolute_error: 2659.878906, mean_q: 3238.243408\n",
      "   55585/5000000: episode: 986, duration: 1.687s, episode steps: 35, steps per second: 21, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.229 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 4184662.500000, mean_absolute_error: 2567.232178, mean_q: 3136.918701\n",
      "   55611/5000000: episode: 987, duration: 1.203s, episode steps: 26, steps per second: 22, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.000 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 2684489.250000, mean_absolute_error: 2592.648682, mean_q: 3141.678223\n",
      "   55636/5000000: episode: 988, duration: 1.262s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.400 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 2719200.750000, mean_absolute_error: 2571.941650, mean_q: 3150.121582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   55761/5000000: episode: 989, duration: 6.234s, episode steps: 125, steps per second: 20, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.488 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 4129789.000000, mean_absolute_error: 2504.731445, mean_q: 3060.682861\n",
      "   55900/5000000: episode: 990, duration: 6.631s, episode steps: 139, steps per second: 21, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 2.619 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 3716076.500000, mean_absolute_error: 2598.209961, mean_q: 3180.192383\n",
      "   55925/5000000: episode: 991, duration: 1.211s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.360 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 1905230.875000, mean_absolute_error: 2482.255859, mean_q: 3040.997559\n",
      "   55986/5000000: episode: 992, duration: 2.952s, episode steps: 61, steps per second: 21, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 2.672 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 2841270.000000, mean_absolute_error: 2760.837891, mean_q: 3388.614746\n",
      "   56016/5000000: episode: 993, duration: 1.421s, episode steps: 30, steps per second: 21, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.233 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 6410843.000000, mean_absolute_error: 2737.938477, mean_q: 3338.478271\n",
      "   56087/5000000: episode: 994, duration: 3.280s, episode steps: 71, steps per second: 22, episode reward: -1.000, mean reward: -0.014 [-1.000, 0.000], mean action: 2.423 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 4376969.000000, mean_absolute_error: 2617.007568, mean_q: 3202.116455\n",
      "   56112/5000000: episode: 995, duration: 1.233s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.440 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 3107653.500000, mean_absolute_error: 2409.564209, mean_q: 2952.280518\n",
      "   56151/5000000: episode: 996, duration: 1.878s, episode steps: 39, steps per second: 21, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 2.513 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 1728224.375000, mean_absolute_error: 2886.599609, mean_q: 3526.118896\n",
      "   56180/5000000: episode: 997, duration: 1.409s, episode steps: 29, steps per second: 21, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.655 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 3098204.250000, mean_absolute_error: 2643.303711, mean_q: 3247.580811\n",
      "   56286/5000000: episode: 998, duration: 4.994s, episode steps: 106, steps per second: 21, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.547 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 3071754.250000, mean_absolute_error: 2777.261230, mean_q: 3413.252686\n",
      "   56341/5000000: episode: 999, duration: 2.533s, episode steps: 55, steps per second: 22, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.618 [0.000, 5.000], mean observation: 0.074 [0.000, 24.000], loss: 7234603.500000, mean_absolute_error: 2807.803467, mean_q: 3435.828125\n",
      "   56372/5000000: episode: 1000, duration: 1.477s, episode steps: 31, steps per second: 21, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 1.903 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 4999946.000000, mean_absolute_error: 2843.669189, mean_q: 3488.058105\n",
      "   56452/5000000: episode: 1001, duration: 4.155s, episode steps: 80, steps per second: 19, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 2.725 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 5136143.500000, mean_absolute_error: 2828.426270, mean_q: 3464.957764\n",
      "   56477/5000000: episode: 1002, duration: 1.185s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.960 [1.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 2552263.250000, mean_absolute_error: 3381.442871, mean_q: 4090.854736\n",
      "   56503/5000000: episode: 1003, duration: 1.279s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.346 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 10186660.000000, mean_absolute_error: 2721.282471, mean_q: 3347.163574\n",
      "   56599/5000000: episode: 1004, duration: 4.761s, episode steps: 96, steps per second: 20, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.396 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 5091397.500000, mean_absolute_error: 2758.318359, mean_q: 3392.032471\n",
      "   56656/5000000: episode: 1005, duration: 2.695s, episode steps: 57, steps per second: 21, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.105 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 3970805.250000, mean_absolute_error: 2842.767090, mean_q: 3499.671631\n",
      "   56813/5000000: episode: 1006, duration: 7.827s, episode steps: 157, steps per second: 20, episode reward: -1.000, mean reward: -0.006 [-1.000, 0.000], mean action: 2.338 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 5175661.000000, mean_absolute_error: 2877.798096, mean_q: 3533.191895\n",
      "   56868/5000000: episode: 1007, duration: 2.566s, episode steps: 55, steps per second: 21, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.655 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 2333729.500000, mean_absolute_error: 2907.552734, mean_q: 3555.042969\n",
      "   56907/5000000: episode: 1008, duration: 1.789s, episode steps: 39, steps per second: 22, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 2.385 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 5514276.500000, mean_absolute_error: 3382.376709, mean_q: 4113.834473\n",
      "   56932/5000000: episode: 1009, duration: 1.215s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.920 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 3958432.000000, mean_absolute_error: 2788.340088, mean_q: 3420.764893\n",
      "   56991/5000000: episode: 1010, duration: 2.781s, episode steps: 59, steps per second: 21, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.220 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1699562.250000, mean_absolute_error: 2754.955566, mean_q: 3388.155762\n",
      "   57018/5000000: episode: 1011, duration: 11.014s, episode steps: 27, steps per second: 2, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.519 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 3548478.250000, mean_absolute_error: 3357.044678, mean_q: 4130.993164\n",
      "   57069/5000000: episode: 1012, duration: 2.362s, episode steps: 51, steps per second: 22, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 2.588 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 7109989.000000, mean_absolute_error: 3011.040527, mean_q: 3658.737793\n",
      "   57135/5000000: episode: 1013, duration: 3.155s, episode steps: 66, steps per second: 21, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 2.227 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 3995221.000000, mean_absolute_error: 2999.142090, mean_q: 3675.807129\n",
      "   57282/5000000: episode: 1014, duration: 7.032s, episode steps: 147, steps per second: 21, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 2.578 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 4172983.250000, mean_absolute_error: 3150.312012, mean_q: 3868.951416\n",
      "   57307/5000000: episode: 1015, duration: 1.183s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.120 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 3063609.250000, mean_absolute_error: 3204.644287, mean_q: 3842.630859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   57344/5000000: episode: 1016, duration: 1.793s, episode steps: 37, steps per second: 21, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 2.135 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 4161709.500000, mean_absolute_error: 3122.834229, mean_q: 3824.934082\n",
      "   57370/5000000: episode: 1017, duration: 1.224s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.615 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 3046029.000000, mean_absolute_error: 3641.902344, mean_q: 4442.998047\n",
      "   57406/5000000: episode: 1018, duration: 1.678s, episode steps: 36, steps per second: 21, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 2.806 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 6631984.500000, mean_absolute_error: 3217.614258, mean_q: 3924.109375\n",
      "   57547/5000000: episode: 1019, duration: 6.793s, episode steps: 141, steps per second: 21, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 2.596 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 6645901.500000, mean_absolute_error: 3329.717285, mean_q: 4078.287598\n",
      "   57601/5000000: episode: 1020, duration: 2.553s, episode steps: 54, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.630 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 2319571.250000, mean_absolute_error: 3193.208984, mean_q: 3917.179199\n",
      "   57684/5000000: episode: 1021, duration: 4.115s, episode steps: 83, steps per second: 20, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.096 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 4262031.000000, mean_absolute_error: 3322.507812, mean_q: 4069.962402\n",
      "   57709/5000000: episode: 1022, duration: 1.239s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 1.840 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 3356778.500000, mean_absolute_error: 3332.973145, mean_q: 4079.189453\n",
      "   57740/5000000: episode: 1023, duration: 1.495s, episode steps: 31, steps per second: 21, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 2.161 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 6132326.000000, mean_absolute_error: 3219.673584, mean_q: 3959.195312\n",
      "   57868/5000000: episode: 1024, duration: 5.821s, episode steps: 128, steps per second: 22, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.766 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 7111843.000000, mean_absolute_error: 3638.751465, mean_q: 4438.270996\n",
      "   57893/5000000: episode: 1025, duration: 1.406s, episode steps: 25, steps per second: 18, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.720 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 8163602.000000, mean_absolute_error: 3897.749268, mean_q: 4718.743652\n",
      "   58026/5000000: episode: 1026, duration: 6.483s, episode steps: 133, steps per second: 21, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.429 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 4691570.000000, mean_absolute_error: 3447.334473, mean_q: 4231.496582\n",
      "   58062/5000000: episode: 1027, duration: 1.653s, episode steps: 36, steps per second: 22, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 2.528 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 2410921.000000, mean_absolute_error: 3513.728027, mean_q: 4326.351562\n",
      "   58087/5000000: episode: 1028, duration: 1.135s, episode steps: 25, steps per second: 22, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.040 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 1478872.125000, mean_absolute_error: 3319.588135, mean_q: 4083.573242\n",
      "   58117/5000000: episode: 1029, duration: 1.419s, episode steps: 30, steps per second: 21, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.267 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 2896919.000000, mean_absolute_error: 3492.027100, mean_q: 4285.857910\n",
      "   58272/5000000: episode: 1030, duration: 7.548s, episode steps: 155, steps per second: 21, episode reward: -1.000, mean reward: -0.006 [-1.000, 0.000], mean action: 2.406 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 8817449.000000, mean_absolute_error: 3640.817383, mean_q: 4457.702148\n",
      "   58485/5000000: episode: 1031, duration: 10.550s, episode steps: 213, steps per second: 20, episode reward: -1.000, mean reward: -0.005 [-1.000, 0.000], mean action: 2.587 [0.000, 5.000], mean observation: 0.057 [0.000, 24.000], loss: 7897897.000000, mean_absolute_error: 3664.227051, mean_q: 4493.660156\n",
      "   58603/5000000: episode: 1032, duration: 5.761s, episode steps: 118, steps per second: 20, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.424 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 8075354.000000, mean_absolute_error: 3846.628174, mean_q: 4694.406738\n",
      "   58660/5000000: episode: 1033, duration: 2.683s, episode steps: 57, steps per second: 21, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.474 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 7861918.500000, mean_absolute_error: 3917.643311, mean_q: 4782.929688\n",
      "   58685/5000000: episode: 1034, duration: 1.336s, episode steps: 25, steps per second: 19, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.400 [1.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 7089193.500000, mean_absolute_error: 3636.210938, mean_q: 4472.443359\n",
      "   58788/5000000: episode: 1035, duration: 4.925s, episode steps: 103, steps per second: 21, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.670 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 5945884.000000, mean_absolute_error: 3732.681152, mean_q: 4570.384277\n",
      "   59127/5000000: episode: 1036, duration: 16.033s, episode steps: 339, steps per second: 21, episode reward: -1.000, mean reward: -0.003 [-1.000, 0.000], mean action: 2.510 [0.000, 5.000], mean observation: 0.060 [0.000, 24.000], loss: 6852401.000000, mean_absolute_error: 3953.836182, mean_q: 4849.327637\n",
      "   59210/5000000: episode: 1037, duration: 4.183s, episode steps: 83, steps per second: 20, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.241 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 3153267.000000, mean_absolute_error: 3937.904297, mean_q: 4835.041504\n",
      "   59236/5000000: episode: 1038, duration: 1.268s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.538 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 10440673.000000, mean_absolute_error: 4300.671387, mean_q: 5298.122070\n",
      "   59288/5000000: episode: 1039, duration: 2.455s, episode steps: 52, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.788 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 12095803.000000, mean_absolute_error: 4130.386230, mean_q: 5073.016602\n",
      "   59323/5000000: episode: 1040, duration: 1.627s, episode steps: 35, steps per second: 22, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.571 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 10800166.000000, mean_absolute_error: 3836.030273, mean_q: 4691.831543\n",
      "   59382/5000000: episode: 1041, duration: 2.918s, episode steps: 59, steps per second: 20, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.814 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 10536476.000000, mean_absolute_error: 4319.480469, mean_q: 5321.462891\n",
      "   59436/5000000: episode: 1042, duration: 2.545s, episode steps: 54, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.556 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 8193118.000000, mean_absolute_error: 4045.420898, mean_q: 4969.453125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   59461/5000000: episode: 1043, duration: 1.185s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.200 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 9408040.000000, mean_absolute_error: 5183.107422, mean_q: 6349.385742\n",
      "   59532/5000000: episode: 1044, duration: 3.320s, episode steps: 71, steps per second: 21, episode reward: -1.000, mean reward: -0.014 [-1.000, 0.000], mean action: 2.493 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 7765318.500000, mean_absolute_error: 4405.760254, mean_q: 5374.086426\n",
      "   59557/5000000: episode: 1045, duration: 1.210s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.640 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 2894445.500000, mean_absolute_error: 4139.917969, mean_q: 5102.597168\n",
      "   59594/5000000: episode: 1046, duration: 1.642s, episode steps: 37, steps per second: 23, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 2.676 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 9977791.000000, mean_absolute_error: 4135.327148, mean_q: 5093.823242\n",
      "   59625/5000000: episode: 1047, duration: 1.483s, episode steps: 31, steps per second: 21, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 2.129 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 3773472.250000, mean_absolute_error: 4755.511230, mean_q: 5850.465820\n",
      "   59683/5000000: episode: 1048, duration: 2.836s, episode steps: 58, steps per second: 20, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.603 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 7295170.000000, mean_absolute_error: 4255.626465, mean_q: 5236.791016\n",
      "   59788/5000000: episode: 1049, duration: 4.565s, episode steps: 105, steps per second: 23, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.686 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 13364120.000000, mean_absolute_error: 4519.128418, mean_q: 5547.624023\n",
      "   59814/5000000: episode: 1050, duration: 1.232s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.538 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 2829358.750000, mean_absolute_error: 4894.152832, mean_q: 6035.156738\n",
      "   59911/5000000: episode: 1051, duration: 4.672s, episode steps: 97, steps per second: 21, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.237 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 10516076.000000, mean_absolute_error: 4802.490234, mean_q: 5867.792480\n",
      "   59938/5000000: episode: 1052, duration: 1.281s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.778 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 13574140.000000, mean_absolute_error: 5067.434082, mean_q: 6132.351074\n",
      "   59963/5000000: episode: 1053, duration: 1.216s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 1.960 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 7844551.000000, mean_absolute_error: 4557.018066, mean_q: 5589.592285\n",
      "   60016/5000000: episode: 1054, duration: 2.643s, episode steps: 53, steps per second: 20, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.245 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 13554207.000000, mean_absolute_error: 4510.006348, mean_q: 5481.869141\n",
      "   60042/5000000: episode: 1055, duration: 1.189s, episode steps: 26, steps per second: 22, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.731 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 4561312.000000, mean_absolute_error: 4103.765137, mean_q: 5039.986328\n",
      "   60068/5000000: episode: 1056, duration: 1.312s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.346 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 15018974.000000, mean_absolute_error: 4712.737305, mean_q: 5778.297852\n",
      "   60096/5000000: episode: 1057, duration: 1.433s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.536 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 5010713.500000, mean_absolute_error: 5091.508301, mean_q: 6266.975586\n",
      "   60154/5000000: episode: 1058, duration: 2.846s, episode steps: 58, steps per second: 20, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.638 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 3642384.000000, mean_absolute_error: 4396.767090, mean_q: 5409.836426\n",
      "   60253/5000000: episode: 1059, duration: 4.726s, episode steps: 99, steps per second: 21, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.424 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 13493763.000000, mean_absolute_error: 4965.995117, mean_q: 6097.234375\n",
      "   60348/5000000: episode: 1060, duration: 4.733s, episode steps: 95, steps per second: 20, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.305 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 10417009.000000, mean_absolute_error: 4516.738281, mean_q: 5545.894043\n",
      "   60373/5000000: episode: 1061, duration: 1.196s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.080 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 13949486.000000, mean_absolute_error: 4748.781250, mean_q: 5834.627441\n",
      "   60580/5000000: episode: 1062, duration: 9.541s, episode steps: 207, steps per second: 22, episode reward: -1.000, mean reward: -0.005 [-1.000, 0.000], mean action: 2.469 [0.000, 5.000], mean observation: 0.060 [0.000, 24.000], loss: 12854911.000000, mean_absolute_error: 4841.976074, mean_q: 5930.249512\n",
      "   60673/5000000: episode: 1063, duration: 4.461s, episode steps: 93, steps per second: 21, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.527 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 8945319.000000, mean_absolute_error: 5009.478027, mean_q: 6149.640625\n",
      "   60714/5000000: episode: 1064, duration: 1.958s, episode steps: 41, steps per second: 21, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 2.512 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 11761274.000000, mean_absolute_error: 5185.758789, mean_q: 6322.401855\n",
      "   60767/5000000: episode: 1065, duration: 2.647s, episode steps: 53, steps per second: 20, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.698 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 9115376.000000, mean_absolute_error: 4948.604492, mean_q: 6085.479980\n",
      "   60796/5000000: episode: 1066, duration: 1.465s, episode steps: 29, steps per second: 20, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.448 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 17045490.000000, mean_absolute_error: 4679.333008, mean_q: 5738.113281\n",
      "   60824/5000000: episode: 1067, duration: 1.419s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.393 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 11675680.000000, mean_absolute_error: 5128.670898, mean_q: 6314.063477\n",
      "   60869/5000000: episode: 1068, duration: 2.237s, episode steps: 45, steps per second: 20, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 2.333 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 3544831.750000, mean_absolute_error: 4997.765625, mean_q: 6145.842285\n",
      "   60898/5000000: episode: 1069, duration: 1.458s, episode steps: 29, steps per second: 20, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.172 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 6835340.500000, mean_absolute_error: 5538.037109, mean_q: 6822.027344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   60923/5000000: episode: 1070, duration: 1.261s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.840 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 4349967.000000, mean_absolute_error: 5089.900391, mean_q: 6272.134766\n",
      "   61015/5000000: episode: 1071, duration: 4.435s, episode steps: 92, steps per second: 21, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.522 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 11761504.000000, mean_absolute_error: 5375.973633, mean_q: 6607.303223\n",
      "   61043/5000000: episode: 1072, duration: 1.321s, episode steps: 28, steps per second: 21, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.607 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 7766805.000000, mean_absolute_error: 5089.201660, mean_q: 6249.171875\n",
      "   61212/5000000: episode: 1073, duration: 8.077s, episode steps: 169, steps per second: 21, episode reward: -1.000, mean reward: -0.006 [-1.000, 0.000], mean action: 2.728 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 10209716.000000, mean_absolute_error: 5154.372559, mean_q: 6324.190918\n",
      "   61244/5000000: episode: 1074, duration: 1.558s, episode steps: 32, steps per second: 21, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 2.750 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 22481976.000000, mean_absolute_error: 5519.094727, mean_q: 6771.656250\n",
      "   61302/5000000: episode: 1075, duration: 2.905s, episode steps: 58, steps per second: 20, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.638 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 4989544.500000, mean_absolute_error: 5009.689453, mean_q: 6145.846680\n",
      "   61358/5000000: episode: 1076, duration: 2.739s, episode steps: 56, steps per second: 20, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.518 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 13840394.000000, mean_absolute_error: 5400.025879, mean_q: 6621.195312\n",
      "   61439/5000000: episode: 1077, duration: 3.791s, episode steps: 81, steps per second: 21, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.420 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 14579993.000000, mean_absolute_error: 5536.013672, mean_q: 6801.451172\n",
      "   61529/5000000: episode: 1078, duration: 4.307s, episode steps: 90, steps per second: 21, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.244 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 9057115.000000, mean_absolute_error: 5326.562988, mean_q: 6544.516602\n",
      "   61559/5000000: episode: 1079, duration: 1.410s, episode steps: 30, steps per second: 21, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.000 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 3854951.500000, mean_absolute_error: 5577.087402, mean_q: 6855.809082\n",
      "   61655/5000000: episode: 1080, duration: 4.807s, episode steps: 96, steps per second: 20, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.542 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 13021560.000000, mean_absolute_error: 5735.430176, mean_q: 7037.664062\n",
      "   61684/5000000: episode: 1081, duration: 1.366s, episode steps: 29, steps per second: 21, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.034 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 8518388.000000, mean_absolute_error: 5719.683105, mean_q: 7022.256836\n",
      "   61739/5000000: episode: 1082, duration: 2.627s, episode steps: 55, steps per second: 21, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.255 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 9323362.000000, mean_absolute_error: 5643.336426, mean_q: 6919.830078\n",
      "   61768/5000000: episode: 1083, duration: 1.457s, episode steps: 29, steps per second: 20, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 1.862 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 7740842.000000, mean_absolute_error: 5768.627930, mean_q: 7079.840332\n",
      "   61800/5000000: episode: 1084, duration: 1.612s, episode steps: 32, steps per second: 20, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 2.344 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 14437143.000000, mean_absolute_error: 5507.215820, mean_q: 6748.164062\n",
      "   61833/5000000: episode: 1085, duration: 1.562s, episode steps: 33, steps per second: 21, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 2.394 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 3017568.000000, mean_absolute_error: 5295.872070, mean_q: 6494.420410\n",
      "   61861/5000000: episode: 1086, duration: 1.337s, episode steps: 28, steps per second: 21, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 3.036 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 7854625.500000, mean_absolute_error: 5687.812988, mean_q: 6969.967285\n",
      "   61995/5000000: episode: 1087, duration: 6.483s, episode steps: 134, steps per second: 21, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 2.828 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 7858256.500000, mean_absolute_error: 5568.904785, mean_q: 6828.206055\n",
      "   62031/5000000: episode: 1088, duration: 1.746s, episode steps: 36, steps per second: 21, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 2.417 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 14840541.000000, mean_absolute_error: 5675.491699, mean_q: 6951.460938\n",
      "   62067/5000000: episode: 1089, duration: 1.639s, episode steps: 36, steps per second: 22, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 2.639 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 9506330.000000, mean_absolute_error: 6113.761230, mean_q: 7500.235352\n",
      "   62150/5000000: episode: 1090, duration: 3.959s, episode steps: 83, steps per second: 21, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.578 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 9270576.000000, mean_absolute_error: 5931.979492, mean_q: 7267.985840\n",
      "   62232/5000000: episode: 1091, duration: 3.857s, episode steps: 82, steps per second: 21, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.476 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 17658026.000000, mean_absolute_error: 6033.736328, mean_q: 7377.849609\n",
      "   62261/5000000: episode: 1092, duration: 1.511s, episode steps: 29, steps per second: 19, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.586 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 6339024.500000, mean_absolute_error: 6650.004883, mean_q: 8196.070312\n",
      "   62287/5000000: episode: 1093, duration: 1.334s, episode steps: 26, steps per second: 19, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.500 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 11047934.000000, mean_absolute_error: 6399.200684, mean_q: 7870.927734\n",
      "   62349/5000000: episode: 1094, duration: 3.174s, episode steps: 62, steps per second: 20, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 2.403 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 13128461.000000, mean_absolute_error: 6434.416992, mean_q: 7891.922363\n",
      "   62381/5000000: episode: 1095, duration: 1.612s, episode steps: 32, steps per second: 20, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 2.188 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 42451104.000000, mean_absolute_error: 6303.251953, mean_q: 7691.083008\n",
      "   62477/5000000: episode: 1096, duration: 4.505s, episode steps: 96, steps per second: 21, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.417 [0.000, 5.000], mean observation: 0.059 [0.000, 24.000], loss: 10310311.000000, mean_absolute_error: 6213.614746, mean_q: 7624.657715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   62518/5000000: episode: 1097, duration: 1.970s, episode steps: 41, steps per second: 21, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 2.146 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 16104214.000000, mean_absolute_error: 5972.692383, mean_q: 7312.963379\n",
      "   62628/5000000: episode: 1098, duration: 5.545s, episode steps: 110, steps per second: 20, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.727 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 20855808.000000, mean_absolute_error: 6229.128418, mean_q: 7632.004395\n",
      "   62792/5000000: episode: 1099, duration: 8.054s, episode steps: 164, steps per second: 20, episode reward: -1.000, mean reward: -0.006 [-1.000, 0.000], mean action: 2.780 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 15853947.000000, mean_absolute_error: 6277.824219, mean_q: 7702.210449\n",
      "   62817/5000000: episode: 1100, duration: 1.229s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.680 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 12690016.000000, mean_absolute_error: 6106.403809, mean_q: 7483.542969\n",
      "   62866/5000000: episode: 1101, duration: 2.455s, episode steps: 49, steps per second: 20, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 2.673 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 23193192.000000, mean_absolute_error: 6464.341797, mean_q: 7939.152344\n",
      "   62892/5000000: episode: 1102, duration: 1.312s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.500 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 7039875.500000, mean_absolute_error: 6384.391113, mean_q: 7841.874023\n",
      "   62917/5000000: episode: 1103, duration: 1.238s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.480 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 5764851.000000, mean_absolute_error: 6079.971680, mean_q: 7462.529785\n",
      "   62948/5000000: episode: 1104, duration: 1.444s, episode steps: 31, steps per second: 21, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 3.000 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 8778157.000000, mean_absolute_error: 6435.704102, mean_q: 7907.990723\n",
      "   62976/5000000: episode: 1105, duration: 1.364s, episode steps: 28, steps per second: 21, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.107 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 4101062.750000, mean_absolute_error: 6512.813477, mean_q: 8014.457520\n",
      "   63010/5000000: episode: 1106, duration: 11.709s, episode steps: 34, steps per second: 3, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.265 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 8878315.000000, mean_absolute_error: 6604.458496, mean_q: 8115.279297\n",
      "   63037/5000000: episode: 1107, duration: 1.396s, episode steps: 27, steps per second: 19, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.630 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 19952476.000000, mean_absolute_error: 6355.590332, mean_q: 7785.415039\n",
      "   63077/5000000: episode: 1108, duration: 2.075s, episode steps: 40, steps per second: 19, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 2.450 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 21209088.000000, mean_absolute_error: 7293.573242, mean_q: 8956.258789\n",
      "   63161/5000000: episode: 1109, duration: 4.165s, episode steps: 84, steps per second: 20, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.345 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 19856440.000000, mean_absolute_error: 6652.030273, mean_q: 8159.212891\n",
      "   63290/5000000: episode: 1110, duration: 6.346s, episode steps: 129, steps per second: 20, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.434 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 11211093.000000, mean_absolute_error: 6768.795410, mean_q: 8290.338867\n",
      "   63346/5000000: episode: 1111, duration: 2.703s, episode steps: 56, steps per second: 21, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.375 [0.000, 5.000], mean observation: 0.073 [0.000, 24.000], loss: 10783673.000000, mean_absolute_error: 7036.215332, mean_q: 8629.904297\n",
      "   63377/5000000: episode: 1112, duration: 1.564s, episode steps: 31, steps per second: 20, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 2.677 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 20888832.000000, mean_absolute_error: 6545.111328, mean_q: 8012.411133\n",
      "   63465/5000000: episode: 1113, duration: 4.288s, episode steps: 88, steps per second: 21, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.580 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 17516702.000000, mean_absolute_error: 6718.668457, mean_q: 8228.618164\n",
      "   63491/5000000: episode: 1114, duration: 1.327s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.731 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 9344102.000000, mean_absolute_error: 6457.036133, mean_q: 7905.775391\n",
      "   63582/5000000: episode: 1115, duration: 4.454s, episode steps: 91, steps per second: 20, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.473 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 15258344.000000, mean_absolute_error: 6520.822266, mean_q: 7985.276855\n",
      "   63636/5000000: episode: 1116, duration: 2.504s, episode steps: 54, steps per second: 22, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.370 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 26213462.000000, mean_absolute_error: 6921.530273, mean_q: 8477.905273\n",
      "   63725/5000000: episode: 1117, duration: 4.239s, episode steps: 89, steps per second: 21, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.674 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 11601369.000000, mean_absolute_error: 6789.786621, mean_q: 8313.436523\n",
      "   63750/5000000: episode: 1118, duration: 1.235s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.760 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 12877773.000000, mean_absolute_error: 6899.980469, mean_q: 8468.898438\n",
      "   63776/5000000: episode: 1119, duration: 1.277s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.577 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 15269963.000000, mean_absolute_error: 7385.858887, mean_q: 9071.116211\n",
      "   63810/5000000: episode: 1120, duration: 1.703s, episode steps: 34, steps per second: 20, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.706 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 9332955.000000, mean_absolute_error: 6499.122070, mean_q: 7957.875000\n",
      "   63838/5000000: episode: 1121, duration: 1.325s, episode steps: 28, steps per second: 21, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.750 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 7622727.500000, mean_absolute_error: 6854.302246, mean_q: 8394.873047\n",
      "   63863/5000000: episode: 1122, duration: 1.254s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.120 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 11546824.000000, mean_absolute_error: 6654.241699, mean_q: 8153.867676\n",
      "   64003/5000000: episode: 1123, duration: 6.996s, episode steps: 140, steps per second: 20, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 2.493 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 12547452.000000, mean_absolute_error: 7412.541992, mean_q: 9097.474609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   64030/5000000: episode: 1124, duration: 1.431s, episode steps: 27, steps per second: 19, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.370 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 44208800.000000, mean_absolute_error: 7303.944336, mean_q: 8928.365234\n",
      "   64087/5000000: episode: 1125, duration: 2.833s, episode steps: 57, steps per second: 20, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.088 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 19394342.000000, mean_absolute_error: 7189.373047, mean_q: 8802.469727\n",
      "   64259/5000000: episode: 1126, duration: 8.252s, episode steps: 172, steps per second: 21, episode reward: -1.000, mean reward: -0.006 [-1.000, 0.000], mean action: 2.407 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 15897176.000000, mean_absolute_error: 7433.559570, mean_q: 9116.130859\n",
      "   64287/5000000: episode: 1127, duration: 1.418s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.750 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 7892113.500000, mean_absolute_error: 7703.309082, mean_q: 9466.210938\n",
      "   64325/5000000: episode: 1128, duration: 1.860s, episode steps: 38, steps per second: 20, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 2.474 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 24585132.000000, mean_absolute_error: 6993.027344, mean_q: 8556.707031\n",
      "   64376/5000000: episode: 1129, duration: 2.494s, episode steps: 51, steps per second: 20, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 2.588 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 8657926.000000, mean_absolute_error: 7082.366211, mean_q: 8669.139648\n",
      "   64512/5000000: episode: 1130, duration: 6.581s, episode steps: 136, steps per second: 21, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 2.522 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 16463070.000000, mean_absolute_error: 7397.767090, mean_q: 9058.128906\n",
      "   64538/5000000: episode: 1131, duration: 1.303s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.192 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 15804562.000000, mean_absolute_error: 7544.281250, mean_q: 9246.684570\n",
      "   64638/5000000: episode: 1132, duration: 4.821s, episode steps: 100, steps per second: 21, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.360 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 10589155.000000, mean_absolute_error: 7516.294922, mean_q: 9212.502930\n",
      "   64753/5000000: episode: 1133, duration: 5.326s, episode steps: 115, steps per second: 22, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.287 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 24623788.000000, mean_absolute_error: 7358.750000, mean_q: 9008.781250\n",
      "   64836/5000000: episode: 1134, duration: 3.778s, episode steps: 83, steps per second: 22, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.145 [0.000, 5.000], mean observation: 0.073 [0.000, 24.000], loss: 14626793.000000, mean_absolute_error: 7590.456543, mean_q: 9302.178711\n",
      "   64922/5000000: episode: 1135, duration: 4.105s, episode steps: 86, steps per second: 21, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.581 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 16112496.000000, mean_absolute_error: 7554.691895, mean_q: 9255.183594\n",
      "   64948/5000000: episode: 1136, duration: 1.194s, episode steps: 26, steps per second: 22, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.577 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 8522153.000000, mean_absolute_error: 7880.559570, mean_q: 9645.083008\n",
      "   64974/5000000: episode: 1137, duration: 1.270s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.577 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 16577150.000000, mean_absolute_error: 7292.928711, mean_q: 8908.708984\n",
      "   65079/5000000: episode: 1138, duration: 5.334s, episode steps: 105, steps per second: 20, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.648 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 12565015.000000, mean_absolute_error: 7514.859375, mean_q: 9202.439453\n",
      "   65202/5000000: episode: 1139, duration: 5.705s, episode steps: 123, steps per second: 22, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.528 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 23228426.000000, mean_absolute_error: 8097.300293, mean_q: 9922.973633\n",
      "   65412/5000000: episode: 1140, duration: 10.276s, episode steps: 210, steps per second: 20, episode reward: -1.000, mean reward: -0.005 [-1.000, 0.000], mean action: 2.524 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 21716338.000000, mean_absolute_error: 8367.904297, mean_q: 10235.833008\n",
      "   65452/5000000: episode: 1141, duration: 1.908s, episode steps: 40, steps per second: 21, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 2.275 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 10937846.000000, mean_absolute_error: 7803.618652, mean_q: 9538.504883\n",
      "   65479/5000000: episode: 1142, duration: 1.349s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.519 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 13942767.000000, mean_absolute_error: 7916.796875, mean_q: 9692.303711\n",
      "   65507/5000000: episode: 1143, duration: 1.413s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.893 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 20820122.000000, mean_absolute_error: 7471.891113, mean_q: 9127.231445\n",
      "   65621/5000000: episode: 1144, duration: 5.683s, episode steps: 114, steps per second: 20, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.474 [0.000, 5.000], mean observation: 0.073 [0.000, 24.000], loss: 14237451.000000, mean_absolute_error: 8194.119141, mean_q: 10036.289062\n",
      "   65651/5000000: episode: 1145, duration: 1.483s, episode steps: 30, steps per second: 20, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.367 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 31720022.000000, mean_absolute_error: 9158.582031, mean_q: 11222.574219\n",
      "   65681/5000000: episode: 1146, duration: 1.396s, episode steps: 30, steps per second: 21, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.100 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 14950340.000000, mean_absolute_error: 8473.848633, mean_q: 10387.221680\n",
      "   65735/5000000: episode: 1147, duration: 2.419s, episode steps: 54, steps per second: 22, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.630 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 26525912.000000, mean_absolute_error: 8313.919922, mean_q: 10183.224609\n",
      "   65775/5000000: episode: 1148, duration: 1.976s, episode steps: 40, steps per second: 20, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 2.350 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 19101096.000000, mean_absolute_error: 8869.885742, mean_q: 10885.607422\n",
      "   65806/5000000: episode: 1149, duration: 1.423s, episode steps: 31, steps per second: 22, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 2.903 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 14631105.000000, mean_absolute_error: 8946.240234, mean_q: 10963.964844\n",
      "   65908/5000000: episode: 1150, duration: 4.834s, episode steps: 102, steps per second: 21, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.706 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 17365520.000000, mean_absolute_error: 8426.347656, mean_q: 10320.547852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   65933/5000000: episode: 1151, duration: 1.234s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.320 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 31486562.000000, mean_absolute_error: 8938.031250, mean_q: 10944.764648\n",
      "   66028/5000000: episode: 1152, duration: 4.728s, episode steps: 95, steps per second: 20, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.758 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 24618448.000000, mean_absolute_error: 8993.960938, mean_q: 11032.444336\n",
      "   66060/5000000: episode: 1153, duration: 1.584s, episode steps: 32, steps per second: 20, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 2.375 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 26294880.000000, mean_absolute_error: 8918.624023, mean_q: 10912.685547\n",
      "   66122/5000000: episode: 1154, duration: 3.088s, episode steps: 62, steps per second: 20, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 2.484 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 29339076.000000, mean_absolute_error: 8625.793945, mean_q: 10546.181641\n",
      "   66148/5000000: episode: 1155, duration: 1.382s, episode steps: 26, steps per second: 19, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.154 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 28018358.000000, mean_absolute_error: 8453.808594, mean_q: 10342.769531\n",
      "   66380/5000000: episode: 1156, duration: 11.146s, episode steps: 232, steps per second: 21, episode reward: -1.000, mean reward: -0.004 [-1.000, 0.000], mean action: 2.358 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 25775548.000000, mean_absolute_error: 9000.330078, mean_q: 11019.164062\n",
      "   66458/5000000: episode: 1157, duration: 3.668s, episode steps: 78, steps per second: 21, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 2.397 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 28637138.000000, mean_absolute_error: 8529.216797, mean_q: 10416.474609\n",
      "   66516/5000000: episode: 1158, duration: 2.879s, episode steps: 58, steps per second: 20, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.483 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 16688664.000000, mean_absolute_error: 9096.152344, mean_q: 11153.220703\n",
      "   66546/5000000: episode: 1159, duration: 1.576s, episode steps: 30, steps per second: 19, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.500 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 3835443.500000, mean_absolute_error: 8240.895508, mean_q: 10085.661133\n",
      "   66571/5000000: episode: 1160, duration: 1.185s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.880 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 45474712.000000, mean_absolute_error: 9711.160156, mean_q: 11903.405273\n",
      "   66812/5000000: episode: 1161, duration: 12.049s, episode steps: 241, steps per second: 20, episode reward: -1.000, mean reward: -0.004 [-1.000, 0.000], mean action: 2.643 [0.000, 5.000], mean observation: 0.060 [0.000, 24.000], loss: 18378640.000000, mean_absolute_error: 9195.539062, mean_q: 11265.151367\n",
      "   66940/5000000: episode: 1162, duration: 6.295s, episode steps: 128, steps per second: 20, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.430 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 23701352.000000, mean_absolute_error: 9334.357422, mean_q: 11436.683594\n",
      "   66972/5000000: episode: 1163, duration: 1.517s, episode steps: 32, steps per second: 21, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 3.031 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 6194615.000000, mean_absolute_error: 9068.029297, mean_q: 11111.742188\n",
      "   67002/5000000: episode: 1164, duration: 1.500s, episode steps: 30, steps per second: 20, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.500 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 22446684.000000, mean_absolute_error: 9056.949219, mean_q: 11072.996094\n",
      "   67056/5000000: episode: 1165, duration: 2.600s, episode steps: 54, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.370 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 27948758.000000, mean_absolute_error: 9340.263672, mean_q: 11431.202148\n",
      "   67095/5000000: episode: 1166, duration: 1.925s, episode steps: 39, steps per second: 20, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 2.564 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 22086172.000000, mean_absolute_error: 9733.728516, mean_q: 11937.984375\n",
      "   67199/5000000: episode: 1167, duration: 5.061s, episode steps: 104, steps per second: 21, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.721 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 27950462.000000, mean_absolute_error: 9065.001953, mean_q: 11086.995117\n",
      "   67260/5000000: episode: 1168, duration: 2.895s, episode steps: 61, steps per second: 21, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 2.541 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 11174105.000000, mean_absolute_error: 9596.547852, mean_q: 11745.285156\n",
      "   67428/5000000: episode: 1169, duration: 8.062s, episode steps: 168, steps per second: 21, episode reward: -1.000, mean reward: -0.006 [-1.000, 0.000], mean action: 2.589 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 32607378.000000, mean_absolute_error: 9754.160156, mean_q: 11927.922852\n",
      "   67453/5000000: episode: 1170, duration: 1.223s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.240 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 35832004.000000, mean_absolute_error: 9003.862305, mean_q: 10990.958984\n",
      "   67483/5000000: episode: 1171, duration: 1.456s, episode steps: 30, steps per second: 21, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.333 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 17306292.000000, mean_absolute_error: 9404.747070, mean_q: 11493.996094\n",
      "   67509/5000000: episode: 1172, duration: 1.310s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.923 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 64550084.000000, mean_absolute_error: 9642.258789, mean_q: 11769.222656\n",
      "   67534/5000000: episode: 1173, duration: 1.213s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.000 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 46871900.000000, mean_absolute_error: 9367.471680, mean_q: 11439.793945\n",
      "   67560/5000000: episode: 1174, duration: 1.305s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.615 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 33636008.000000, mean_absolute_error: 9841.147461, mean_q: 12045.915039\n",
      "   67589/5000000: episode: 1175, duration: 1.475s, episode steps: 29, steps per second: 20, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.897 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 22005360.000000, mean_absolute_error: 10098.887695, mean_q: 12367.758789\n",
      "   67816/5000000: episode: 1176, duration: 10.996s, episode steps: 227, steps per second: 21, episode reward: -1.000, mean reward: -0.004 [-1.000, 0.000], mean action: 2.577 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 21327728.000000, mean_absolute_error: 10081.121094, mean_q: 12347.006836\n",
      "   67846/5000000: episode: 1177, duration: 1.521s, episode steps: 30, steps per second: 20, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.333 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 56841792.000000, mean_absolute_error: 9793.927734, mean_q: 11952.421875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   67940/5000000: episode: 1178, duration: 4.688s, episode steps: 94, steps per second: 20, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.691 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 9355460.000000, mean_absolute_error: 9540.197266, mean_q: 11662.978516\n",
      "   67980/5000000: episode: 1179, duration: 1.969s, episode steps: 40, steps per second: 20, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 2.450 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 15890754.000000, mean_absolute_error: 10820.423828, mean_q: 13282.443359\n",
      "   68008/5000000: episode: 1180, duration: 1.446s, episode steps: 28, steps per second: 19, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.250 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 20952142.000000, mean_absolute_error: 10700.806641, mean_q: 13102.129883\n",
      "   68062/5000000: episode: 1181, duration: 2.640s, episode steps: 54, steps per second: 20, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.630 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 30134264.000000, mean_absolute_error: 9982.032227, mean_q: 12198.316406\n",
      "   68149/5000000: episode: 1182, duration: 4.403s, episode steps: 87, steps per second: 20, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.391 [0.000, 5.000], mean observation: 0.073 [0.000, 24.000], loss: 39412204.000000, mean_absolute_error: 10440.169922, mean_q: 12768.880859\n",
      "   68234/5000000: episode: 1183, duration: 4.079s, episode steps: 85, steps per second: 21, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.565 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 36981340.000000, mean_absolute_error: 10604.385742, mean_q: 12969.209961\n",
      "   68266/5000000: episode: 1184, duration: 1.518s, episode steps: 32, steps per second: 21, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 2.875 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 43027728.000000, mean_absolute_error: 10425.600586, mean_q: 12758.410156\n",
      "   68296/5000000: episode: 1185, duration: 1.433s, episode steps: 30, steps per second: 21, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.267 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 36772936.000000, mean_absolute_error: 10363.910156, mean_q: 12673.620117\n",
      "   68328/5000000: episode: 1186, duration: 1.575s, episode steps: 32, steps per second: 20, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 2.938 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 48439728.000000, mean_absolute_error: 11296.670898, mean_q: 13833.668945\n",
      "   68353/5000000: episode: 1187, duration: 1.216s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.680 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 16288643.000000, mean_absolute_error: 11194.025391, mean_q: 13739.162109\n",
      "   68496/5000000: episode: 1188, duration: 7.053s, episode steps: 143, steps per second: 20, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 2.594 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 24551722.000000, mean_absolute_error: 11118.815430, mean_q: 13614.325195\n",
      "   68549/5000000: episode: 1189, duration: 2.661s, episode steps: 53, steps per second: 20, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.170 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 25872546.000000, mean_absolute_error: 10624.408203, mean_q: 12986.298828\n",
      "   68578/5000000: episode: 1190, duration: 1.572s, episode steps: 29, steps per second: 18, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.345 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 42155376.000000, mean_absolute_error: 10947.887695, mean_q: 13386.287109\n",
      "   68638/5000000: episode: 1191, duration: 2.865s, episode steps: 60, steps per second: 21, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.267 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 25955700.000000, mean_absolute_error: 11072.064453, mean_q: 13551.661133\n",
      "   68703/5000000: episode: 1192, duration: 3.165s, episode steps: 65, steps per second: 21, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 2.662 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 28003868.000000, mean_absolute_error: 10877.670898, mean_q: 13297.181641\n",
      "   68879/5000000: episode: 1193, duration: 8.539s, episode steps: 176, steps per second: 21, episode reward: -1.000, mean reward: -0.006 [-1.000, 0.000], mean action: 2.500 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 32947546.000000, mean_absolute_error: 10679.468750, mean_q: 13055.253906\n",
      "   68909/5000000: episode: 1194, duration: 1.508s, episode steps: 30, steps per second: 20, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.667 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 55702336.000000, mean_absolute_error: 10457.996094, mean_q: 12768.680664\n",
      "   68953/5000000: episode: 1195, duration: 2.243s, episode steps: 44, steps per second: 20, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 2.068 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 33655256.000000, mean_absolute_error: 10872.735352, mean_q: 13289.747070\n",
      "   68979/5000000: episode: 1196, duration: 1.212s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.846 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 27584724.000000, mean_absolute_error: 12790.530273, mean_q: 15698.979492\n",
      "   69006/5000000: episode: 1197, duration: 8.609s, episode steps: 27, steps per second: 3, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.407 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 7375354.000000, mean_absolute_error: 10296.224609, mean_q: 12577.624023\n",
      "   69032/5000000: episode: 1198, duration: 1.297s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.615 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 21754460.000000, mean_absolute_error: 11131.374023, mean_q: 13617.123047\n",
      "   69066/5000000: episode: 1199, duration: 1.688s, episode steps: 34, steps per second: 20, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.559 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 70360576.000000, mean_absolute_error: 10810.577148, mean_q: 13197.760742\n",
      "   69091/5000000: episode: 1200, duration: 1.160s, episode steps: 25, steps per second: 22, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.960 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 15652170.000000, mean_absolute_error: 11969.927734, mean_q: 14676.776367\n",
      "   69116/5000000: episode: 1201, duration: 1.176s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.720 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 5486679.000000, mean_absolute_error: 10447.498047, mean_q: 12779.320312\n",
      "   69156/5000000: episode: 1202, duration: 1.917s, episode steps: 40, steps per second: 21, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 2.500 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 37021624.000000, mean_absolute_error: 11238.140625, mean_q: 13736.685547\n",
      "   69184/5000000: episode: 1203, duration: 1.396s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.214 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 23698070.000000, mean_absolute_error: 10702.516602, mean_q: 13069.178711\n",
      "   69282/5000000: episode: 1204, duration: 4.586s, episode steps: 98, steps per second: 21, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.612 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 43211224.000000, mean_absolute_error: 11252.179688, mean_q: 13737.849609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   69403/5000000: episode: 1205, duration: 5.832s, episode steps: 121, steps per second: 21, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.612 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 30870304.000000, mean_absolute_error: 11172.405273, mean_q: 13651.576172\n",
      "   69431/5000000: episode: 1206, duration: 1.415s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 1.893 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 16395562.000000, mean_absolute_error: 11482.123047, mean_q: 14050.315430\n",
      "   69463/5000000: episode: 1207, duration: 1.487s, episode steps: 32, steps per second: 22, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 2.594 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 19550456.000000, mean_absolute_error: 10817.205078, mean_q: 13194.735352\n",
      "   69491/5000000: episode: 1208, duration: 1.389s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.571 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 12622730.000000, mean_absolute_error: 11403.467773, mean_q: 13943.092773\n",
      "   69557/5000000: episode: 1209, duration: 3.168s, episode steps: 66, steps per second: 21, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 2.576 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 43830976.000000, mean_absolute_error: 11017.328125, mean_q: 13445.014648\n",
      "   69619/5000000: episode: 1210, duration: 2.926s, episode steps: 62, steps per second: 21, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 2.161 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 60723188.000000, mean_absolute_error: 11960.615234, mean_q: 14625.145508\n",
      "   69888/5000000: episode: 1211, duration: 13.319s, episode steps: 269, steps per second: 20, episode reward: -1.000, mean reward: -0.004 [-1.000, 0.000], mean action: 2.413 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 53963896.000000, mean_absolute_error: 11727.189453, mean_q: 14326.749023\n",
      "   70142/5000000: episode: 1212, duration: 12.945s, episode steps: 254, steps per second: 20, episode reward: -1.000, mean reward: -0.004 [-1.000, 0.000], mean action: 2.512 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 26811864.000000, mean_absolute_error: 11732.248047, mean_q: 14340.548828\n",
      "   70236/5000000: episode: 1213, duration: 4.449s, episode steps: 94, steps per second: 21, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.383 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 52651532.000000, mean_absolute_error: 12379.999023, mean_q: 15134.875977\n",
      "   70370/5000000: episode: 1214, duration: 5.940s, episode steps: 134, steps per second: 23, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 2.373 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 30826038.000000, mean_absolute_error: 12449.114258, mean_q: 15224.083984\n",
      "   70424/5000000: episode: 1215, duration: 2.533s, episode steps: 54, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.463 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 34529824.000000, mean_absolute_error: 12239.014648, mean_q: 14948.020508\n",
      "   70483/5000000: episode: 1216, duration: 2.852s, episode steps: 59, steps per second: 21, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.712 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 47523852.000000, mean_absolute_error: 12151.898438, mean_q: 14839.671875\n",
      "   70516/5000000: episode: 1217, duration: 1.598s, episode steps: 33, steps per second: 21, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 2.727 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 16971626.000000, mean_absolute_error: 11340.059570, mean_q: 13825.273438\n",
      "   70599/5000000: episode: 1218, duration: 4.113s, episode steps: 83, steps per second: 20, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.566 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 43358388.000000, mean_absolute_error: 12014.324219, mean_q: 14660.733398\n",
      "   70625/5000000: episode: 1219, duration: 1.286s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.269 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 31752526.000000, mean_absolute_error: 11495.744141, mean_q: 14024.628906\n",
      "   70650/5000000: episode: 1220, duration: 1.190s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.760 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 11643098.000000, mean_absolute_error: 12109.625000, mean_q: 14824.583984\n",
      "   70706/5000000: episode: 1221, duration: 2.701s, episode steps: 56, steps per second: 21, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.143 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 21098434.000000, mean_absolute_error: 12663.192383, mean_q: 15506.072266\n",
      "   70757/5000000: episode: 1222, duration: 2.428s, episode steps: 51, steps per second: 21, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 3.078 [0.000, 5.000], mean observation: 0.073 [0.000, 24.000], loss: 64131444.000000, mean_absolute_error: 12126.744141, mean_q: 14771.284180\n",
      "   70791/5000000: episode: 1223, duration: 1.672s, episode steps: 34, steps per second: 20, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 3.088 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 13592262.000000, mean_absolute_error: 11934.774414, mean_q: 14571.062500\n",
      "   70820/5000000: episode: 1224, duration: 1.511s, episode steps: 29, steps per second: 19, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.655 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 51854300.000000, mean_absolute_error: 12896.499023, mean_q: 15787.227539\n",
      "   70913/5000000: episode: 1225, duration: 4.459s, episode steps: 93, steps per second: 21, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.226 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 34165584.000000, mean_absolute_error: 12796.240234, mean_q: 15642.365234\n",
      "   71285/5000000: episode: 1226, duration: 17.590s, episode steps: 372, steps per second: 21, episode reward: -1.000, mean reward: -0.003 [-1.000, 0.000], mean action: 2.341 [0.000, 5.000], mean observation: 0.058 [0.000, 24.000], loss: 56055784.000000, mean_absolute_error: 12685.889648, mean_q: 15486.596680\n",
      "   71321/5000000: episode: 1227, duration: 1.824s, episode steps: 36, steps per second: 20, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 2.611 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 73184352.000000, mean_absolute_error: 12866.877930, mean_q: 15701.708008\n",
      "   71440/5000000: episode: 1228, duration: 5.711s, episode steps: 119, steps per second: 21, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.513 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 84088224.000000, mean_absolute_error: 12969.509766, mean_q: 15813.696289\n",
      "   71667/5000000: episode: 1229, duration: 10.272s, episode steps: 227, steps per second: 22, episode reward: -1.000, mean reward: -0.004 [-1.000, 0.000], mean action: 2.449 [0.000, 5.000], mean observation: 0.058 [0.000, 24.000], loss: 34569860.000000, mean_absolute_error: 13165.292969, mean_q: 16086.381836\n",
      "   71759/5000000: episode: 1230, duration: 4.314s, episode steps: 92, steps per second: 21, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.272 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 47975832.000000, mean_absolute_error: 13221.176758, mean_q: 16144.017578\n",
      "   72064/5000000: episode: 1231, duration: 14.727s, episode steps: 305, steps per second: 21, episode reward: -1.000, mean reward: -0.003 [-1.000, 0.000], mean action: 2.456 [0.000, 5.000], mean observation: 0.055 [0.000, 24.000], loss: 41401704.000000, mean_absolute_error: 13363.140625, mean_q: 16304.411133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   72097/5000000: episode: 1232, duration: 1.665s, episode steps: 33, steps per second: 20, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 2.848 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 33505498.000000, mean_absolute_error: 14104.012695, mean_q: 17239.218750\n",
      "   72312/5000000: episode: 1233, duration: 10.328s, episode steps: 215, steps per second: 21, episode reward: -1.000, mean reward: -0.005 [-1.000, 0.000], mean action: 2.544 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 64815236.000000, mean_absolute_error: 13281.637695, mean_q: 16180.841797\n",
      "   72426/5000000: episode: 1234, duration: 5.258s, episode steps: 114, steps per second: 22, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.342 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 68230888.000000, mean_absolute_error: 13649.189453, mean_q: 16631.513672\n",
      "   72521/5000000: episode: 1235, duration: 4.664s, episode steps: 95, steps per second: 20, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.558 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 35706956.000000, mean_absolute_error: 13762.330078, mean_q: 16792.882812\n",
      "   72554/5000000: episode: 1236, duration: 1.588s, episode steps: 33, steps per second: 21, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 2.727 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 79023256.000000, mean_absolute_error: 14001.805664, mean_q: 17073.367188\n",
      "   72584/5000000: episode: 1237, duration: 1.405s, episode steps: 30, steps per second: 21, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.300 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 19415242.000000, mean_absolute_error: 13913.958984, mean_q: 16981.527344\n",
      "   72616/5000000: episode: 1238, duration: 1.565s, episode steps: 32, steps per second: 20, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 2.188 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 47967672.000000, mean_absolute_error: 14903.708984, mean_q: 18244.757812\n",
      "   72667/5000000: episode: 1239, duration: 2.482s, episode steps: 51, steps per second: 21, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 2.255 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 22387644.000000, mean_absolute_error: 14108.757812, mean_q: 17240.164062\n",
      "   72724/5000000: episode: 1240, duration: 2.839s, episode steps: 57, steps per second: 20, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.912 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 39525568.000000, mean_absolute_error: 14326.702148, mean_q: 17477.306641\n",
      "   72790/5000000: episode: 1241, duration: 3.308s, episode steps: 66, steps per second: 20, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 2.621 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 48826804.000000, mean_absolute_error: 14286.621094, mean_q: 17422.568359\n",
      "   72877/5000000: episode: 1242, duration: 4.063s, episode steps: 87, steps per second: 21, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.552 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 52153432.000000, mean_absolute_error: 14301.195312, mean_q: 17437.875000\n",
      "   72950/5000000: episode: 1243, duration: 3.535s, episode steps: 73, steps per second: 21, episode reward: -1.000, mean reward: -0.014 [-1.000, 0.000], mean action: 2.233 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 81947768.000000, mean_absolute_error: 14313.274414, mean_q: 17447.925781\n",
      "   72976/5000000: episode: 1244, duration: 1.253s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.615 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 82986416.000000, mean_absolute_error: 14642.847656, mean_q: 17887.841797\n",
      "   73104/5000000: episode: 1245, duration: 6.025s, episode steps: 128, steps per second: 21, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.570 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 47579288.000000, mean_absolute_error: 13504.193359, mean_q: 16437.607422\n",
      "   73131/5000000: episode: 1246, duration: 1.375s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.296 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 48356208.000000, mean_absolute_error: 15096.057617, mean_q: 18425.695312\n",
      "   73195/5000000: episode: 1247, duration: 3.088s, episode steps: 64, steps per second: 21, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 2.562 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 46770536.000000, mean_absolute_error: 14252.625000, mean_q: 17386.250000\n",
      "   73221/5000000: episode: 1248, duration: 1.243s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.692 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 18117706.000000, mean_absolute_error: 13616.732422, mean_q: 16590.017578\n",
      "   73248/5000000: episode: 1249, duration: 1.279s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.593 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 129846544.000000, mean_absolute_error: 13680.214844, mean_q: 16651.308594\n",
      "   73326/5000000: episode: 1250, duration: 3.630s, episode steps: 78, steps per second: 21, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 2.385 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 75730120.000000, mean_absolute_error: 14865.214844, mean_q: 18124.830078\n",
      "   73449/5000000: episode: 1251, duration: 5.909s, episode steps: 123, steps per second: 21, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.504 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 35894484.000000, mean_absolute_error: 14142.440430, mean_q: 17228.703125\n",
      "   73518/5000000: episode: 1252, duration: 3.351s, episode steps: 69, steps per second: 21, episode reward: -1.000, mean reward: -0.014 [-1.000, 0.000], mean action: 2.246 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 36156832.000000, mean_absolute_error: 14962.687500, mean_q: 18262.136719\n",
      "   73733/5000000: episode: 1253, duration: 10.320s, episode steps: 215, steps per second: 21, episode reward: -1.000, mean reward: -0.005 [-1.000, 0.000], mean action: 2.442 [0.000, 5.000], mean observation: 0.061 [0.000, 24.000], loss: 86535600.000000, mean_absolute_error: 14776.463867, mean_q: 18008.210938\n",
      "   73759/5000000: episode: 1254, duration: 1.324s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.423 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 180204272.000000, mean_absolute_error: 16041.383789, mean_q: 19579.976562\n",
      "   73789/5000000: episode: 1255, duration: 1.437s, episode steps: 30, steps per second: 21, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.400 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 67275648.000000, mean_absolute_error: 14258.362305, mean_q: 17368.285156\n",
      "   73815/5000000: episode: 1256, duration: 1.160s, episode steps: 26, steps per second: 22, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.538 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 37690020.000000, mean_absolute_error: 15263.124023, mean_q: 18635.945312\n",
      "   73841/5000000: episode: 1257, duration: 1.274s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.538 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 140151392.000000, mean_absolute_error: 15316.813477, mean_q: 18657.074219\n",
      "   73868/5000000: episode: 1258, duration: 1.310s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.741 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 105882384.000000, mean_absolute_error: 15917.533203, mean_q: 19411.535156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   73900/5000000: episode: 1259, duration: 1.624s, episode steps: 32, steps per second: 20, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 2.656 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 97810592.000000, mean_absolute_error: 15346.121094, mean_q: 18721.466797\n",
      "   73932/5000000: episode: 1260, duration: 1.575s, episode steps: 32, steps per second: 20, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 2.969 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 80768104.000000, mean_absolute_error: 14953.630859, mean_q: 18221.798828\n",
      "   74019/5000000: episode: 1261, duration: 4.218s, episode steps: 87, steps per second: 21, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.483 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 50922944.000000, mean_absolute_error: 14683.000977, mean_q: 17900.441406\n",
      "   74044/5000000: episode: 1262, duration: 1.285s, episode steps: 25, steps per second: 19, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.200 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 17150926.000000, mean_absolute_error: 14485.651367, mean_q: 17623.917969\n",
      "   74075/5000000: episode: 1263, duration: 1.521s, episode steps: 31, steps per second: 20, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 2.161 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 95861856.000000, mean_absolute_error: 15560.909180, mean_q: 18981.671875\n",
      "   74159/5000000: episode: 1264, duration: 4.164s, episode steps: 84, steps per second: 20, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.738 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 116803440.000000, mean_absolute_error: 15216.864258, mean_q: 18532.972656\n",
      "   74186/5000000: episode: 1265, duration: 1.299s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.741 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 25961346.000000, mean_absolute_error: 14082.217773, mean_q: 17107.607422\n",
      "   74211/5000000: episode: 1266, duration: 1.129s, episode steps: 25, steps per second: 22, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.680 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 34396400.000000, mean_absolute_error: 14724.750977, mean_q: 17917.882812\n",
      "   74273/5000000: episode: 1267, duration: 2.976s, episode steps: 62, steps per second: 21, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 2.532 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 18597412.000000, mean_absolute_error: 15616.869141, mean_q: 19069.138672\n",
      "   74298/5000000: episode: 1268, duration: 1.263s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 1.840 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 27877606.000000, mean_absolute_error: 15682.918945, mean_q: 19156.335938\n",
      "   74323/5000000: episode: 1269, duration: 1.329s, episode steps: 25, steps per second: 19, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.840 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 12721290.000000, mean_absolute_error: 14515.662109, mean_q: 17691.074219\n",
      "   74389/5000000: episode: 1270, duration: 3.149s, episode steps: 66, steps per second: 21, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 2.348 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 59670976.000000, mean_absolute_error: 15525.461914, mean_q: 18922.697266\n",
      "   74414/5000000: episode: 1271, duration: 1.236s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.520 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 49678448.000000, mean_absolute_error: 14925.207031, mean_q: 18174.333984\n",
      "   74513/5000000: episode: 1272, duration: 4.727s, episode steps: 99, steps per second: 21, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.343 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 83047104.000000, mean_absolute_error: 14792.526367, mean_q: 17994.621094\n",
      "   74566/5000000: episode: 1273, duration: 2.460s, episode steps: 53, steps per second: 22, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.434 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 69885448.000000, mean_absolute_error: 15422.538086, mean_q: 18790.066406\n",
      "   74699/5000000: episode: 1274, duration: 6.502s, episode steps: 133, steps per second: 20, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.278 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 92554200.000000, mean_absolute_error: 15268.638672, mean_q: 18575.564453\n",
      "   74725/5000000: episode: 1275, duration: 1.394s, episode steps: 26, steps per second: 19, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.808 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 20808814.000000, mean_absolute_error: 14819.301758, mean_q: 18046.222656\n",
      "   74896/5000000: episode: 1276, duration: 8.300s, episode steps: 171, steps per second: 21, episode reward: -1.000, mean reward: -0.006 [-1.000, 0.000], mean action: 2.550 [0.000, 5.000], mean observation: 0.062 [0.000, 24.000], loss: 43959524.000000, mean_absolute_error: 15178.140625, mean_q: 18475.728516\n",
      "   74922/5000000: episode: 1277, duration: 1.220s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.500 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 60324224.000000, mean_absolute_error: 17017.214844, mean_q: 20782.246094\n",
      "   74974/5000000: episode: 1278, duration: 2.556s, episode steps: 52, steps per second: 20, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 3.154 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 69596048.000000, mean_absolute_error: 14815.529297, mean_q: 18016.501953\n",
      "   75043/5000000: episode: 1279, duration: 13.932s, episode steps: 69, steps per second: 5, episode reward: -1.000, mean reward: -0.014 [-1.000, 0.000], mean action: 2.638 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 158759568.000000, mean_absolute_error: 15699.916992, mean_q: 19090.273438\n",
      "   75075/5000000: episode: 1280, duration: 1.675s, episode steps: 32, steps per second: 19, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 1.938 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 18253200.000000, mean_absolute_error: 14492.989258, mean_q: 17641.568359\n",
      "   75101/5000000: episode: 1281, duration: 1.326s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.385 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 79288184.000000, mean_absolute_error: 16618.388672, mean_q: 20285.455078\n",
      "   75135/5000000: episode: 1282, duration: 1.681s, episode steps: 34, steps per second: 20, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.118 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 42174624.000000, mean_absolute_error: 16591.621094, mean_q: 20242.613281\n",
      "   75224/5000000: episode: 1283, duration: 4.355s, episode steps: 89, steps per second: 20, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.865 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 73725096.000000, mean_absolute_error: 15876.079102, mean_q: 19352.640625\n",
      "   75255/5000000: episode: 1284, duration: 1.562s, episode steps: 31, steps per second: 20, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 1.968 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 108230296.000000, mean_absolute_error: 15837.816406, mean_q: 19276.255859\n",
      "   75355/5000000: episode: 1285, duration: 4.727s, episode steps: 100, steps per second: 21, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.500 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 33647440.000000, mean_absolute_error: 16170.693359, mean_q: 19708.132812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   75380/5000000: episode: 1286, duration: 1.206s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.960 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 94120472.000000, mean_absolute_error: 17114.148438, mean_q: 20865.685547\n",
      "   75405/5000000: episode: 1287, duration: 1.201s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.040 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 30277570.000000, mean_absolute_error: 16272.572266, mean_q: 19863.201172\n",
      "   75496/5000000: episode: 1288, duration: 4.444s, episode steps: 91, steps per second: 20, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.549 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 52641352.000000, mean_absolute_error: 16076.980469, mean_q: 19593.343750\n",
      "   75530/5000000: episode: 1289, duration: 1.586s, episode steps: 34, steps per second: 21, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.824 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 152757680.000000, mean_absolute_error: 15958.882812, mean_q: 19414.142578\n",
      "   75562/5000000: episode: 1290, duration: 1.582s, episode steps: 32, steps per second: 20, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 2.812 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 51112680.000000, mean_absolute_error: 15407.503906, mean_q: 18752.148438\n",
      "   75616/5000000: episode: 1291, duration: 2.619s, episode steps: 54, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.759 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 72246248.000000, mean_absolute_error: 16643.472656, mean_q: 20290.261719\n",
      "   75646/5000000: episode: 1292, duration: 1.522s, episode steps: 30, steps per second: 20, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.967 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 37776068.000000, mean_absolute_error: 16190.146484, mean_q: 19707.929688\n",
      "   75673/5000000: episode: 1293, duration: 1.360s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.852 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 24560092.000000, mean_absolute_error: 16136.047852, mean_q: 19659.708984\n",
      "   75751/5000000: episode: 1294, duration: 3.627s, episode steps: 78, steps per second: 22, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 2.449 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 61339536.000000, mean_absolute_error: 15557.993164, mean_q: 18920.423828\n",
      "   75803/5000000: episode: 1295, duration: 2.414s, episode steps: 52, steps per second: 22, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.385 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 112592856.000000, mean_absolute_error: 16847.800781, mean_q: 20554.365234\n",
      "   75828/5000000: episode: 1296, duration: 1.192s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 1.880 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 62394756.000000, mean_absolute_error: 16892.507812, mean_q: 20611.986328\n",
      "   75893/5000000: episode: 1297, duration: 3.198s, episode steps: 65, steps per second: 20, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 2.369 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 42252500.000000, mean_absolute_error: 16355.788086, mean_q: 19900.943359\n",
      "   75920/5000000: episode: 1298, duration: 1.299s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.222 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 44561992.000000, mean_absolute_error: 17175.326172, mean_q: 20936.955078\n",
      "   75947/5000000: episode: 1299, duration: 1.375s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.519 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 16296958.000000, mean_absolute_error: 16904.982422, mean_q: 20616.574219\n",
      "   75984/5000000: episode: 1300, duration: 1.834s, episode steps: 37, steps per second: 20, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 2.676 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 22111506.000000, mean_absolute_error: 16555.691406, mean_q: 20181.890625\n",
      "   76065/5000000: episode: 1301, duration: 3.829s, episode steps: 81, steps per second: 21, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.321 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 96363688.000000, mean_absolute_error: 17220.759766, mean_q: 20993.664062\n",
      "   76091/5000000: episode: 1302, duration: 1.260s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.500 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 128879392.000000, mean_absolute_error: 16564.277344, mean_q: 20133.847656\n",
      "   76149/5000000: episode: 1303, duration: 2.666s, episode steps: 58, steps per second: 22, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.672 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 94597816.000000, mean_absolute_error: 16880.382812, mean_q: 20529.925781\n",
      "   76181/5000000: episode: 1304, duration: 1.719s, episode steps: 32, steps per second: 19, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 2.594 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 166327664.000000, mean_absolute_error: 18129.171875, mean_q: 22107.574219\n",
      "   76234/5000000: episode: 1305, duration: 2.638s, episode steps: 53, steps per second: 20, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.679 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 73447744.000000, mean_absolute_error: 17604.347656, mean_q: 21430.140625\n",
      "   76263/5000000: episode: 1306, duration: 1.295s, episode steps: 29, steps per second: 22, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.276 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 93080728.000000, mean_absolute_error: 15668.934570, mean_q: 18996.787109\n",
      "   76377/5000000: episode: 1307, duration: 5.491s, episode steps: 114, steps per second: 21, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.605 [0.000, 5.000], mean observation: 0.062 [0.000, 24.000], loss: 112335536.000000, mean_absolute_error: 17492.847656, mean_q: 21289.035156\n",
      "   76586/5000000: episode: 1308, duration: 10.166s, episode steps: 209, steps per second: 21, episode reward: -1.000, mean reward: -0.005 [-1.000, 0.000], mean action: 2.421 [0.000, 5.000], mean observation: 0.061 [0.000, 24.000], loss: 74788864.000000, mean_absolute_error: 17197.687500, mean_q: 20931.468750\n",
      "   76641/5000000: episode: 1309, duration: 2.602s, episode steps: 55, steps per second: 21, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.473 [0.000, 5.000], mean observation: 0.073 [0.000, 24.000], loss: 42613216.000000, mean_absolute_error: 17024.337891, mean_q: 20724.824219\n",
      "   76756/5000000: episode: 1310, duration: 5.485s, episode steps: 115, steps per second: 21, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.330 [0.000, 5.000], mean observation: 0.061 [0.000, 24.000], loss: 76693696.000000, mean_absolute_error: 17218.441406, mean_q: 20943.425781\n",
      "   76782/5000000: episode: 1311, duration: 1.318s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.077 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 50520112.000000, mean_absolute_error: 17983.714844, mean_q: 21889.447266\n",
      "   76809/5000000: episode: 1312, duration: 1.292s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.667 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 36304264.000000, mean_absolute_error: 18015.951172, mean_q: 21956.402344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   76865/5000000: episode: 1313, duration: 2.932s, episode steps: 56, steps per second: 19, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.411 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 95799000.000000, mean_absolute_error: 17604.847656, mean_q: 21402.529297\n",
      "   76897/5000000: episode: 1314, duration: 1.562s, episode steps: 32, steps per second: 20, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 2.094 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 26331156.000000, mean_absolute_error: 16878.955078, mean_q: 20502.660156\n",
      "   77082/5000000: episode: 1315, duration: 9.113s, episode steps: 185, steps per second: 20, episode reward: -1.000, mean reward: -0.005 [-1.000, 0.000], mean action: 2.670 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 63307620.000000, mean_absolute_error: 17077.177734, mean_q: 20766.339844\n",
      "   77109/5000000: episode: 1316, duration: 1.218s, episode steps: 27, steps per second: 22, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.593 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 137407968.000000, mean_absolute_error: 18514.962891, mean_q: 22536.767578\n",
      "   77173/5000000: episode: 1317, duration: 3.152s, episode steps: 64, steps per second: 20, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 2.484 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 148064496.000000, mean_absolute_error: 19330.730469, mean_q: 23555.357422\n",
      "   77201/5000000: episode: 1318, duration: 1.385s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.179 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 37262136.000000, mean_absolute_error: 17074.619141, mean_q: 20777.697266\n",
      "   77230/5000000: episode: 1319, duration: 1.440s, episode steps: 29, steps per second: 20, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.586 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 38422288.000000, mean_absolute_error: 16912.888672, mean_q: 20581.130859\n",
      "   77381/5000000: episode: 1320, duration: 6.965s, episode steps: 151, steps per second: 22, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 2.397 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 59168580.000000, mean_absolute_error: 17899.142578, mean_q: 21768.933594\n",
      "   77432/5000000: episode: 1321, duration: 2.309s, episode steps: 51, steps per second: 22, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 2.843 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 58288244.000000, mean_absolute_error: 18693.095703, mean_q: 22775.298828\n",
      "   77526/5000000: episode: 1322, duration: 4.543s, episode steps: 94, steps per second: 21, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.351 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 38773704.000000, mean_absolute_error: 17419.578125, mean_q: 21165.251953\n",
      "   77552/5000000: episode: 1323, duration: 1.191s, episode steps: 26, steps per second: 22, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.731 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 46488600.000000, mean_absolute_error: 18750.025391, mean_q: 22852.027344\n",
      "   77603/5000000: episode: 1324, duration: 2.503s, episode steps: 51, steps per second: 20, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 2.451 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 62165872.000000, mean_absolute_error: 17573.539062, mean_q: 21321.265625\n",
      "   77631/5000000: episode: 1325, duration: 1.426s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.964 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 477044064.000000, mean_absolute_error: 20070.271484, mean_q: 24389.830078\n",
      "   77657/5000000: episode: 1326, duration: 1.294s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.308 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 63217864.000000, mean_absolute_error: 18078.144531, mean_q: 21958.185547\n",
      "   77683/5000000: episode: 1327, duration: 1.277s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.538 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 101903304.000000, mean_absolute_error: 20368.398438, mean_q: 24500.320312\n",
      "   77792/5000000: episode: 1328, duration: 5.179s, episode steps: 109, steps per second: 21, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.284 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 112337648.000000, mean_absolute_error: 18210.304688, mean_q: 22153.564453\n",
      "   77877/5000000: episode: 1329, duration: 4.115s, episode steps: 85, steps per second: 21, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.259 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 96431312.000000, mean_absolute_error: 18364.175781, mean_q: 22314.179688\n",
      "   77913/5000000: episode: 1330, duration: 1.753s, episode steps: 36, steps per second: 21, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 2.639 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 23649652.000000, mean_absolute_error: 18226.966797, mean_q: 22156.380859\n",
      "   77938/5000000: episode: 1331, duration: 1.209s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.680 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 116760968.000000, mean_absolute_error: 19500.488281, mean_q: 23721.335938\n",
      "   77971/5000000: episode: 1332, duration: 1.651s, episode steps: 33, steps per second: 20, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 2.788 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 114773688.000000, mean_absolute_error: 18889.433594, mean_q: 22981.050781\n",
      "   78069/5000000: episode: 1333, duration: 4.713s, episode steps: 98, steps per second: 21, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.724 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 152407792.000000, mean_absolute_error: 18822.285156, mean_q: 22876.427734\n",
      "   78095/5000000: episode: 1334, duration: 1.236s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.615 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 59940324.000000, mean_absolute_error: 18326.132812, mean_q: 22275.095703\n",
      "   78121/5000000: episode: 1335, duration: 1.423s, episode steps: 26, steps per second: 18, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.615 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 279657248.000000, mean_absolute_error: 20101.552734, mean_q: 24390.060547\n",
      "   78186/5000000: episode: 1336, duration: 3.038s, episode steps: 65, steps per second: 21, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 2.492 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 69428000.000000, mean_absolute_error: 18635.007812, mean_q: 22659.554688\n",
      "   78219/5000000: episode: 1337, duration: 1.494s, episode steps: 33, steps per second: 22, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 2.818 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 173132304.000000, mean_absolute_error: 18697.523438, mean_q: 22721.417969\n",
      "   78246/5000000: episode: 1338, duration: 1.350s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.296 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 123220640.000000, mean_absolute_error: 18929.638672, mean_q: 23019.083984\n",
      "   78301/5000000: episode: 1339, duration: 2.687s, episode steps: 55, steps per second: 20, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.455 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 105667360.000000, mean_absolute_error: 19440.044922, mean_q: 23630.240234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   78329/5000000: episode: 1340, duration: 1.374s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.179 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 234478752.000000, mean_absolute_error: 18761.826172, mean_q: 22765.853516\n",
      "   78359/5000000: episode: 1341, duration: 1.470s, episode steps: 30, steps per second: 20, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.433 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 22978942.000000, mean_absolute_error: 18865.542969, mean_q: 22970.519531\n",
      "   78621/5000000: episode: 1342, duration: 13.176s, episode steps: 262, steps per second: 20, episode reward: -1.000, mean reward: -0.004 [-1.000, 0.000], mean action: 2.550 [0.000, 5.000], mean observation: 0.062 [0.000, 24.000], loss: 94585088.000000, mean_absolute_error: 18657.166016, mean_q: 22680.679688\n",
      "   78687/5000000: episode: 1343, duration: 3.254s, episode steps: 66, steps per second: 20, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 2.333 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 151040320.000000, mean_absolute_error: 19980.994141, mean_q: 24283.498047\n",
      "   78781/5000000: episode: 1344, duration: 4.449s, episode steps: 94, steps per second: 21, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.351 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 249194784.000000, mean_absolute_error: 18983.785156, mean_q: 23050.986328\n",
      "   78811/5000000: episode: 1345, duration: 1.424s, episode steps: 30, steps per second: 21, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.233 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 109614184.000000, mean_absolute_error: 21423.662109, mean_q: 26072.525391\n",
      "   78837/5000000: episode: 1346, duration: 1.232s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.577 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 231889136.000000, mean_absolute_error: 18592.484375, mean_q: 22561.214844\n",
      "   78956/5000000: episode: 1347, duration: 5.936s, episode steps: 119, steps per second: 20, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.471 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 125485744.000000, mean_absolute_error: 20249.212891, mean_q: 24669.984375\n",
      "   78984/5000000: episode: 1348, duration: 1.344s, episode steps: 28, steps per second: 21, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.429 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 59508108.000000, mean_absolute_error: 19432.746094, mean_q: 23636.298828\n",
      "   79009/5000000: episode: 1349, duration: 1.289s, episode steps: 25, steps per second: 19, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.280 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 77718304.000000, mean_absolute_error: 19304.337891, mean_q: 23467.562500\n",
      "   79062/5000000: episode: 1350, duration: 2.399s, episode steps: 53, steps per second: 22, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.151 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 101684536.000000, mean_absolute_error: 19269.753906, mean_q: 23433.281250\n",
      "   79113/5000000: episode: 1351, duration: 2.404s, episode steps: 51, steps per second: 21, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 2.490 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 62321484.000000, mean_absolute_error: 18744.462891, mean_q: 22812.607422\n",
      "   79189/5000000: episode: 1352, duration: 3.744s, episode steps: 76, steps per second: 20, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 2.329 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 89901592.000000, mean_absolute_error: 20789.677734, mean_q: 25328.009766\n",
      "   79214/5000000: episode: 1353, duration: 1.138s, episode steps: 25, steps per second: 22, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.240 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 40346332.000000, mean_absolute_error: 18732.277344, mean_q: 22777.775391\n",
      "   79244/5000000: episode: 1354, duration: 1.519s, episode steps: 30, steps per second: 20, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.833 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 84980464.000000, mean_absolute_error: 19696.457031, mean_q: 23978.642578\n",
      "   79272/5000000: episode: 1355, duration: 1.373s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.214 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 138390416.000000, mean_absolute_error: 19907.457031, mean_q: 24252.390625\n",
      "   79314/5000000: episode: 1356, duration: 1.915s, episode steps: 42, steps per second: 22, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 1.643 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 71689432.000000, mean_absolute_error: 19234.925781, mean_q: 23321.964844\n",
      "   79346/5000000: episode: 1357, duration: 1.583s, episode steps: 32, steps per second: 20, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 2.406 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 60760108.000000, mean_absolute_error: 19806.619141, mean_q: 24067.693359\n",
      "   79372/5000000: episode: 1358, duration: 1.300s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.846 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 421668224.000000, mean_absolute_error: 19628.923828, mean_q: 23791.785156\n",
      "   79402/5000000: episode: 1359, duration: 4.586s, episode steps: 30, steps per second: 7, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.833 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 198041376.000000, mean_absolute_error: 18932.574219, mean_q: 22942.130859\n",
      "   79491/5000000: episode: 1360, duration: 4.068s, episode steps: 89, steps per second: 22, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.315 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 128104688.000000, mean_absolute_error: 20713.169922, mean_q: 25228.703125\n",
      "   79549/5000000: episode: 1361, duration: 2.725s, episode steps: 58, steps per second: 21, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.517 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 139623712.000000, mean_absolute_error: 20160.789062, mean_q: 24518.337891\n",
      "   79577/5000000: episode: 1362, duration: 1.348s, episode steps: 28, steps per second: 21, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.464 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 110056320.000000, mean_absolute_error: 18560.210938, mean_q: 22474.968750\n",
      "   79614/5000000: episode: 1363, duration: 1.755s, episode steps: 37, steps per second: 21, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 2.243 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 86036352.000000, mean_absolute_error: 20157.105469, mean_q: 24516.384766\n",
      "   79643/5000000: episode: 1364, duration: 1.396s, episode steps: 29, steps per second: 21, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.345 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 16107806.000000, mean_absolute_error: 18013.455078, mean_q: 21888.500000\n",
      "   79770/5000000: episode: 1365, duration: 6.116s, episode steps: 127, steps per second: 21, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.543 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 141008736.000000, mean_absolute_error: 19706.748047, mean_q: 23934.333984\n",
      "   79795/5000000: episode: 1366, duration: 1.204s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.120 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 302543328.000000, mean_absolute_error: 19471.453125, mean_q: 23617.775391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   79821/5000000: episode: 1367, duration: 1.334s, episode steps: 26, steps per second: 19, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.615 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 139215456.000000, mean_absolute_error: 20080.126953, mean_q: 24374.080078\n",
      "   79886/5000000: episode: 1368, duration: 2.973s, episode steps: 65, steps per second: 22, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 2.231 [0.000, 5.000], mean observation: 0.073 [0.000, 24.000], loss: 93204176.000000, mean_absolute_error: 19488.478516, mean_q: 23672.904297\n",
      "   79916/5000000: episode: 1369, duration: 1.314s, episode steps: 30, steps per second: 23, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.200 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 74033504.000000, mean_absolute_error: 20496.388672, mean_q: 24921.054688\n",
      "   79945/5000000: episode: 1370, duration: 1.476s, episode steps: 29, steps per second: 20, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.759 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 43943928.000000, mean_absolute_error: 20102.312500, mean_q: 24439.808594\n",
      "   79970/5000000: episode: 1371, duration: 1.184s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.600 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 105823160.000000, mean_absolute_error: 21356.597656, mean_q: 25992.117188\n",
      "   79995/5000000: episode: 1372, duration: 1.203s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.800 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 74188520.000000, mean_absolute_error: 18828.541016, mean_q: 22829.800781\n",
      "   80027/5000000: episode: 1373, duration: 1.539s, episode steps: 32, steps per second: 21, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 2.281 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 41996424.000000, mean_absolute_error: 20378.837891, mean_q: 24785.111328\n",
      "   80138/5000000: episode: 1374, duration: 5.381s, episode steps: 111, steps per second: 21, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.315 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 133623640.000000, mean_absolute_error: 20230.568359, mean_q: 24579.259766\n",
      "   80189/5000000: episode: 1375, duration: 2.376s, episode steps: 51, steps per second: 21, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 2.275 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 222100192.000000, mean_absolute_error: 20816.078125, mean_q: 25276.916016\n",
      "   80218/5000000: episode: 1376, duration: 1.469s, episode steps: 29, steps per second: 20, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.759 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 40170984.000000, mean_absolute_error: 18789.994141, mean_q: 22797.654297\n",
      "   80281/5000000: episode: 1377, duration: 3.079s, episode steps: 63, steps per second: 20, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 2.000 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 108554208.000000, mean_absolute_error: 20233.458984, mean_q: 24565.689453\n",
      "   80319/5000000: episode: 1378, duration: 1.768s, episode steps: 38, steps per second: 21, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 2.158 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 45452560.000000, mean_absolute_error: 21101.457031, mean_q: 25701.111328\n",
      "   80370/5000000: episode: 1379, duration: 2.454s, episode steps: 51, steps per second: 21, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 2.667 [0.000, 5.000], mean observation: 0.073 [0.000, 24.000], loss: 158423232.000000, mean_absolute_error: 21130.970703, mean_q: 25699.929688\n",
      "   80397/5000000: episode: 1380, duration: 1.313s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.296 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 62223620.000000, mean_absolute_error: 20683.740234, mean_q: 25137.046875\n",
      "   80426/5000000: episode: 1381, duration: 1.380s, episode steps: 29, steps per second: 21, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.241 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 43063280.000000, mean_absolute_error: 21295.005859, mean_q: 25969.921875\n",
      "   80452/5000000: episode: 1382, duration: 1.323s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.346 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 224010976.000000, mean_absolute_error: 21199.937500, mean_q: 25738.623047\n",
      "   80479/5000000: episode: 1383, duration: 1.357s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.333 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 56878660.000000, mean_absolute_error: 20040.572266, mean_q: 24340.669922\n",
      "   80597/5000000: episode: 1384, duration: 5.431s, episode steps: 118, steps per second: 22, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.636 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 170848624.000000, mean_absolute_error: 21255.605469, mean_q: 25843.169922\n",
      "   80626/5000000: episode: 1385, duration: 1.450s, episode steps: 29, steps per second: 20, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.276 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 91290384.000000, mean_absolute_error: 21721.376953, mean_q: 26414.658203\n",
      "   80655/5000000: episode: 1386, duration: 1.437s, episode steps: 29, steps per second: 20, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 3.034 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 123343304.000000, mean_absolute_error: 22089.173828, mean_q: 26877.916016\n",
      "   80717/5000000: episode: 1387, duration: 3.069s, episode steps: 62, steps per second: 20, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 2.339 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 154839088.000000, mean_absolute_error: 20244.812500, mean_q: 24543.984375\n",
      "   80834/5000000: episode: 1388, duration: 5.579s, episode steps: 117, steps per second: 21, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.231 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 178055360.000000, mean_absolute_error: 21348.263672, mean_q: 25945.332031\n",
      "   80861/5000000: episode: 1389, duration: 1.393s, episode steps: 27, steps per second: 19, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.444 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 23638142.000000, mean_absolute_error: 21949.890625, mean_q: 26764.222656\n",
      "   80926/5000000: episode: 1390, duration: 3.051s, episode steps: 65, steps per second: 21, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 2.523 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 186213216.000000, mean_absolute_error: 20596.837891, mean_q: 24994.593750\n",
      "   80951/5000000: episode: 1391, duration: 1.196s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.480 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 54444208.000000, mean_absolute_error: 20664.210938, mean_q: 25130.539062\n",
      "   80979/5000000: episode: 1392, duration: 1.307s, episode steps: 28, steps per second: 21, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.321 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 20066274.000000, mean_absolute_error: 19963.248047, mean_q: 24263.384766\n",
      "   81005/5000000: episode: 1393, duration: 27.998s, episode steps: 26, steps per second: 1, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.769 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 168635072.000000, mean_absolute_error: 20560.951172, mean_q: 24978.183594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   81060/5000000: episode: 1394, duration: 2.673s, episode steps: 55, steps per second: 21, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.545 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 22002194.000000, mean_absolute_error: 19342.843750, mean_q: 23439.794922\n",
      "   81085/5000000: episode: 1395, duration: 1.298s, episode steps: 25, steps per second: 19, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.320 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 25633846.000000, mean_absolute_error: 20094.410156, mean_q: 24357.939453\n",
      "   81111/5000000: episode: 1396, duration: 1.248s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.346 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 65258616.000000, mean_absolute_error: 20484.613281, mean_q: 24871.716797\n",
      "   81143/5000000: episode: 1397, duration: 1.562s, episode steps: 32, steps per second: 20, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 2.344 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 40226780.000000, mean_absolute_error: 21775.126953, mean_q: 26476.369141\n",
      "   81175/5000000: episode: 1398, duration: 1.689s, episode steps: 32, steps per second: 19, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 2.500 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 68294368.000000, mean_absolute_error: 21398.976562, mean_q: 26026.542969\n",
      "   81203/5000000: episode: 1399, duration: 1.386s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.357 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 106019832.000000, mean_absolute_error: 22018.763672, mean_q: 26750.890625\n",
      "   81267/5000000: episode: 1400, duration: 3.129s, episode steps: 64, steps per second: 20, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 2.516 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 245286576.000000, mean_absolute_error: 21609.572266, mean_q: 26239.484375\n",
      "   81292/5000000: episode: 1401, duration: 1.121s, episode steps: 25, steps per second: 22, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.640 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 197362736.000000, mean_absolute_error: 21889.601562, mean_q: 26628.341797\n",
      "   81345/5000000: episode: 1402, duration: 2.564s, episode steps: 53, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.415 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 85818800.000000, mean_absolute_error: 21755.093750, mean_q: 26458.728516\n",
      "   81378/5000000: episode: 1403, duration: 1.620s, episode steps: 33, steps per second: 20, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 2.394 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 182694336.000000, mean_absolute_error: 21135.527344, mean_q: 25644.183594\n",
      "   81444/5000000: episode: 1404, duration: 3.211s, episode steps: 66, steps per second: 21, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 2.667 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 134691568.000000, mean_absolute_error: 22145.525391, mean_q: 26924.109375\n",
      "   81478/5000000: episode: 1405, duration: 1.674s, episode steps: 34, steps per second: 20, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.500 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 78579600.000000, mean_absolute_error: 23178.216797, mean_q: 28236.488281\n",
      "   81508/5000000: episode: 1406, duration: 1.539s, episode steps: 30, steps per second: 19, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.600 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 204399536.000000, mean_absolute_error: 21964.244141, mean_q: 26663.273438\n",
      "   81582/5000000: episode: 1407, duration: 3.538s, episode steps: 74, steps per second: 21, episode reward: -1.000, mean reward: -0.014 [-1.000, 0.000], mean action: 2.514 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 154250784.000000, mean_absolute_error: 22105.570312, mean_q: 26848.664062\n",
      "   81609/5000000: episode: 1408, duration: 1.325s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.074 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 89427512.000000, mean_absolute_error: 21128.273438, mean_q: 25622.070312\n",
      "   81727/5000000: episode: 1409, duration: 5.626s, episode steps: 118, steps per second: 21, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.415 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 104669864.000000, mean_absolute_error: 22059.152344, mean_q: 26812.193359\n",
      "   81870/5000000: episode: 1410, duration: 6.771s, episode steps: 143, steps per second: 21, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 2.657 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 207696752.000000, mean_absolute_error: 22146.933594, mean_q: 26882.150391\n",
      "   81898/5000000: episode: 1411, duration: 1.394s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.857 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 229119120.000000, mean_absolute_error: 23344.996094, mean_q: 28365.224609\n",
      "   81980/5000000: episode: 1412, duration: 4.260s, episode steps: 82, steps per second: 19, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.537 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 100259144.000000, mean_absolute_error: 21120.636719, mean_q: 25647.886719\n",
      "   82008/5000000: episode: 1413, duration: 1.418s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.643 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 350269856.000000, mean_absolute_error: 22593.767578, mean_q: 27395.330078\n",
      "   82033/5000000: episode: 1414, duration: 1.263s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.880 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 165413152.000000, mean_absolute_error: 23184.472656, mean_q: 28144.136719\n",
      "   82096/5000000: episode: 1415, duration: 3.212s, episode steps: 63, steps per second: 20, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 2.984 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 231403344.000000, mean_absolute_error: 22473.964844, mean_q: 27269.884766\n",
      "   82197/5000000: episode: 1416, duration: 4.808s, episode steps: 101, steps per second: 21, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.366 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 89120224.000000, mean_absolute_error: 22202.126953, mean_q: 26962.287109\n",
      "   82260/5000000: episode: 1417, duration: 2.772s, episode steps: 63, steps per second: 23, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 2.762 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 139063088.000000, mean_absolute_error: 22380.019531, mean_q: 27172.667969\n",
      "   82335/5000000: episode: 1418, duration: 3.615s, episode steps: 75, steps per second: 21, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 2.640 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 191740416.000000, mean_absolute_error: 23519.929688, mean_q: 28575.003906\n",
      "   82360/5000000: episode: 1419, duration: 1.285s, episode steps: 25, steps per second: 19, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.800 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 103925016.000000, mean_absolute_error: 24509.578125, mean_q: 29821.802734\n",
      "   82550/5000000: episode: 1420, duration: 9.173s, episode steps: 190, steps per second: 21, episode reward: -1.000, mean reward: -0.005 [-1.000, 0.000], mean action: 2.511 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 120074544.000000, mean_absolute_error: 21878.707031, mean_q: 26534.794922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   82610/5000000: episode: 1421, duration: 3.102s, episode steps: 60, steps per second: 19, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.583 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 239608768.000000, mean_absolute_error: 21919.082031, mean_q: 26556.464844\n",
      "   82663/5000000: episode: 1422, duration: 2.637s, episode steps: 53, steps per second: 20, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.774 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 106391792.000000, mean_absolute_error: 24932.144531, mean_q: 30370.201172\n",
      "   82774/5000000: episode: 1423, duration: 5.329s, episode steps: 111, steps per second: 21, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.730 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 134280896.000000, mean_absolute_error: 21981.650391, mean_q: 26656.152344\n",
      "   82847/5000000: episode: 1424, duration: 3.478s, episode steps: 73, steps per second: 21, episode reward: -1.000, mean reward: -0.014 [-1.000, 0.000], mean action: 2.795 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 185956784.000000, mean_absolute_error: 23575.621094, mean_q: 28628.947266\n",
      "   82934/5000000: episode: 1425, duration: 3.990s, episode steps: 87, steps per second: 22, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.494 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 253849936.000000, mean_absolute_error: 22687.408203, mean_q: 27523.433594\n",
      "   82993/5000000: episode: 1426, duration: 2.967s, episode steps: 59, steps per second: 20, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.661 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 279669696.000000, mean_absolute_error: 24990.806641, mean_q: 30435.845703\n",
      "   83075/5000000: episode: 1427, duration: 4.144s, episode steps: 82, steps per second: 20, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.634 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 72870440.000000, mean_absolute_error: 23543.421875, mean_q: 28621.226562\n",
      "   83100/5000000: episode: 1428, duration: 1.309s, episode steps: 25, steps per second: 19, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.440 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 57131380.000000, mean_absolute_error: 22229.740234, mean_q: 27012.363281\n",
      "   83250/5000000: episode: 1429, duration: 7.471s, episode steps: 150, steps per second: 20, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 2.573 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 251630192.000000, mean_absolute_error: 23176.839844, mean_q: 28141.599609\n",
      "   83277/5000000: episode: 1430, duration: 1.339s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.444 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 336346848.000000, mean_absolute_error: 22875.261719, mean_q: 27756.890625\n",
      "   83303/5000000: episode: 1431, duration: 1.188s, episode steps: 26, steps per second: 22, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.615 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 192030480.000000, mean_absolute_error: 24169.972656, mean_q: 29372.312500\n",
      "   83339/5000000: episode: 1432, duration: 1.643s, episode steps: 36, steps per second: 22, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 2.083 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 40189100.000000, mean_absolute_error: 20805.636719, mean_q: 25219.148438\n",
      "   83449/5000000: episode: 1433, duration: 5.477s, episode steps: 110, steps per second: 20, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.364 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 181510608.000000, mean_absolute_error: 24307.164062, mean_q: 29533.107422\n",
      "   83579/5000000: episode: 1434, duration: 6.201s, episode steps: 130, steps per second: 21, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.354 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 272517856.000000, mean_absolute_error: 24098.646484, mean_q: 29224.935547\n",
      "   83611/5000000: episode: 1435, duration: 1.589s, episode steps: 32, steps per second: 20, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 2.469 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 396917600.000000, mean_absolute_error: 23638.345703, mean_q: 28674.898438\n",
      "   83706/5000000: episode: 1436, duration: 4.459s, episode steps: 95, steps per second: 21, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.358 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 166047744.000000, mean_absolute_error: 22882.880859, mean_q: 27781.576172\n",
      "   83736/5000000: episode: 1437, duration: 1.412s, episode steps: 30, steps per second: 21, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.467 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 158048960.000000, mean_absolute_error: 21986.490234, mean_q: 26633.748047\n",
      "   83764/5000000: episode: 1438, duration: 1.416s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.607 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 183715360.000000, mean_absolute_error: 24065.597656, mean_q: 29243.503906\n",
      "   83803/5000000: episode: 1439, duration: 1.905s, episode steps: 39, steps per second: 20, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 2.513 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 244512240.000000, mean_absolute_error: 22520.796875, mean_q: 27253.923828\n",
      "   83835/5000000: episode: 1440, duration: 1.590s, episode steps: 32, steps per second: 20, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 2.281 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 50850424.000000, mean_absolute_error: 21599.050781, mean_q: 26176.787109\n",
      "   83867/5000000: episode: 1441, duration: 1.571s, episode steps: 32, steps per second: 20, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 2.344 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 534403136.000000, mean_absolute_error: 25268.537109, mean_q: 30718.535156\n",
      "   83963/5000000: episode: 1442, duration: 4.472s, episode steps: 96, steps per second: 21, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.260 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 200596560.000000, mean_absolute_error: 24397.593750, mean_q: 29652.828125\n",
      "   83989/5000000: episode: 1443, duration: 1.243s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.000 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 410545216.000000, mean_absolute_error: 24726.308594, mean_q: 30001.039062\n",
      "   84014/5000000: episode: 1444, duration: 1.157s, episode steps: 25, steps per second: 22, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.160 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 115848048.000000, mean_absolute_error: 22859.705078, mean_q: 27679.437500\n",
      "   84071/5000000: episode: 1445, duration: 2.783s, episode steps: 57, steps per second: 20, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.614 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 157871568.000000, mean_absolute_error: 24015.994141, mean_q: 29146.585938\n",
      "   84096/5000000: episode: 1446, duration: 1.311s, episode steps: 25, steps per second: 19, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.400 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 299551008.000000, mean_absolute_error: 24337.849609, mean_q: 29546.625000\n",
      "   84169/5000000: episode: 1447, duration: 3.397s, episode steps: 73, steps per second: 21, episode reward: -1.000, mean reward: -0.014 [-1.000, 0.000], mean action: 2.233 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 239631520.000000, mean_absolute_error: 24070.427734, mean_q: 29194.541016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   84254/5000000: episode: 1448, duration: 4.063s, episode steps: 85, steps per second: 21, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.529 [0.000, 5.000], mean observation: 0.073 [0.000, 24.000], loss: 116431888.000000, mean_absolute_error: 24614.867188, mean_q: 29928.128906\n",
      "   84279/5000000: episode: 1449, duration: 1.207s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.000 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 184468688.000000, mean_absolute_error: 25272.839844, mean_q: 30718.835938\n",
      "   84307/5000000: episode: 1450, duration: 1.345s, episode steps: 28, steps per second: 21, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.214 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 99299696.000000, mean_absolute_error: 23142.250000, mean_q: 28055.494141\n",
      "   84441/5000000: episode: 1451, duration: 6.647s, episode steps: 134, steps per second: 20, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 2.224 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 189196544.000000, mean_absolute_error: 24285.974609, mean_q: 29473.369141\n",
      "   84511/5000000: episode: 1452, duration: 3.251s, episode steps: 70, steps per second: 22, episode reward: -1.000, mean reward: -0.014 [-1.000, 0.000], mean action: 2.671 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 128759720.000000, mean_absolute_error: 23444.253906, mean_q: 28394.066406\n",
      "   84537/5000000: episode: 1453, duration: 1.296s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.077 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 93463640.000000, mean_absolute_error: 24021.937500, mean_q: 29147.281250\n",
      "   84624/5000000: episode: 1454, duration: 4.260s, episode steps: 87, steps per second: 20, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.287 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 229529936.000000, mean_absolute_error: 26117.488281, mean_q: 31736.755859\n",
      "   84855/5000000: episode: 1455, duration: 10.904s, episode steps: 231, steps per second: 21, episode reward: -1.000, mean reward: -0.004 [-1.000, 0.000], mean action: 2.619 [0.000, 5.000], mean observation: 0.062 [0.000, 24.000], loss: 190091664.000000, mean_absolute_error: 24937.601562, mean_q: 30269.345703\n",
      "   84914/5000000: episode: 1456, duration: 2.999s, episode steps: 59, steps per second: 20, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.339 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 266899600.000000, mean_absolute_error: 24010.859375, mean_q: 29126.185547\n",
      "   84967/5000000: episode: 1457, duration: 2.475s, episode steps: 53, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.340 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 249646080.000000, mean_absolute_error: 27173.474609, mean_q: 33047.132812\n",
      "   85021/5000000: episode: 1458, duration: 2.668s, episode steps: 54, steps per second: 20, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.333 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 266654096.000000, mean_absolute_error: 24371.367188, mean_q: 29546.966797\n",
      "   85076/5000000: episode: 1459, duration: 2.725s, episode steps: 55, steps per second: 20, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.927 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 201659408.000000, mean_absolute_error: 25613.689453, mean_q: 31141.908203\n",
      "   85102/5000000: episode: 1460, duration: 1.229s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.154 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 447182880.000000, mean_absolute_error: 24131.896484, mean_q: 29134.691406\n",
      "   85209/5000000: episode: 1461, duration: 4.983s, episode steps: 107, steps per second: 21, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.467 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 122097456.000000, mean_absolute_error: 24466.347656, mean_q: 29721.941406\n",
      "   85244/5000000: episode: 1462, duration: 1.656s, episode steps: 35, steps per second: 21, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.257 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 243610880.000000, mean_absolute_error: 26248.652344, mean_q: 31883.822266\n",
      "   85277/5000000: episode: 1463, duration: 1.689s, episode steps: 33, steps per second: 20, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 2.152 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 458628928.000000, mean_absolute_error: 26401.472656, mean_q: 32043.230469\n",
      "   85302/5000000: episode: 1464, duration: 1.152s, episode steps: 25, steps per second: 22, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.760 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 143866208.000000, mean_absolute_error: 26587.587891, mean_q: 32360.974609\n",
      "   85341/5000000: episode: 1465, duration: 1.729s, episode steps: 39, steps per second: 23, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 2.179 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 90292904.000000, mean_absolute_error: 24344.906250, mean_q: 29531.013672\n",
      "   85371/5000000: episode: 1466, duration: 1.497s, episode steps: 30, steps per second: 20, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.067 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 540197824.000000, mean_absolute_error: 23112.128906, mean_q: 27953.058594\n",
      "   85439/5000000: episode: 1467, duration: 3.431s, episode steps: 68, steps per second: 20, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 2.515 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 332859520.000000, mean_absolute_error: 25193.945312, mean_q: 30596.736328\n",
      "   85467/5000000: episode: 1468, duration: 1.331s, episode steps: 28, steps per second: 21, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.821 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 109157376.000000, mean_absolute_error: 25164.183594, mean_q: 30539.724609\n",
      "   85496/5000000: episode: 1469, duration: 1.404s, episode steps: 29, steps per second: 21, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.448 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 59142108.000000, mean_absolute_error: 25894.589844, mean_q: 31508.394531\n",
      "   85521/5000000: episode: 1470, duration: 1.322s, episode steps: 25, steps per second: 19, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.840 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 157508208.000000, mean_absolute_error: 25704.966797, mean_q: 31219.285156\n",
      "   85560/5000000: episode: 1471, duration: 1.933s, episode steps: 39, steps per second: 20, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 2.436 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 69085384.000000, mean_absolute_error: 23741.638672, mean_q: 28769.029297\n",
      "   85591/5000000: episode: 1472, duration: 1.546s, episode steps: 31, steps per second: 20, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 2.290 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 167388112.000000, mean_absolute_error: 23344.060547, mean_q: 28269.062500\n",
      "   85617/5000000: episode: 1473, duration: 1.254s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.615 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 575148160.000000, mean_absolute_error: 27460.986328, mean_q: 33353.273438\n",
      "   85652/5000000: episode: 1474, duration: 1.672s, episode steps: 35, steps per second: 21, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.457 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 256632912.000000, mean_absolute_error: 27740.427734, mean_q: 33702.394531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   85707/5000000: episode: 1475, duration: 2.737s, episode steps: 55, steps per second: 20, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.164 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 240941152.000000, mean_absolute_error: 27819.296875, mean_q: 33841.441406\n",
      "   85733/5000000: episode: 1476, duration: 1.296s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.231 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 152537920.000000, mean_absolute_error: 25702.251953, mean_q: 31255.160156\n",
      "   85759/5000000: episode: 1477, duration: 1.275s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.615 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 325322528.000000, mean_absolute_error: 26145.646484, mean_q: 31719.123047\n",
      "   85817/5000000: episode: 1478, duration: 2.876s, episode steps: 58, steps per second: 20, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.276 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 185725104.000000, mean_absolute_error: 24856.529297, mean_q: 30149.089844\n",
      "   85853/5000000: episode: 1479, duration: 1.786s, episode steps: 36, steps per second: 20, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 2.833 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 91228832.000000, mean_absolute_error: 24141.404297, mean_q: 29292.326172\n",
      "   85882/5000000: episode: 1480, duration: 1.250s, episode steps: 29, steps per second: 23, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.448 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 299169184.000000, mean_absolute_error: 26303.027344, mean_q: 31906.685547\n",
      "   85909/5000000: episode: 1481, duration: 1.283s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.815 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 654130304.000000, mean_absolute_error: 25524.669922, mean_q: 30844.419922\n",
      "   86109/5000000: episode: 1482, duration: 9.879s, episode steps: 200, steps per second: 20, episode reward: -1.000, mean reward: -0.005 [-1.000, 0.000], mean action: 2.700 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 189170368.000000, mean_absolute_error: 25671.669922, mean_q: 31138.205078\n",
      "   86166/5000000: episode: 1483, duration: 2.794s, episode steps: 57, steps per second: 20, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.579 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 321595904.000000, mean_absolute_error: 26034.373047, mean_q: 31554.716797\n",
      "   86196/5000000: episode: 1484, duration: 1.325s, episode steps: 30, steps per second: 23, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.600 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 30667716.000000, mean_absolute_error: 24519.240234, mean_q: 29775.019531\n",
      "   86224/5000000: episode: 1485, duration: 1.424s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.536 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 53459912.000000, mean_absolute_error: 25451.287109, mean_q: 30917.566406\n",
      "   86255/5000000: episode: 1486, duration: 1.496s, episode steps: 31, steps per second: 21, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 2.710 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 76698616.000000, mean_absolute_error: 24311.312500, mean_q: 29421.625000\n",
      "   86280/5000000: episode: 1487, duration: 1.233s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.440 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 104656136.000000, mean_absolute_error: 27097.529297, mean_q: 32908.917969\n",
      "   86309/5000000: episode: 1488, duration: 1.443s, episode steps: 29, steps per second: 20, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.138 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 104663480.000000, mean_absolute_error: 27024.062500, mean_q: 32801.964844\n",
      "   86334/5000000: episode: 1489, duration: 1.471s, episode steps: 25, steps per second: 17, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.560 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 182943504.000000, mean_absolute_error: 25429.125000, mean_q: 30850.511719\n",
      "   86364/5000000: episode: 1490, duration: 1.601s, episode steps: 30, steps per second: 19, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.500 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 185246304.000000, mean_absolute_error: 27274.187500, mean_q: 33106.574219\n",
      "   86501/5000000: episode: 1491, duration: 6.638s, episode steps: 137, steps per second: 21, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 2.606 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 266289808.000000, mean_absolute_error: 26181.785156, mean_q: 31789.513672\n",
      "   86527/5000000: episode: 1492, duration: 1.289s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.000 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 82917168.000000, mean_absolute_error: 27601.927734, mean_q: 33600.660156\n",
      "   86561/5000000: episode: 1493, duration: 1.585s, episode steps: 34, steps per second: 21, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.441 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 233402384.000000, mean_absolute_error: 28334.287109, mean_q: 34449.214844\n",
      "   86590/5000000: episode: 1494, duration: 1.347s, episode steps: 29, steps per second: 22, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.310 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 146632896.000000, mean_absolute_error: 26832.621094, mean_q: 32619.363281\n",
      "   86619/5000000: episode: 1495, duration: 1.425s, episode steps: 29, steps per second: 20, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.276 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 239117184.000000, mean_absolute_error: 25932.820312, mean_q: 31467.308594\n",
      "   86647/5000000: episode: 1496, duration: 1.336s, episode steps: 28, steps per second: 21, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.286 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 216785232.000000, mean_absolute_error: 26180.072266, mean_q: 31717.636719\n",
      "   86682/5000000: episode: 1497, duration: 1.794s, episode steps: 35, steps per second: 20, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 1.886 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 187519744.000000, mean_absolute_error: 26352.789062, mean_q: 31938.060547\n",
      "   86717/5000000: episode: 1498, duration: 1.594s, episode steps: 35, steps per second: 22, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.400 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 234909552.000000, mean_absolute_error: 26835.820312, mean_q: 32530.310547\n",
      "   86746/5000000: episode: 1499, duration: 1.406s, episode steps: 29, steps per second: 21, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.414 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 244135872.000000, mean_absolute_error: 29413.169922, mean_q: 35615.953125\n",
      "   86771/5000000: episode: 1500, duration: 1.346s, episode steps: 25, steps per second: 19, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.320 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 605262784.000000, mean_absolute_error: 26358.865234, mean_q: 31889.589844\n",
      "   86805/5000000: episode: 1501, duration: 1.739s, episode steps: 34, steps per second: 20, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.206 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 155568576.000000, mean_absolute_error: 28060.615234, mean_q: 33931.972656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   86833/5000000: episode: 1502, duration: 1.425s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.536 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 170441936.000000, mean_absolute_error: 26375.970703, mean_q: 31961.937500\n",
      "   86861/5000000: episode: 1503, duration: 1.311s, episode steps: 28, steps per second: 21, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.964 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 384197728.000000, mean_absolute_error: 29396.359375, mean_q: 35722.332031\n",
      "   86941/5000000: episode: 1504, duration: 3.869s, episode steps: 80, steps per second: 21, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 2.350 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 589706688.000000, mean_absolute_error: 27424.683594, mean_q: 33238.722656\n",
      "   86969/5000000: episode: 1505, duration: 1.357s, episode steps: 28, steps per second: 21, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.643 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 173303920.000000, mean_absolute_error: 25696.199219, mean_q: 31164.615234\n",
      "   86997/5000000: episode: 1506, duration: 1.425s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.571 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 194465408.000000, mean_absolute_error: 22371.355469, mean_q: 27005.222656\n",
      "   87063/5000000: episode: 1507, duration: 3.165s, episode steps: 66, steps per second: 21, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 2.273 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 333957696.000000, mean_absolute_error: 26904.843750, mean_q: 32623.027344\n",
      "   87147/5000000: episode: 1508, duration: 3.800s, episode steps: 84, steps per second: 22, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.762 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 331136256.000000, mean_absolute_error: 26323.091797, mean_q: 31883.681641\n",
      "   87172/5000000: episode: 1509, duration: 1.156s, episode steps: 25, steps per second: 22, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.120 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 209156384.000000, mean_absolute_error: 30020.632812, mean_q: 36528.449219\n",
      "   87255/5000000: episode: 1510, duration: 3.987s, episode steps: 83, steps per second: 21, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.048 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 247963184.000000, mean_absolute_error: 26097.529297, mean_q: 31632.583984\n",
      "   87281/5000000: episode: 1511, duration: 1.234s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.692 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 400476704.000000, mean_absolute_error: 27431.906250, mean_q: 33246.863281\n",
      "   87309/5000000: episode: 1512, duration: 1.449s, episode steps: 28, steps per second: 19, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.750 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 291032256.000000, mean_absolute_error: 26778.837891, mean_q: 32466.873047\n",
      "   87337/5000000: episode: 1513, duration: 1.379s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.607 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 266763488.000000, mean_absolute_error: 29917.347656, mean_q: 36331.980469\n",
      "   87365/5000000: episode: 1514, duration: 1.440s, episode steps: 28, steps per second: 19, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 1.929 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 384612640.000000, mean_absolute_error: 28501.183594, mean_q: 34604.117188\n",
      "   87390/5000000: episode: 1515, duration: 1.244s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.000 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 625382336.000000, mean_absolute_error: 28391.056641, mean_q: 34439.398438\n",
      "   87427/5000000: episode: 1516, duration: 1.850s, episode steps: 37, steps per second: 20, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 2.649 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 228622816.000000, mean_absolute_error: 26635.251953, mean_q: 32332.462891\n",
      "   87559/5000000: episode: 1517, duration: 6.357s, episode steps: 132, steps per second: 21, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.455 [0.000, 5.000], mean observation: 0.073 [0.000, 24.000], loss: 258659904.000000, mean_absolute_error: 26975.257812, mean_q: 32703.287109\n",
      "   87739/5000000: episode: 1518, duration: 8.340s, episode steps: 180, steps per second: 22, episode reward: -1.000, mean reward: -0.006 [-1.000, 0.000], mean action: 2.483 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 282387520.000000, mean_absolute_error: 27100.039062, mean_q: 32846.593750\n",
      "   87768/5000000: episode: 1519, duration: 1.530s, episode steps: 29, steps per second: 19, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.483 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 421268096.000000, mean_absolute_error: 29302.824219, mean_q: 35636.171875\n",
      "   87800/5000000: episode: 1520, duration: 1.700s, episode steps: 32, steps per second: 19, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 2.438 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 403298816.000000, mean_absolute_error: 27091.673828, mean_q: 32842.593750\n",
      "   87834/5000000: episode: 1521, duration: 1.671s, episode steps: 34, steps per second: 20, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.471 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 165733888.000000, mean_absolute_error: 26108.519531, mean_q: 31646.925781\n",
      "   87973/5000000: episode: 1522, duration: 6.005s, episode steps: 139, steps per second: 23, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 2.331 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 337711904.000000, mean_absolute_error: 27768.867188, mean_q: 33669.617188\n",
      "   88035/5000000: episode: 1523, duration: 2.749s, episode steps: 62, steps per second: 23, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 2.097 [0.000, 5.000], mean observation: 0.061 [0.000, 24.000], loss: 215291024.000000, mean_absolute_error: 26968.337891, mean_q: 32667.587891\n",
      "   88087/5000000: episode: 1524, duration: 2.431s, episode steps: 52, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.577 [0.000, 5.000], mean observation: 0.073 [0.000, 24.000], loss: 343206944.000000, mean_absolute_error: 28399.876953, mean_q: 34454.527344\n",
      "   88154/5000000: episode: 1525, duration: 3.243s, episode steps: 67, steps per second: 21, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 2.567 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 158232064.000000, mean_absolute_error: 26681.830078, mean_q: 32323.396484\n",
      "   88181/5000000: episode: 1526, duration: 1.337s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.630 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 79308968.000000, mean_absolute_error: 28932.367188, mean_q: 35115.707031\n",
      "   88305/5000000: episode: 1527, duration: 5.987s, episode steps: 124, steps per second: 21, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.492 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 275645152.000000, mean_absolute_error: 28363.625000, mean_q: 34406.171875\n",
      "   88460/5000000: episode: 1528, duration: 7.569s, episode steps: 155, steps per second: 20, episode reward: -1.000, mean reward: -0.006 [-1.000, 0.000], mean action: 2.477 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 275635904.000000, mean_absolute_error: 28740.039062, mean_q: 34835.222656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   88524/5000000: episode: 1529, duration: 2.886s, episode steps: 64, steps per second: 22, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 2.625 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 175745104.000000, mean_absolute_error: 29137.664062, mean_q: 35339.898438\n",
      "   88557/5000000: episode: 1530, duration: 1.652s, episode steps: 33, steps per second: 20, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 2.303 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 269241280.000000, mean_absolute_error: 28865.453125, mean_q: 34960.578125\n",
      "   88582/5000000: episode: 1531, duration: 1.254s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.440 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 242753632.000000, mean_absolute_error: 28960.800781, mean_q: 35132.496094\n",
      "   88608/5000000: episode: 1532, duration: 1.308s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.654 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 238183584.000000, mean_absolute_error: 26625.562500, mean_q: 32126.205078\n",
      "   88662/5000000: episode: 1533, duration: 2.517s, episode steps: 54, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.444 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 320088832.000000, mean_absolute_error: 28319.826172, mean_q: 34346.679688\n",
      "   88701/5000000: episode: 1534, duration: 1.889s, episode steps: 39, steps per second: 21, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 2.308 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 186907808.000000, mean_absolute_error: 30607.000000, mean_q: 37218.414062\n",
      "   88733/5000000: episode: 1535, duration: 1.623s, episode steps: 32, steps per second: 20, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 2.500 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 699237376.000000, mean_absolute_error: 26456.392578, mean_q: 31957.003906\n",
      "   88761/5000000: episode: 1536, duration: 1.449s, episode steps: 28, steps per second: 19, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.071 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 411540704.000000, mean_absolute_error: 25722.353516, mean_q: 31096.359375\n",
      "   88791/5000000: episode: 1537, duration: 1.426s, episode steps: 30, steps per second: 21, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.333 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 216424208.000000, mean_absolute_error: 30214.386719, mean_q: 36709.425781\n",
      "   88882/5000000: episode: 1538, duration: 4.370s, episode steps: 91, steps per second: 21, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.143 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 203518128.000000, mean_absolute_error: 28684.027344, mean_q: 34805.429688\n",
      "   88914/5000000: episode: 1539, duration: 1.613s, episode steps: 32, steps per second: 20, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 2.594 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 37900872.000000, mean_absolute_error: 26121.970703, mean_q: 31621.859375\n",
      "   88968/5000000: episode: 1540, duration: 2.589s, episode steps: 54, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.241 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 413022560.000000, mean_absolute_error: 30795.496094, mean_q: 37404.093750\n",
      "   89088/5000000: episode: 1541, duration: 5.763s, episode steps: 120, steps per second: 21, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.483 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 467451744.000000, mean_absolute_error: 28083.003906, mean_q: 33975.343750\n",
      "   89159/5000000: episode: 1542, duration: 3.312s, episode steps: 71, steps per second: 21, episode reward: -1.000, mean reward: -0.014 [-1.000, 0.000], mean action: 2.451 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 188560896.000000, mean_absolute_error: 28005.933594, mean_q: 33921.785156\n",
      "   89190/5000000: episode: 1543, duration: 1.488s, episode steps: 31, steps per second: 21, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 2.097 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 126447728.000000, mean_absolute_error: 29417.972656, mean_q: 35682.671875\n",
      "   89278/5000000: episode: 1544, duration: 4.382s, episode steps: 88, steps per second: 20, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.330 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 240448880.000000, mean_absolute_error: 28231.367188, mean_q: 34213.886719\n",
      "   89313/5000000: episode: 1545, duration: 1.641s, episode steps: 35, steps per second: 21, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.057 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 117143904.000000, mean_absolute_error: 28597.750000, mean_q: 34708.804688\n",
      "   89340/5000000: episode: 1546, duration: 1.328s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.481 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 815185984.000000, mean_absolute_error: 30227.878906, mean_q: 36631.023438\n",
      "   89404/5000000: episode: 1547, duration: 3.224s, episode steps: 64, steps per second: 20, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 2.234 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 412630656.000000, mean_absolute_error: 29573.675781, mean_q: 35833.824219\n",
      "   89431/5000000: episode: 1548, duration: 1.297s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 3.074 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 77402848.000000, mean_absolute_error: 27703.175781, mean_q: 33486.820312\n",
      "   89549/5000000: episode: 1549, duration: 5.294s, episode steps: 118, steps per second: 22, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.542 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 351043264.000000, mean_absolute_error: 31161.355469, mean_q: 37853.968750\n",
      "   89576/5000000: episode: 1550, duration: 1.338s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.741 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 436196960.000000, mean_absolute_error: 29943.525391, mean_q: 36289.628906\n",
      "   89603/5000000: episode: 1551, duration: 1.335s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.148 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 433631520.000000, mean_absolute_error: 28346.898438, mean_q: 34176.433594\n",
      "   89631/5000000: episode: 1552, duration: 1.348s, episode steps: 28, steps per second: 21, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.571 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 364745792.000000, mean_absolute_error: 28970.980469, mean_q: 35142.101562\n",
      "   89668/5000000: episode: 1553, duration: 1.715s, episode steps: 37, steps per second: 22, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 2.162 [0.000, 5.000], mean observation: 0.062 [0.000, 24.000], loss: 547750208.000000, mean_absolute_error: 30476.082031, mean_q: 36986.089844\n",
      "   89694/5000000: episode: 1554, duration: 1.239s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 1.923 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 465622688.000000, mean_absolute_error: 28552.933594, mean_q: 34612.894531\n",
      "   89720/5000000: episode: 1555, duration: 1.316s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.577 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 457285440.000000, mean_absolute_error: 29138.673828, mean_q: 35258.359375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   89798/5000000: episode: 1556, duration: 3.551s, episode steps: 78, steps per second: 22, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 2.846 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 515730080.000000, mean_absolute_error: 28204.888672, mean_q: 34085.835938\n",
      "   89831/5000000: episode: 1557, duration: 1.526s, episode steps: 33, steps per second: 22, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 2.667 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 276592864.000000, mean_absolute_error: 27504.937500, mean_q: 33274.968750\n",
      "   89858/5000000: episode: 1558, duration: 1.302s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.444 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 503873408.000000, mean_absolute_error: 28487.972656, mean_q: 34507.257812\n",
      "   89887/5000000: episode: 1559, duration: 1.441s, episode steps: 29, steps per second: 20, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.379 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 246693520.000000, mean_absolute_error: 32187.080078, mean_q: 39042.218750\n",
      "   89941/5000000: episode: 1560, duration: 2.656s, episode steps: 54, steps per second: 20, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.667 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 163874400.000000, mean_absolute_error: 28144.564453, mean_q: 34076.070312\n",
      "   90059/5000000: episode: 1561, duration: 5.750s, episode steps: 118, steps per second: 21, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.500 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 433128672.000000, mean_absolute_error: 29985.232422, mean_q: 36356.230469\n",
      "   90187/5000000: episode: 1562, duration: 5.914s, episode steps: 128, steps per second: 22, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.391 [0.000, 5.000], mean observation: 0.062 [0.000, 24.000], loss: 308941280.000000, mean_absolute_error: 28988.476562, mean_q: 35073.433594\n",
      "   90212/5000000: episode: 1563, duration: 1.259s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.640 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 78948816.000000, mean_absolute_error: 31360.062500, mean_q: 38183.023438\n",
      "   90240/5000000: episode: 1564, duration: 1.433s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.321 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 99800632.000000, mean_absolute_error: 32793.464844, mean_q: 39944.730469\n",
      "   90273/5000000: episode: 1565, duration: 1.557s, episode steps: 33, steps per second: 21, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 2.242 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 440812160.000000, mean_absolute_error: 28248.994141, mean_q: 34139.910156\n",
      "   90353/5000000: episode: 1566, duration: 3.870s, episode steps: 80, steps per second: 21, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 2.538 [0.000, 5.000], mean observation: 0.073 [0.000, 24.000], loss: 545834304.000000, mean_absolute_error: 30873.808594, mean_q: 37391.070312\n",
      "   90406/5000000: episode: 1567, duration: 2.526s, episode steps: 53, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.415 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 231588512.000000, mean_absolute_error: 28508.486328, mean_q: 34523.992188\n",
      "   90484/5000000: episode: 1568, duration: 3.672s, episode steps: 78, steps per second: 21, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 2.423 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 277735968.000000, mean_absolute_error: 31317.000000, mean_q: 38034.714844\n",
      "   90513/5000000: episode: 1569, duration: 1.476s, episode steps: 29, steps per second: 20, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.310 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 560839360.000000, mean_absolute_error: 27480.152344, mean_q: 33083.703125\n",
      "   90621/5000000: episode: 1570, duration: 5.011s, episode steps: 108, steps per second: 22, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.713 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 254250976.000000, mean_absolute_error: 30461.550781, mean_q: 36940.593750\n",
      "   90657/5000000: episode: 1571, duration: 1.710s, episode steps: 36, steps per second: 21, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 2.389 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 923659264.000000, mean_absolute_error: 31004.625000, mean_q: 37549.363281\n",
      "   90695/5000000: episode: 1572, duration: 1.894s, episode steps: 38, steps per second: 20, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 2.763 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 682920448.000000, mean_absolute_error: 31124.904297, mean_q: 37710.738281\n",
      "   90720/5000000: episode: 1573, duration: 1.262s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.200 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 57180172.000000, mean_absolute_error: 30063.560547, mean_q: 36560.621094\n",
      "   90754/5000000: episode: 1574, duration: 1.571s, episode steps: 34, steps per second: 22, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.588 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 464340160.000000, mean_absolute_error: 29687.847656, mean_q: 35785.281250\n",
      "   90827/5000000: episode: 1575, duration: 3.522s, episode steps: 73, steps per second: 21, episode reward: -1.000, mean reward: -0.014 [-1.000, 0.000], mean action: 2.356 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 342678400.000000, mean_absolute_error: 30770.619141, mean_q: 37331.730469\n",
      "   90881/5000000: episode: 1576, duration: 2.605s, episode steps: 54, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.704 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 270729760.000000, mean_absolute_error: 29618.546875, mean_q: 35834.203125\n",
      "   90915/5000000: episode: 1577, duration: 1.616s, episode steps: 34, steps per second: 21, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.235 [0.000, 5.000], mean observation: 0.062 [0.000, 24.000], loss: 63725628.000000, mean_absolute_error: 29398.515625, mean_q: 35670.730469\n",
      "   90966/5000000: episode: 1578, duration: 2.339s, episode steps: 51, steps per second: 22, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 2.333 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 351177472.000000, mean_absolute_error: 31132.738281, mean_q: 37719.996094\n",
      "   90991/5000000: episode: 1579, duration: 1.186s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.320 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 855868160.000000, mean_absolute_error: 30452.814453, mean_q: 36773.746094\n",
      "   91025/5000000: episode: 1580, duration: 1.660s, episode steps: 34, steps per second: 20, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.441 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 374228704.000000, mean_absolute_error: 29782.140625, mean_q: 36027.816406\n",
      "   91107/5000000: episode: 1581, duration: 3.823s, episode steps: 82, steps per second: 21, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.268 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 219560736.000000, mean_absolute_error: 28855.212891, mean_q: 34951.664062\n",
      "   91146/5000000: episode: 1582, duration: 1.862s, episode steps: 39, steps per second: 21, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 1.641 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 504680768.000000, mean_absolute_error: 31438.984375, mean_q: 38146.527344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   91176/5000000: episode: 1583, duration: 1.571s, episode steps: 30, steps per second: 19, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.200 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 320247264.000000, mean_absolute_error: 32503.089844, mean_q: 39332.574219\n",
      "   91219/5000000: episode: 1584, duration: 2.123s, episode steps: 43, steps per second: 20, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 2.256 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 168354112.000000, mean_absolute_error: 31952.837891, mean_q: 38790.632812\n",
      "   91271/5000000: episode: 1585, duration: 2.403s, episode steps: 52, steps per second: 22, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.942 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 340802944.000000, mean_absolute_error: 29646.888672, mean_q: 35905.542969\n",
      "   91300/5000000: episode: 1586, duration: 1.466s, episode steps: 29, steps per second: 20, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.517 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 216836496.000000, mean_absolute_error: 31055.976562, mean_q: 37674.113281\n",
      "   91341/5000000: episode: 1587, duration: 1.940s, episode steps: 41, steps per second: 21, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 2.390 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 191290528.000000, mean_absolute_error: 28698.580078, mean_q: 34659.859375\n",
      "   91369/5000000: episode: 1588, duration: 1.383s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 1.929 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 116830040.000000, mean_absolute_error: 31090.005859, mean_q: 37742.375000\n",
      "   91395/5000000: episode: 1589, duration: 1.285s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.654 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 290418112.000000, mean_absolute_error: 28950.392578, mean_q: 35029.644531\n",
      "   91420/5000000: episode: 1590, duration: 1.177s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.040 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 142459504.000000, mean_absolute_error: 30521.654297, mean_q: 37020.359375\n",
      "   91478/5000000: episode: 1591, duration: 2.693s, episode steps: 58, steps per second: 22, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.741 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 780193088.000000, mean_absolute_error: 33732.125000, mean_q: 40905.113281\n",
      "   91504/5000000: episode: 1592, duration: 1.219s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.385 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 196913248.000000, mean_absolute_error: 31573.914062, mean_q: 38324.769531\n",
      "   91627/5000000: episode: 1593, duration: 6.005s, episode steps: 123, steps per second: 20, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.220 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 455593056.000000, mean_absolute_error: 32042.800781, mean_q: 38772.027344\n",
      "   91653/5000000: episode: 1594, duration: 1.289s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.269 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 277805536.000000, mean_absolute_error: 30328.919922, mean_q: 36714.726562\n",
      "   91849/5000000: episode: 1595, duration: 9.480s, episode steps: 196, steps per second: 21, episode reward: -1.000, mean reward: -0.005 [-1.000, 0.000], mean action: 2.490 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 355317728.000000, mean_absolute_error: 30245.609375, mean_q: 36594.210938\n",
      "   91908/5000000: episode: 1596, duration: 2.845s, episode steps: 59, steps per second: 21, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.898 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 483297664.000000, mean_absolute_error: 30285.298828, mean_q: 36640.031250\n",
      "   91962/5000000: episode: 1597, duration: 2.521s, episode steps: 54, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.796 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 257546336.000000, mean_absolute_error: 31329.484375, mean_q: 37995.093750\n",
      "   91987/5000000: episode: 1598, duration: 1.239s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.520 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 270687776.000000, mean_absolute_error: 33415.011719, mean_q: 40673.125000\n",
      "   92012/5000000: episode: 1599, duration: 1.236s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 1.920 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 112558776.000000, mean_absolute_error: 31218.908203, mean_q: 37894.597656\n",
      "   92100/5000000: episode: 1600, duration: 4.197s, episode steps: 88, steps per second: 21, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.545 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 381620608.000000, mean_absolute_error: 32573.408203, mean_q: 39435.406250\n",
      "   92131/5000000: episode: 1601, duration: 1.555s, episode steps: 31, steps per second: 20, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 2.774 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 600582784.000000, mean_absolute_error: 30062.695312, mean_q: 36326.914062\n",
      "   92157/5000000: episode: 1602, duration: 1.370s, episode steps: 26, steps per second: 19, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.077 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 113579568.000000, mean_absolute_error: 32218.738281, mean_q: 39086.726562\n",
      "   92230/5000000: episode: 1603, duration: 3.568s, episode steps: 73, steps per second: 20, episode reward: -1.000, mean reward: -0.014 [-1.000, 0.000], mean action: 2.219 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 329501984.000000, mean_absolute_error: 31517.191406, mean_q: 38207.964844\n",
      "   92255/5000000: episode: 1604, duration: 1.174s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.720 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 872892032.000000, mean_absolute_error: 33001.105469, mean_q: 39953.023438\n",
      "   92307/5000000: episode: 1605, duration: 2.705s, episode steps: 52, steps per second: 19, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.269 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 271989440.000000, mean_absolute_error: 29312.111328, mean_q: 35441.121094\n",
      "   92458/5000000: episode: 1606, duration: 7.576s, episode steps: 151, steps per second: 20, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 2.291 [0.000, 5.000], mean observation: 0.073 [0.000, 24.000], loss: 362311072.000000, mean_absolute_error: 32223.833984, mean_q: 39056.816406\n",
      "   92700/5000000: episode: 1607, duration: 11.141s, episode steps: 242, steps per second: 22, episode reward: -1.000, mean reward: -0.004 [-1.000, 0.000], mean action: 2.500 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 505266720.000000, mean_absolute_error: 31424.859375, mean_q: 38069.558594\n",
      "   92729/5000000: episode: 1608, duration: 1.365s, episode steps: 29, steps per second: 21, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.034 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 1083641984.000000, mean_absolute_error: 32414.476562, mean_q: 39243.324219\n",
      "   92754/5000000: episode: 1609, duration: 1.219s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.880 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 539174656.000000, mean_absolute_error: 35172.609375, mean_q: 42550.929688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   92838/5000000: episode: 1610, duration: 4.037s, episode steps: 84, steps per second: 21, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.524 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 338110976.000000, mean_absolute_error: 32974.890625, mean_q: 39929.859375\n",
      "   92873/5000000: episode: 1611, duration: 1.742s, episode steps: 35, steps per second: 20, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.571 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 453296800.000000, mean_absolute_error: 31905.931641, mean_q: 38462.546875\n",
      "   92917/5000000: episode: 1612, duration: 2.065s, episode steps: 44, steps per second: 21, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 1.909 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 157684560.000000, mean_absolute_error: 31888.257812, mean_q: 38650.152344\n",
      "   92942/5000000: episode: 1613, duration: 1.206s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.480 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 1243112320.000000, mean_absolute_error: 34341.750000, mean_q: 41638.164062\n",
      "   92971/5000000: episode: 1614, duration: 1.436s, episode steps: 29, steps per second: 20, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.138 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 359592288.000000, mean_absolute_error: 30444.111328, mean_q: 36836.710938\n",
      "   93004/5000000: episode: 1615, duration: 1.796s, episode steps: 33, steps per second: 18, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 2.030 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 359127456.000000, mean_absolute_error: 32692.587891, mean_q: 39715.230469\n",
      "   93069/5000000: episode: 1616, duration: 2.851s, episode steps: 65, steps per second: 23, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 2.431 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 888152832.000000, mean_absolute_error: 32508.285156, mean_q: 39349.796875\n",
      "   93094/5000000: episode: 1617, duration: 1.246s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.120 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 70784320.000000, mean_absolute_error: 31012.976562, mean_q: 37643.691406\n",
      "   93127/5000000: episode: 1618, duration: 1.722s, episode steps: 33, steps per second: 19, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 2.636 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 119068720.000000, mean_absolute_error: 34557.839844, mean_q: 42104.933594\n",
      "   93188/5000000: episode: 1619, duration: 2.948s, episode steps: 61, steps per second: 21, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 2.492 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 104284968.000000, mean_absolute_error: 31400.050781, mean_q: 38062.656250\n",
      "   93293/5000000: episode: 1620, duration: 5.129s, episode steps: 105, steps per second: 20, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.533 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 204879104.000000, mean_absolute_error: 31778.806641, mean_q: 38497.691406\n",
      "   93392/5000000: episode: 1621, duration: 4.869s, episode steps: 99, steps per second: 20, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.232 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 493875872.000000, mean_absolute_error: 33827.074219, mean_q: 41011.453125\n",
      "   93425/5000000: episode: 1622, duration: 1.574s, episode steps: 33, steps per second: 21, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 2.364 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 689388736.000000, mean_absolute_error: 33018.343750, mean_q: 39941.421875\n",
      "   93450/5000000: episode: 1623, duration: 1.169s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.360 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 72981912.000000, mean_absolute_error: 32149.234375, mean_q: 39054.140625\n",
      "   93543/5000000: episode: 1624, duration: 4.790s, episode steps: 93, steps per second: 19, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.613 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 660742528.000000, mean_absolute_error: 33525.199219, mean_q: 40614.363281\n",
      "   93569/5000000: episode: 1625, duration: 1.317s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.192 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 812768896.000000, mean_absolute_error: 32895.519531, mean_q: 39854.851562\n",
      "   93594/5000000: episode: 1626, duration: 1.336s, episode steps: 25, steps per second: 19, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.920 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 567148032.000000, mean_absolute_error: 30152.632812, mean_q: 36445.828125\n",
      "   93622/5000000: episode: 1627, duration: 1.434s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.464 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 402565088.000000, mean_absolute_error: 35563.285156, mean_q: 43165.394531\n",
      "   93809/5000000: episode: 1628, duration: 9.097s, episode steps: 187, steps per second: 21, episode reward: -1.000, mean reward: -0.005 [-1.000, 0.000], mean action: 2.615 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 416289152.000000, mean_absolute_error: 33571.148438, mean_q: 40699.859375\n",
      "   93841/5000000: episode: 1629, duration: 1.582s, episode steps: 32, steps per second: 20, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 1.688 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 138362240.000000, mean_absolute_error: 31219.740234, mean_q: 37520.644531\n",
      "   93904/5000000: episode: 1630, duration: 2.949s, episode steps: 63, steps per second: 21, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 2.175 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 634657344.000000, mean_absolute_error: 34112.476562, mean_q: 41349.648438\n",
      "   93959/5000000: episode: 1631, duration: 2.497s, episode steps: 55, steps per second: 22, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.473 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 1004459264.000000, mean_absolute_error: 34499.312500, mean_q: 41796.871094\n",
      "   94089/5000000: episode: 1632, duration: 6.343s, episode steps: 130, steps per second: 20, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.746 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 194967600.000000, mean_absolute_error: 34531.000000, mean_q: 41926.835938\n",
      "   94157/5000000: episode: 1633, duration: 3.186s, episode steps: 68, steps per second: 21, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 2.382 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 475013856.000000, mean_absolute_error: 33769.937500, mean_q: 40944.636719\n",
      "   94263/5000000: episode: 1634, duration: 5.219s, episode steps: 106, steps per second: 20, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.292 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 223983120.000000, mean_absolute_error: 32878.691406, mean_q: 39799.250000\n",
      "   94290/5000000: episode: 1635, duration: 1.337s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.556 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 603594240.000000, mean_absolute_error: 38952.703125, mean_q: 47385.933594\n",
      "   94323/5000000: episode: 1636, duration: 1.631s, episode steps: 33, steps per second: 20, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 1.970 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 475495936.000000, mean_absolute_error: 35538.128906, mean_q: 43167.187500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   94348/5000000: episode: 1637, duration: 1.220s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.440 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1095552896.000000, mean_absolute_error: 32477.748047, mean_q: 39248.183594\n",
      "   94382/5000000: episode: 1638, duration: 1.638s, episode steps: 34, steps per second: 21, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.500 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 806925440.000000, mean_absolute_error: 33263.773438, mean_q: 40202.792969\n",
      "   94439/5000000: episode: 1639, duration: 2.643s, episode steps: 57, steps per second: 22, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.772 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 419697920.000000, mean_absolute_error: 31953.486328, mean_q: 38639.578125\n",
      "   94464/5000000: episode: 1640, duration: 1.189s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.440 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 232809984.000000, mean_absolute_error: 32988.617188, mean_q: 40026.433594\n",
      "   94497/5000000: episode: 1641, duration: 1.654s, episode steps: 33, steps per second: 20, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 2.667 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 302543456.000000, mean_absolute_error: 35987.617188, mean_q: 43766.992188\n",
      "   94535/5000000: episode: 1642, duration: 1.858s, episode steps: 38, steps per second: 20, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 2.447 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 125393328.000000, mean_absolute_error: 31483.796875, mean_q: 38107.238281\n",
      "   94574/5000000: episode: 1643, duration: 1.937s, episode steps: 39, steps per second: 20, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 1.897 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 718184768.000000, mean_absolute_error: 33085.617188, mean_q: 39974.757812\n",
      "   94605/5000000: episode: 1644, duration: 1.517s, episode steps: 31, steps per second: 20, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 2.290 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 101961608.000000, mean_absolute_error: 37137.179688, mean_q: 45187.480469\n",
      "   94631/5000000: episode: 1645, duration: 1.269s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.808 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 1043391488.000000, mean_absolute_error: 34705.234375, mean_q: 41991.542969\n",
      "   94656/5000000: episode: 1646, duration: 1.240s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.520 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 427667488.000000, mean_absolute_error: 37107.902344, mean_q: 45152.074219\n",
      "   94688/5000000: episode: 1647, duration: 1.587s, episode steps: 32, steps per second: 20, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 2.906 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 442785376.000000, mean_absolute_error: 32420.888672, mean_q: 39249.953125\n",
      "   94718/5000000: episode: 1648, duration: 1.520s, episode steps: 30, steps per second: 20, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.267 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 328917568.000000, mean_absolute_error: 34337.781250, mean_q: 41523.277344\n",
      "   94806/5000000: episode: 1649, duration: 4.469s, episode steps: 88, steps per second: 20, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.443 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 287589376.000000, mean_absolute_error: 34160.964844, mean_q: 41220.011719\n",
      "   94832/5000000: episode: 1650, duration: 1.261s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.577 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 136449984.000000, mean_absolute_error: 32923.871094, mean_q: 39860.390625\n",
      "   94919/5000000: episode: 1651, duration: 4.186s, episode steps: 87, steps per second: 21, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.747 [0.000, 5.000], mean observation: 0.074 [0.000, 24.000], loss: 552440064.000000, mean_absolute_error: 35864.343750, mean_q: 43464.242188\n",
      "   94953/5000000: episode: 1652, duration: 1.626s, episode steps: 34, steps per second: 21, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.676 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 209117680.000000, mean_absolute_error: 32036.205078, mean_q: 38716.320312\n",
      "   95011/5000000: episode: 1653, duration: 2.764s, episode steps: 58, steps per second: 21, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.138 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 481332608.000000, mean_absolute_error: 37455.511719, mean_q: 45351.152344\n",
      "   95039/5000000: episode: 1654, duration: 1.472s, episode steps: 28, steps per second: 19, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.679 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 377746624.000000, mean_absolute_error: 32556.064453, mean_q: 39375.519531\n",
      "   95152/5000000: episode: 1655, duration: 5.230s, episode steps: 113, steps per second: 22, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.513 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 155642000.000000, mean_absolute_error: 32639.189453, mean_q: 39500.578125\n",
      "   95177/5000000: episode: 1656, duration: 1.242s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.480 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 917636736.000000, mean_absolute_error: 34137.675781, mean_q: 41282.210938\n",
      "   95283/5000000: episode: 1657, duration: 5.159s, episode steps: 106, steps per second: 21, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.358 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 647968960.000000, mean_absolute_error: 34730.847656, mean_q: 41973.003906\n",
      "   95368/5000000: episode: 1658, duration: 4.209s, episode steps: 85, steps per second: 20, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.482 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 723687360.000000, mean_absolute_error: 36409.492188, mean_q: 44053.742188\n",
      "   95397/5000000: episode: 1659, duration: 1.436s, episode steps: 29, steps per second: 20, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.793 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 417513024.000000, mean_absolute_error: 34217.832031, mean_q: 41431.492188\n",
      "   95454/5000000: episode: 1660, duration: 2.715s, episode steps: 57, steps per second: 21, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.596 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 274057952.000000, mean_absolute_error: 35980.847656, mean_q: 43651.242188\n",
      "   95517/5000000: episode: 1661, duration: 2.838s, episode steps: 63, steps per second: 22, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 2.365 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 577383488.000000, mean_absolute_error: 35298.644531, mean_q: 42751.636719\n",
      "   95551/5000000: episode: 1662, duration: 1.718s, episode steps: 34, steps per second: 20, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.765 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 996874880.000000, mean_absolute_error: 36015.359375, mean_q: 43603.113281\n",
      "   95626/5000000: episode: 1663, duration: 3.447s, episode steps: 75, steps per second: 22, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 2.587 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 358265312.000000, mean_absolute_error: 33591.105469, mean_q: 40686.484375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   95723/5000000: episode: 1664, duration: 4.768s, episode steps: 97, steps per second: 20, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.485 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 952296704.000000, mean_absolute_error: 36553.472656, mean_q: 44234.050781\n",
      "   95813/5000000: episode: 1665, duration: 4.398s, episode steps: 90, steps per second: 20, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.378 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 607291072.000000, mean_absolute_error: 36825.910156, mean_q: 44300.398438\n",
      "   95929/5000000: episode: 1666, duration: 5.769s, episode steps: 116, steps per second: 20, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.578 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 703453696.000000, mean_absolute_error: 34899.476562, mean_q: 42237.167969\n",
      "   96035/5000000: episode: 1667, duration: 5.322s, episode steps: 106, steps per second: 20, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.160 [0.000, 5.000], mean observation: 0.062 [0.000, 24.000], loss: 425581408.000000, mean_absolute_error: 36632.234375, mean_q: 44116.062500\n",
      "   96085/5000000: episode: 1668, duration: 2.504s, episode steps: 50, steps per second: 20, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 2.100 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 500905792.000000, mean_absolute_error: 34429.261719, mean_q: 41566.605469\n",
      "   96112/5000000: episode: 1669, duration: 1.283s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.778 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 770688640.000000, mean_absolute_error: 35769.109375, mean_q: 43268.621094\n",
      "   96137/5000000: episode: 1670, duration: 1.203s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.640 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 109970360.000000, mean_absolute_error: 35670.250000, mean_q: 43264.914062\n",
      "   96234/5000000: episode: 1671, duration: 4.956s, episode steps: 97, steps per second: 20, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.454 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 716496448.000000, mean_absolute_error: 37727.839844, mean_q: 45714.222656\n",
      "   96300/5000000: episode: 1672, duration: 3.233s, episode steps: 66, steps per second: 20, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 2.606 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 271452928.000000, mean_absolute_error: 35556.933594, mean_q: 43036.921875\n",
      "   96325/5000000: episode: 1673, duration: 1.190s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.240 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 183240704.000000, mean_absolute_error: 41055.878906, mean_q: 49655.585938\n",
      "   96457/5000000: episode: 1674, duration: 6.413s, episode steps: 132, steps per second: 21, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.508 [0.000, 5.000], mean observation: 0.073 [0.000, 24.000], loss: 987591552.000000, mean_absolute_error: 36717.312500, mean_q: 44410.082031\n",
      "   96488/5000000: episode: 1675, duration: 1.445s, episode steps: 31, steps per second: 21, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 2.129 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 1155037696.000000, mean_absolute_error: 37828.042969, mean_q: 45787.746094\n",
      "   96561/5000000: episode: 1676, duration: 3.527s, episode steps: 73, steps per second: 21, episode reward: -1.000, mean reward: -0.014 [-1.000, 0.000], mean action: 2.753 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 835797440.000000, mean_absolute_error: 37460.023438, mean_q: 45226.179688\n",
      "   96589/5000000: episode: 1677, duration: 1.402s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.429 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 882210880.000000, mean_absolute_error: 34228.648438, mean_q: 41294.324219\n",
      "   96617/5000000: episode: 1678, duration: 1.403s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.964 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 610549888.000000, mean_absolute_error: 34988.800781, mean_q: 42207.816406\n",
      "   96670/5000000: episode: 1679, duration: 2.668s, episode steps: 53, steps per second: 20, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.264 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 626497088.000000, mean_absolute_error: 37070.390625, mean_q: 44820.253906\n",
      "   96722/5000000: episode: 1680, duration: 2.415s, episode steps: 52, steps per second: 22, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.827 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 745157312.000000, mean_absolute_error: 35106.152344, mean_q: 42473.828125\n",
      "   96811/5000000: episode: 1681, duration: 4.355s, episode steps: 89, steps per second: 20, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.674 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 498656384.000000, mean_absolute_error: 38444.464844, mean_q: 46601.675781\n",
      "   96843/5000000: episode: 1682, duration: 1.544s, episode steps: 32, steps per second: 21, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 1.938 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 921819968.000000, mean_absolute_error: 36288.843750, mean_q: 43831.640625\n",
      "   96897/5000000: episode: 1683, duration: 2.671s, episode steps: 54, steps per second: 20, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.241 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 432438752.000000, mean_absolute_error: 37230.621094, mean_q: 45053.726562\n",
      "   96922/5000000: episode: 1684, duration: 1.365s, episode steps: 25, steps per second: 18, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.960 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 748575872.000000, mean_absolute_error: 39225.003906, mean_q: 47490.449219\n",
      "   97009/5000000: episode: 1685, duration: 4.256s, episode steps: 87, steps per second: 20, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.460 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 621977728.000000, mean_absolute_error: 36641.000000, mean_q: 44342.027344\n",
      "   97040/5000000: episode: 1686, duration: 1.439s, episode steps: 31, steps per second: 22, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 2.161 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 952726208.000000, mean_absolute_error: 37011.980469, mean_q: 44644.023438\n",
      "   97070/5000000: episode: 1687, duration: 1.500s, episode steps: 30, steps per second: 20, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.500 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 399468992.000000, mean_absolute_error: 37800.097656, mean_q: 45843.464844\n",
      "   97097/5000000: episode: 1688, duration: 1.216s, episode steps: 27, steps per second: 22, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.259 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 656223744.000000, mean_absolute_error: 37753.472656, mean_q: 45759.265625\n",
      "   97188/5000000: episode: 1689, duration: 4.296s, episode steps: 91, steps per second: 21, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.407 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 1234595968.000000, mean_absolute_error: 37303.152344, mean_q: 44984.550781\n",
      "   97245/5000000: episode: 1690, duration: 2.886s, episode steps: 57, steps per second: 20, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.561 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 497330496.000000, mean_absolute_error: 36974.933594, mean_q: 44671.359375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   97353/5000000: episode: 1691, duration: 5.344s, episode steps: 108, steps per second: 20, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.398 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 569980800.000000, mean_absolute_error: 39505.250000, mean_q: 47768.292969\n",
      "   97537/5000000: episode: 1692, duration: 8.871s, episode steps: 184, steps per second: 21, episode reward: -1.000, mean reward: -0.005 [-1.000, 0.000], mean action: 2.380 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 776005184.000000, mean_absolute_error: 36615.265625, mean_q: 44241.648438\n",
      "   97674/5000000: episode: 1693, duration: 6.692s, episode steps: 137, steps per second: 20, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 2.372 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 540547520.000000, mean_absolute_error: 36658.234375, mean_q: 44353.671875\n",
      "   97770/5000000: episode: 1694, duration: 4.802s, episode steps: 96, steps per second: 20, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.448 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 644538624.000000, mean_absolute_error: 37823.359375, mean_q: 45752.625000\n",
      "   97825/5000000: episode: 1695, duration: 2.732s, episode steps: 55, steps per second: 20, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.364 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 238168176.000000, mean_absolute_error: 36721.367188, mean_q: 44495.812500\n",
      "   97855/5000000: episode: 1696, duration: 1.500s, episode steps: 30, steps per second: 20, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.300 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 432510368.000000, mean_absolute_error: 40123.140625, mean_q: 48746.769531\n",
      "   97923/5000000: episode: 1697, duration: 3.075s, episode steps: 68, steps per second: 22, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 2.235 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 542561088.000000, mean_absolute_error: 36862.285156, mean_q: 44631.667969\n",
      "   97948/5000000: episode: 1698, duration: 1.319s, episode steps: 25, steps per second: 19, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.880 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1307463296.000000, mean_absolute_error: 37412.144531, mean_q: 45165.675781\n",
      "   97973/5000000: episode: 1699, duration: 1.278s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.080 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 1743107328.000000, mean_absolute_error: 39237.914062, mean_q: 47290.875000\n",
      "   97999/5000000: episode: 1700, duration: 1.293s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.385 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1351344384.000000, mean_absolute_error: 41537.195312, mean_q: 50229.101562\n",
      "   98026/5000000: episode: 1701, duration: 1.286s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.889 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 744414144.000000, mean_absolute_error: 39374.519531, mean_q: 47619.859375\n",
      "   98086/5000000: episode: 1702, duration: 2.931s, episode steps: 60, steps per second: 20, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.100 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 830300160.000000, mean_absolute_error: 40055.507812, mean_q: 48472.003906\n",
      "   98153/5000000: episode: 1703, duration: 3.212s, episode steps: 67, steps per second: 21, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 2.403 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 393400096.000000, mean_absolute_error: 38175.906250, mean_q: 46222.421875\n",
      "   98185/5000000: episode: 1704, duration: 1.513s, episode steps: 32, steps per second: 21, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 2.438 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 643270912.000000, mean_absolute_error: 37964.207031, mean_q: 46001.492188\n",
      "   98211/5000000: episode: 1705, duration: 1.331s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.115 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 1511062656.000000, mean_absolute_error: 36752.140625, mean_q: 44357.062500\n",
      "   98241/5000000: episode: 1706, duration: 1.514s, episode steps: 30, steps per second: 20, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.200 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 625661952.000000, mean_absolute_error: 39539.488281, mean_q: 47805.109375\n",
      "   98287/5000000: episode: 1707, duration: 2.321s, episode steps: 46, steps per second: 20, episode reward: -1.000, mean reward: -0.022 [-1.000, 0.000], mean action: 1.957 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 726710720.000000, mean_absolute_error: 37268.886719, mean_q: 45031.390625\n",
      "   98394/5000000: episode: 1708, duration: 4.961s, episode steps: 107, steps per second: 22, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.617 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 1457725056.000000, mean_absolute_error: 39270.234375, mean_q: 47443.078125\n",
      "   98419/5000000: episode: 1709, duration: 1.247s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.320 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 787297344.000000, mean_absolute_error: 38916.058594, mean_q: 47021.140625\n",
      "   98444/5000000: episode: 1710, duration: 1.258s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.480 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 331660384.000000, mean_absolute_error: 33884.968750, mean_q: 40845.753906\n",
      "   98472/5000000: episode: 1711, duration: 1.283s, episode steps: 28, steps per second: 22, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.107 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 702633472.000000, mean_absolute_error: 37906.445312, mean_q: 45822.667969\n",
      "   98500/5000000: episode: 1712, duration: 1.305s, episode steps: 28, steps per second: 21, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.036 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1189833472.000000, mean_absolute_error: 37874.667969, mean_q: 45491.898438\n",
      "   98530/5000000: episode: 1713, duration: 1.407s, episode steps: 30, steps per second: 21, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 3.000 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 1125522176.000000, mean_absolute_error: 39838.492188, mean_q: 48222.550781\n",
      "   98555/5000000: episode: 1714, duration: 1.213s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.400 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 299796800.000000, mean_absolute_error: 36183.453125, mean_q: 43833.300781\n",
      "   98580/5000000: episode: 1715, duration: 1.198s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.920 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1351244928.000000, mean_absolute_error: 36301.449219, mean_q: 43690.816406\n",
      "   98607/5000000: episode: 1716, duration: 1.382s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.481 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 702585216.000000, mean_absolute_error: 35242.335938, mean_q: 42620.515625\n",
      "   98694/5000000: episode: 1717, duration: 4.103s, episode steps: 87, steps per second: 21, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.609 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 896961856.000000, mean_absolute_error: 40598.558594, mean_q: 49224.937500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   98751/5000000: episode: 1718, duration: 2.782s, episode steps: 57, steps per second: 20, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.789 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 984883840.000000, mean_absolute_error: 37729.558594, mean_q: 45652.968750\n",
      "   98825/5000000: episode: 1719, duration: 3.513s, episode steps: 74, steps per second: 21, episode reward: -1.000, mean reward: -0.014 [-1.000, 0.000], mean action: 2.297 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 1025396544.000000, mean_absolute_error: 40253.742188, mean_q: 48745.761719\n",
      "   98853/5000000: episode: 1720, duration: 1.424s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.536 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 343118784.000000, mean_absolute_error: 38023.804688, mean_q: 46022.226562\n",
      "   99038/5000000: episode: 1721, duration: 16.460s, episode steps: 185, steps per second: 11, episode reward: -1.000, mean reward: -0.005 [-1.000, 0.000], mean action: 2.573 [0.000, 5.000], mean observation: 0.058 [0.000, 24.000], loss: 868610304.000000, mean_absolute_error: 39081.511719, mean_q: 47263.042969\n",
      "   99069/5000000: episode: 1722, duration: 1.537s, episode steps: 31, steps per second: 20, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 2.387 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 859697216.000000, mean_absolute_error: 37369.593750, mean_q: 45084.828125\n",
      "   99101/5000000: episode: 1723, duration: 1.678s, episode steps: 32, steps per second: 19, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 2.281 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 572819712.000000, mean_absolute_error: 39862.394531, mean_q: 48207.226562\n",
      "   99237/5000000: episode: 1724, duration: 6.363s, episode steps: 136, steps per second: 21, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 2.537 [0.000, 5.000], mean observation: 0.074 [0.000, 24.000], loss: 834596160.000000, mean_absolute_error: 38373.226562, mean_q: 46308.421875\n",
      "   99263/5000000: episode: 1725, duration: 1.270s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.269 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1282074752.000000, mean_absolute_error: 40126.210938, mean_q: 48521.082031\n",
      "   99296/5000000: episode: 1726, duration: 1.628s, episode steps: 33, steps per second: 20, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 2.697 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 265709568.000000, mean_absolute_error: 42322.542969, mean_q: 51415.160156\n",
      "   99321/5000000: episode: 1727, duration: 1.226s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.680 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 897577280.000000, mean_absolute_error: 39998.535156, mean_q: 48373.789062\n",
      "   99346/5000000: episode: 1728, duration: 1.220s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.280 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 141936256.000000, mean_absolute_error: 41144.585938, mean_q: 49754.843750\n",
      "   99372/5000000: episode: 1729, duration: 1.173s, episode steps: 26, steps per second: 22, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.308 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 152858896.000000, mean_absolute_error: 36813.144531, mean_q: 44532.878906\n",
      "   99461/5000000: episode: 1730, duration: 4.467s, episode steps: 89, steps per second: 20, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.528 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 903877184.000000, mean_absolute_error: 41148.085938, mean_q: 49802.714844\n",
      "   99486/5000000: episode: 1731, duration: 1.199s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.640 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 722202112.000000, mean_absolute_error: 41551.300781, mean_q: 50444.691406\n",
      "   99512/5000000: episode: 1732, duration: 1.309s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.808 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 435445376.000000, mean_absolute_error: 37482.902344, mean_q: 45338.835938\n",
      "   99537/5000000: episode: 1733, duration: 1.238s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 1.920 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 1642329344.000000, mean_absolute_error: 43015.996094, mean_q: 51973.730469\n",
      "   99564/5000000: episode: 1734, duration: 1.516s, episode steps: 27, steps per second: 18, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.889 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 597073344.000000, mean_absolute_error: 41493.023438, mean_q: 49547.730469\n",
      "   99596/5000000: episode: 1735, duration: 1.604s, episode steps: 32, steps per second: 20, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 2.469 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 640410368.000000, mean_absolute_error: 38468.757812, mean_q: 46591.539062\n",
      "   99719/5000000: episode: 1736, duration: 6.152s, episode steps: 123, steps per second: 20, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.715 [0.000, 5.000], mean observation: 0.075 [0.000, 24.000], loss: 980974848.000000, mean_absolute_error: 41623.566406, mean_q: 50169.703125\n",
      "   99751/5000000: episode: 1737, duration: 1.413s, episode steps: 32, steps per second: 23, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 2.156 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 694287360.000000, mean_absolute_error: 37038.546875, mean_q: 44768.410156\n",
      "   99776/5000000: episode: 1738, duration: 1.184s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.880 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 197122496.000000, mean_absolute_error: 43121.765625, mean_q: 52523.855469\n",
      "   99807/5000000: episode: 1739, duration: 1.485s, episode steps: 31, steps per second: 21, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 2.516 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 514222400.000000, mean_absolute_error: 38063.792969, mean_q: 45983.984375\n",
      "   99834/5000000: episode: 1740, duration: 1.319s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.593 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 647329856.000000, mean_absolute_error: 45571.019531, mean_q: 55022.550781\n",
      "   99952/5000000: episode: 1741, duration: 5.423s, episode steps: 118, steps per second: 22, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.339 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 842901760.000000, mean_absolute_error: 41831.898438, mean_q: 50647.199219\n",
      "  100028/5000000: episode: 1742, duration: 3.749s, episode steps: 76, steps per second: 20, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 2.461 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1014027968.000000, mean_absolute_error: 41299.257812, mean_q: 49908.835938\n",
      "  100055/5000000: episode: 1743, duration: 1.488s, episode steps: 27, steps per second: 18, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.519 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 905870464.000000, mean_absolute_error: 42172.148438, mean_q: 51103.507812\n",
      "  100111/5000000: episode: 1744, duration: 2.667s, episode steps: 56, steps per second: 21, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.607 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 552311936.000000, mean_absolute_error: 42082.593750, mean_q: 50562.152344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  100289/5000000: episode: 1745, duration: 8.640s, episode steps: 178, steps per second: 21, episode reward: -1.000, mean reward: -0.006 [-1.000, 0.000], mean action: 2.522 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 628511616.000000, mean_absolute_error: 39694.976562, mean_q: 47916.718750\n",
      "  100314/5000000: episode: 1746, duration: 1.349s, episode steps: 25, steps per second: 19, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.720 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 389578304.000000, mean_absolute_error: 44283.488281, mean_q: 53579.019531\n",
      "  100348/5000000: episode: 1747, duration: 1.684s, episode steps: 34, steps per second: 20, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.147 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1628201472.000000, mean_absolute_error: 45147.042969, mean_q: 54349.003906\n",
      "  100481/5000000: episode: 1748, duration: 6.437s, episode steps: 133, steps per second: 21, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.346 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 763904960.000000, mean_absolute_error: 40407.148438, mean_q: 48644.292969\n",
      "  100507/5000000: episode: 1749, duration: 1.330s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.538 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 624091072.000000, mean_absolute_error: 39867.679688, mean_q: 48095.792969\n",
      "  100677/5000000: episode: 1750, duration: 8.097s, episode steps: 170, steps per second: 21, episode reward: -1.000, mean reward: -0.006 [-1.000, 0.000], mean action: 2.747 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 516420128.000000, mean_absolute_error: 41390.695312, mean_q: 50034.769531\n",
      "  100702/5000000: episode: 1751, duration: 1.243s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.360 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 592531136.000000, mean_absolute_error: 42184.218750, mean_q: 50926.351562\n",
      "  100735/5000000: episode: 1752, duration: 1.587s, episode steps: 33, steps per second: 21, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 2.545 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 661594048.000000, mean_absolute_error: 42796.246094, mean_q: 51453.699219\n",
      "  100762/5000000: episode: 1753, duration: 1.388s, episode steps: 27, steps per second: 19, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.704 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 2250418176.000000, mean_absolute_error: 43706.070312, mean_q: 52648.832031\n",
      "  100853/5000000: episode: 1754, duration: 4.485s, episode steps: 91, steps per second: 20, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.484 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 1211644672.000000, mean_absolute_error: 41299.722656, mean_q: 49864.808594\n",
      "  100930/5000000: episode: 1755, duration: 3.697s, episode steps: 77, steps per second: 21, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 2.558 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 638742144.000000, mean_absolute_error: 40897.855469, mean_q: 49406.648438\n",
      "  100959/5000000: episode: 1756, duration: 1.463s, episode steps: 29, steps per second: 20, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.379 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1021813184.000000, mean_absolute_error: 40709.867188, mean_q: 49153.511719\n",
      "  101024/5000000: episode: 1757, duration: 3.262s, episode steps: 65, steps per second: 20, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 2.338 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 467655072.000000, mean_absolute_error: 42456.574219, mean_q: 51397.726562\n",
      "  101050/5000000: episode: 1758, duration: 1.346s, episode steps: 26, steps per second: 19, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.077 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 456962560.000000, mean_absolute_error: 41636.792969, mean_q: 50051.394531\n",
      "  101107/5000000: episode: 1759, duration: 2.648s, episode steps: 57, steps per second: 22, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.614 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 215158432.000000, mean_absolute_error: 40818.539062, mean_q: 49323.765625\n",
      "  101134/5000000: episode: 1760, duration: 1.313s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 3.111 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 222429360.000000, mean_absolute_error: 46099.972656, mean_q: 55719.359375\n",
      "  101191/5000000: episode: 1761, duration: 2.743s, episode steps: 57, steps per second: 21, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.772 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 1583070464.000000, mean_absolute_error: 43210.300781, mean_q: 52272.425781\n",
      "  101389/5000000: episode: 1762, duration: 9.454s, episode steps: 198, steps per second: 21, episode reward: -1.000, mean reward: -0.005 [-1.000, 0.000], mean action: 2.424 [0.000, 5.000], mean observation: 0.062 [0.000, 24.000], loss: 1063059648.000000, mean_absolute_error: 41949.722656, mean_q: 50643.546875\n",
      "  101467/5000000: episode: 1763, duration: 3.669s, episode steps: 78, steps per second: 21, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 2.795 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 1191807104.000000, mean_absolute_error: 41720.644531, mean_q: 50373.761719\n",
      "  101538/5000000: episode: 1764, duration: 3.498s, episode steps: 71, steps per second: 20, episode reward: -1.000, mean reward: -0.014 [-1.000, 0.000], mean action: 2.296 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 431501536.000000, mean_absolute_error: 40208.375000, mean_q: 48574.652344\n",
      "  101597/5000000: episode: 1765, duration: 2.823s, episode steps: 59, steps per second: 21, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.576 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 527148672.000000, mean_absolute_error: 44275.878906, mean_q: 53589.457031\n",
      "  101635/5000000: episode: 1766, duration: 1.788s, episode steps: 38, steps per second: 21, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 2.158 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 537025984.000000, mean_absolute_error: 40214.292969, mean_q: 48584.863281\n",
      "  101688/5000000: episode: 1767, duration: 2.539s, episode steps: 53, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.660 [0.000, 5.000], mean observation: 0.073 [0.000, 24.000], loss: 977937792.000000, mean_absolute_error: 42569.027344, mean_q: 51479.769531\n",
      "  101772/5000000: episode: 1768, duration: 4.182s, episode steps: 84, steps per second: 20, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.714 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1506196736.000000, mean_absolute_error: 45240.566406, mean_q: 54620.968750\n",
      "  101858/5000000: episode: 1769, duration: 4.015s, episode steps: 86, steps per second: 21, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.453 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 760878400.000000, mean_absolute_error: 44051.871094, mean_q: 53332.714844\n",
      "  101883/5000000: episode: 1770, duration: 1.214s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.640 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 614890752.000000, mean_absolute_error: 46166.613281, mean_q: 55947.101562\n",
      "  101910/5000000: episode: 1771, duration: 1.327s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.741 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1984262784.000000, mean_absolute_error: 42613.351562, mean_q: 51266.226562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  101938/5000000: episode: 1772, duration: 1.313s, episode steps: 28, steps per second: 21, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.750 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 847160512.000000, mean_absolute_error: 49514.914062, mean_q: 59589.777344\n",
      "  102023/5000000: episode: 1773, duration: 3.995s, episode steps: 85, steps per second: 21, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.635 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 1567071744.000000, mean_absolute_error: 44480.535156, mean_q: 53756.347656\n",
      "  102167/5000000: episode: 1774, duration: 6.744s, episode steps: 144, steps per second: 21, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 2.590 [0.000, 5.000], mean observation: 0.073 [0.000, 24.000], loss: 843705088.000000, mean_absolute_error: 44343.492188, mean_q: 53578.765625\n",
      "  102204/5000000: episode: 1775, duration: 1.755s, episode steps: 37, steps per second: 21, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 2.514 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 1129492864.000000, mean_absolute_error: 41532.679688, mean_q: 50145.375000\n",
      "  102429/5000000: episode: 1776, duration: 10.919s, episode steps: 225, steps per second: 21, episode reward: -1.000, mean reward: -0.004 [-1.000, 0.000], mean action: 2.529 [0.000, 5.000], mean observation: 0.062 [0.000, 24.000], loss: 1648517888.000000, mean_absolute_error: 45130.851562, mean_q: 54520.507812\n",
      "  102454/5000000: episode: 1777, duration: 1.242s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.360 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 3265612800.000000, mean_absolute_error: 43527.144531, mean_q: 52403.128906\n",
      "  102481/5000000: episode: 1778, duration: 1.416s, episode steps: 27, steps per second: 19, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.926 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 2530262784.000000, mean_absolute_error: 48661.785156, mean_q: 58490.281250\n",
      "  102506/5000000: episode: 1779, duration: 1.222s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.320 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 522952448.000000, mean_absolute_error: 41451.351562, mean_q: 50074.285156\n",
      "  102597/5000000: episode: 1780, duration: 4.262s, episode steps: 91, steps per second: 21, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.571 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 1144853632.000000, mean_absolute_error: 45118.777344, mean_q: 54488.179688\n",
      "  102623/5000000: episode: 1781, duration: 1.283s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.038 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 3264361984.000000, mean_absolute_error: 45618.023438, mean_q: 55028.210938\n",
      "  102648/5000000: episode: 1782, duration: 1.248s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.520 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 466204064.000000, mean_absolute_error: 47137.195312, mean_q: 56901.261719\n",
      "  102673/5000000: episode: 1783, duration: 1.113s, episode steps: 25, steps per second: 22, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.600 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 829558400.000000, mean_absolute_error: 45617.054688, mean_q: 55214.781250\n",
      "  102703/5000000: episode: 1784, duration: 1.443s, episode steps: 30, steps per second: 21, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.400 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 4278276096.000000, mean_absolute_error: 43405.828125, mean_q: 51999.164062\n",
      "  102759/5000000: episode: 1785, duration: 2.716s, episode steps: 56, steps per second: 21, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.161 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 1840840320.000000, mean_absolute_error: 48997.222656, mean_q: 59006.554688\n",
      "  102791/5000000: episode: 1786, duration: 1.762s, episode steps: 32, steps per second: 18, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 2.656 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1832672640.000000, mean_absolute_error: 46777.054688, mean_q: 56349.011719\n",
      "  102950/5000000: episode: 1787, duration: 7.602s, episode steps: 159, steps per second: 21, episode reward: -1.000, mean reward: -0.006 [-1.000, 0.000], mean action: 2.497 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 1741799936.000000, mean_absolute_error: 46646.242188, mean_q: 56386.226562\n",
      "  102977/5000000: episode: 1788, duration: 1.360s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.222 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 1149386752.000000, mean_absolute_error: 42885.472656, mean_q: 51753.582031\n",
      "  103004/5000000: episode: 1789, duration: 1.195s, episode steps: 27, steps per second: 23, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.741 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 2863159040.000000, mean_absolute_error: 48054.578125, mean_q: 58026.101562\n",
      "  103035/5000000: episode: 1790, duration: 1.481s, episode steps: 31, steps per second: 21, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 2.677 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 1420645504.000000, mean_absolute_error: 47714.191406, mean_q: 57496.660156\n",
      "  103266/5000000: episode: 1791, duration: 10.557s, episode steps: 231, steps per second: 22, episode reward: -1.000, mean reward: -0.004 [-1.000, 0.000], mean action: 2.511 [0.000, 5.000], mean observation: 0.061 [0.000, 24.000], loss: 1582902784.000000, mean_absolute_error: 47625.351562, mean_q: 57535.316406\n",
      "  103322/5000000: episode: 1792, duration: 2.708s, episode steps: 56, steps per second: 21, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.714 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 1030966912.000000, mean_absolute_error: 46263.980469, mean_q: 55948.617188\n",
      "  103353/5000000: episode: 1793, duration: 1.514s, episode steps: 31, steps per second: 20, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 2.484 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 1516068224.000000, mean_absolute_error: 44574.640625, mean_q: 53899.675781\n",
      "  103412/5000000: episode: 1794, duration: 2.748s, episode steps: 59, steps per second: 21, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.169 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 1161513344.000000, mean_absolute_error: 47620.769531, mean_q: 57610.363281\n",
      "  103456/5000000: episode: 1795, duration: 2.058s, episode steps: 44, steps per second: 21, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 2.523 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 791061888.000000, mean_absolute_error: 48028.699219, mean_q: 58201.535156\n",
      "  103484/5000000: episode: 1796, duration: 1.331s, episode steps: 28, steps per second: 21, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.607 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 1392354432.000000, mean_absolute_error: 50895.285156, mean_q: 61728.570312\n",
      "  103511/5000000: episode: 1797, duration: 1.348s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.481 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 2360014592.000000, mean_absolute_error: 44807.417969, mean_q: 54109.820312\n",
      "  103567/5000000: episode: 1798, duration: 2.596s, episode steps: 56, steps per second: 22, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.429 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1306029312.000000, mean_absolute_error: 49086.242188, mean_q: 59458.777344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  103647/5000000: episode: 1799, duration: 3.847s, episode steps: 80, steps per second: 21, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 3.237 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 2063023744.000000, mean_absolute_error: 48526.054688, mean_q: 58566.210938\n",
      "  103673/5000000: episode: 1800, duration: 1.190s, episode steps: 26, steps per second: 22, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.654 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 525544096.000000, mean_absolute_error: 43680.414062, mean_q: 52762.742188\n",
      "  103736/5000000: episode: 1801, duration: 2.901s, episode steps: 63, steps per second: 22, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 2.238 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 1540161792.000000, mean_absolute_error: 44140.878906, mean_q: 53147.058594\n",
      "  103771/5000000: episode: 1802, duration: 1.696s, episode steps: 35, steps per second: 21, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.200 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 2542155008.000000, mean_absolute_error: 48830.835938, mean_q: 58970.855469\n",
      "  103796/5000000: episode: 1803, duration: 1.203s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.280 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 3849948928.000000, mean_absolute_error: 51759.945312, mean_q: 62508.328125\n",
      "  103862/5000000: episode: 1804, duration: 2.972s, episode steps: 66, steps per second: 22, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 2.303 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1896690304.000000, mean_absolute_error: 46067.496094, mean_q: 55538.496094\n",
      "  103917/5000000: episode: 1805, duration: 2.651s, episode steps: 55, steps per second: 21, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.691 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1485009280.000000, mean_absolute_error: 48688.195312, mean_q: 58839.855469\n",
      "  103997/5000000: episode: 1806, duration: 3.770s, episode steps: 80, steps per second: 21, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 2.450 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 2229988864.000000, mean_absolute_error: 47787.230469, mean_q: 57725.710938\n",
      "  104024/5000000: episode: 1807, duration: 1.306s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.519 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 2223322368.000000, mean_absolute_error: 50068.921875, mean_q: 60267.425781\n",
      "  104086/5000000: episode: 1808, duration: 3.034s, episode steps: 62, steps per second: 20, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 2.645 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1352844544.000000, mean_absolute_error: 46959.210938, mean_q: 56799.144531\n",
      "  104155/5000000: episode: 1809, duration: 3.332s, episode steps: 69, steps per second: 21, episode reward: -1.000, mean reward: -0.014 [-1.000, 0.000], mean action: 2.725 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 670435648.000000, mean_absolute_error: 47751.843750, mean_q: 57669.171875\n",
      "  104377/5000000: episode: 1810, duration: 10.854s, episode steps: 222, steps per second: 20, episode reward: -1.000, mean reward: -0.005 [-1.000, 0.000], mean action: 2.477 [0.000, 5.000], mean observation: 0.058 [0.000, 24.000], loss: 1178963584.000000, mean_absolute_error: 46970.460938, mean_q: 56711.082031\n",
      "  104529/5000000: episode: 1811, duration: 6.957s, episode steps: 152, steps per second: 22, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 2.632 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 1446735232.000000, mean_absolute_error: 48352.718750, mean_q: 58502.093750\n",
      "  104736/5000000: episode: 1812, duration: 10.096s, episode steps: 207, steps per second: 21, episode reward: -1.000, mean reward: -0.005 [-1.000, 0.000], mean action: 2.502 [0.000, 5.000], mean observation: 0.057 [0.000, 24.000], loss: 1380327296.000000, mean_absolute_error: 47797.882812, mean_q: 57713.738281\n",
      "  104773/5000000: episode: 1813, duration: 1.766s, episode steps: 37, steps per second: 21, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 2.108 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 1837189632.000000, mean_absolute_error: 49255.996094, mean_q: 59544.609375\n",
      "  104799/5000000: episode: 1814, duration: 1.253s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.077 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1937233152.000000, mean_absolute_error: 48325.289062, mean_q: 58369.046875\n",
      "  104852/5000000: episode: 1815, duration: 2.445s, episode steps: 53, steps per second: 22, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.453 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 1141192576.000000, mean_absolute_error: 46536.304688, mean_q: 56197.062500\n",
      "  104882/5000000: episode: 1816, duration: 1.556s, episode steps: 30, steps per second: 19, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.567 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 880394240.000000, mean_absolute_error: 46737.136719, mean_q: 56383.574219\n",
      "  104945/5000000: episode: 1817, duration: 2.956s, episode steps: 63, steps per second: 21, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 2.778 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 2591676928.000000, mean_absolute_error: 51173.734375, mean_q: 61880.453125\n",
      "  105109/5000000: episode: 1818, duration: 19.328s, episode steps: 164, steps per second: 8, episode reward: -1.000, mean reward: -0.006 [-1.000, 0.000], mean action: 2.823 [0.000, 5.000], mean observation: 0.060 [0.000, 24.000], loss: 2216132608.000000, mean_absolute_error: 48914.851562, mean_q: 58950.855469\n",
      "  105169/5000000: episode: 1819, duration: 2.816s, episode steps: 60, steps per second: 21, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.400 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 1741494272.000000, mean_absolute_error: 52648.941406, mean_q: 63701.585938\n",
      "  105198/5000000: episode: 1820, duration: 1.323s, episode steps: 29, steps per second: 22, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.690 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1050537408.000000, mean_absolute_error: 49922.875000, mean_q: 60421.632812\n",
      "  105223/5000000: episode: 1821, duration: 1.322s, episode steps: 25, steps per second: 19, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.000 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 833812672.000000, mean_absolute_error: 51898.984375, mean_q: 62698.781250\n",
      "  105253/5000000: episode: 1822, duration: 1.509s, episode steps: 30, steps per second: 20, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.133 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 1486406400.000000, mean_absolute_error: 52183.753906, mean_q: 62939.121094\n",
      "  105307/5000000: episode: 1823, duration: 2.530s, episode steps: 54, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.704 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1317025664.000000, mean_absolute_error: 51094.683594, mean_q: 61711.402344\n",
      "  105399/5000000: episode: 1824, duration: 4.345s, episode steps: 92, steps per second: 21, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.478 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 1734489984.000000, mean_absolute_error: 53021.191406, mean_q: 64128.906250\n",
      "  105429/5000000: episode: 1825, duration: 1.417s, episode steps: 30, steps per second: 21, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.400 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 3343094272.000000, mean_absolute_error: 54776.636719, mean_q: 66224.109375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  105457/5000000: episode: 1826, duration: 1.360s, episode steps: 28, steps per second: 21, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.643 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 2007639680.000000, mean_absolute_error: 50443.238281, mean_q: 60814.863281\n",
      "  105569/5000000: episode: 1827, duration: 5.678s, episode steps: 112, steps per second: 20, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.277 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 1309631616.000000, mean_absolute_error: 48549.550781, mean_q: 58631.367188\n",
      "  105628/5000000: episode: 1828, duration: 2.820s, episode steps: 59, steps per second: 21, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.780 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 674077952.000000, mean_absolute_error: 50768.683594, mean_q: 61219.878906\n",
      "  105683/5000000: episode: 1829, duration: 2.595s, episode steps: 55, steps per second: 21, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.873 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 2857540096.000000, mean_absolute_error: 55050.742188, mean_q: 65000.890625\n",
      "  105771/5000000: episode: 1830, duration: 4.236s, episode steps: 88, steps per second: 21, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.284 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 1322246784.000000, mean_absolute_error: 53641.734375, mean_q: 64861.703125\n",
      "  105800/5000000: episode: 1831, duration: 1.454s, episode steps: 29, steps per second: 20, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.897 [1.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 4547486720.000000, mean_absolute_error: 59384.953125, mean_q: 71256.843750\n",
      "  105856/5000000: episode: 1832, duration: 2.839s, episode steps: 56, steps per second: 20, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.411 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 2534085120.000000, mean_absolute_error: 51164.222656, mean_q: 61628.871094\n",
      "  105922/5000000: episode: 1833, duration: 3.284s, episode steps: 66, steps per second: 20, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 2.348 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1072616896.000000, mean_absolute_error: 46981.335938, mean_q: 56666.199219\n",
      "  105947/5000000: episode: 1834, duration: 1.234s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.840 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 376926656.000000, mean_absolute_error: 46995.828125, mean_q: 56733.410156\n",
      "  105980/5000000: episode: 1835, duration: 1.556s, episode steps: 33, steps per second: 21, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 2.091 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 1851122944.000000, mean_absolute_error: 49716.429688, mean_q: 59965.128906\n",
      "  106070/5000000: episode: 1836, duration: 4.340s, episode steps: 90, steps per second: 21, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.333 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 2831120640.000000, mean_absolute_error: 49365.167969, mean_q: 59437.511719\n",
      "  106165/5000000: episode: 1837, duration: 4.532s, episode steps: 95, steps per second: 21, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.337 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 724605184.000000, mean_absolute_error: 51741.574219, mean_q: 62457.132812\n",
      "  106225/5000000: episode: 1838, duration: 2.758s, episode steps: 60, steps per second: 22, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.417 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1656038912.000000, mean_absolute_error: 55725.222656, mean_q: 67471.867188\n",
      "  106350/5000000: episode: 1839, duration: 5.984s, episode steps: 125, steps per second: 21, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.296 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1187327360.000000, mean_absolute_error: 48500.722656, mean_q: 58578.164062\n",
      "  106424/5000000: episode: 1840, duration: 3.741s, episode steps: 74, steps per second: 20, episode reward: -1.000, mean reward: -0.014 [-1.000, 0.000], mean action: 2.554 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 2556612608.000000, mean_absolute_error: 54388.761719, mean_q: 65589.859375\n",
      "  106449/5000000: episode: 1841, duration: 1.187s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.560 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 1831737728.000000, mean_absolute_error: 50954.011719, mean_q: 61469.496094\n",
      "  106476/5000000: episode: 1842, duration: 1.334s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.074 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 499789280.000000, mean_absolute_error: 54477.707031, mean_q: 65971.468750\n",
      "  106504/5000000: episode: 1843, duration: 1.411s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.321 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 2316504064.000000, mean_absolute_error: 52365.875000, mean_q: 63184.523438\n",
      "  106538/5000000: episode: 1844, duration: 1.599s, episode steps: 34, steps per second: 21, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.206 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 1816956800.000000, mean_absolute_error: 50729.746094, mean_q: 61127.468750\n",
      "  106646/5000000: episode: 1845, duration: 5.117s, episode steps: 108, steps per second: 21, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.444 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 1295032320.000000, mean_absolute_error: 52890.019531, mean_q: 63841.867188\n",
      "  106674/5000000: episode: 1846, duration: 1.462s, episode steps: 28, steps per second: 19, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 3.250 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 631776768.000000, mean_absolute_error: 51995.687500, mean_q: 63009.511719\n",
      "  106883/5000000: episode: 1847, duration: 9.323s, episode steps: 209, steps per second: 22, episode reward: -1.000, mean reward: -0.005 [-1.000, 0.000], mean action: 2.388 [0.000, 5.000], mean observation: 0.061 [0.000, 24.000], loss: 1561176320.000000, mean_absolute_error: 51488.503906, mean_q: 62083.656250\n",
      "  106919/5000000: episode: 1848, duration: 1.714s, episode steps: 36, steps per second: 21, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 2.194 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1692787328.000000, mean_absolute_error: 53052.035156, mean_q: 63732.207031\n",
      "  106944/5000000: episode: 1849, duration: 1.241s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.640 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 2098980096.000000, mean_absolute_error: 50789.093750, mean_q: 61326.531250\n",
      "  106970/5000000: episode: 1850, duration: 1.316s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.923 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1241302272.000000, mean_absolute_error: 55021.703125, mean_q: 66449.804688\n",
      "  107009/5000000: episode: 1851, duration: 1.890s, episode steps: 39, steps per second: 21, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 2.282 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 1124155520.000000, mean_absolute_error: 53230.894531, mean_q: 64385.121094\n",
      "  107043/5000000: episode: 1852, duration: 1.734s, episode steps: 34, steps per second: 20, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.500 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 1225037440.000000, mean_absolute_error: 52798.585938, mean_q: 63694.082031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  107184/5000000: episode: 1853, duration: 7.006s, episode steps: 141, steps per second: 20, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 2.553 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 2039426816.000000, mean_absolute_error: 53927.296875, mean_q: 64905.566406\n",
      "  107215/5000000: episode: 1854, duration: 1.610s, episode steps: 31, steps per second: 19, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 3.194 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 613900096.000000, mean_absolute_error: 51205.953125, mean_q: 61929.500000\n",
      "  107281/5000000: episode: 1855, duration: 3.248s, episode steps: 66, steps per second: 20, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 2.242 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 2002827264.000000, mean_absolute_error: 53390.410156, mean_q: 64525.507812\n",
      "  107357/5000000: episode: 1856, duration: 3.692s, episode steps: 76, steps per second: 21, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 2.303 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 1422468096.000000, mean_absolute_error: 48911.464844, mean_q: 59002.027344\n",
      "  107507/5000000: episode: 1857, duration: 7.159s, episode steps: 150, steps per second: 21, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 2.460 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 1263449600.000000, mean_absolute_error: 50439.160156, mean_q: 60974.574219\n",
      "  107534/5000000: episode: 1858, duration: 1.295s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.963 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 3188086784.000000, mean_absolute_error: 62936.746094, mean_q: 76015.703125\n",
      "  107570/5000000: episode: 1859, duration: 1.772s, episode steps: 36, steps per second: 20, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 1.972 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 974683712.000000, mean_absolute_error: 50172.058594, mean_q: 60589.769531\n",
      "  107601/5000000: episode: 1860, duration: 1.422s, episode steps: 31, steps per second: 22, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 2.355 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 2615145728.000000, mean_absolute_error: 53568.250000, mean_q: 64565.566406\n",
      "  107627/5000000: episode: 1861, duration: 1.352s, episode steps: 26, steps per second: 19, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.385 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1691237504.000000, mean_absolute_error: 53488.714844, mean_q: 64719.019531\n",
      "  107829/5000000: episode: 1862, duration: 10.181s, episode steps: 202, steps per second: 20, episode reward: -1.000, mean reward: -0.005 [-1.000, 0.000], mean action: 2.337 [0.000, 5.000], mean observation: 0.061 [0.000, 24.000], loss: 1415944064.000000, mean_absolute_error: 52956.078125, mean_q: 63815.003906\n",
      "  107855/5000000: episode: 1863, duration: 1.338s, episode steps: 26, steps per second: 19, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.962 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 3400936960.000000, mean_absolute_error: 57218.765625, mean_q: 68968.468750\n",
      "  107933/5000000: episode: 1864, duration: 3.693s, episode steps: 78, steps per second: 21, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 2.846 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1388981248.000000, mean_absolute_error: 53614.183594, mean_q: 64720.828125\n",
      "  107998/5000000: episode: 1865, duration: 3.106s, episode steps: 65, steps per second: 21, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 2.462 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1549690624.000000, mean_absolute_error: 57344.335938, mean_q: 69145.984375\n",
      "  108050/5000000: episode: 1866, duration: 2.380s, episode steps: 52, steps per second: 22, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.865 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 513816448.000000, mean_absolute_error: 50205.867188, mean_q: 60719.855469\n",
      "  108078/5000000: episode: 1867, duration: 1.412s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.321 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 3068108800.000000, mean_absolute_error: 51588.800781, mean_q: 62169.382812\n",
      "  108105/5000000: episode: 1868, duration: 1.290s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.222 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 1020204992.000000, mean_absolute_error: 54473.277344, mean_q: 66076.281250\n",
      "  108161/5000000: episode: 1869, duration: 2.644s, episode steps: 56, steps per second: 21, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.625 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 1302988416.000000, mean_absolute_error: 54052.500000, mean_q: 65323.304688\n",
      "  108246/5000000: episode: 1870, duration: 3.821s, episode steps: 85, steps per second: 22, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.165 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 2022837632.000000, mean_absolute_error: 54774.312500, mean_q: 66207.554688\n",
      "  108281/5000000: episode: 1871, duration: 1.612s, episode steps: 35, steps per second: 22, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.543 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 2235469824.000000, mean_absolute_error: 52377.589844, mean_q: 63231.613281\n",
      "  108380/5000000: episode: 1872, duration: 4.686s, episode steps: 99, steps per second: 21, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.273 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1837328512.000000, mean_absolute_error: 51819.089844, mean_q: 62563.003906\n",
      "  108514/5000000: episode: 1873, duration: 6.461s, episode steps: 134, steps per second: 21, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 2.425 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1887587840.000000, mean_absolute_error: 51946.031250, mean_q: 62592.804688\n",
      "  108542/5000000: episode: 1874, duration: 1.322s, episode steps: 28, steps per second: 21, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.464 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 730289600.000000, mean_absolute_error: 53954.972656, mean_q: 65307.875000\n",
      "  108568/5000000: episode: 1875, duration: 1.233s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.923 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1216215680.000000, mean_absolute_error: 53628.257812, mean_q: 64900.847656\n",
      "  108628/5000000: episode: 1876, duration: 2.914s, episode steps: 60, steps per second: 21, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.550 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 1700065408.000000, mean_absolute_error: 57070.132812, mean_q: 68551.453125\n",
      "  108653/5000000: episode: 1877, duration: 1.220s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.240 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 1235095296.000000, mean_absolute_error: 52350.644531, mean_q: 63171.398438\n",
      "  108683/5000000: episode: 1878, duration: 1.474s, episode steps: 30, steps per second: 20, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 3.000 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1423541504.000000, mean_absolute_error: 52733.539062, mean_q: 63649.707031\n",
      "  108714/5000000: episode: 1879, duration: 1.444s, episode steps: 31, steps per second: 21, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 2.161 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 1859224192.000000, mean_absolute_error: 56345.054688, mean_q: 66896.976562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  108744/5000000: episode: 1880, duration: 1.451s, episode steps: 30, steps per second: 21, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.500 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 981051264.000000, mean_absolute_error: 51776.296875, mean_q: 62461.570312\n",
      "  108796/5000000: episode: 1881, duration: 2.514s, episode steps: 52, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.827 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 675325696.000000, mean_absolute_error: 55240.585938, mean_q: 66828.679688\n",
      "  108821/5000000: episode: 1882, duration: 1.132s, episode steps: 25, steps per second: 22, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.440 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 3530383104.000000, mean_absolute_error: 61918.671875, mean_q: 74269.218750\n",
      "  108874/5000000: episode: 1883, duration: 2.627s, episode steps: 53, steps per second: 20, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.132 [0.000, 5.000], mean observation: 0.073 [0.000, 24.000], loss: 632525696.000000, mean_absolute_error: 57018.207031, mean_q: 68638.585938\n",
      "  108900/5000000: episode: 1884, duration: 1.274s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.269 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1664663808.000000, mean_absolute_error: 52695.203125, mean_q: 63720.628906\n",
      "  108939/5000000: episode: 1885, duration: 1.952s, episode steps: 39, steps per second: 20, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 2.410 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 913031872.000000, mean_absolute_error: 52718.597656, mean_q: 63714.511719\n",
      "  108969/5000000: episode: 1886, duration: 1.450s, episode steps: 30, steps per second: 21, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.767 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1425179520.000000, mean_absolute_error: 60804.558594, mean_q: 73580.546875\n",
      "  109061/5000000: episode: 1887, duration: 4.408s, episode steps: 92, steps per second: 21, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.315 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 2190733568.000000, mean_absolute_error: 58260.390625, mean_q: 70453.632812\n",
      "  109092/5000000: episode: 1888, duration: 1.504s, episode steps: 31, steps per second: 21, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 2.452 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 2127230720.000000, mean_absolute_error: 54617.882812, mean_q: 65823.007812\n",
      "  109161/5000000: episode: 1889, duration: 3.321s, episode steps: 69, steps per second: 21, episode reward: -1.000, mean reward: -0.014 [-1.000, 0.000], mean action: 2.609 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 1575992320.000000, mean_absolute_error: 56855.500000, mean_q: 68698.257812\n",
      "  109186/5000000: episode: 1890, duration: 1.184s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.680 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1641243008.000000, mean_absolute_error: 53022.355469, mean_q: 64016.218750\n",
      "  109346/5000000: episode: 1891, duration: 7.838s, episode steps: 160, steps per second: 20, episode reward: -1.000, mean reward: -0.006 [-1.000, 0.000], mean action: 2.506 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 2179762944.000000, mean_absolute_error: 56092.117188, mean_q: 67740.015625\n",
      "  109373/5000000: episode: 1892, duration: 1.369s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.667 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 754336512.000000, mean_absolute_error: 53734.078125, mean_q: 64997.621094\n",
      "  109402/5000000: episode: 1893, duration: 1.430s, episode steps: 29, steps per second: 20, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.172 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1141438720.000000, mean_absolute_error: 59314.132812, mean_q: 71722.242188\n",
      "  109461/5000000: episode: 1894, duration: 2.801s, episode steps: 59, steps per second: 21, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.814 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 2837668352.000000, mean_absolute_error: 57156.933594, mean_q: 68996.640625\n",
      "  109497/5000000: episode: 1895, duration: 1.844s, episode steps: 36, steps per second: 20, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 2.833 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 1600170496.000000, mean_absolute_error: 57350.183594, mean_q: 69122.734375\n",
      "  109522/5000000: episode: 1896, duration: 1.184s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.720 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 2637058304.000000, mean_absolute_error: 51802.718750, mean_q: 62385.343750\n",
      "  109547/5000000: episode: 1897, duration: 1.158s, episode steps: 25, steps per second: 22, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.840 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1079275392.000000, mean_absolute_error: 55935.214844, mean_q: 67021.570312\n",
      "  109683/5000000: episode: 1898, duration: 6.589s, episode steps: 136, steps per second: 21, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 2.301 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 2172190208.000000, mean_absolute_error: 54134.113281, mean_q: 65206.281250\n",
      "  109739/5000000: episode: 1899, duration: 2.766s, episode steps: 56, steps per second: 20, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.429 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 1367390848.000000, mean_absolute_error: 58400.312500, mean_q: 70626.054688\n",
      "  109766/5000000: episode: 1900, duration: 1.371s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.444 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1248353664.000000, mean_absolute_error: 57293.597656, mean_q: 69328.914062\n",
      "  109792/5000000: episode: 1901, duration: 1.256s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.038 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1318499968.000000, mean_absolute_error: 53735.773438, mean_q: 64964.589844\n",
      "  109877/5000000: episode: 1902, duration: 4.141s, episode steps: 85, steps per second: 21, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.365 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1528600448.000000, mean_absolute_error: 53599.242188, mean_q: 64751.453125\n",
      "  109945/5000000: episode: 1903, duration: 3.392s, episode steps: 68, steps per second: 20, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 2.221 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1484334848.000000, mean_absolute_error: 56237.378906, mean_q: 67883.359375\n",
      "  109975/5000000: episode: 1904, duration: 1.474s, episode steps: 30, steps per second: 20, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.700 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 2118886144.000000, mean_absolute_error: 60848.148438, mean_q: 73748.132812\n",
      "  110000/5000000: episode: 1905, duration: 1.219s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.200 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 792436672.000000, mean_absolute_error: 53525.203125, mean_q: 64662.136719\n",
      "  110026/5000000: episode: 1906, duration: 1.286s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.731 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1408537728.000000, mean_absolute_error: 57563.234375, mean_q: 69608.593750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  110280/5000000: episode: 1907, duration: 11.944s, episode steps: 254, steps per second: 21, episode reward: -1.000, mean reward: -0.004 [-1.000, 0.000], mean action: 2.280 [0.000, 5.000], mean observation: 0.062 [0.000, 24.000], loss: 2279296256.000000, mean_absolute_error: 55622.222656, mean_q: 67158.898438\n",
      "  110453/5000000: episode: 1908, duration: 7.516s, episode steps: 173, steps per second: 23, episode reward: -1.000, mean reward: -0.006 [-1.000, 0.000], mean action: 2.572 [0.000, 5.000], mean observation: 0.060 [0.000, 24.000], loss: 1909885312.000000, mean_absolute_error: 55275.273438, mean_q: 66783.664062\n",
      "  110545/5000000: episode: 1909, duration: 4.308s, episode steps: 92, steps per second: 21, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.641 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 468031456.000000, mean_absolute_error: 54083.175781, mean_q: 65418.433594\n",
      "  110571/5000000: episode: 1910, duration: 1.369s, episode steps: 26, steps per second: 19, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.962 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 831951552.000000, mean_absolute_error: 56304.042969, mean_q: 68113.734375\n",
      "  110651/5000000: episode: 1911, duration: 3.977s, episode steps: 80, steps per second: 20, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 1.988 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 662754112.000000, mean_absolute_error: 56786.898438, mean_q: 68602.750000\n",
      "  110676/5000000: episode: 1912, duration: 1.237s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.760 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 476064032.000000, mean_absolute_error: 61714.289062, mean_q: 74696.492188\n",
      "  110734/5000000: episode: 1913, duration: 2.978s, episode steps: 58, steps per second: 19, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.569 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 4758137344.000000, mean_absolute_error: 57704.027344, mean_q: 69570.351562\n",
      "  110761/5000000: episode: 1914, duration: 1.396s, episode steps: 27, steps per second: 19, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.185 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1156073344.000000, mean_absolute_error: 58199.785156, mean_q: 70456.945312\n",
      "  110819/5000000: episode: 1915, duration: 2.710s, episode steps: 58, steps per second: 21, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.793 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 1084315904.000000, mean_absolute_error: 59811.816406, mean_q: 72473.296875\n",
      "  110875/5000000: episode: 1916, duration: 2.727s, episode steps: 56, steps per second: 21, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.696 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1850656768.000000, mean_absolute_error: 57608.445312, mean_q: 69580.609375\n",
      "  110907/5000000: episode: 1917, duration: 1.550s, episode steps: 32, steps per second: 21, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 2.094 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 2166436608.000000, mean_absolute_error: 55929.078125, mean_q: 67614.492188\n",
      "  110976/5000000: episode: 1918, duration: 3.285s, episode steps: 69, steps per second: 21, episode reward: -1.000, mean reward: -0.014 [-1.000, 0.000], mean action: 2.420 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 3114599168.000000, mean_absolute_error: 58197.777344, mean_q: 70333.718750\n",
      "  111045/5000000: episode: 1919, duration: 13.112s, episode steps: 69, steps per second: 5, episode reward: -1.000, mean reward: -0.014 [-1.000, 0.000], mean action: 2.391 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 1266463360.000000, mean_absolute_error: 61198.421875, mean_q: 73842.453125\n",
      "  111154/5000000: episode: 1920, duration: 5.396s, episode steps: 109, steps per second: 20, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.532 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 1919105408.000000, mean_absolute_error: 58496.445312, mean_q: 70712.539062\n",
      "  111182/5000000: episode: 1921, duration: 1.391s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.893 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 282866784.000000, mean_absolute_error: 56757.605469, mean_q: 68694.335938\n",
      "  111211/5000000: episode: 1922, duration: 1.501s, episode steps: 29, steps per second: 19, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.172 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 2799941376.000000, mean_absolute_error: 64906.097656, mean_q: 77541.546875\n",
      "  111242/5000000: episode: 1923, duration: 1.574s, episode steps: 31, steps per second: 20, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 2.355 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1884216448.000000, mean_absolute_error: 57613.582031, mean_q: 69438.156250\n",
      "  111443/5000000: episode: 1924, duration: 9.671s, episode steps: 201, steps per second: 21, episode reward: -1.000, mean reward: -0.005 [-1.000, 0.000], mean action: 2.408 [0.000, 5.000], mean observation: 0.061 [0.000, 24.000], loss: 1705934336.000000, mean_absolute_error: 58766.894531, mean_q: 71030.109375\n",
      "  111470/5000000: episode: 1925, duration: 1.290s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.333 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 2460672256.000000, mean_absolute_error: 63261.847656, mean_q: 76441.140625\n",
      "  111569/5000000: episode: 1926, duration: 5.030s, episode steps: 99, steps per second: 20, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.707 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 2981781504.000000, mean_absolute_error: 60065.968750, mean_q: 72221.609375\n",
      "  111634/5000000: episode: 1927, duration: 2.923s, episode steps: 65, steps per second: 22, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 2.692 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 1338099456.000000, mean_absolute_error: 59813.593750, mean_q: 72441.500000\n",
      "  111830/5000000: episode: 1928, duration: 9.312s, episode steps: 196, steps per second: 21, episode reward: -1.000, mean reward: -0.005 [-1.000, 0.000], mean action: 2.577 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 1585027584.000000, mean_absolute_error: 56378.148438, mean_q: 67981.062500\n",
      "  111860/5000000: episode: 1929, duration: 1.439s, episode steps: 30, steps per second: 21, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.333 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 2994657792.000000, mean_absolute_error: 57778.847656, mean_q: 69756.273438\n",
      "  111894/5000000: episode: 1930, duration: 1.697s, episode steps: 34, steps per second: 20, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.735 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 2270896384.000000, mean_absolute_error: 61423.304688, mean_q: 73844.132812\n",
      "  111928/5000000: episode: 1931, duration: 1.586s, episode steps: 34, steps per second: 21, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 3.000 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 2055728640.000000, mean_absolute_error: 56806.828125, mean_q: 68715.015625\n",
      "  111959/5000000: episode: 1932, duration: 1.483s, episode steps: 31, steps per second: 21, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 2.452 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 1866411776.000000, mean_absolute_error: 57669.660156, mean_q: 69680.898438\n",
      "  112209/5000000: episode: 1933, duration: 12.254s, episode steps: 250, steps per second: 20, episode reward: -1.000, mean reward: -0.004 [-1.000, 0.000], mean action: 2.676 [0.000, 5.000], mean observation: 0.060 [0.000, 24.000], loss: 2179038208.000000, mean_absolute_error: 59764.871094, mean_q: 72137.421875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  112351/5000000: episode: 1934, duration: 6.606s, episode steps: 142, steps per second: 21, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 2.373 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 1581553792.000000, mean_absolute_error: 60481.437500, mean_q: 73123.593750\n",
      "  112502/5000000: episode: 1935, duration: 7.464s, episode steps: 151, steps per second: 20, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 2.450 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 2025570432.000000, mean_absolute_error: 59710.531250, mean_q: 72108.281250\n",
      "  112623/5000000: episode: 1936, duration: 5.767s, episode steps: 121, steps per second: 21, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.388 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 2068395136.000000, mean_absolute_error: 60799.667969, mean_q: 73443.835938\n",
      "  112651/5000000: episode: 1937, duration: 1.308s, episode steps: 28, steps per second: 21, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 3.036 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 695044480.000000, mean_absolute_error: 60847.132812, mean_q: 73701.460938\n",
      "  112676/5000000: episode: 1938, duration: 1.229s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.800 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 924540928.000000, mean_absolute_error: 56513.929688, mean_q: 68268.179688\n",
      "  112801/5000000: episode: 1939, duration: 5.675s, episode steps: 125, steps per second: 22, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.328 [0.000, 5.000], mean observation: 0.061 [0.000, 24.000], loss: 2141956480.000000, mean_absolute_error: 59355.781250, mean_q: 71743.812500\n",
      "  112826/5000000: episode: 1940, duration: 1.184s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.640 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1344950912.000000, mean_absolute_error: 67521.726562, mean_q: 81415.617188\n",
      "  112883/5000000: episode: 1941, duration: 2.580s, episode steps: 57, steps per second: 22, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.842 [0.000, 5.000], mean observation: 0.074 [0.000, 24.000], loss: 1398915968.000000, mean_absolute_error: 62900.960938, mean_q: 76021.710938\n",
      "  112908/5000000: episode: 1942, duration: 1.306s, episode steps: 25, steps per second: 19, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.880 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 394630912.000000, mean_absolute_error: 58386.968750, mean_q: 70511.125000\n",
      "  113023/5000000: episode: 1943, duration: 5.652s, episode steps: 115, steps per second: 20, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.565 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 3717387008.000000, mean_absolute_error: 59808.355469, mean_q: 72088.312500\n",
      "  113048/5000000: episode: 1944, duration: 1.231s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.600 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 1540196864.000000, mean_absolute_error: 61130.671875, mean_q: 73887.078125\n",
      "  113074/5000000: episode: 1945, duration: 1.316s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.077 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 233499808.000000, mean_absolute_error: 53614.000000, mean_q: 64967.015625\n",
      "  113099/5000000: episode: 1946, duration: 1.190s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.640 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 3189234432.000000, mean_absolute_error: 70392.179688, mean_q: 84424.328125\n",
      "  113125/5000000: episode: 1947, duration: 1.193s, episode steps: 26, steps per second: 22, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.346 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 2688110080.000000, mean_absolute_error: 65176.398438, mean_q: 78355.445312\n",
      "  113261/5000000: episode: 1948, duration: 6.735s, episode steps: 136, steps per second: 20, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 2.669 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 3535175424.000000, mean_absolute_error: 60952.261719, mean_q: 73455.750000\n",
      "  113289/5000000: episode: 1949, duration: 1.369s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.786 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 609591424.000000, mean_absolute_error: 63061.589844, mean_q: 76420.109375\n",
      "  113384/5000000: episode: 1950, duration: 4.595s, episode steps: 95, steps per second: 21, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.358 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 2976035328.000000, mean_absolute_error: 65189.242188, mean_q: 78814.632812\n",
      "  113409/5000000: episode: 1951, duration: 1.315s, episode steps: 25, steps per second: 19, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.440 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 2407713280.000000, mean_absolute_error: 67292.437500, mean_q: 78622.218750\n",
      "  113435/5000000: episode: 1952, duration: 1.294s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.038 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 5611834368.000000, mean_absolute_error: 59076.984375, mean_q: 71149.195312\n",
      "  113522/5000000: episode: 1953, duration: 3.958s, episode steps: 87, steps per second: 22, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.701 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 1475505152.000000, mean_absolute_error: 62285.488281, mean_q: 75383.554688\n",
      "  113719/5000000: episode: 1954, duration: 9.250s, episode steps: 197, steps per second: 21, episode reward: -1.000, mean reward: -0.005 [-1.000, 0.000], mean action: 2.533 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 1733075456.000000, mean_absolute_error: 60989.785156, mean_q: 73507.117188\n",
      "  113759/5000000: episode: 1955, duration: 1.856s, episode steps: 40, steps per second: 22, episode reward: -1.000, mean reward: -0.025 [-1.000, 0.000], mean action: 2.675 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 1006712832.000000, mean_absolute_error: 64314.062500, mean_q: 77745.851562\n",
      "  113796/5000000: episode: 1956, duration: 1.765s, episode steps: 37, steps per second: 21, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 2.297 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 3210953984.000000, mean_absolute_error: 59074.917969, mean_q: 71212.273438\n",
      "  113892/5000000: episode: 1957, duration: 4.617s, episode steps: 96, steps per second: 21, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.708 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 2024214912.000000, mean_absolute_error: 58430.433594, mean_q: 70609.132812\n",
      "  113926/5000000: episode: 1958, duration: 1.582s, episode steps: 34, steps per second: 21, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.412 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 752906432.000000, mean_absolute_error: 62334.726562, mean_q: 75279.289062\n",
      "  113964/5000000: episode: 1959, duration: 1.904s, episode steps: 38, steps per second: 20, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 2.105 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 3513522944.000000, mean_absolute_error: 61360.066406, mean_q: 73570.000000\n",
      "  114071/5000000: episode: 1960, duration: 5.093s, episode steps: 107, steps per second: 21, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.636 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1521417344.000000, mean_absolute_error: 59464.976562, mean_q: 71710.398438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  114153/5000000: episode: 1961, duration: 4.150s, episode steps: 82, steps per second: 20, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.585 [0.000, 5.000], mean observation: 0.073 [0.000, 24.000], loss: 973254848.000000, mean_absolute_error: 60121.089844, mean_q: 72646.929688\n",
      "  114189/5000000: episode: 1962, duration: 1.653s, episode steps: 36, steps per second: 22, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 2.056 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 483753248.000000, mean_absolute_error: 57882.386719, mean_q: 69895.843750\n",
      "  114289/5000000: episode: 1963, duration: 4.780s, episode steps: 100, steps per second: 21, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.500 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 2118797440.000000, mean_absolute_error: 64079.863281, mean_q: 77463.796875\n",
      "  114323/5000000: episode: 1964, duration: 1.576s, episode steps: 34, steps per second: 22, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.382 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 3043298816.000000, mean_absolute_error: 67949.875000, mean_q: 82198.875000\n",
      "  114376/5000000: episode: 1965, duration: 2.582s, episode steps: 53, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.472 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1023665856.000000, mean_absolute_error: 60993.980469, mean_q: 73818.695312\n",
      "  114459/5000000: episode: 1966, duration: 4.136s, episode steps: 83, steps per second: 20, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.373 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 2708461568.000000, mean_absolute_error: 64749.398438, mean_q: 77925.210938\n",
      "  114486/5000000: episode: 1967, duration: 1.318s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.481 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 5271822848.000000, mean_absolute_error: 61392.199219, mean_q: 74080.773438\n",
      "  114515/5000000: episode: 1968, duration: 1.385s, episode steps: 29, steps per second: 21, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.241 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 2271537408.000000, mean_absolute_error: 57864.710938, mean_q: 69853.046875\n",
      "  114819/5000000: episode: 1969, duration: 13.615s, episode steps: 304, steps per second: 22, episode reward: -1.000, mean reward: -0.003 [-1.000, 0.000], mean action: 2.457 [0.000, 5.000], mean observation: 0.060 [0.000, 24.000], loss: 2336500736.000000, mean_absolute_error: 61804.460938, mean_q: 74565.921875\n",
      "  114845/5000000: episode: 1970, duration: 1.242s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.923 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 872622848.000000, mean_absolute_error: 59844.105469, mean_q: 72280.156250\n",
      "  114873/5000000: episode: 1971, duration: 1.361s, episode steps: 28, steps per second: 21, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.679 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 2452144128.000000, mean_absolute_error: 57454.210938, mean_q: 69348.429688\n",
      "  114924/5000000: episode: 1972, duration: 2.409s, episode steps: 51, steps per second: 21, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 2.510 [0.000, 5.000], mean observation: 0.074 [0.000, 24.000], loss: 1879363200.000000, mean_absolute_error: 62301.695312, mean_q: 75358.757812\n",
      "  114953/5000000: episode: 1973, duration: 1.519s, episode steps: 29, steps per second: 19, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.552 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 3115919616.000000, mean_absolute_error: 64802.003906, mean_q: 78149.726562\n",
      "  115076/5000000: episode: 1974, duration: 5.948s, episode steps: 123, steps per second: 21, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.618 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 2121708672.000000, mean_absolute_error: 62155.859375, mean_q: 75085.859375\n",
      "  115103/5000000: episode: 1975, duration: 1.408s, episode steps: 27, steps per second: 19, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 1.889 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 2962048512.000000, mean_absolute_error: 57497.367188, mean_q: 69261.640625\n",
      "  115133/5000000: episode: 1976, duration: 1.487s, episode steps: 30, steps per second: 20, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.633 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 463436096.000000, mean_absolute_error: 54420.402344, mean_q: 65706.984375\n",
      "  115172/5000000: episode: 1977, duration: 1.944s, episode steps: 39, steps per second: 20, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 2.385 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 893469184.000000, mean_absolute_error: 63408.367188, mean_q: 76623.000000\n",
      "  115202/5000000: episode: 1978, duration: 1.468s, episode steps: 30, steps per second: 20, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.833 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 711324032.000000, mean_absolute_error: 62119.933594, mean_q: 75115.031250\n",
      "  115229/5000000: episode: 1979, duration: 1.311s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.667 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1083138304.000000, mean_absolute_error: 62974.218750, mean_q: 76226.867188\n",
      "  115293/5000000: episode: 1980, duration: 3.189s, episode steps: 64, steps per second: 20, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 2.250 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 1311915264.000000, mean_absolute_error: 62808.265625, mean_q: 75934.687500\n",
      "  115379/5000000: episode: 1981, duration: 4.355s, episode steps: 86, steps per second: 20, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.465 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 1031272768.000000, mean_absolute_error: 62123.261719, mean_q: 75081.351562\n",
      "  115466/5000000: episode: 1982, duration: 4.438s, episode steps: 87, steps per second: 20, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.448 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 1319443584.000000, mean_absolute_error: 61919.601562, mean_q: 74760.195312\n",
      "  115491/5000000: episode: 1983, duration: 1.232s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.720 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 2887498752.000000, mean_absolute_error: 55375.503906, mean_q: 66621.328125\n",
      "  115551/5000000: episode: 1984, duration: 2.968s, episode steps: 60, steps per second: 20, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.133 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 1480322176.000000, mean_absolute_error: 63773.777344, mean_q: 76975.835938\n",
      "  115578/5000000: episode: 1985, duration: 1.275s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 3.333 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 3761237248.000000, mean_absolute_error: 59289.722656, mean_q: 71454.171875\n",
      "  115639/5000000: episode: 1986, duration: 2.913s, episode steps: 61, steps per second: 21, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 2.705 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 1189048192.000000, mean_absolute_error: 60847.437500, mean_q: 73686.234375\n",
      "  115664/5000000: episode: 1987, duration: 1.235s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.000 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 630589376.000000, mean_absolute_error: 59946.105469, mean_q: 72400.460938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  115798/5000000: episode: 1988, duration: 6.383s, episode steps: 134, steps per second: 21, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 2.604 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 2776354048.000000, mean_absolute_error: 60859.140625, mean_q: 73484.593750\n",
      "  115860/5000000: episode: 1989, duration: 2.975s, episode steps: 62, steps per second: 21, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 2.435 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 2480535808.000000, mean_absolute_error: 64942.375000, mean_q: 78559.164062\n",
      "  115915/5000000: episode: 1990, duration: 2.765s, episode steps: 55, steps per second: 20, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.109 [0.000, 5.000], mean observation: 0.076 [0.000, 24.000], loss: 1452426368.000000, mean_absolute_error: 62925.769531, mean_q: 76205.312500\n",
      "  115966/5000000: episode: 1991, duration: 2.648s, episode steps: 51, steps per second: 19, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 2.980 [0.000, 5.000], mean observation: 0.074 [0.000, 24.000], loss: 1608301568.000000, mean_absolute_error: 66173.484375, mean_q: 80141.609375\n",
      "  116193/5000000: episode: 1992, duration: 11.463s, episode steps: 227, steps per second: 20, episode reward: -1.000, mean reward: -0.004 [-1.000, 0.000], mean action: 2.542 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 1634033024.000000, mean_absolute_error: 62428.527344, mean_q: 75429.304688\n",
      "  116279/5000000: episode: 1993, duration: 4.143s, episode steps: 86, steps per second: 21, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.558 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1196879232.000000, mean_absolute_error: 65033.574219, mean_q: 78785.304688\n",
      "  116410/5000000: episode: 1994, duration: 6.378s, episode steps: 131, steps per second: 21, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.313 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 2557758464.000000, mean_absolute_error: 65158.105469, mean_q: 78771.523438\n",
      "  116435/5000000: episode: 1995, duration: 1.281s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.000 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 1254255488.000000, mean_absolute_error: 58121.894531, mean_q: 70110.968750\n",
      "  116461/5000000: episode: 1996, duration: 1.280s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.577 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 570920512.000000, mean_absolute_error: 64092.703125, mean_q: 77695.226562\n",
      "  116611/5000000: episode: 1997, duration: 7.139s, episode steps: 150, steps per second: 21, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 2.567 [0.000, 5.000], mean observation: 0.060 [0.000, 24.000], loss: 1990289920.000000, mean_absolute_error: 64698.921875, mean_q: 78265.867188\n",
      "  116720/5000000: episode: 1998, duration: 5.538s, episode steps: 109, steps per second: 20, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.422 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1450372352.000000, mean_absolute_error: 62884.468750, mean_q: 75825.523438\n",
      "  116747/5000000: episode: 1999, duration: 1.295s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.556 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 2559234560.000000, mean_absolute_error: 63497.941406, mean_q: 76670.890625\n",
      "  116799/5000000: episode: 2000, duration: 2.603s, episode steps: 52, steps per second: 20, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.519 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 2173812992.000000, mean_absolute_error: 62166.546875, mean_q: 75051.609375\n",
      "  116900/5000000: episode: 2001, duration: 6.575s, episode steps: 101, steps per second: 15, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.465 [0.000, 5.000], mean observation: 0.073 [0.000, 24.000], loss: 1597308160.000000, mean_absolute_error: 64292.871094, mean_q: 77873.179688\n",
      "  116939/5000000: episode: 2002, duration: 1.894s, episode steps: 39, steps per second: 21, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 2.333 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 3264718592.000000, mean_absolute_error: 64061.992188, mean_q: 77382.195312\n",
      "  116993/5000000: episode: 2003, duration: 2.582s, episode steps: 54, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.556 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 917281856.000000, mean_absolute_error: 65074.144531, mean_q: 78699.289062\n",
      "  117047/5000000: episode: 2004, duration: 11.054s, episode steps: 54, steps per second: 5, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.370 [0.000, 5.000], mean observation: 0.073 [0.000, 24.000], loss: 1072147264.000000, mean_absolute_error: 65132.027344, mean_q: 78650.046875\n",
      "  117072/5000000: episode: 2005, duration: 1.252s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.000 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 1230603776.000000, mean_absolute_error: 71118.390625, mean_q: 86025.453125\n",
      "  117105/5000000: episode: 2006, duration: 1.502s, episode steps: 33, steps per second: 22, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 2.818 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 1935750656.000000, mean_absolute_error: 62528.718750, mean_q: 75644.359375\n",
      "  117154/5000000: episode: 2007, duration: 2.415s, episode steps: 49, steps per second: 20, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 2.082 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 622654720.000000, mean_absolute_error: 62423.148438, mean_q: 75598.398438\n",
      "  117215/5000000: episode: 2008, duration: 2.985s, episode steps: 61, steps per second: 20, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 2.361 [0.000, 5.000], mean observation: 0.073 [0.000, 24.000], loss: 1519585024.000000, mean_absolute_error: 65521.644531, mean_q: 79279.429688\n",
      "  117244/5000000: episode: 2009, duration: 1.461s, episode steps: 29, steps per second: 20, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.310 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 2726155520.000000, mean_absolute_error: 66419.210938, mean_q: 80422.906250\n",
      "  117453/5000000: episode: 2010, duration: 10.387s, episode steps: 209, steps per second: 20, episode reward: -1.000, mean reward: -0.005 [-1.000, 0.000], mean action: 2.445 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 1749411072.000000, mean_absolute_error: 65098.929688, mean_q: 78739.742188\n",
      "  117478/5000000: episode: 2011, duration: 1.284s, episode steps: 25, steps per second: 19, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.680 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 2426253824.000000, mean_absolute_error: 69031.101562, mean_q: 83661.257812\n",
      "  117505/5000000: episode: 2012, duration: 1.292s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 3.185 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 5000729088.000000, mean_absolute_error: 63294.058594, mean_q: 76254.890625\n",
      "  117530/5000000: episode: 2013, duration: 1.329s, episode steps: 25, steps per second: 19, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.360 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 982176832.000000, mean_absolute_error: 68659.945312, mean_q: 83012.507812\n",
      "  117577/5000000: episode: 2014, duration: 2.247s, episode steps: 47, steps per second: 21, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 2.128 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 1748260864.000000, mean_absolute_error: 62735.601562, mean_q: 75811.859375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  117602/5000000: episode: 2015, duration: 1.179s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.120 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 2563503360.000000, mean_absolute_error: 63035.589844, mean_q: 75692.148438\n",
      "  117627/5000000: episode: 2016, duration: 1.256s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.160 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 2025995520.000000, mean_absolute_error: 69690.601562, mean_q: 84367.890625\n",
      "  117663/5000000: episode: 2017, duration: 1.709s, episode steps: 36, steps per second: 21, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 2.389 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 2355513088.000000, mean_absolute_error: 69763.929688, mean_q: 83998.523438\n",
      "  117691/5000000: episode: 2018, duration: 1.380s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.679 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1105644416.000000, mean_absolute_error: 64784.714844, mean_q: 78521.414062\n",
      "  117791/5000000: episode: 2019, duration: 5.023s, episode steps: 100, steps per second: 20, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.280 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 1761972224.000000, mean_absolute_error: 64616.984375, mean_q: 78175.843750\n",
      "  117873/5000000: episode: 2020, duration: 4.148s, episode steps: 82, steps per second: 20, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.537 [0.000, 5.000], mean observation: 0.074 [0.000, 24.000], loss: 1787449472.000000, mean_absolute_error: 65709.570312, mean_q: 79495.039062\n",
      "  117923/5000000: episode: 2021, duration: 2.505s, episode steps: 50, steps per second: 20, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 2.120 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 904015680.000000, mean_absolute_error: 68531.867188, mean_q: 82993.093750\n",
      "  117952/5000000: episode: 2022, duration: 1.495s, episode steps: 29, steps per second: 19, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.138 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 5139433472.000000, mean_absolute_error: 63540.746094, mean_q: 76644.468750\n",
      "  117978/5000000: episode: 2023, duration: 1.298s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.346 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 3275544064.000000, mean_absolute_error: 70525.679688, mean_q: 85090.960938\n",
      "  118007/5000000: episode: 2024, duration: 1.480s, episode steps: 29, steps per second: 20, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.586 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 2758829056.000000, mean_absolute_error: 60196.046875, mean_q: 72534.445312\n",
      "  118140/5000000: episode: 2025, duration: 6.420s, episode steps: 133, steps per second: 21, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.383 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 1954837120.000000, mean_absolute_error: 65366.058594, mean_q: 78901.101562\n",
      "  118170/5000000: episode: 2026, duration: 1.534s, episode steps: 30, steps per second: 20, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.633 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 3408462592.000000, mean_absolute_error: 62834.367188, mean_q: 75639.281250\n",
      "  118224/5000000: episode: 2027, duration: 2.541s, episode steps: 54, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.704 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 2939038976.000000, mean_absolute_error: 65807.281250, mean_q: 79545.679688\n",
      "  118528/5000000: episode: 2028, duration: 14.487s, episode steps: 304, steps per second: 21, episode reward: -1.000, mean reward: -0.003 [-1.000, 0.000], mean action: 2.510 [0.000, 5.000], mean observation: 0.054 [0.000, 24.000], loss: 1967695744.000000, mean_absolute_error: 68465.148438, mean_q: 82772.156250\n",
      "  118557/5000000: episode: 2029, duration: 1.337s, episode steps: 29, steps per second: 22, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.310 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 2756447232.000000, mean_absolute_error: 68768.421875, mean_q: 83025.531250\n",
      "  118584/5000000: episode: 2030, duration: 1.303s, episode steps: 27, steps per second: 21, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.667 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1478224128.000000, mean_absolute_error: 64781.792969, mean_q: 78185.140625\n",
      "  118615/5000000: episode: 2031, duration: 1.529s, episode steps: 31, steps per second: 20, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 2.613 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 3967771392.000000, mean_absolute_error: 67723.648438, mean_q: 81821.359375\n",
      "  118693/5000000: episode: 2032, duration: 3.822s, episode steps: 78, steps per second: 20, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 2.590 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 2071518976.000000, mean_absolute_error: 64485.449219, mean_q: 77950.914062\n",
      "  118718/5000000: episode: 2033, duration: 1.188s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.040 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 807387776.000000, mean_absolute_error: 64466.550781, mean_q: 78012.648438\n",
      "  118826/5000000: episode: 2034, duration: 5.226s, episode steps: 108, steps per second: 21, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.546 [0.000, 5.000], mean observation: 0.074 [0.000, 24.000], loss: 927100032.000000, mean_absolute_error: 67455.179688, mean_q: 81604.531250\n",
      "  118851/5000000: episode: 2035, duration: 1.208s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.640 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 1008103424.000000, mean_absolute_error: 63348.039062, mean_q: 76577.156250\n",
      "  118881/5000000: episode: 2036, duration: 1.332s, episode steps: 30, steps per second: 23, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.333 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 2766513664.000000, mean_absolute_error: 71187.960938, mean_q: 86132.601562\n",
      "  118909/5000000: episode: 2037, duration: 1.340s, episode steps: 28, steps per second: 21, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.536 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 2045784960.000000, mean_absolute_error: 69280.023438, mean_q: 83955.023438\n",
      "  118957/5000000: episode: 2038, duration: 2.158s, episode steps: 48, steps per second: 22, episode reward: -1.000, mean reward: -0.021 [-1.000, 0.000], mean action: 2.333 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 3426010368.000000, mean_absolute_error: 73266.328125, mean_q: 88608.054688\n",
      "  119023/5000000: episode: 2039, duration: 3.192s, episode steps: 66, steps per second: 21, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 2.682 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 954453696.000000, mean_absolute_error: 68414.812500, mean_q: 82773.835938\n",
      "  119052/5000000: episode: 2040, duration: 1.407s, episode steps: 29, steps per second: 21, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.345 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 169352976.000000, mean_absolute_error: 65279.031250, mean_q: 79115.445312\n",
      "  119109/5000000: episode: 2041, duration: 2.730s, episode steps: 57, steps per second: 21, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.754 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 3888707840.000000, mean_absolute_error: 70685.429688, mean_q: 85415.062500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  119135/5000000: episode: 2042, duration: 1.289s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.462 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 947488064.000000, mean_absolute_error: 65535.480469, mean_q: 79417.437500\n",
      "  119165/5000000: episode: 2043, duration: 1.513s, episode steps: 30, steps per second: 20, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.400 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 984055488.000000, mean_absolute_error: 73587.343750, mean_q: 89350.789062\n",
      "  119237/5000000: episode: 2044, duration: 3.130s, episode steps: 72, steps per second: 23, episode reward: -1.000, mean reward: -0.014 [-1.000, 0.000], mean action: 2.542 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 945761600.000000, mean_absolute_error: 66867.476562, mean_q: 80934.960938\n",
      "  119267/5000000: episode: 2045, duration: 1.388s, episode steps: 30, steps per second: 22, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.700 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 2756032256.000000, mean_absolute_error: 69601.398438, mean_q: 84011.164062\n",
      "  119292/5000000: episode: 2046, duration: 1.298s, episode steps: 25, steps per second: 19, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.840 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 2332935424.000000, mean_absolute_error: 61997.121094, mean_q: 74826.617188\n",
      "  119325/5000000: episode: 2047, duration: 1.494s, episode steps: 33, steps per second: 22, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 1.970 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 393265792.000000, mean_absolute_error: 64387.531250, mean_q: 77576.921875\n",
      "  119351/5000000: episode: 2048, duration: 1.138s, episode steps: 26, steps per second: 23, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.615 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 4561804800.000000, mean_absolute_error: 66121.578125, mean_q: 79703.304688\n",
      "  119378/5000000: episode: 2049, duration: 1.367s, episode steps: 27, steps per second: 20, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 2.481 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 2960452352.000000, mean_absolute_error: 63498.433594, mean_q: 76548.296875\n",
      "  119406/5000000: episode: 2050, duration: 1.432s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.000 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1344054144.000000, mean_absolute_error: 69005.726562, mean_q: 83353.273438\n",
      "  119436/5000000: episode: 2051, duration: 1.380s, episode steps: 30, steps per second: 22, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.433 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1871320448.000000, mean_absolute_error: 65274.953125, mean_q: 78927.367188\n",
      "  119551/5000000: episode: 2052, duration: 5.167s, episode steps: 115, steps per second: 22, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.487 [0.000, 5.000], mean observation: 0.060 [0.000, 24.000], loss: 2564763648.000000, mean_absolute_error: 67319.726562, mean_q: 81283.617188\n",
      "  119604/5000000: episode: 2053, duration: 2.579s, episode steps: 53, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.340 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 2439722752.000000, mean_absolute_error: 66946.437500, mean_q: 80889.359375\n",
      "  119629/5000000: episode: 2054, duration: 1.193s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.680 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 918640448.000000, mean_absolute_error: 71085.718750, mean_q: 86064.546875\n",
      "  119654/5000000: episode: 2055, duration: 1.223s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.640 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 1308238848.000000, mean_absolute_error: 66095.867188, mean_q: 79842.203125\n",
      "  119743/5000000: episode: 2056, duration: 4.155s, episode steps: 89, steps per second: 21, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.685 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 1622028928.000000, mean_absolute_error: 68626.968750, mean_q: 83008.859375\n",
      "  119798/5000000: episode: 2057, duration: 2.627s, episode steps: 55, steps per second: 21, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.509 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 3260405504.000000, mean_absolute_error: 63052.605469, mean_q: 76077.570312\n",
      "  119824/5000000: episode: 2058, duration: 1.325s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.500 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 456816128.000000, mean_absolute_error: 70639.296875, mean_q: 85636.992188\n",
      "  119854/5000000: episode: 2059, duration: 1.415s, episode steps: 30, steps per second: 21, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.500 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 774679168.000000, mean_absolute_error: 72763.429688, mean_q: 88231.734375\n",
      "  119879/5000000: episode: 2060, duration: 1.159s, episode steps: 25, steps per second: 22, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.960 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 1358798336.000000, mean_absolute_error: 76461.929688, mean_q: 92444.757812\n",
      "  119908/5000000: episode: 2061, duration: 1.402s, episode steps: 29, steps per second: 21, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.897 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 1949560448.000000, mean_absolute_error: 71468.570312, mean_q: 86427.898438\n",
      "  119971/5000000: episode: 2062, duration: 3.066s, episode steps: 63, steps per second: 21, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 2.746 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 909068352.000000, mean_absolute_error: 66236.781250, mean_q: 79870.695312\n",
      "  120075/5000000: episode: 2063, duration: 4.964s, episode steps: 104, steps per second: 21, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.875 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 2953095680.000000, mean_absolute_error: 71906.359375, mean_q: 86692.335938\n",
      "  120131/5000000: episode: 2064, duration: 2.730s, episode steps: 56, steps per second: 21, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.018 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 1385342976.000000, mean_absolute_error: 70584.351562, mean_q: 85266.210938\n",
      "  120157/5000000: episode: 2065, duration: 1.366s, episode steps: 26, steps per second: 19, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.346 [1.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 934822336.000000, mean_absolute_error: 64747.230469, mean_q: 78178.109375\n",
      "  120182/5000000: episode: 2066, duration: 1.283s, episode steps: 25, steps per second: 19, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.480 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1688676608.000000, mean_absolute_error: 67375.101562, mean_q: 81287.046875\n",
      "  120302/5000000: episode: 2067, duration: 5.762s, episode steps: 120, steps per second: 21, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.542 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 1280703744.000000, mean_absolute_error: 65592.679688, mean_q: 79363.078125\n",
      "  120384/5000000: episode: 2068, duration: 3.791s, episode steps: 82, steps per second: 22, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.744 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 1689697280.000000, mean_absolute_error: 68408.820312, mean_q: 82924.875000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  120410/5000000: episode: 2069, duration: 1.309s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 1.692 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 780189312.000000, mean_absolute_error: 64786.398438, mean_q: 78260.515625\n",
      "  120436/5000000: episode: 2070, duration: 1.303s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.885 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1224515584.000000, mean_absolute_error: 73126.203125, mean_q: 88431.460938\n",
      "  120462/5000000: episode: 2071, duration: 1.240s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.538 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1473502976.000000, mean_absolute_error: 65384.972656, mean_q: 78934.093750\n",
      "  120495/5000000: episode: 2072, duration: 1.652s, episode steps: 33, steps per second: 20, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 2.606 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 2426320640.000000, mean_absolute_error: 69367.468750, mean_q: 83940.710938\n",
      "  120523/5000000: episode: 2073, duration: 1.393s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.429 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 631876544.000000, mean_absolute_error: 68517.476562, mean_q: 82989.875000\n",
      "  120580/5000000: episode: 2074, duration: 2.567s, episode steps: 57, steps per second: 22, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.509 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 3724267520.000000, mean_absolute_error: 70798.390625, mean_q: 85689.351562\n",
      "  120605/5000000: episode: 2075, duration: 1.272s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.640 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 1167280384.000000, mean_absolute_error: 67580.062500, mean_q: 81721.726562\n",
      "  120648/5000000: episode: 2076, duration: 1.956s, episode steps: 43, steps per second: 22, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 2.163 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 348233856.000000, mean_absolute_error: 63407.347656, mean_q: 76861.648438\n",
      "  120677/5000000: episode: 2077, duration: 1.394s, episode steps: 29, steps per second: 21, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.069 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 7909426176.000000, mean_absolute_error: 72062.679688, mean_q: 86896.718750\n",
      "  120703/5000000: episode: 2078, duration: 1.240s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.731 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 2047373952.000000, mean_absolute_error: 72089.351562, mean_q: 87291.367188\n",
      "  120780/5000000: episode: 2079, duration: 3.768s, episode steps: 77, steps per second: 20, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 2.260 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 1606366976.000000, mean_absolute_error: 70543.187500, mean_q: 85492.132812\n",
      "  120833/5000000: episode: 2080, duration: 2.703s, episode steps: 53, steps per second: 20, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.849 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 1700516352.000000, mean_absolute_error: 63975.859375, mean_q: 77367.742188\n",
      "  120895/5000000: episode: 2081, duration: 2.930s, episode steps: 62, steps per second: 21, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 2.823 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 4734081024.000000, mean_absolute_error: 74020.156250, mean_q: 89153.562500\n",
      "  120920/5000000: episode: 2082, duration: 1.296s, episode steps: 25, steps per second: 19, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.200 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 403006464.000000, mean_absolute_error: 66499.421875, mean_q: 80412.921875\n",
      "  120988/5000000: episode: 2083, duration: 3.347s, episode steps: 68, steps per second: 20, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 2.309 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 3055309312.000000, mean_absolute_error: 68648.367188, mean_q: 82899.898438\n",
      "  121018/5000000: episode: 2084, duration: 1.510s, episode steps: 30, steps per second: 20, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.800 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 1045528576.000000, mean_absolute_error: 77649.828125, mean_q: 93006.226562\n",
      "  121043/5000000: episode: 2085, duration: 1.260s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 1.520 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1177492736.000000, mean_absolute_error: 68663.671875, mean_q: 83090.890625\n",
      "  121141/5000000: episode: 2086, duration: 4.749s, episode steps: 98, steps per second: 21, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.776 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 2747852800.000000, mean_absolute_error: 67135.781250, mean_q: 81076.445312\n",
      "  121194/5000000: episode: 2087, duration: 2.521s, episode steps: 53, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.491 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 651870592.000000, mean_absolute_error: 68286.539062, mean_q: 82459.328125\n",
      "  121249/5000000: episode: 2088, duration: 2.611s, episode steps: 55, steps per second: 21, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.527 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 3673454336.000000, mean_absolute_error: 69391.453125, mean_q: 83582.820312\n",
      "  121299/5000000: episode: 2089, duration: 2.326s, episode steps: 50, steps per second: 21, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 2.300 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1425881344.000000, mean_absolute_error: 67391.531250, mean_q: 81490.242188\n",
      "  121325/5000000: episode: 2090, duration: 1.240s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.115 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1070289024.000000, mean_absolute_error: 65078.167969, mean_q: 78664.546875\n",
      "  121477/5000000: episode: 2091, duration: 7.305s, episode steps: 152, steps per second: 21, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 2.664 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 2989962752.000000, mean_absolute_error: 68604.789062, mean_q: 82854.406250\n",
      "  121506/5000000: episode: 2092, duration: 1.439s, episode steps: 29, steps per second: 20, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.172 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 578758272.000000, mean_absolute_error: 64704.167969, mean_q: 78183.398438\n",
      "  121556/5000000: episode: 2093, duration: 2.413s, episode steps: 50, steps per second: 21, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 2.560 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1751702528.000000, mean_absolute_error: 75884.890625, mean_q: 91615.460938\n",
      "  121582/5000000: episode: 2094, duration: 1.238s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.962 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1283438592.000000, mean_absolute_error: 76883.429688, mean_q: 92962.585938\n",
      "  121672/5000000: episode: 2095, duration: 4.140s, episode steps: 90, steps per second: 22, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.133 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1912771968.000000, mean_absolute_error: 71126.609375, mean_q: 85954.570312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  121777/5000000: episode: 2096, duration: 5.055s, episode steps: 105, steps per second: 21, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.505 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1843809536.000000, mean_absolute_error: 71978.031250, mean_q: 86988.257812\n",
      "  121803/5000000: episode: 2097, duration: 1.292s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.808 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1016218688.000000, mean_absolute_error: 67887.609375, mean_q: 82032.320312\n",
      "  121828/5000000: episode: 2098, duration: 1.261s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.200 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 1277765120.000000, mean_absolute_error: 71812.492188, mean_q: 86966.179688\n",
      "  121899/5000000: episode: 2099, duration: 3.515s, episode steps: 71, steps per second: 20, episode reward: -1.000, mean reward: -0.014 [-1.000, 0.000], mean action: 2.479 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 2972993792.000000, mean_absolute_error: 72036.414062, mean_q: 87214.484375\n",
      "  121928/5000000: episode: 2100, duration: 1.378s, episode steps: 29, steps per second: 21, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.379 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 2365809664.000000, mean_absolute_error: 70610.046875, mean_q: 85515.687500\n",
      "  121961/5000000: episode: 2101, duration: 1.550s, episode steps: 33, steps per second: 21, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 2.152 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 2067909504.000000, mean_absolute_error: 76922.992188, mean_q: 92858.500000\n",
      "  121991/5000000: episode: 2102, duration: 1.449s, episode steps: 30, steps per second: 21, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 3.000 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 3620657920.000000, mean_absolute_error: 78912.226562, mean_q: 93252.750000\n",
      "  122186/5000000: episode: 2103, duration: 9.323s, episode steps: 195, steps per second: 21, episode reward: -1.000, mean reward: -0.005 [-1.000, 0.000], mean action: 2.610 [0.000, 5.000], mean observation: 0.060 [0.000, 24.000], loss: 3480962816.000000, mean_absolute_error: 73777.125000, mean_q: 89073.234375\n",
      "  122316/5000000: episode: 2104, duration: 6.452s, episode steps: 130, steps per second: 20, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.285 [0.000, 5.000], mean observation: 0.061 [0.000, 24.000], loss: 2201123072.000000, mean_absolute_error: 74029.781250, mean_q: 89630.539062\n",
      "  122342/5000000: episode: 2105, duration: 1.296s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.462 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 496791712.000000, mean_absolute_error: 75309.515625, mean_q: 90711.859375\n",
      "  122395/5000000: episode: 2106, duration: 2.680s, episode steps: 53, steps per second: 20, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.528 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1357596416.000000, mean_absolute_error: 73830.992188, mean_q: 88800.453125\n",
      "  122683/5000000: episode: 2107, duration: 14.063s, episode steps: 288, steps per second: 20, episode reward: -1.000, mean reward: -0.003 [-1.000, 0.000], mean action: 2.479 [0.000, 5.000], mean observation: 0.058 [0.000, 24.000], loss: 1413926912.000000, mean_absolute_error: 72379.203125, mean_q: 87449.054688\n",
      "  122934/5000000: episode: 2108, duration: 12.108s, episode steps: 251, steps per second: 21, episode reward: -1.000, mean reward: -0.004 [-1.000, 0.000], mean action: 2.426 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 2206435328.000000, mean_absolute_error: 76869.007812, mean_q: 92943.140625\n",
      "  122960/5000000: episode: 2109, duration: 1.336s, episode steps: 26, steps per second: 19, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.615 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 3150278656.000000, mean_absolute_error: 74741.781250, mean_q: 90283.554688\n",
      "  123069/5000000: episode: 2110, duration: 15.142s, episode steps: 109, steps per second: 7, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.174 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 2648907520.000000, mean_absolute_error: 74338.085938, mean_q: 89902.257812\n",
      "  123181/5000000: episode: 2111, duration: 5.569s, episode steps: 112, steps per second: 20, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.411 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 978779456.000000, mean_absolute_error: 71491.492188, mean_q: 86521.812500\n",
      "  123241/5000000: episode: 2112, duration: 2.899s, episode steps: 60, steps per second: 21, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.550 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1045102208.000000, mean_absolute_error: 72384.750000, mean_q: 87603.625000\n",
      "  123272/5000000: episode: 2113, duration: 1.530s, episode steps: 31, steps per second: 20, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 2.774 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 6141373440.000000, mean_absolute_error: 76136.679688, mean_q: 91890.601562\n",
      "  123332/5000000: episode: 2114, duration: 2.787s, episode steps: 60, steps per second: 22, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.567 [0.000, 5.000], mean observation: 0.073 [0.000, 24.000], loss: 2743290112.000000, mean_absolute_error: 79084.984375, mean_q: 95914.218750\n",
      "  123360/5000000: episode: 2115, duration: 1.477s, episode steps: 28, steps per second: 19, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.929 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 6184889856.000000, mean_absolute_error: 74982.914062, mean_q: 90651.007812\n",
      "  123387/5000000: episode: 2116, duration: 1.236s, episode steps: 27, steps per second: 22, episode reward: -1.000, mean reward: -0.037 [-1.000, 0.000], mean action: 3.074 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 2898986496.000000, mean_absolute_error: 75169.046875, mean_q: 90856.953125\n",
      "  123450/5000000: episode: 2117, duration: 2.892s, episode steps: 63, steps per second: 22, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 2.286 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 2985157632.000000, mean_absolute_error: 75536.609375, mean_q: 91437.976562\n",
      "  123492/5000000: episode: 2118, duration: 2.012s, episode steps: 42, steps per second: 21, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 2.286 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 5850044928.000000, mean_absolute_error: 80265.406250, mean_q: 97061.234375\n",
      "  123686/5000000: episode: 2119, duration: 9.596s, episode steps: 194, steps per second: 20, episode reward: -1.000, mean reward: -0.005 [-1.000, 0.000], mean action: 2.567 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 3117553152.000000, mean_absolute_error: 74818.062500, mean_q: 90315.171875\n",
      "  123711/5000000: episode: 2120, duration: 1.223s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.200 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 3113614592.000000, mean_absolute_error: 74154.367188, mean_q: 89754.460938\n",
      "  123748/5000000: episode: 2121, duration: 1.816s, episode steps: 37, steps per second: 20, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 2.459 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 7405049856.000000, mean_absolute_error: 75386.382812, mean_q: 91038.960938\n",
      "  123785/5000000: episode: 2122, duration: 1.678s, episode steps: 37, steps per second: 22, episode reward: -1.000, mean reward: -0.027 [-1.000, 0.000], mean action: 2.378 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 1712615168.000000, mean_absolute_error: 72681.484375, mean_q: 87857.789062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  123876/5000000: episode: 2123, duration: 4.736s, episode steps: 91, steps per second: 19, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.385 [0.000, 5.000], mean observation: 0.073 [0.000, 24.000], loss: 1930273920.000000, mean_absolute_error: 76606.132812, mean_q: 92624.132812\n",
      "  123974/5000000: episode: 2124, duration: 4.625s, episode steps: 98, steps per second: 21, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.316 [0.000, 5.000], mean observation: 0.061 [0.000, 24.000], loss: 1138994816.000000, mean_absolute_error: 72600.914062, mean_q: 87601.726562\n",
      "  124027/5000000: episode: 2125, duration: 2.578s, episode steps: 53, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.245 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 1149771520.000000, mean_absolute_error: 73419.093750, mean_q: 88814.757812\n",
      "  124090/5000000: episode: 2126, duration: 2.989s, episode steps: 63, steps per second: 21, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 2.349 [0.000, 5.000], mean observation: 0.074 [0.000, 24.000], loss: 2449294336.000000, mean_absolute_error: 73330.281250, mean_q: 88670.843750\n",
      "  124115/5000000: episode: 2127, duration: 1.179s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.080 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 3012645120.000000, mean_absolute_error: 85712.656250, mean_q: 101675.757812\n",
      "  124147/5000000: episode: 2128, duration: 1.579s, episode steps: 32, steps per second: 20, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 2.719 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 793917184.000000, mean_absolute_error: 70866.828125, mean_q: 85778.531250\n",
      "  124448/5000000: episode: 2129, duration: 14.741s, episode steps: 301, steps per second: 20, episode reward: -1.000, mean reward: -0.003 [-1.000, 0.000], mean action: 2.568 [0.000, 5.000], mean observation: 0.060 [0.000, 24.000], loss: 2026583168.000000, mean_absolute_error: 74046.960938, mean_q: 89420.070312\n",
      "  124503/5000000: episode: 2130, duration: 2.542s, episode steps: 55, steps per second: 22, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.327 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 3566138880.000000, mean_absolute_error: 76704.257812, mean_q: 92532.164062\n",
      "  124537/5000000: episode: 2131, duration: 1.647s, episode steps: 34, steps per second: 21, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.824 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 3862558720.000000, mean_absolute_error: 69451.781250, mean_q: 83727.687500\n",
      "  124562/5000000: episode: 2132, duration: 1.211s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.440 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 4387876352.000000, mean_absolute_error: 79071.937500, mean_q: 95298.601562\n",
      "  124588/5000000: episode: 2133, duration: 1.286s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 3.077 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 870677120.000000, mean_absolute_error: 75787.554688, mean_q: 91598.406250\n",
      "  124646/5000000: episode: 2134, duration: 2.913s, episode steps: 58, steps per second: 20, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.483 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 2482050048.000000, mean_absolute_error: 70083.539062, mean_q: 84484.414062\n",
      "  124678/5000000: episode: 2135, duration: 1.536s, episode steps: 32, steps per second: 21, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 2.219 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 3523624448.000000, mean_absolute_error: 73905.390625, mean_q: 89299.039062\n",
      "  124703/5000000: episode: 2136, duration: 1.215s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.520 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 7301533696.000000, mean_absolute_error: 75461.328125, mean_q: 90393.679688\n",
      "  124875/5000000: episode: 2137, duration: 8.446s, episode steps: 172, steps per second: 20, episode reward: -1.000, mean reward: -0.006 [-1.000, 0.000], mean action: 2.314 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 1449512960.000000, mean_absolute_error: 73584.031250, mean_q: 88421.898438\n",
      "  124928/5000000: episode: 2138, duration: 2.645s, episode steps: 53, steps per second: 20, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.453 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 4506307072.000000, mean_absolute_error: 79922.492188, mean_q: 96263.265625\n",
      "  124954/5000000: episode: 2139, duration: 1.266s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.577 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 3995525632.000000, mean_absolute_error: 72608.632812, mean_q: 87615.289062\n",
      "  124990/5000000: episode: 2140, duration: 1.838s, episode steps: 36, steps per second: 20, episode reward: -1.000, mean reward: -0.028 [-1.000, 0.000], mean action: 2.528 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 4124827136.000000, mean_absolute_error: 73938.539062, mean_q: 89011.281250\n",
      "  125015/5000000: episode: 2141, duration: 1.181s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.680 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 5197417472.000000, mean_absolute_error: 66319.617188, mean_q: 79762.906250\n",
      "  125097/5000000: episode: 2142, duration: 3.674s, episode steps: 82, steps per second: 22, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.439 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 3835054848.000000, mean_absolute_error: 74687.609375, mean_q: 90336.742188\n",
      "  125140/5000000: episode: 2143, duration: 2.078s, episode steps: 43, steps per second: 21, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 2.767 [0.000, 5.000], mean observation: 0.060 [0.000, 24.000], loss: 1277365376.000000, mean_absolute_error: 74254.906250, mean_q: 89942.257812\n",
      "  125235/5000000: episode: 2144, duration: 4.458s, episode steps: 95, steps per second: 21, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.526 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 2778859776.000000, mean_absolute_error: 75053.648438, mean_q: 90722.968750\n",
      "  125260/5000000: episode: 2145, duration: 1.161s, episode steps: 25, steps per second: 22, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.800 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 1327857280.000000, mean_absolute_error: 77029.898438, mean_q: 93300.718750\n",
      "  125290/5000000: episode: 2146, duration: 1.497s, episode steps: 30, steps per second: 20, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.800 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 3479551232.000000, mean_absolute_error: 75213.906250, mean_q: 90832.796875\n",
      "  125381/5000000: episode: 2147, duration: 4.437s, episode steps: 91, steps per second: 21, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.846 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 3115136768.000000, mean_absolute_error: 74695.445312, mean_q: 90248.398438\n",
      "  125699/5000000: episode: 2148, duration: 15.631s, episode steps: 318, steps per second: 20, episode reward: -1.000, mean reward: -0.003 [-1.000, 0.000], mean action: 2.431 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 2773696512.000000, mean_absolute_error: 75028.171875, mean_q: 90658.015625\n",
      "  125731/5000000: episode: 2149, duration: 1.556s, episode steps: 32, steps per second: 21, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 2.250 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 6884696064.000000, mean_absolute_error: 73948.148438, mean_q: 89237.203125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  125756/5000000: episode: 2150, duration: 1.377s, episode steps: 25, steps per second: 18, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.040 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 3182950400.000000, mean_absolute_error: 80817.328125, mean_q: 97944.539062\n",
      "  125811/5000000: episode: 2151, duration: 2.685s, episode steps: 55, steps per second: 20, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.382 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 1945446272.000000, mean_absolute_error: 74712.265625, mean_q: 90274.406250\n",
      "  125864/5000000: episode: 2152, duration: 2.559s, episode steps: 53, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.472 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 2133671936.000000, mean_absolute_error: 75564.039062, mean_q: 91307.148438\n",
      "  125980/5000000: episode: 2153, duration: 5.538s, episode steps: 116, steps per second: 21, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.440 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 2443528704.000000, mean_absolute_error: 74461.695312, mean_q: 89963.468750\n",
      "  126044/5000000: episode: 2154, duration: 3.015s, episode steps: 64, steps per second: 21, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 2.562 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 1339531008.000000, mean_absolute_error: 73949.015625, mean_q: 89534.460938\n",
      "  126069/5000000: episode: 2155, duration: 1.209s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.600 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 1485128832.000000, mean_absolute_error: 78234.390625, mean_q: 94679.007812\n",
      "  126121/5000000: episode: 2156, duration: 2.552s, episode steps: 52, steps per second: 20, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.404 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 2869796352.000000, mean_absolute_error: 69613.203125, mean_q: 84064.000000\n",
      "  126241/5000000: episode: 2157, duration: 5.771s, episode steps: 120, steps per second: 21, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.642 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 1230902016.000000, mean_absolute_error: 74263.843750, mean_q: 89930.148438\n",
      "  126331/5000000: episode: 2158, duration: 4.257s, episode steps: 90, steps per second: 21, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.644 [0.000, 5.000], mean observation: 0.077 [0.000, 24.000], loss: 1697012480.000000, mean_absolute_error: 72588.625000, mean_q: 87631.125000\n",
      "  126357/5000000: episode: 2159, duration: 1.364s, episode steps: 26, steps per second: 19, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.692 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1246150912.000000, mean_absolute_error: 76848.359375, mean_q: 93268.570312\n",
      "  126386/5000000: episode: 2160, duration: 1.373s, episode steps: 29, steps per second: 21, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 3.000 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1068068992.000000, mean_absolute_error: 72952.234375, mean_q: 88312.632812\n",
      "  126429/5000000: episode: 2161, duration: 1.998s, episode steps: 43, steps per second: 22, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 2.000 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 2321669376.000000, mean_absolute_error: 74116.218750, mean_q: 89628.406250\n",
      "  126513/5000000: episode: 2162, duration: 3.723s, episode steps: 84, steps per second: 23, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.821 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 703235008.000000, mean_absolute_error: 73114.968750, mean_q: 88551.148438\n",
      "  126577/5000000: episode: 2163, duration: 3.162s, episode steps: 64, steps per second: 20, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 2.359 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 3001768704.000000, mean_absolute_error: 80219.250000, mean_q: 97099.015625\n",
      "  126603/5000000: episode: 2164, duration: 1.281s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.615 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 2577842944.000000, mean_absolute_error: 70954.640625, mean_q: 85685.054688\n",
      "  126772/5000000: episode: 2165, duration: 8.417s, episode steps: 169, steps per second: 20, episode reward: -1.000, mean reward: -0.006 [-1.000, 0.000], mean action: 2.669 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 2459264768.000000, mean_absolute_error: 75613.078125, mean_q: 91400.132812\n",
      "  126911/5000000: episode: 2166, duration: 6.959s, episode steps: 139, steps per second: 20, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 2.317 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 2606381056.000000, mean_absolute_error: 78166.015625, mean_q: 94410.054688\n",
      "  126942/5000000: episode: 2167, duration: 1.556s, episode steps: 31, steps per second: 20, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 2.323 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 2324213504.000000, mean_absolute_error: 75924.296875, mean_q: 91926.562500\n",
      "  126983/5000000: episode: 2168, duration: 1.897s, episode steps: 41, steps per second: 22, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 2.439 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 2742226176.000000, mean_absolute_error: 75464.851562, mean_q: 91308.593750\n",
      "  127008/5000000: episode: 2169, duration: 1.234s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.320 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 4209234688.000000, mean_absolute_error: 79285.492188, mean_q: 95867.570312\n",
      "  127153/5000000: episode: 2170, duration: 7.210s, episode steps: 145, steps per second: 20, episode reward: -1.000, mean reward: -0.007 [-1.000, 0.000], mean action: 2.545 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 2947225856.000000, mean_absolute_error: 79525.531250, mean_q: 96173.242188\n",
      "  127260/5000000: episode: 2171, duration: 5.056s, episode steps: 107, steps per second: 21, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.589 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1690470400.000000, mean_absolute_error: 76609.687500, mean_q: 92778.187500\n",
      "  127345/5000000: episode: 2172, duration: 4.071s, episode steps: 85, steps per second: 21, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.871 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 2684340224.000000, mean_absolute_error: 79379.820312, mean_q: 95801.460938\n",
      "  127370/5000000: episode: 2173, duration: 1.210s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.720 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1001604928.000000, mean_absolute_error: 81608.460938, mean_q: 98993.140625\n",
      "  127396/5000000: episode: 2174, duration: 1.241s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.231 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 960928064.000000, mean_absolute_error: 75897.695312, mean_q: 91763.203125\n",
      "  127437/5000000: episode: 2175, duration: 1.910s, episode steps: 41, steps per second: 21, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 2.146 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 897311232.000000, mean_absolute_error: 71610.242188, mean_q: 86599.187500\n",
      "  127537/5000000: episode: 2176, duration: 4.853s, episode steps: 100, steps per second: 21, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.250 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 3587039488.000000, mean_absolute_error: 77489.085938, mean_q: 93594.617188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  127563/5000000: episode: 2177, duration: 1.377s, episode steps: 26, steps per second: 19, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.731 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 4002885376.000000, mean_absolute_error: 77310.265625, mean_q: 93186.906250\n",
      "  127595/5000000: episode: 2178, duration: 1.575s, episode steps: 32, steps per second: 20, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 2.281 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 3474752256.000000, mean_absolute_error: 82016.039062, mean_q: 99118.757812\n",
      "  127620/5000000: episode: 2179, duration: 1.167s, episode steps: 25, steps per second: 21, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.800 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 784787136.000000, mean_absolute_error: 78134.882812, mean_q: 94534.000000\n",
      "  127655/5000000: episode: 2180, duration: 1.728s, episode steps: 35, steps per second: 20, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 1.543 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 1538188416.000000, mean_absolute_error: 78447.078125, mean_q: 94980.765625\n",
      "  127681/5000000: episode: 2181, duration: 1.290s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.038 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 936617280.000000, mean_absolute_error: 81685.000000, mean_q: 98991.859375\n",
      "  127711/5000000: episode: 2182, duration: 1.502s, episode steps: 30, steps per second: 20, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.767 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 2014829440.000000, mean_absolute_error: 81784.382812, mean_q: 98997.390625\n",
      "  127772/5000000: episode: 2183, duration: 2.902s, episode steps: 61, steps per second: 21, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 2.164 [0.000, 5.000], mean observation: 0.073 [0.000, 24.000], loss: 2176512768.000000, mean_absolute_error: 75732.023438, mean_q: 91605.695312\n",
      "  127848/5000000: episode: 2184, duration: 3.827s, episode steps: 76, steps per second: 20, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 2.553 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1608239488.000000, mean_absolute_error: 71846.312500, mean_q: 86888.429688\n",
      "  127911/5000000: episode: 2185, duration: 3.153s, episode steps: 63, steps per second: 20, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 2.651 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 1511323904.000000, mean_absolute_error: 78948.273438, mean_q: 95726.945312\n",
      "  127944/5000000: episode: 2186, duration: 1.563s, episode steps: 33, steps per second: 21, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 2.061 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 7649050624.000000, mean_absolute_error: 74563.695312, mean_q: 89828.632812\n",
      "  128073/5000000: episode: 2187, duration: 6.228s, episode steps: 129, steps per second: 21, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.372 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 1300294144.000000, mean_absolute_error: 77210.046875, mean_q: 93427.054688\n",
      "  128125/5000000: episode: 2188, duration: 2.590s, episode steps: 52, steps per second: 20, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.577 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 5563025920.000000, mean_absolute_error: 78422.382812, mean_q: 94473.132812\n",
      "  128258/5000000: episode: 2189, duration: 6.598s, episode steps: 133, steps per second: 20, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.564 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 4795826688.000000, mean_absolute_error: 79087.562500, mean_q: 95657.734375\n",
      "  128512/5000000: episode: 2190, duration: 12.081s, episode steps: 254, steps per second: 21, episode reward: -1.000, mean reward: -0.004 [-1.000, 0.000], mean action: 2.343 [0.000, 5.000], mean observation: 0.059 [0.000, 24.000], loss: 2538270720.000000, mean_absolute_error: 77420.515625, mean_q: 93652.023438\n",
      "  128845/5000000: episode: 2191, duration: 14.881s, episode steps: 333, steps per second: 22, episode reward: -1.000, mean reward: -0.003 [-1.000, 0.000], mean action: 2.333 [0.000, 5.000], mean observation: 0.056 [0.000, 24.000], loss: 2358647040.000000, mean_absolute_error: 78275.179688, mean_q: 94757.515625\n",
      "  128880/5000000: episode: 2192, duration: 1.726s, episode steps: 35, steps per second: 20, episode reward: -1.000, mean reward: -0.029 [-1.000, 0.000], mean action: 2.257 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 1355319424.000000, mean_absolute_error: 71296.546875, mean_q: 86077.234375\n",
      "  128946/5000000: episode: 2193, duration: 3.143s, episode steps: 66, steps per second: 21, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 2.106 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 3620547328.000000, mean_absolute_error: 81539.757812, mean_q: 98784.539062\n",
      "  128988/5000000: episode: 2194, duration: 1.996s, episode steps: 42, steps per second: 21, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 2.524 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 3262241536.000000, mean_absolute_error: 80515.656250, mean_q: 97390.210938\n",
      "  129019/5000000: episode: 2195, duration: 11.049s, episode steps: 31, steps per second: 3, episode reward: -1.000, mean reward: -0.032 [-1.000, 0.000], mean action: 2.806 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 3097690880.000000, mean_absolute_error: 85782.992188, mean_q: 103975.554688\n",
      "  129051/5000000: episode: 2196, duration: 1.583s, episode steps: 32, steps per second: 20, episode reward: -1.000, mean reward: -0.031 [-1.000, 0.000], mean action: 2.625 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 3606816000.000000, mean_absolute_error: 79348.820312, mean_q: 95896.007812\n",
      "  129108/5000000: episode: 2197, duration: 2.865s, episode steps: 57, steps per second: 20, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.754 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 2757453312.000000, mean_absolute_error: 77631.843750, mean_q: 93386.445312\n",
      "  129133/5000000: episode: 2198, duration: 1.281s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.720 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 313327936.000000, mean_absolute_error: 78355.171875, mean_q: 95024.609375\n",
      "  129158/5000000: episode: 2199, duration: 1.263s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.480 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 5605432832.000000, mean_absolute_error: 76498.281250, mean_q: 92316.468750\n",
      "  129209/5000000: episode: 2200, duration: 2.554s, episode steps: 51, steps per second: 20, episode reward: -1.000, mean reward: -0.020 [-1.000, 0.000], mean action: 2.333 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 2182617088.000000, mean_absolute_error: 79084.804688, mean_q: 95861.546875\n",
      "  129325/5000000: episode: 2201, duration: 5.816s, episode steps: 116, steps per second: 20, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.578 [0.000, 5.000], mean observation: 0.073 [0.000, 24.000], loss: 1980810880.000000, mean_absolute_error: 80286.164062, mean_q: 97099.328125\n",
      "  129367/5000000: episode: 2202, duration: 2.076s, episode steps: 42, steps per second: 20, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 2.524 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 2213450752.000000, mean_absolute_error: 76339.414062, mean_q: 91988.687500\n",
      "  129443/5000000: episode: 2203, duration: 3.520s, episode steps: 76, steps per second: 22, episode reward: -1.000, mean reward: -0.013 [-1.000, 0.000], mean action: 2.329 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 2799596288.000000, mean_absolute_error: 77751.085938, mean_q: 94083.500000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  129486/5000000: episode: 2204, duration: 2.064s, episode steps: 43, steps per second: 21, episode reward: -1.000, mean reward: -0.023 [-1.000, 0.000], mean action: 1.884 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 3559604224.000000, mean_absolute_error: 86296.078125, mean_q: 103856.468750\n",
      "  129512/5000000: episode: 2205, duration: 1.221s, episode steps: 26, steps per second: 21, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.615 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 356098336.000000, mean_absolute_error: 80555.125000, mean_q: 97303.953125\n",
      "  129553/5000000: episode: 2206, duration: 2.008s, episode steps: 41, steps per second: 20, episode reward: -1.000, mean reward: -0.024 [-1.000, 0.000], mean action: 2.488 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 1658797440.000000, mean_absolute_error: 74536.593750, mean_q: 89984.164062\n",
      "  129586/5000000: episode: 2207, duration: 1.561s, episode steps: 33, steps per second: 21, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 1.970 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 2862338816.000000, mean_absolute_error: 83667.476562, mean_q: 101070.812500\n",
      "  129763/5000000: episode: 2208, duration: 8.473s, episode steps: 177, steps per second: 21, episode reward: -1.000, mean reward: -0.006 [-1.000, 0.000], mean action: 2.571 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 2457687040.000000, mean_absolute_error: 76736.351562, mean_q: 92752.929688\n",
      "  129817/5000000: episode: 2209, duration: 2.571s, episode steps: 54, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.519 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 3443537664.000000, mean_absolute_error: 75671.148438, mean_q: 91416.546875\n",
      "  129933/5000000: episode: 2210, duration: 5.480s, episode steps: 116, steps per second: 21, episode reward: -1.000, mean reward: -0.009 [-1.000, 0.000], mean action: 2.353 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 3087053568.000000, mean_absolute_error: 80240.664062, mean_q: 97063.968750\n",
      "  129966/5000000: episode: 2211, duration: 1.617s, episode steps: 33, steps per second: 20, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 2.606 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 1283510272.000000, mean_absolute_error: 78715.609375, mean_q: 94561.976562\n",
      "  129992/5000000: episode: 2212, duration: 1.357s, episode steps: 26, steps per second: 19, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 1.885 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 407955328.000000, mean_absolute_error: 73470.820312, mean_q: 88827.585938\n",
      "  130020/5000000: episode: 2213, duration: 1.372s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 3.036 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 733268352.000000, mean_absolute_error: 74011.117188, mean_q: 89512.304688\n",
      "  130085/5000000: episode: 2214, duration: 3.207s, episode steps: 65, steps per second: 20, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 2.600 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 934601216.000000, mean_absolute_error: 79379.734375, mean_q: 95947.265625\n",
      "  130111/5000000: episode: 2215, duration: 1.274s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.731 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 641024128.000000, mean_absolute_error: 71592.070312, mean_q: 86344.203125\n",
      "  130198/5000000: episode: 2216, duration: 4.029s, episode steps: 87, steps per second: 22, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.368 [0.000, 5.000], mean observation: 0.068 [0.000, 24.000], loss: 1436972288.000000, mean_absolute_error: 78936.796875, mean_q: 95374.476562\n",
      "  130285/5000000: episode: 2217, duration: 4.189s, episode steps: 87, steps per second: 21, episode reward: -1.000, mean reward: -0.011 [-1.000, 0.000], mean action: 2.678 [0.000, 5.000], mean observation: 0.073 [0.000, 24.000], loss: 1688863616.000000, mean_absolute_error: 79715.750000, mean_q: 96273.398438\n",
      "  130315/5000000: episode: 2218, duration: 1.592s, episode steps: 30, steps per second: 19, episode reward: -1.000, mean reward: -0.033 [-1.000, 0.000], mean action: 2.633 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 885782336.000000, mean_absolute_error: 82051.531250, mean_q: 99385.382812\n",
      "  130370/5000000: episode: 2219, duration: 2.702s, episode steps: 55, steps per second: 20, episode reward: -1.000, mean reward: -0.018 [-1.000, 0.000], mean action: 2.818 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 1158659328.000000, mean_absolute_error: 82095.976562, mean_q: 98653.164062\n",
      "  130403/5000000: episode: 2220, duration: 1.605s, episode steps: 33, steps per second: 21, episode reward: -1.000, mean reward: -0.030 [-1.000, 0.000], mean action: 2.515 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 5616014336.000000, mean_absolute_error: 83447.929688, mean_q: 99317.156250\n",
      "  130432/5000000: episode: 2221, duration: 1.405s, episode steps: 29, steps per second: 21, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.931 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 3222973696.000000, mean_absolute_error: 86675.351562, mean_q: 104803.398438\n",
      "  130458/5000000: episode: 2222, duration: 1.373s, episode steps: 26, steps per second: 19, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.423 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1384450816.000000, mean_absolute_error: 72814.195312, mean_q: 87917.828125\n",
      "  130497/5000000: episode: 2223, duration: 1.922s, episode steps: 39, steps per second: 20, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 2.538 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 1060835456.000000, mean_absolute_error: 81166.460938, mean_q: 98036.390625\n",
      "  130525/5000000: episode: 2224, duration: 1.380s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 3.071 [0.000, 5.000], mean observation: 0.064 [0.000, 24.000], loss: 2352357632.000000, mean_absolute_error: 97631.585938, mean_q: 115659.789062\n",
      "  130551/5000000: episode: 2225, duration: 1.279s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.154 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 5075855360.000000, mean_absolute_error: 86708.031250, mean_q: 104757.515625\n",
      "  130576/5000000: episode: 2226, duration: 1.306s, episode steps: 25, steps per second: 19, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 3.040 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 986387520.000000, mean_absolute_error: 79903.796875, mean_q: 96399.453125\n",
      "  130602/5000000: episode: 2227, duration: 1.322s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.654 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 506659040.000000, mean_absolute_error: 78027.781250, mean_q: 94416.203125\n",
      "  130686/5000000: episode: 2228, duration: 4.154s, episode steps: 84, steps per second: 20, episode reward: -1.000, mean reward: -0.012 [-1.000, 0.000], mean action: 2.810 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 6792239104.000000, mean_absolute_error: 83971.687500, mean_q: 101343.093750\n",
      "  130753/5000000: episode: 2229, duration: 3.351s, episode steps: 67, steps per second: 20, episode reward: -1.000, mean reward: -0.015 [-1.000, 0.000], mean action: 2.746 [0.000, 5.000], mean observation: 0.073 [0.000, 24.000], loss: 3186273792.000000, mean_absolute_error: 86219.085938, mean_q: 104082.585938\n",
      "  130778/5000000: episode: 2230, duration: 1.285s, episode steps: 25, steps per second: 19, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.640 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 2037331584.000000, mean_absolute_error: 84744.171875, mean_q: 102606.320312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  130803/5000000: episode: 2231, duration: 1.326s, episode steps: 25, steps per second: 19, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.400 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 1035378624.000000, mean_absolute_error: 83653.914062, mean_q: 101197.476562\n",
      "  130828/5000000: episode: 2232, duration: 1.230s, episode steps: 25, steps per second: 20, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.680 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 672488960.000000, mean_absolute_error: 88950.523438, mean_q: 107831.257812\n",
      "  130853/5000000: episode: 2233, duration: 1.316s, episode steps: 25, steps per second: 19, episode reward: -1.000, mean reward: -0.040 [-1.000, 0.000], mean action: 2.400 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 3348037888.000000, mean_absolute_error: 81222.109375, mean_q: 98250.359375\n",
      "  130972/5000000: episode: 2234, duration: 5.737s, episode steps: 119, steps per second: 21, episode reward: -1.000, mean reward: -0.008 [-1.000, 0.000], mean action: 2.403 [0.000, 5.000], mean observation: 0.059 [0.000, 24.000], loss: 1740649728.000000, mean_absolute_error: 78623.976562, mean_q: 95088.679688\n",
      "  131035/5000000: episode: 2235, duration: 3.069s, episode steps: 63, steps per second: 21, episode reward: -1.000, mean reward: -0.016 [-1.000, 0.000], mean action: 2.508 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 3072595712.000000, mean_absolute_error: 84632.687500, mean_q: 102462.437500\n",
      "  131140/5000000: episode: 2236, duration: 5.295s, episode steps: 105, steps per second: 20, episode reward: -1.000, mean reward: -0.010 [-1.000, 0.000], mean action: 2.343 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 1620831360.000000, mean_absolute_error: 76897.765625, mean_q: 93046.664062\n",
      "  131296/5000000: episode: 2237, duration: 7.861s, episode steps: 156, steps per second: 20, episode reward: -1.000, mean reward: -0.006 [-1.000, 0.000], mean action: 2.750 [0.000, 5.000], mean observation: 0.063 [0.000, 24.000], loss: 2964182272.000000, mean_absolute_error: 79043.101562, mean_q: 95402.484375\n",
      "  131324/5000000: episode: 2238, duration: 1.397s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.536 [0.000, 5.000], mean observation: 0.065 [0.000, 24.000], loss: 2784953856.000000, mean_absolute_error: 79240.007812, mean_q: 95609.500000\n",
      "  131350/5000000: episode: 2239, duration: 1.298s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.192 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 5363374080.000000, mean_absolute_error: 80786.718750, mean_q: 97717.539062\n",
      "  131379/5000000: episode: 2240, duration: 1.408s, episode steps: 29, steps per second: 21, episode reward: -1.000, mean reward: -0.034 [-1.000, 0.000], mean action: 2.586 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 1228113792.000000, mean_absolute_error: 86523.796875, mean_q: 104990.132812\n",
      "  131432/5000000: episode: 2241, duration: 2.565s, episode steps: 53, steps per second: 21, episode reward: -1.000, mean reward: -0.019 [-1.000, 0.000], mean action: 2.755 [0.000, 5.000], mean observation: 0.067 [0.000, 24.000], loss: 2210280960.000000, mean_absolute_error: 79062.257812, mean_q: 95809.960938\n",
      "  131460/5000000: episode: 2242, duration: 1.377s, episode steps: 28, steps per second: 20, episode reward: -1.000, mean reward: -0.036 [-1.000, 0.000], mean action: 2.786 [0.000, 5.000], mean observation: 0.069 [0.000, 24.000], loss: 1675476096.000000, mean_absolute_error: 89546.734375, mean_q: 107883.179688\n",
      "  131486/5000000: episode: 2243, duration: 1.288s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.192 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 5586621952.000000, mean_absolute_error: 80973.578125, mean_q: 97767.921875\n",
      "  131512/5000000: episode: 2244, duration: 1.327s, episode steps: 26, steps per second: 20, episode reward: -1.000, mean reward: -0.038 [-1.000, 0.000], mean action: 2.692 [0.000, 5.000], mean observation: 0.071 [0.000, 24.000], loss: 657522624.000000, mean_absolute_error: 73765.820312, mean_q: 89315.789062\n",
      "  131572/5000000: episode: 2245, duration: 2.720s, episode steps: 60, steps per second: 22, episode reward: -1.000, mean reward: -0.017 [-1.000, 0.000], mean action: 2.483 [0.000, 5.000], mean observation: 0.066 [0.000, 24.000], loss: 1653705344.000000, mean_absolute_error: 79396.945312, mean_q: 96129.960938\n",
      "  131610/5000000: episode: 2246, duration: 1.933s, episode steps: 38, steps per second: 20, episode reward: -1.000, mean reward: -0.026 [-1.000, 0.000], mean action: 2.237 [0.000, 5.000], mean observation: 0.072 [0.000, 24.000], loss: 1945267328.000000, mean_absolute_error: 77442.937500, mean_q: 93773.679688\n",
      "  131681/5000000: episode: 2247, duration: 3.390s, episode steps: 71, steps per second: 21, episode reward: -1.000, mean reward: -0.014 [-1.000, 0.000], mean action: 2.437 [0.000, 5.000], mean observation: 0.070 [0.000, 24.000], loss: 3716174336.000000, mean_absolute_error: 81286.125000, mean_q: 98344.648438\n"
     ]
    }
   ],
   "source": [
    "history = dqn.fit(env_wrapper, nb_steps=number_of_training_steps, visualize=False, verbose=2,\n",
    "        nb_max_episode_steps=env._max_steps,\n",
    "                  callbacks=callbacks)\n",
    "dqn.save_weights(model_path, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
