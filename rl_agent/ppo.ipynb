{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/hpchome/anton95/.conda/envs/pommerman/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/gpfs/hpchome/anton95/.conda/envs/pommerman/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from pommerman.envs.v0 import Pomme\n",
    "from pommerman.agents import SimpleAgent, BaseAgent\n",
    "from pommerman.configs import ffa_v0_env\n",
    "from pommerman.constants import BOARD_SIZE, GameType\n",
    "from tensorforce.agents import PPOAgent\n",
    "from tensorforce.execution import Runner\n",
    "from tensorforce.contrib.openai_gym import OpenAIGym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 30000\n",
    "batching_capacity = 1000\n",
    "save_seconds = 300\n",
    "main_dir = './ppo/'\n",
    "log_path = main_dir + 'logs/'\n",
    "model_path = main_dir + 'model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(main_dir):\n",
    "    os.mkdir(main_dir)\n",
    "if os.path.isdir(log_path):\n",
    "    shutil.rmtree(log_path, ignore_errors=True)\n",
    "os.mkdir(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into ./ppo/model/model.ckpt.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the environment\n",
    "config = ffa_v0_env()\n",
    "env = Pomme(**config[\"env_kwargs\"])\n",
    "env.seed(0)\n",
    "\n",
    "# Create a Proximal Policy Optimization agent\n",
    "network = dict(type='pomm_network.PommNetwork')\n",
    "states = {\n",
    "    \"board\": dict(shape=(BOARD_SIZE, BOARD_SIZE, 3, ), type='float'),\n",
    "    \"state\": dict(shape=(3,), type='float')\n",
    "}\n",
    "saver = {\n",
    "    \"directory\": model_path,\n",
    "    \"seconds\": save_seconds,\n",
    "    \"load\": os.path.isdir(model_path)\n",
    "}\n",
    "agent = PPOAgent(\n",
    "    states=states,\n",
    "    actions=dict(type='int', num_actions=env.action_space.n),\n",
    "    network=network,\n",
    "    batching_capacity=batching_capacity,\n",
    "    step_optimizer=dict(\n",
    "        type='adam',\n",
    "        learning_rate=1e-4\n",
    "    ),\n",
    "    saver=saver\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorforceAgent(BaseAgent):\n",
    "    def act(self, obs, action_space):\n",
    "        pass\n",
    "# Add 3 random agents\n",
    "agents = []\n",
    "for agent_id in range(3):\n",
    "    agents.append(SimpleAgent(config[\"agent\"](agent_id, config[\"game_type\"])))\n",
    "\n",
    "# Add TensorforceAgent\n",
    "agent_id += 1\n",
    "agents.append(TensorforceAgent(config[\"agent\"](agent_id, config[\"game_type\"])))\n",
    "env.set_agents(agents)\n",
    "env.set_training_agent(agents[-1].agent_id)\n",
    "env.set_init_game_state(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedEnv(OpenAIGym):\n",
    "    def __init__(self, gym, visualize=False):\n",
    "        self.gym = gym\n",
    "        self.visualize = visualize\n",
    "\n",
    "    def execute(self, actions):\n",
    "        if self.visualize:\n",
    "            self.gym.render()\n",
    "\n",
    "        obs = self.gym.get_observations()\n",
    "        all_actions = self.gym.act(obs)\n",
    "        all_actions.insert(self.gym.training_agent, actions)\n",
    "        state, reward, terminal, _ = self.gym.step(all_actions)\n",
    "        agent_state = WrappedEnv.featurize(state[self.gym.training_agent])\n",
    "        agent_reward = reward[self.gym.training_agent]\n",
    "        # If nobody die, use some \"smart\" reward\n",
    "        if agent_reward == 0:\n",
    "            agent_reward = self.gym.train_reward\n",
    "        return agent_state, terminal, agent_reward\n",
    "\n",
    "    def reset(self):\n",
    "        obs = self.gym.reset()\n",
    "        agent_obs = WrappedEnv.featurize(obs[3])\n",
    "        return agent_obs\n",
    "\n",
    "    @staticmethod\n",
    "    def featurize(obs):\n",
    "        def get_matrix(dict, key):\n",
    "            res = dict[key]\n",
    "            return res.reshape(res.shape[0], res.shape[1], 1).astype(np.float32)\n",
    "\n",
    "        board = get_matrix(obs, 'board')\n",
    "        teammate_position = None\n",
    "        teammate = obs[\"teammate\"]\n",
    "        if teammate is not None:\n",
    "            teammate = teammate.value\n",
    "            if teammate > 10 and teammate < 15:\n",
    "                teammate_position = np.argwhere(board == teammate)[0]\n",
    "        else:\n",
    "            teammate = None\n",
    "        # My self - 11\n",
    "        # Team mate - 12\n",
    "        # Enemy - 13\n",
    "\n",
    "        # Everyone enemy\n",
    "        board[(board > 10) & (board < 15)] = 13\n",
    "        # I'm not enemy\n",
    "        my_position = obs['position']\n",
    "        board[my_position[0], my_position[1], 0] = 11\n",
    "        # Set teammate\n",
    "        if teammate_position is not None:\n",
    "            board[teammate_position[0], teammate_position[1], teammate_position[2]] = 12\n",
    "\n",
    "        bomb_blast_strength = get_matrix(obs, 'bomb_blast_strength')\n",
    "        bomb_life = get_matrix(obs, 'bomb_life')\n",
    "        conv_inp = np.concatenate([board, bomb_blast_strength, bomb_life], axis=2)\n",
    "        state = np.array([obs[\"ammo\"], obs[\"blast_strength\"], obs[\"can_kick\"]]).astype(np.float32)\n",
    "        return dict(board=conv_inp, state=state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode_finished(r):\n",
    "    if r.episode % 10 == 0:\n",
    "        print(\"Finished episode {ep} after {ts} timesteps\".format(ep=r.episode + 1, ts = r.timestep + 1))\n",
    "        print(\"Episode reward: {}\".format(r.episode_rewards[-1]))\n",
    "        print(\"Average of last 10 rewards: {}\".format(np.mean(r.episode_rewards[10:])))\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 11 after 509 timesteps\n",
      "Episode reward: -1.041\n",
      "Average of last 10 rewards: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/hpchome/anton95/.conda/envs/pommerman/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/gpfs/hpchome/anton95/.conda/envs/pommerman/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 21 after 1215 timesteps\n",
      "Episode reward: -1.033\n",
      "Average of last 10 rewards: -1.1287\n",
      "Finished episode 31 after 1755 timesteps\n",
      "Episode reward: -1.08\n",
      "Average of last 10 rewards: -1.0877500000000002\n",
      "Finished episode 41 after 2342 timesteps\n",
      "Episode reward: -1.051\n",
      "Average of last 10 rewards: -1.0775000000000001\n",
      "Finished episode 51 after 2770 timesteps\n",
      "Episode reward: -1.112\n",
      "Average of last 10 rewards: -1.0662749999999999\n",
      "Finished episode 61 after 3589 timesteps\n",
      "Episode reward: -1.065\n",
      "Average of last 10 rewards: -1.0710799999999998\n",
      "Finished episode 71 after 4270 timesteps\n",
      "Episode reward: -1.095\n",
      "Average of last 10 rewards: -1.0727499999999999\n",
      "Finished episode 81 after 4695 timesteps\n",
      "Episode reward: -1.072\n",
      "Average of last 10 rewards: -1.0671714285714284\n",
      "Finished episode 91 after 5136 timesteps\n",
      "Episode reward: -1.115\n",
      "Average of last 10 rewards: -1.0688625\n",
      "Finished episode 101 after 5899 timesteps\n",
      "Episode reward: -1.058\n",
      "Average of last 10 rewards: -1.0764333333333331\n",
      "Finished episode 111 after 6350 timesteps\n",
      "Episode reward: -1.3220000000000003\n",
      "Average of last 10 rewards: -1.0772500000000003\n",
      "Finished episode 121 after 7137 timesteps\n",
      "Episode reward: -1.127\n",
      "Average of last 10 rewards: -1.076509090909091\n",
      "Finished episode 131 after 7646 timesteps\n",
      "Episode reward: -1.062\n",
      "Average of last 10 rewards: -1.0748416666666667\n",
      "Finished episode 141 after 8733 timesteps\n",
      "Episode reward: -1.099\n",
      "Average of last 10 rewards: -1.0797076923076925\n",
      "Finished episode 151 after 9737 timesteps\n",
      "Episode reward: -1.3620000000000003\n",
      "Average of last 10 rewards: -1.0865714285714285\n",
      "Finished episode 161 after 10308 timesteps\n",
      "Episode reward: -1.045\n",
      "Average of last 10 rewards: -1.083626666666667\n",
      "Finished episode 171 after 10778 timesteps\n",
      "Episode reward: -1.029\n",
      "Average of last 10 rewards: -1.08203125\n",
      "Finished episode 181 after 11540 timesteps\n",
      "Episode reward: -1.063\n",
      "Average of last 10 rewards: -1.083323529411765\n",
      "INFO:tensorflow:Saving checkpoints for 12521 into ./ppo/model/model.ckpt.\n",
      "Finished episode 191 after 12681 timesteps\n",
      "Episode reward: -1.0250000000000006\n",
      "Average of last 10 rewards: -1.0817944444444443\n",
      "Finished episode 201 after 13328 timesteps\n",
      "Episode reward: -1.118\n",
      "Average of last 10 rewards: -1.0843421052631579\n",
      "Finished episode 211 after 13862 timesteps\n",
      "Episode reward: -0.358\n",
      "Average of last 10 rewards: -1.081585\n",
      "Finished episode 221 after 14371 timesteps\n",
      "Episode reward: -1.049\n",
      "Average of last 10 rewards: -1.0808857142857144\n",
      "Finished episode 231 after 14824 timesteps\n",
      "Episode reward: -1.197\n",
      "Average of last 10 rewards: -1.0794772727272728\n",
      "Finished episode 241 after 15941 timesteps\n",
      "Episode reward: -1.1380000000000001\n",
      "Average of last 10 rewards: -1.0829\n",
      "Finished episode 251 after 16520 timesteps\n",
      "Episode reward: -1.1500000000000001\n",
      "Average of last 10 rewards: -1.0843416666666665\n",
      "Finished episode 261 after 17186 timesteps\n",
      "Episode reward: -1.023\n",
      "Average of last 10 rewards: -1.084252\n",
      "Finished episode 271 after 17904 timesteps\n",
      "Episode reward: -1.093\n",
      "Average of last 10 rewards: -1.0801961538461538\n",
      "Finished episode 281 after 18876 timesteps\n",
      "Episode reward: -1.025\n",
      "Average of last 10 rewards: -1.0831814814814815\n",
      "Finished episode 291 after 19552 timesteps\n",
      "Episode reward: -1.101\n",
      "Average of last 10 rewards: -1.0845035714285716\n",
      "Finished episode 301 after 20057 timesteps\n",
      "Episode reward: -1.157\n",
      "Average of last 10 rewards: -1.0834241379310345\n",
      "Finished episode 311 after 20687 timesteps\n",
      "Episode reward: -1.113\n",
      "Average of last 10 rewards: -1.0847433333333334\n",
      "Finished episode 321 after 21217 timesteps\n",
      "Episode reward: -1.097\n",
      "Average of last 10 rewards: -1.0853645161290322\n",
      "Finished episode 331 after 21609 timesteps\n",
      "Episode reward: -1.129\n",
      "Average of last 10 rewards: -1.0839625\n",
      "Finished episode 341 after 22606 timesteps\n",
      "Episode reward: -1.097\n",
      "Average of last 10 rewards: -1.085621212121212\n",
      "Finished episode 351 after 23108 timesteps\n",
      "Episode reward: -1.135\n",
      "Average of last 10 rewards: -1.0842735294117647\n",
      "Finished episode 361 after 24024 timesteps\n",
      "Episode reward: -0.8380000000000001\n",
      "Average of last 10 rewards: -1.0840771428571427\n",
      "Finished episode 371 after 24497 timesteps\n",
      "Episode reward: -1.062\n",
      "Average of last 10 rewards: -1.081711111111111\n",
      "INFO:tensorflow:Saving checkpoints for 25236 into ./ppo/model/model.ckpt.\n",
      "Finished episode 381 after 25237 timesteps\n",
      "Episode reward: -0.643\n",
      "Average of last 10 rewards: -1.0824297297297298\n",
      "Finished episode 391 after 25746 timesteps\n",
      "Episode reward: -1.033\n",
      "Average of last 10 rewards: -1.0815552631578946\n",
      "Finished episode 401 after 26719 timesteps\n",
      "Episode reward: -0.8630000000000002\n",
      "Average of last 10 rewards: -1.078202564102564\n",
      "Finished episode 411 after 27387 timesteps\n",
      "Episode reward: -0.387\n",
      "Average of last 10 rewards: -1.0750325\n",
      "Finished episode 421 after 27973 timesteps\n",
      "Episode reward: -1.065\n",
      "Average of last 10 rewards: -1.075648780487805\n",
      "Finished episode 431 after 28467 timesteps\n",
      "Episode reward: -1.2080000000000002\n",
      "Average of last 10 rewards: -1.07505\n",
      "Finished episode 441 after 29116 timesteps\n",
      "Episode reward: -1.116\n",
      "Average of last 10 rewards: -1.0762720930232559\n",
      "Finished episode 451 after 29690 timesteps\n",
      "Episode reward: -1.066\n",
      "Average of last 10 rewards: -1.076209090909091\n",
      "Finished episode 461 after 30234 timesteps\n",
      "Episode reward: -1.015\n",
      "Average of last 10 rewards: -1.0750822222222223\n",
      "Finished episode 471 after 30993 timesteps\n",
      "Episode reward: -1.108\n",
      "Average of last 10 rewards: -1.0756630434782608\n",
      "Finished episode 481 after 31404 timesteps\n",
      "Episode reward: -1.083\n",
      "Average of last 10 rewards: -1.0747617021276596\n",
      "Finished episode 491 after 32192 timesteps\n",
      "Episode reward: -1.062\n",
      "Average of last 10 rewards: -1.0749354166666665\n",
      "Finished episode 501 after 33103 timesteps\n",
      "Episode reward: -1.3360000000000003\n",
      "Average of last 10 rewards: -1.0767510204081632\n",
      "Finished episode 511 after 33764 timesteps\n",
      "Episode reward: -1.217\n",
      "Average of last 10 rewards: -1.077694\n",
      "Finished episode 521 after 34252 timesteps\n",
      "Episode reward: -1.125\n",
      "Average of last 10 rewards: -1.078072549019608\n",
      "Finished episode 531 after 34646 timesteps\n",
      "Episode reward: -1.068\n",
      "Average of last 10 rewards: -1.0778096153846155\n",
      "Finished episode 541 after 35205 timesteps\n",
      "Episode reward: -1.039\n",
      "Average of last 10 rewards: -1.0772528301886792\n",
      "Finished episode 551 after 35605 timesteps\n",
      "Episode reward: -0.6659999999999999\n",
      "Average of last 10 rewards: -1.076275925925926\n",
      "Finished episode 561 after 36098 timesteps\n",
      "Episode reward: -0.9120000000000001\n",
      "Average of last 10 rewards: -1.0750472727272729\n",
      "Finished episode 571 after 36624 timesteps\n",
      "Episode reward: -1.043\n",
      "Average of last 10 rewards: -1.0747160714285717\n",
      "Finished episode 581 after 37174 timesteps\n",
      "Episode reward: -1.114\n",
      "Average of last 10 rewards: -1.075124561403509\n",
      "Finished episode 591 after 37695 timesteps\n",
      "Episode reward: -1.1820000000000002\n",
      "Average of last 10 rewards: -1.0741879310344828\n",
      "INFO:tensorflow:Saving checkpoints for 38022 into ./ppo/model/model.ckpt.\n",
      "Finished episode 601 after 38715 timesteps\n",
      "Episode reward: 0.5310000000000041\n",
      "Average of last 10 rewards: -1.071591525423729\n",
      "Finished episode 611 after 39447 timesteps\n",
      "Episode reward: -1.3520000000000003\n",
      "Average of last 10 rewards: -1.071675\n",
      "Finished episode 621 after 39845 timesteps\n",
      "Episode reward: -1.044\n",
      "Average of last 10 rewards: -1.0718688524590163\n",
      "Finished episode 631 after 40177 timesteps\n",
      "Episode reward: -1.053\n",
      "Average of last 10 rewards: -1.07175\n",
      "Finished episode 641 after 40661 timesteps\n",
      "Episode reward: -1.055\n",
      "Average of last 10 rewards: -1.0721492063492064\n",
      "Finished episode 651 after 41011 timesteps\n",
      "Episode reward: -1.066\n",
      "Average of last 10 rewards: -1.0721453125\n",
      "Finished episode 661 after 41623 timesteps\n",
      "Episode reward: -1.219\n",
      "Average of last 10 rewards: -1.0729907692307692\n",
      "Finished episode 671 after 42128 timesteps\n",
      "Episode reward: -1.035\n",
      "Average of last 10 rewards: -1.0733363636363638\n",
      "Finished episode 681 after 43016 timesteps\n",
      "Episode reward: -1.061\n",
      "Average of last 10 rewards: -1.074471641791045\n",
      "Finished episode 691 after 43374 timesteps\n",
      "Episode reward: -1.038\n",
      "Average of last 10 rewards: -1.0744279411764708\n",
      "Finished episode 701 after 44151 timesteps\n",
      "Episode reward: -1.1880000000000002\n",
      "Average of last 10 rewards: -1.0746159420289856\n",
      "Finished episode 711 after 44746 timesteps\n",
      "Episode reward: -1.084\n",
      "Average of last 10 rewards: -1.0742057142857142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 721 after 45280 timesteps\n",
      "Episode reward: -1.057\n",
      "Average of last 10 rewards: -1.0741732394366197\n",
      "Finished episode 731 after 45738 timesteps\n",
      "Episode reward: -1.137\n",
      "Average of last 10 rewards: -1.0744194444444444\n",
      "Finished episode 741 after 46238 timesteps\n",
      "Episode reward: -1.061\n",
      "Average of last 10 rewards: -1.0742205479452056\n",
      "Finished episode 751 after 47109 timesteps\n",
      "Episode reward: -1.1980000000000002\n",
      "Average of last 10 rewards: -1.074372972972973\n",
      "Finished episode 761 after 48084 timesteps\n",
      "Episode reward: -0.7010000000000001\n",
      "Average of last 10 rewards: -1.0748146666666667\n",
      "Finished episode 771 after 48440 timesteps\n",
      "Episode reward: -1.077\n",
      "Average of last 10 rewards: -1.074775\n",
      "Finished episode 781 after 48745 timesteps\n",
      "Episode reward: -1.111\n",
      "Average of last 10 rewards: -1.0746376623376623\n",
      "Finished episode 791 after 49140 timesteps\n",
      "Episode reward: -0.655\n",
      "Average of last 10 rewards: -1.073575641025641\n",
      "Finished episode 801 after 49494 timesteps\n",
      "Episode reward: -1.053\n",
      "Average of last 10 rewards: -1.073621518987342\n",
      "Finished episode 811 after 50176 timesteps\n",
      "Episode reward: -1.052\n",
      "Average of last 10 rewards: -1.074255\n",
      "INFO:tensorflow:Saving checkpoints for 50520 into ./ppo/model/model.ckpt.\n",
      "Finished episode 821 after 51235 timesteps\n",
      "Episode reward: -1.112\n",
      "Average of last 10 rewards: -1.0729506172839507\n",
      "Finished episode 831 after 51986 timesteps\n",
      "Episode reward: -1.163\n",
      "Average of last 10 rewards: -1.0726475609756096\n",
      "Finished episode 841 after 52787 timesteps\n",
      "Episode reward: -1.059\n",
      "Average of last 10 rewards: -1.0731337349397592\n",
      "Finished episode 851 after 53440 timesteps\n",
      "Episode reward: -1.062\n",
      "Average of last 10 rewards: -1.0716726190476191\n",
      "Finished episode 861 after 53920 timesteps\n",
      "Episode reward: -1.042\n",
      "Average of last 10 rewards: -1.0714129411764706\n",
      "Finished episode 871 after 54670 timesteps\n",
      "Episode reward: -0.634\n",
      "Average of last 10 rewards: -1.0710220930232557\n",
      "Finished episode 881 after 55179 timesteps\n",
      "Episode reward: -1.028\n",
      "Average of last 10 rewards: -1.0712241379310345\n",
      "Finished episode 891 after 55644 timesteps\n",
      "Episode reward: -1.181\n",
      "Average of last 10 rewards: -1.0714306818181818\n",
      "Finished episode 901 after 56219 timesteps\n",
      "Episode reward: -1.121\n",
      "Average of last 10 rewards: -1.0719808988764046\n",
      "Finished episode 911 after 56731 timesteps\n",
      "Episode reward: -1.052\n",
      "Average of last 10 rewards: -1.0722144444444446\n",
      "Finished episode 921 after 57383 timesteps\n",
      "Episode reward: -1.223\n",
      "Average of last 10 rewards: -1.07229010989011\n",
      "Finished episode 931 after 58087 timesteps\n",
      "Episode reward: -0.7010000000000001\n",
      "Average of last 10 rewards: -1.072441304347826\n",
      "Finished episode 941 after 58820 timesteps\n",
      "Episode reward: -1.044\n",
      "Average of last 10 rewards: -1.0730881720430108\n",
      "Finished episode 951 after 59454 timesteps\n",
      "Episode reward: -1.125\n",
      "Average of last 10 rewards: -1.0732436170212767\n",
      "Finished episode 961 after 60226 timesteps\n",
      "Episode reward: -1.072\n",
      "Average of last 10 rewards: -1.0732010526315792\n",
      "Finished episode 971 after 60973 timesteps\n",
      "Episode reward: -1.1280000000000001\n",
      "Average of last 10 rewards: -1.073540625\n",
      "Finished episode 981 after 61485 timesteps\n",
      "Episode reward: -1.117\n",
      "Average of last 10 rewards: -1.072962886597938\n",
      "Finished episode 991 after 62282 timesteps\n",
      "Episode reward: -0.802\n",
      "Average of last 10 rewards: -1.0726755102040817\n",
      "Finished episode 1001 after 62757 timesteps\n",
      "Episode reward: -1.108\n",
      "Average of last 10 rewards: -1.0729424242424241\n",
      "INFO:tensorflow:Saving checkpoints for 63370 into ./ppo/model/model.ckpt.\n",
      "Finished episode 1011 after 63518 timesteps\n",
      "Episode reward: -1.034\n",
      "Average of last 10 rewards: -1.07294\n",
      "Finished episode 1021 after 63984 timesteps\n",
      "Episode reward: -1.086\n",
      "Average of last 10 rewards: -1.073129702970297\n",
      "Finished episode 1031 after 64535 timesteps\n",
      "Episode reward: -1.047\n",
      "Average of last 10 rewards: -1.0733127450980393\n",
      "Finished episode 1041 after 65424 timesteps\n",
      "Episode reward: -0.8450000000000001\n",
      "Average of last 10 rewards: -1.0735601941747572\n",
      "Finished episode 1051 after 65946 timesteps\n",
      "Episode reward: -0.643\n",
      "Average of last 10 rewards: -1.0733067307692308\n",
      "Finished episode 1061 after 66506 timesteps\n",
      "Episode reward: -1.044\n",
      "Average of last 10 rewards: -1.0733314285714286\n",
      "Finished episode 1071 after 67250 timesteps\n",
      "Episode reward: -1.031\n",
      "Average of last 10 rewards: -1.0734669811320754\n",
      "Finished episode 1081 after 67660 timesteps\n",
      "Episode reward: -1.031\n",
      "Average of last 10 rewards: -1.073455140186916\n",
      "Finished episode 1091 after 68458 timesteps\n",
      "Episode reward: -1.3130000000000002\n",
      "Average of last 10 rewards: -1.0737166666666667\n",
      "Finished episode 1101 after 69188 timesteps\n",
      "Episode reward: -1.213\n",
      "Average of last 10 rewards: -1.0735119266055044\n",
      "Finished episode 1111 after 69860 timesteps\n",
      "Episode reward: -1.262\n",
      "Average of last 10 rewards: -1.07365\n",
      "Finished episode 1121 after 70610 timesteps\n",
      "Episode reward: -1.2300000000000002\n",
      "Average of last 10 rewards: -1.0743090090090093\n",
      "Finished episode 1131 after 71280 timesteps\n",
      "Episode reward: -1.031\n",
      "Average of last 10 rewards: -1.0742919642857145\n",
      "Finished episode 1141 after 71868 timesteps\n",
      "Episode reward: -0.6970000000000001\n",
      "Average of last 10 rewards: -1.0734716814159293\n",
      "Finished episode 1151 after 72682 timesteps\n",
      "Episode reward: -1.181\n",
      "Average of last 10 rewards: -1.0743991228070175\n",
      "Finished episode 1161 after 72961 timesteps\n",
      "Episode reward: -1.062\n",
      "Average of last 10 rewards: -1.074197391304348\n",
      "Finished episode 1171 after 73591 timesteps\n",
      "Episode reward: -1.1460000000000001\n",
      "Average of last 10 rewards: -1.0742275862068964\n",
      "Finished episode 1181 after 74709 timesteps\n",
      "Episode reward: -1.3200000000000003\n",
      "Average of last 10 rewards: -1.0751068376068376\n",
      "Finished episode 1191 after 75673 timesteps\n",
      "Episode reward: -1.3270000000000002\n",
      "Average of last 10 rewards: -1.0754940677966103\n",
      "INFO:tensorflow:Saving checkpoints for 75977 into ./ppo/model/model.ckpt.\n",
      "Finished episode 1201 after 76072 timesteps\n",
      "Episode reward: -0.7250000000000001\n",
      "Average of last 10 rewards: -1.0751932773109243\n",
      "Finished episode 1211 after 76686 timesteps\n",
      "Episode reward: -1.1340000000000001\n",
      "Average of last 10 rewards: -1.0745058333333335\n",
      "Finished episode 1221 after 77170 timesteps\n",
      "Episode reward: -1.167\n",
      "Average of last 10 rewards: -1.0742694214876034\n",
      "Finished episode 1231 after 77958 timesteps\n",
      "Episode reward: -1.073\n",
      "Average of last 10 rewards: -1.0741926229508199\n",
      "Finished episode 1241 after 78404 timesteps\n",
      "Episode reward: -1.063\n",
      "Average of last 10 rewards: -1.0741422764227644\n",
      "Finished episode 1251 after 79095 timesteps\n",
      "Episode reward: -1.1280000000000001\n",
      "Average of last 10 rewards: -1.0744379032258065\n",
      "Finished episode 1261 after 79750 timesteps\n",
      "Episode reward: -1.098\n",
      "Average of last 10 rewards: -1.0745008000000003\n",
      "Finished episode 1271 after 80409 timesteps\n",
      "Episode reward: -1.055\n",
      "Average of last 10 rewards: -1.0747333333333335\n",
      "Finished episode 1281 after 80948 timesteps\n",
      "Episode reward: -1.079\n",
      "Average of last 10 rewards: -1.0747952755905514\n",
      "Finished episode 1291 after 81508 timesteps\n",
      "Episode reward: -1.049\n",
      "Average of last 10 rewards: -1.0730945312500002\n",
      "Finished episode 1301 after 82259 timesteps\n",
      "Episode reward: -1.039\n",
      "Average of last 10 rewards: -1.073782945736434\n",
      "Finished episode 1311 after 82747 timesteps\n",
      "Episode reward: -1.102\n",
      "Average of last 10 rewards: -1.0737615384615384\n",
      "Finished episode 1321 after 83310 timesteps\n",
      "Episode reward: -1.042\n",
      "Average of last 10 rewards: -1.0736480916030535\n",
      "Finished episode 1331 after 83756 timesteps\n",
      "Episode reward: -1.073\n",
      "Average of last 10 rewards: -1.0732568181818183\n",
      "Finished episode 1341 after 84501 timesteps\n",
      "Episode reward: -1.1580000000000001\n",
      "Average of last 10 rewards: -1.0733443609022557\n",
      "Finished episode 1351 after 85310 timesteps\n",
      "Episode reward: -1.046\n",
      "Average of last 10 rewards: -1.073858208955224\n",
      "Finished episode 1361 after 85973 timesteps\n",
      "Episode reward: -1.12\n",
      "Average of last 10 rewards: -1.0736237037037037\n",
      "Finished episode 1371 after 86660 timesteps\n",
      "Episode reward: -0.8620000000000001\n",
      "Average of last 10 rewards: -1.0737536764705884\n",
      "Finished episode 1381 after 87380 timesteps\n",
      "Episode reward: -1.024\n",
      "Average of last 10 rewards: -1.0742102189781022\n",
      "Finished episode 1391 after 88172 timesteps\n",
      "Episode reward: -1.1680000000000001\n",
      "Average of last 10 rewards: -1.0744500000000001\n",
      "Finished episode 1401 after 88582 timesteps\n",
      "Episode reward: -1.053\n",
      "Average of last 10 rewards: -1.0741402877697843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 88617 into ./ppo/model/model.ckpt.\n",
      "Finished episode 1411 after 89472 timesteps\n",
      "Episode reward: -1.065\n",
      "Average of last 10 rewards: -1.0741828571428573\n",
      "Finished episode 1421 after 90376 timesteps\n",
      "Episode reward: -1.342\n",
      "Average of last 10 rewards: -1.0744836879432624\n",
      "Finished episode 1431 after 90829 timesteps\n",
      "Episode reward: -1.112\n",
      "Average of last 10 rewards: -1.0745577464788734\n",
      "Finished episode 1441 after 91526 timesteps\n",
      "Episode reward: -1.07\n",
      "Average of last 10 rewards: -1.074323776223776\n",
      "Finished episode 1451 after 92118 timesteps\n",
      "Episode reward: -1.047\n",
      "Average of last 10 rewards: -1.0745701388888889\n",
      "Finished episode 1461 after 92931 timesteps\n",
      "Episode reward: -1.2240000000000002\n",
      "Average of last 10 rewards: -1.075353103448276\n",
      "Finished episode 1471 after 93553 timesteps\n",
      "Episode reward: -0.7690000000000001\n",
      "Average of last 10 rewards: -1.0753027397260273\n",
      "Finished episode 1481 after 94210 timesteps\n",
      "Episode reward: -1.11\n",
      "Average of last 10 rewards: -1.0752646258503402\n",
      "Finished episode 1491 after 94685 timesteps\n",
      "Episode reward: -1.1480000000000001\n",
      "Average of last 10 rewards: -1.075418918918919\n",
      "Finished episode 1501 after 95263 timesteps\n",
      "Episode reward: -0.673\n",
      "Average of last 10 rewards: -1.0748302013422817\n",
      "Finished episode 1511 after 96010 timesteps\n",
      "Episode reward: -1.243\n",
      "Average of last 10 rewards: -1.0752213333333334\n",
      "Finished episode 1521 after 96515 timesteps\n",
      "Episode reward: -1.024\n",
      "Average of last 10 rewards: -1.0750735099337747\n",
      "Finished episode 1531 after 97083 timesteps\n",
      "Episode reward: -1.094\n",
      "Average of last 10 rewards: -1.0751164473684212\n",
      "Finished episode 1541 after 97668 timesteps\n",
      "Episode reward: -1.05\n",
      "Average of last 10 rewards: -1.0751784313725492\n",
      "Finished episode 1551 after 97980 timesteps\n",
      "Episode reward: -1.054\n",
      "Average of last 10 rewards: -1.074977922077922\n",
      "Finished episode 1561 after 98481 timesteps\n",
      "Episode reward: -1.133\n",
      "Average of last 10 rewards: -1.0751077419354838\n",
      "Finished episode 1571 after 99129 timesteps\n",
      "Episode reward: -1.1860000000000002\n",
      "Average of last 10 rewards: -1.075500641025641\n",
      "Finished episode 1581 after 99578 timesteps\n",
      "Episode reward: -0.675\n",
      "Average of last 10 rewards: -1.0752076433121018\n",
      "Finished episode 1591 after 100140 timesteps\n",
      "Episode reward: -1.091\n",
      "Average of last 10 rewards: -1.074946835443038\n",
      "Finished episode 1601 after 100920 timesteps\n",
      "Episode reward: -1.036\n",
      "Average of last 10 rewards: -1.0752220125786165\n",
      "INFO:tensorflow:Saving checkpoints for 101350 into ./ppo/model/model.ckpt.\n",
      "Finished episode 1611 after 101579 timesteps\n",
      "Episode reward: -1.065\n",
      "Average of last 10 rewards: -1.075565625\n",
      "Finished episode 1621 after 101923 timesteps\n",
      "Episode reward: -1.029\n",
      "Average of last 10 rewards: -1.07522049689441\n",
      "Finished episode 1631 after 102559 timesteps\n",
      "Episode reward: -1.032\n",
      "Average of last 10 rewards: -1.0750783950617284\n",
      "Finished episode 1641 after 103178 timesteps\n",
      "Episode reward: -1.062\n",
      "Average of last 10 rewards: -1.075258282208589\n",
      "Finished episode 1651 after 103842 timesteps\n",
      "Episode reward: -1.074\n",
      "Average of last 10 rewards: -1.0756298780487805\n",
      "Finished episode 1661 after 104878 timesteps\n",
      "Episode reward: -1.09\n",
      "Average of last 10 rewards: -1.0760515151515153\n",
      "Finished episode 1671 after 105403 timesteps\n",
      "Episode reward: -1.033\n",
      "Average of last 10 rewards: -1.0758006024096387\n",
      "Finished episode 1681 after 106003 timesteps\n",
      "Episode reward: -1.1560000000000001\n",
      "Average of last 10 rewards: -1.0757185628742516\n",
      "Finished episode 1691 after 106516 timesteps\n",
      "Episode reward: -1.034\n",
      "Average of last 10 rewards: -1.075732142857143\n",
      "Finished episode 1701 after 106995 timesteps\n",
      "Episode reward: -1.245\n",
      "Average of last 10 rewards: -1.07563550295858\n",
      "Finished episode 1711 after 107479 timesteps\n",
      "Episode reward: -1.034\n",
      "Average of last 10 rewards: -1.075525294117647\n",
      "Finished episode 1721 after 107965 timesteps\n",
      "Episode reward: -1.046\n",
      "Average of last 10 rewards: -1.0749239766081873\n",
      "Finished episode 1731 after 108683 timesteps\n",
      "Episode reward: -1.1820000000000002\n",
      "Average of last 10 rewards: -1.0749738372093023\n",
      "Finished episode 1741 after 109354 timesteps\n",
      "Episode reward: -1.073\n",
      "Average of last 10 rewards: -1.0751884393063584\n",
      "Finished episode 1751 after 109977 timesteps\n",
      "Episode reward: -1.183\n",
      "Average of last 10 rewards: -1.0748586206896553\n",
      "Finished episode 1761 after 110624 timesteps\n",
      "Episode reward: -1.1480000000000001\n",
      "Average of last 10 rewards: -1.0749920000000002\n",
      "Finished episode 1771 after 111260 timesteps\n",
      "Episode reward: -1.059\n",
      "Average of last 10 rewards: -1.075080681818182\n",
      "Finished episode 1781 after 111710 timesteps\n",
      "Episode reward: -1.056\n",
      "Average of last 10 rewards: -1.0751824858757062\n",
      "Finished episode 1791 after 112283 timesteps\n",
      "Episode reward: -1.025\n",
      "Average of last 10 rewards: -1.075308426966292\n",
      "Finished episode 1801 after 112945 timesteps\n",
      "Episode reward: -1.187\n",
      "Average of last 10 rewards: -1.0755949720670392\n",
      "Finished episode 1811 after 113643 timesteps\n",
      "Episode reward: -1.1280000000000001\n",
      "Average of last 10 rewards: -1.0754777777777778\n",
      "INFO:tensorflow:Saving checkpoints for 114038 into ./ppo/model/model.ckpt.\n",
      "Finished episode 1821 after 114046 timesteps\n",
      "Episode reward: -1.031\n",
      "Average of last 10 rewards: -1.075446961325967\n",
      "Finished episode 1831 after 114790 timesteps\n",
      "Episode reward: -1.1720000000000002\n",
      "Average of last 10 rewards: -1.075718131868132\n",
      "Finished episode 1841 after 115400 timesteps\n",
      "Episode reward: -1.051\n",
      "Average of last 10 rewards: -1.0752387978142077\n",
      "Finished episode 1851 after 116156 timesteps\n",
      "Episode reward: -1.071\n",
      "Average of last 10 rewards: -1.0753445652173914\n",
      "Finished episode 1861 after 116632 timesteps\n",
      "Episode reward: -0.752\n",
      "Average of last 10 rewards: -1.074818918918919\n",
      "Finished episode 1871 after 117060 timesteps\n",
      "Episode reward: -1.227\n",
      "Average of last 10 rewards: -1.0746478494623657\n",
      "Finished episode 1881 after 117542 timesteps\n",
      "Episode reward: -1.097\n",
      "Average of last 10 rewards: -1.0747283422459895\n",
      "Finished episode 1891 after 117960 timesteps\n",
      "Episode reward: -1.125\n",
      "Average of last 10 rewards: -1.0747276595744681\n",
      "Finished episode 1901 after 118429 timesteps\n",
      "Episode reward: -1.103\n",
      "Average of last 10 rewards: -1.0745962962962963\n",
      "Finished episode 1911 after 119177 timesteps\n",
      "Episode reward: -1.4490000000000003\n",
      "Average of last 10 rewards: -1.074753157894737\n",
      "Finished episode 1921 after 119904 timesteps\n",
      "Episode reward: -1.3250000000000002\n",
      "Average of last 10 rewards: -1.0741445026178011\n",
      "Finished episode 1931 after 120438 timesteps\n",
      "Episode reward: -1.073\n",
      "Average of last 10 rewards: -1.0742604166666667\n",
      "Finished episode 1941 after 120967 timesteps\n",
      "Episode reward: -1.092\n",
      "Average of last 10 rewards: -1.0741523316062176\n",
      "Finished episode 1951 after 121318 timesteps\n",
      "Episode reward: -1.037\n",
      "Average of last 10 rewards: -1.07415206185567\n",
      "Finished episode 1961 after 121933 timesteps\n",
      "Episode reward: -1.12\n",
      "Average of last 10 rewards: -1.0738758974358975\n",
      "Finished episode 1971 after 122379 timesteps\n",
      "Episode reward: -1.179\n",
      "Average of last 10 rewards: -1.0737025510204081\n",
      "Finished episode 1981 after 122940 timesteps\n",
      "Episode reward: -1.069\n",
      "Average of last 10 rewards: -1.0737426395939087\n",
      "Finished episode 1991 after 123609 timesteps\n",
      "Episode reward: -0.6539999999999999\n",
      "Average of last 10 rewards: -1.0735621212121214\n",
      "Finished episode 2001 after 124039 timesteps\n",
      "Episode reward: -1.093\n",
      "Average of last 10 rewards: -1.0731778894472361\n",
      "Finished episode 2011 after 124606 timesteps\n",
      "Episode reward: -1.099\n",
      "Average of last 10 rewards: -1.0733825000000001\n",
      "Finished episode 2021 after 125632 timesteps\n",
      "Episode reward: -0.8730000000000002\n",
      "Average of last 10 rewards: -1.0736164179104477\n",
      "Finished episode 2031 after 126273 timesteps\n",
      "Episode reward: -1.191\n",
      "Average of last 10 rewards: -1.0739123762376237\n",
      "INFO:tensorflow:Saving checkpoints for 126813 into ./ppo/model/model.ckpt.\n",
      "Finished episode 2041 after 126939 timesteps\n",
      "Episode reward: -1.2160000000000002\n",
      "Average of last 10 rewards: -1.0737172413793103\n",
      "Finished episode 2051 after 127478 timesteps\n",
      "Episode reward: -1.009\n",
      "Average of last 10 rewards: -1.0738230392156864\n",
      "Finished episode 2061 after 128112 timesteps\n",
      "Episode reward: -1.084\n",
      "Average of last 10 rewards: -1.0732770731707317\n",
      "Finished episode 2071 after 129000 timesteps\n",
      "Episode reward: -1.219\n",
      "Average of last 10 rewards: -1.0736660194174756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 2081 after 129569 timesteps\n",
      "Episode reward: -1.086\n",
      "Average of last 10 rewards: -1.0738859903381643\n",
      "Finished episode 2091 after 130346 timesteps\n",
      "Episode reward: -1.05\n",
      "Average of last 10 rewards: -1.0740298076923076\n",
      "Finished episode 2101 after 130883 timesteps\n",
      "Episode reward: -1.066\n",
      "Average of last 10 rewards: -1.073967942583732\n",
      "Finished episode 2111 after 131269 timesteps\n",
      "Episode reward: -0.673\n",
      "Average of last 10 rewards: -1.0737895238095239\n",
      "Finished episode 2121 after 131695 timesteps\n",
      "Episode reward: -0.671\n",
      "Average of last 10 rewards: -1.0734488151658768\n",
      "Finished episode 2131 after 132114 timesteps\n",
      "Episode reward: -1.037\n",
      "Average of last 10 rewards: -1.073233490566038\n",
      "Finished episode 2141 after 132512 timesteps\n",
      "Episode reward: -1.1560000000000001\n",
      "Average of last 10 rewards: -1.073038028169014\n",
      "Finished episode 2151 after 133011 timesteps\n",
      "Episode reward: -1.117\n",
      "Average of last 10 rewards: -1.0732018691588785\n",
      "Finished episode 2161 after 133691 timesteps\n",
      "Episode reward: -1.032\n",
      "Average of last 10 rewards: -1.0729683720930234\n",
      "Finished episode 2171 after 134164 timesteps\n",
      "Episode reward: -1.054\n",
      "Average of last 10 rewards: -1.0729041666666665\n",
      "Finished episode 2181 after 135257 timesteps\n",
      "Episode reward: -0.8500000000000001\n",
      "Average of last 10 rewards: -1.0719552995391706\n",
      "Finished episode 2191 after 136246 timesteps\n",
      "Episode reward: -0.3590000000000003\n",
      "Average of last 10 rewards: -1.0718949541284404\n",
      "Finished episode 2201 after 136813 timesteps\n",
      "Episode reward: -1.043\n",
      "Average of last 10 rewards: -1.0720283105022832\n",
      "Finished episode 2211 after 137259 timesteps\n",
      "Episode reward: -1.037\n",
      "Average of last 10 rewards: -1.0719986363636362\n",
      "Finished episode 2221 after 137788 timesteps\n",
      "Episode reward: -1.063\n",
      "Average of last 10 rewards: -1.0720778280542989\n",
      "Finished episode 2231 after 138529 timesteps\n",
      "Episode reward: -1.032\n",
      "Average of last 10 rewards: -1.0721117117117118\n",
      "Finished episode 2241 after 139144 timesteps\n",
      "Episode reward: -1.035\n",
      "Average of last 10 rewards: -1.0722807174887894\n",
      "INFO:tensorflow:Saving checkpoints for 139513 into ./ppo/model/model.ckpt.\n",
      "Finished episode 2251 after 139792 timesteps\n",
      "Episode reward: -1.127\n",
      "Average of last 10 rewards: -1.0722464285714288\n",
      "Finished episode 2261 after 140400 timesteps\n",
      "Episode reward: -1.037\n",
      "Average of last 10 rewards: -1.0722733333333334\n",
      "Finished episode 2271 after 141161 timesteps\n",
      "Episode reward: -1.083\n",
      "Average of last 10 rewards: -1.072541150442478\n",
      "Finished episode 2281 after 141574 timesteps\n",
      "Episode reward: -0.655\n",
      "Average of last 10 rewards: -1.0722220264317182\n",
      "Finished episode 2291 after 142085 timesteps\n",
      "Episode reward: -1.04\n",
      "Average of last 10 rewards: -1.0722548245614034\n",
      "Finished episode 2301 after 142650 timesteps\n",
      "Episode reward: -1.045\n",
      "Average of last 10 rewards: -1.0724030567685592\n",
      "Finished episode 2311 after 143439 timesteps\n",
      "Episode reward: -1.3240000000000003\n",
      "Average of last 10 rewards: -1.0724278260869566\n",
      "Finished episode 2321 after 143928 timesteps\n",
      "Episode reward: -1.038\n",
      "Average of last 10 rewards: -1.072338528138528\n",
      "Finished episode 2331 after 144692 timesteps\n",
      "Episode reward: -0.7910000000000001\n",
      "Average of last 10 rewards: -1.0722724137931035\n",
      "Finished episode 2341 after 145286 timesteps\n",
      "Episode reward: -1.033\n",
      "Average of last 10 rewards: -1.0721824034334764\n",
      "Finished episode 2351 after 145922 timesteps\n",
      "Episode reward: -0.766\n",
      "Average of last 10 rewards: -1.0720970085470087\n",
      "Finished episode 2361 after 146627 timesteps\n",
      "Episode reward: -1.27\n",
      "Average of last 10 rewards: -1.0723587234042553\n",
      "Finished episode 2371 after 147220 timesteps\n",
      "Episode reward: -1.069\n",
      "Average of last 10 rewards: -1.072556779661017\n",
      "Finished episode 2381 after 147841 timesteps\n",
      "Episode reward: -1.077\n",
      "Average of last 10 rewards: -1.0721940928270044\n",
      "Finished episode 2391 after 148457 timesteps\n",
      "Episode reward: -1.262\n",
      "Average of last 10 rewards: -1.0721508403361344\n",
      "Finished episode 2401 after 149089 timesteps\n",
      "Episode reward: -1.1720000000000002\n",
      "Average of last 10 rewards: -1.0721719665271967\n",
      "Finished episode 2411 after 149725 timesteps\n",
      "Episode reward: -1.073\n",
      "Average of last 10 rewards: -1.0721533333333333\n",
      "Finished episode 2421 after 150154 timesteps\n",
      "Episode reward: -1.021\n",
      "Average of last 10 rewards: -1.072217427385892\n",
      "Finished episode 2431 after 150546 timesteps\n",
      "Episode reward: -1.048\n",
      "Average of last 10 rewards: -1.072190082644628\n",
      "Finished episode 2441 after 151010 timesteps\n",
      "Episode reward: -1.046\n",
      "Average of last 10 rewards: -1.072237037037037\n",
      "Finished episode 2451 after 152044 timesteps\n",
      "Episode reward: -1.3180000000000003\n",
      "Average of last 10 rewards: -1.0725036885245902\n",
      "INFO:tensorflow:Saving checkpoints for 152238 into ./ppo/model/model.ckpt.\n",
      "Finished episode 2461 after 152719 timesteps\n",
      "Episode reward: -1.12\n",
      "Average of last 10 rewards: -1.0723551020408164\n",
      "Finished episode 2471 after 153189 timesteps\n",
      "Episode reward: -1.058\n",
      "Average of last 10 rewards: -1.072060162601626\n",
      "Finished episode 2481 after 153788 timesteps\n",
      "Episode reward: -1.025\n",
      "Average of last 10 rewards: -1.0719935222672063\n",
      "Finished episode 2491 after 154407 timesteps\n",
      "Episode reward: -1.049\n",
      "Average of last 10 rewards: -1.0721838709677418\n",
      "Finished episode 2501 after 155228 timesteps\n",
      "Episode reward: -1.072\n",
      "Average of last 10 rewards: -1.0718224899598392\n",
      "Finished episode 2511 after 155963 timesteps\n",
      "Episode reward: -1.213\n",
      "Average of last 10 rewards: -1.0720188000000002\n",
      "Finished episode 2521 after 156451 timesteps\n",
      "Episode reward: -1.113\n",
      "Average of last 10 rewards: -1.0721422310756974\n",
      "Finished episode 2531 after 157200 timesteps\n",
      "Episode reward: -1.096\n",
      "Average of last 10 rewards: -1.072068650793651\n",
      "Finished episode 2541 after 158134 timesteps\n",
      "Episode reward: -1.053\n",
      "Average of last 10 rewards: -1.0723695652173915\n",
      "Finished episode 2551 after 158728 timesteps\n",
      "Episode reward: -1.1400000000000001\n",
      "Average of last 10 rewards: -1.072548818897638\n",
      "Finished episode 2561 after 159297 timesteps\n",
      "Episode reward: -0.41100000000000003\n",
      "Average of last 10 rewards: -1.0722517647058825\n",
      "Finished episode 2571 after 159967 timesteps\n",
      "Episode reward: -1.064\n",
      "Average of last 10 rewards: -1.072178125\n",
      "Finished episode 2581 after 160805 timesteps\n",
      "Episode reward: -1.05\n",
      "Average of last 10 rewards: -1.0725307392996108\n",
      "Finished episode 2591 after 161395 timesteps\n",
      "Episode reward: -1.2530000000000001\n",
      "Average of last 10 rewards: -1.072317054263566\n",
      "Finished episode 2601 after 162104 timesteps\n",
      "Episode reward: -1.119\n",
      "Average of last 10 rewards: -1.0724046332046333\n",
      "Finished episode 2611 after 163053 timesteps\n",
      "Episode reward: -1.1300000000000001\n",
      "Average of last 10 rewards: -1.072713076923077\n",
      "Finished episode 2621 after 163586 timesteps\n",
      "Episode reward: -1.052\n",
      "Average of last 10 rewards: -1.072524521072797\n",
      "Finished episode 2631 after 164412 timesteps\n",
      "Episode reward: -1.05\n",
      "Average of last 10 rewards: -1.072713358778626\n",
      "INFO:tensorflow:Saving checkpoints for 165017 into ./ppo/model/model.ckpt.\n",
      "Finished episode 2641 after 165556 timesteps\n",
      "Episode reward: -1.2480000000000002\n",
      "Average of last 10 rewards: -1.0730790874524716\n",
      "Finished episode 2651 after 166069 timesteps\n",
      "Episode reward: -1.089\n",
      "Average of last 10 rewards: -1.073196590909091\n",
      "Finished episode 2661 after 166852 timesteps\n",
      "Episode reward: -1.118\n",
      "Average of last 10 rewards: -1.0734845283018868\n",
      "Finished episode 2671 after 167498 timesteps\n",
      "Episode reward: -1.2280000000000002\n",
      "Average of last 10 rewards: -1.0734338345864662\n",
      "Finished episode 2681 after 167933 timesteps\n",
      "Episode reward: -1.145\n",
      "Average of last 10 rewards: -1.0732895131086142\n",
      "Finished episode 2691 after 168578 timesteps\n",
      "Episode reward: -1.171\n",
      "Average of last 10 rewards: -1.0732630597014927\n",
      "Finished episode 2701 after 169191 timesteps\n",
      "Episode reward: -1.057\n",
      "Average of last 10 rewards: -1.0732368029739776\n",
      "Finished episode 2711 after 169671 timesteps\n",
      "Episode reward: -1.051\n",
      "Average of last 10 rewards: -1.0732822222222222\n",
      "Finished episode 2721 after 170159 timesteps\n",
      "Episode reward: -1.025\n",
      "Average of last 10 rewards: -1.0732845018450186\n",
      "Finished episode 2731 after 170745 timesteps\n",
      "Episode reward: -1.033\n",
      "Average of last 10 rewards: -1.073289705882353\n",
      "Finished episode 2741 after 171384 timesteps\n",
      "Episode reward: -1.059\n",
      "Average of last 10 rewards: -1.07326336996337\n",
      "Finished episode 2751 after 172017 timesteps\n",
      "Episode reward: -1.058\n",
      "Average of last 10 rewards: -1.073384671532847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 2761 after 172674 timesteps\n",
      "Episode reward: -1.2480000000000002\n",
      "Average of last 10 rewards: -1.0733407272727273\n",
      "Finished episode 2771 after 173167 timesteps\n",
      "Episode reward: -1.143\n",
      "Average of last 10 rewards: -1.073253623188406\n",
      "Finished episode 2781 after 173730 timesteps\n",
      "Episode reward: -1.065\n",
      "Average of last 10 rewards: -1.0733064981949458\n",
      "Finished episode 2791 after 174439 timesteps\n",
      "Episode reward: -1.055\n",
      "Average of last 10 rewards: -1.0735061151079137\n",
      "Finished episode 2801 after 175014 timesteps\n",
      "Episode reward: -1.116\n",
      "Average of last 10 rewards: -1.0734491039426521\n",
      "Finished episode 2811 after 175655 timesteps\n",
      "Episode reward: -1.033\n",
      "Average of last 10 rewards: -1.073559285714286\n",
      "Finished episode 2821 after 176180 timesteps\n",
      "Episode reward: -0.677\n",
      "Average of last 10 rewards: -1.0735419928825625\n",
      "Finished episode 2831 after 176881 timesteps\n",
      "Episode reward: -1.2480000000000002\n",
      "Average of last 10 rewards: -1.0735514184397164\n",
      "INFO:tensorflow:Saving checkpoints for 177921 into ./ppo/model/model.ckpt.\n",
      "Finished episode 2841 after 177922 timesteps\n",
      "Episode reward: -0.2210000000000002\n",
      "Average of last 10 rewards: -1.0733639575971732\n",
      "Finished episode 2851 after 178921 timesteps\n",
      "Episode reward: -1.061\n",
      "Average of last 10 rewards: -1.0733838028169014\n",
      "Finished episode 2861 after 179951 timesteps\n",
      "Episode reward: -1.105\n",
      "Average of last 10 rewards: -1.0731691228070177\n",
      "Finished episode 2871 after 180487 timesteps\n",
      "Episode reward: -1.065\n",
      "Average of last 10 rewards: -1.0731290209790212\n",
      "Finished episode 2881 after 181042 timesteps\n",
      "Episode reward: -1.2300000000000002\n",
      "Average of last 10 rewards: -1.0731996515679445\n",
      "Finished episode 2891 after 181874 timesteps\n",
      "Episode reward: -1.2380000000000002\n",
      "Average of last 10 rewards: -1.0733975694444444\n",
      "Finished episode 2901 after 182365 timesteps\n",
      "Episode reward: -1.105\n",
      "Average of last 10 rewards: -1.073457439446367\n",
      "Finished episode 2911 after 182900 timesteps\n",
      "Episode reward: -1.175\n",
      "Average of last 10 rewards: -1.0733106896551725\n",
      "Finished episode 2921 after 183364 timesteps\n",
      "Episode reward: -1.2840000000000003\n",
      "Average of last 10 rewards: -1.0733594501718213\n",
      "Finished episode 2931 after 184055 timesteps\n",
      "Episode reward: -1.103\n",
      "Average of last 10 rewards: -1.073243493150685\n",
      "Finished episode 2941 after 184726 timesteps\n",
      "Episode reward: -1.056\n",
      "Average of last 10 rewards: -1.0734047781569966\n",
      "Finished episode 2951 after 185274 timesteps\n",
      "Episode reward: -1.067\n",
      "Average of last 10 rewards: -1.0735085034013607\n",
      "Finished episode 2961 after 185945 timesteps\n",
      "Episode reward: -1.067\n",
      "Average of last 10 rewards: -1.0731450847457629\n",
      "Finished episode 2971 after 186795 timesteps\n",
      "Episode reward: -1.062\n",
      "Average of last 10 rewards: -1.0732570945945945\n",
      "Finished episode 2981 after 187198 timesteps\n",
      "Episode reward: -1.1440000000000001\n",
      "Average of last 10 rewards: -1.0731656565656567\n",
      "Finished episode 2991 after 187805 timesteps\n",
      "Episode reward: -1.061\n",
      "Average of last 10 rewards: -1.072989932885906\n",
      "Finished episode 3001 after 188366 timesteps\n",
      "Episode reward: -1.048\n",
      "Average of last 10 rewards: -1.0730337792642142\n",
      "Finished episode 3011 after 189064 timesteps\n",
      "Episode reward: -1.063\n",
      "Average of last 10 rewards: -1.0730126666666668\n",
      "Finished episode 3021 after 189687 timesteps\n",
      "Episode reward: -0.43700000000000006\n",
      "Average of last 10 rewards: -1.072846511627907\n",
      "Finished episode 3031 after 190153 timesteps\n",
      "Episode reward: -1.069\n",
      "Average of last 10 rewards: -1.0728311258278147\n",
      "Finished episode 3041 after 190580 timesteps\n",
      "Episode reward: -1.086\n",
      "Average of last 10 rewards: -1.0726933993399341\n",
      "INFO:tensorflow:Saving checkpoints for 190826 into ./ppo/model/model.ckpt.\n",
      "Finished episode 3051 after 191126 timesteps\n",
      "Episode reward: -1.097\n",
      "Average of last 10 rewards: -1.0724565789473683\n",
      "Finished episode 3061 after 191778 timesteps\n",
      "Episode reward: -1.039\n",
      "Average of last 10 rewards: -1.0726222950819673\n",
      "Finished episode 3071 after 192520 timesteps\n",
      "Episode reward: -0.5340000000000003\n",
      "Average of last 10 rewards: -1.0724316993464051\n",
      "Finished episode 3081 after 193146 timesteps\n",
      "Episode reward: -1.1500000000000001\n",
      "Average of last 10 rewards: -1.072257654723127\n",
      "Finished episode 3091 after 193610 timesteps\n",
      "Episode reward: -1.038\n",
      "Average of last 10 rewards: -1.0722659090909092\n",
      "Finished episode 3101 after 194233 timesteps\n",
      "Episode reward: -1.209\n",
      "Average of last 10 rewards: -1.072130420711974\n",
      "Finished episode 3111 after 194856 timesteps\n",
      "Episode reward: -1.2860000000000003\n",
      "Average of last 10 rewards: -1.0721074193548388\n",
      "Finished episode 3121 after 195665 timesteps\n",
      "Episode reward: -1.059\n",
      "Average of last 10 rewards: -1.0723154340836014\n",
      "Finished episode 3131 after 196217 timesteps\n",
      "Episode reward: -1.109\n",
      "Average of last 10 rewards: -1.0722721153846153\n",
      "Finished episode 3141 after 196710 timesteps\n",
      "Episode reward: -1.033\n",
      "Average of last 10 rewards: -1.0719376996805112\n",
      "Finished episode 3151 after 197211 timesteps\n",
      "Episode reward: -1.049\n",
      "Average of last 10 rewards: -1.0719984076433122\n",
      "Finished episode 3161 after 197953 timesteps\n",
      "Episode reward: -0.43900000000000006\n",
      "Average of last 10 rewards: -1.0716819047619048\n",
      "Finished episode 3171 after 198785 timesteps\n",
      "Episode reward: -1.145\n",
      "Average of last 10 rewards: -1.0717522151898735\n",
      "Finished episode 3181 after 199510 timesteps\n",
      "Episode reward: -1.1740000000000002\n",
      "Average of last 10 rewards: -1.0717223974763408\n",
      "Finished episode 3191 after 199792 timesteps\n",
      "Episode reward: -1.08\n",
      "Average of last 10 rewards: -1.0715490566037735\n",
      "Finished episode 3201 after 200640 timesteps\n",
      "Episode reward: -1.113\n",
      "Average of last 10 rewards: -1.071582131661442\n",
      "Finished episode 3211 after 201600 timesteps\n",
      "Episode reward: -1.3200000000000003\n",
      "Average of last 10 rewards: -1.0716553125\n",
      "Finished episode 3221 after 202181 timesteps\n",
      "Episode reward: -1.052\n",
      "Average of last 10 rewards: -1.071597507788162\n",
      "Finished episode 3231 after 202774 timesteps\n",
      "Episode reward: -1.112\n",
      "Average of last 10 rewards: -1.0717767080745342\n",
      "Finished episode 3241 after 203593 timesteps\n",
      "Episode reward: -1.1780000000000002\n",
      "Average of last 10 rewards: -1.0719659442724458\n",
      "INFO:tensorflow:Saving checkpoints for 203762 into ./ppo/model/model.ckpt.\n",
      "Finished episode 3251 after 203966 timesteps\n",
      "Episode reward: -1.067\n",
      "Average of last 10 rewards: -1.0718185185185185\n",
      "Finished episode 3261 after 204750 timesteps\n",
      "Episode reward: -1.057\n",
      "Average of last 10 rewards: -1.0717073846153846\n",
      "Finished episode 3271 after 205270 timesteps\n",
      "Episode reward: -1.05\n",
      "Average of last 10 rewards: -1.0716266871165645\n",
      "Finished episode 3281 after 205763 timesteps\n",
      "Episode reward: -1.131\n",
      "Average of last 10 rewards: -1.0716464831804282\n",
      "Finished episode 3291 after 206588 timesteps\n",
      "Episode reward: -1.048\n",
      "Average of last 10 rewards: -1.071776524390244\n",
      "Finished episode 3301 after 207351 timesteps\n",
      "Episode reward: -1.018\n",
      "Average of last 10 rewards: -1.0714340425531914\n",
      "Finished episode 3311 after 207695 timesteps\n",
      "Episode reward: -1.1420000000000001\n",
      "Average of last 10 rewards: -1.0712851515151516\n",
      "Finished episode 3321 after 208323 timesteps\n",
      "Episode reward: -1.099\n",
      "Average of last 10 rewards: -1.0712728096676738\n",
      "Finished episode 3331 after 208868 timesteps\n",
      "Episode reward: -1.1760000000000002\n",
      "Average of last 10 rewards: -1.0712292168674697\n",
      "Finished episode 3341 after 209433 timesteps\n",
      "Episode reward: -1.2440000000000002\n",
      "Average of last 10 rewards: -1.0713372372372374\n",
      "Finished episode 3351 after 210008 timesteps\n",
      "Episode reward: -1.019\n",
      "Average of last 10 rewards: -1.0714419161676647\n",
      "Finished episode 3361 after 210562 timesteps\n",
      "Episode reward: -1.043\n",
      "Average of last 10 rewards: -1.071422686567164\n",
      "Finished episode 3371 after 211105 timesteps\n",
      "Episode reward: -1.068\n",
      "Average of last 10 rewards: -1.0715035714285714\n",
      "Finished episode 3381 after 211533 timesteps\n",
      "Episode reward: -1.075\n",
      "Average of last 10 rewards: -1.0714881305637982\n",
      "Finished episode 3391 after 212482 timesteps\n",
      "Episode reward: -1.191\n",
      "Average of last 10 rewards: -1.071718047337278\n",
      "Finished episode 3401 after 212844 timesteps\n",
      "Episode reward: -1.055\n",
      "Average of last 10 rewards: -1.0716666666666668\n",
      "Finished episode 3411 after 213668 timesteps\n",
      "Episode reward: -1.062\n",
      "Average of last 10 rewards: -1.071845\n",
      "Finished episode 3421 after 213980 timesteps\n",
      "Episode reward: -1.039\n",
      "Average of last 10 rewards: -1.0718219941348974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 3431 after 214748 timesteps\n",
      "Episode reward: -1.1920000000000002\n",
      "Average of last 10 rewards: -1.072011403508772\n",
      "Finished episode 3441 after 215309 timesteps\n",
      "Episode reward: -1.032\n",
      "Average of last 10 rewards: -1.0719932944606414\n",
      "Finished episode 3451 after 215753 timesteps\n",
      "Episode reward: -1.059\n",
      "Average of last 10 rewards: -1.072011046511628\n",
      "INFO:tensorflow:Saving checkpoints for 216563 into ./ppo/model/model.ckpt.\n",
      "Finished episode 3461 after 216793 timesteps\n",
      "Episode reward: -1.064\n",
      "Average of last 10 rewards: -1.072003768115942\n",
      "Finished episode 3471 after 217535 timesteps\n",
      "Episode reward: -1.06\n",
      "Average of last 10 rewards: -1.0722156069364162\n",
      "Finished episode 3481 after 217925 timesteps\n",
      "Episode reward: -1.045\n",
      "Average of last 10 rewards: -1.0722236311239193\n",
      "Finished episode 3491 after 218587 timesteps\n",
      "Episode reward: -1.072\n",
      "Average of last 10 rewards: -1.0720683908045976\n",
      "Finished episode 3501 after 219082 timesteps\n",
      "Episode reward: -1.098\n",
      "Average of last 10 rewards: -1.0721300859598855\n",
      "Finished episode 3511 after 219527 timesteps\n",
      "Episode reward: -1.053\n",
      "Average of last 10 rewards: -1.0720088571428572\n",
      "Finished episode 3521 after 220212 timesteps\n",
      "Episode reward: -1.06\n",
      "Average of last 10 rewards: -1.0721219373219373\n",
      "Finished episode 3531 after 220660 timesteps\n",
      "Episode reward: -1.1780000000000002\n",
      "Average of last 10 rewards: -1.0719366477272727\n",
      "Finished episode 3541 after 221250 timesteps\n",
      "Episode reward: -1.07\n",
      "Average of last 10 rewards: -1.0719422096317281\n",
      "Finished episode 3551 after 221672 timesteps\n",
      "Episode reward: -1.072\n",
      "Average of last 10 rewards: -1.0717802259887006\n",
      "Finished episode 3561 after 222213 timesteps\n",
      "Episode reward: -1.041\n",
      "Average of last 10 rewards: -1.071754647887324\n",
      "Finished episode 3571 after 222711 timesteps\n",
      "Episode reward: -0.6519999999999999\n",
      "Average of last 10 rewards: -1.0716171348314607\n",
      "Finished episode 3581 after 223257 timesteps\n",
      "Episode reward: -1.229\n",
      "Average of last 10 rewards: -1.0717039215686275\n",
      "Finished episode 3591 after 223848 timesteps\n",
      "Episode reward: -1.042\n",
      "Average of last 10 rewards: -1.0716656424581006\n",
      "Finished episode 3601 after 224489 timesteps\n",
      "Episode reward: -1.053\n",
      "Average of last 10 rewards: -1.0715682451253483\n",
      "Finished episode 3611 after 225061 timesteps\n",
      "Episode reward: -1.1500000000000001\n",
      "Average of last 10 rewards: -1.0716794444444446\n",
      "Finished episode 3621 after 225887 timesteps\n",
      "Episode reward: -1.106\n",
      "Average of last 10 rewards: -1.0717880886426594\n",
      "Finished episode 3631 after 226606 timesteps\n",
      "Episode reward: -1.056\n",
      "Average of last 10 rewards: -1.0716201657458564\n",
      "Finished episode 3641 after 227108 timesteps\n",
      "Episode reward: -1.091\n",
      "Average of last 10 rewards: -1.0715680440771351\n",
      "Finished episode 3651 after 227585 timesteps\n",
      "Episode reward: -1.137\n",
      "Average of last 10 rewards: -1.0714947802197803\n",
      "Finished episode 3661 after 228148 timesteps\n",
      "Episode reward: -1.044\n",
      "Average of last 10 rewards: -1.0714879452054795\n",
      "Finished episode 3671 after 228823 timesteps\n",
      "Episode reward: -1.087\n",
      "Average of last 10 rewards: -1.0713295081967213\n",
      "INFO:tensorflow:Saving checkpoints for 229386 into ./ppo/model/model.ckpt.\n",
      "Finished episode 3681 after 229763 timesteps\n",
      "Episode reward: -1.085\n",
      "Average of last 10 rewards: -1.0714667574931882\n",
      "Finished episode 3691 after 230426 timesteps\n",
      "Episode reward: -1.053\n",
      "Average of last 10 rewards: -1.0715076086956523\n",
      "Finished episode 3701 after 230978 timesteps\n",
      "Episode reward: -1.058\n",
      "Average of last 10 rewards: -1.0714170731707318\n",
      "Finished episode 3711 after 231679 timesteps\n",
      "Episode reward: -1.094\n",
      "Average of last 10 rewards: -1.0715210810810811\n",
      "Finished episode 3721 after 232440 timesteps\n",
      "Episode reward: -0.8520000000000001\n",
      "Average of last 10 rewards: -1.0715256064690026\n",
      "Finished episode 3731 after 233096 timesteps\n",
      "Episode reward: -1.1440000000000001\n",
      "Average of last 10 rewards: -1.0714333333333332\n",
      "Finished episode 3741 after 233743 timesteps\n",
      "Episode reward: -1.3990000000000002\n",
      "Average of last 10 rewards: -1.071535924932976\n",
      "Finished episode 3751 after 234262 timesteps\n",
      "Episode reward: -1.057\n",
      "Average of last 10 rewards: -1.0714524064171123\n",
      "Finished episode 3761 after 234990 timesteps\n",
      "Episode reward: -1.045\n",
      "Average of last 10 rewards: -1.0711866666666667\n",
      "Finished episode 3771 after 235429 timesteps\n",
      "Episode reward: -1.043\n",
      "Average of last 10 rewards: -1.0710808510638299\n",
      "Finished episode 3781 after 235866 timesteps\n",
      "Episode reward: -1.059\n",
      "Average of last 10 rewards: -1.071097877984085\n",
      "Finished episode 3791 after 237128 timesteps\n",
      "Episode reward: -1.076\n",
      "Average of last 10 rewards: -1.0708576719576721\n",
      "Finished episode 3801 after 238219 timesteps\n",
      "Episode reward: -1.074\n",
      "Average of last 10 rewards: -1.0711131926121373\n",
      "Finished episode 3811 after 238746 timesteps\n",
      "Episode reward: -1.064\n",
      "Average of last 10 rewards: -1.0709955263157895\n",
      "Finished episode 3821 after 239170 timesteps\n",
      "Episode reward: -1.048\n",
      "Average of last 10 rewards: -1.070946719160105\n",
      "Finished episode 3831 after 240016 timesteps\n",
      "Episode reward: -1.3480000000000003\n",
      "Average of last 10 rewards: -1.0710356020942409\n",
      "Finished episode 3841 after 240401 timesteps\n",
      "Episode reward: -1.055\n",
      "Average of last 10 rewards: -1.070932637075718\n",
      "Finished episode 3851 after 241237 timesteps\n",
      "Episode reward: -0.44900000000000007\n",
      "Average of last 10 rewards: -1.0708075520833333\n",
      "Finished episode 3861 after 241879 timesteps\n",
      "Episode reward: -1.044\n",
      "Average of last 10 rewards: -1.0706940259740259\n",
      "INFO:tensorflow:Saving checkpoints for 242505 into ./ppo/model/model.ckpt.\n",
      "Finished episode 3871 after 242524 timesteps\n",
      "Episode reward: -1.095\n",
      "Average of last 10 rewards: -1.0708422279792746\n",
      "Finished episode 3881 after 243046 timesteps\n",
      "Episode reward: -1.045\n",
      "Average of last 10 rewards: -1.0709201550387595\n",
      "Finished episode 3891 after 243695 timesteps\n",
      "Episode reward: -1.137\n",
      "Average of last 10 rewards: -1.0708311855670103\n",
      "Finished episode 3901 after 244338 timesteps\n",
      "Episode reward: -1.3480000000000003\n",
      "Average of last 10 rewards: -1.070842673521851\n",
      "Finished episode 3911 after 245064 timesteps\n",
      "Episode reward: -1.044\n",
      "Average of last 10 rewards: -1.0709943589743591\n",
      "Finished episode 3921 after 245536 timesteps\n",
      "Episode reward: -1.098\n",
      "Average of last 10 rewards: -1.07103631713555\n",
      "Finished episode 3931 after 246183 timesteps\n",
      "Episode reward: -1.061\n",
      "Average of last 10 rewards: -1.0711191326530614\n",
      "Finished episode 3941 after 246756 timesteps\n",
      "Episode reward: -1.061\n",
      "Average of last 10 rewards: -1.0712346055979642\n",
      "Finished episode 3951 after 247386 timesteps\n",
      "Episode reward: -1.1320000000000001\n",
      "Average of last 10 rewards: -1.0710421319796954\n",
      "Finished episode 3961 after 247831 timesteps\n",
      "Episode reward: -1.066\n",
      "Average of last 10 rewards: -1.0708779746835442\n",
      "Finished episode 3971 after 248366 timesteps\n",
      "Episode reward: -1.058\n",
      "Average of last 10 rewards: -1.0707282828282827\n",
      "Finished episode 3981 after 248820 timesteps\n",
      "Episode reward: -1.071\n",
      "Average of last 10 rewards: -1.070747355163728\n",
      "Finished episode 3991 after 249679 timesteps\n",
      "Episode reward: -1.4230000000000003\n",
      "Average of last 10 rewards: -1.0708384422110553\n",
      "Finished episode 4001 after 250351 timesteps\n",
      "Episode reward: -1.04\n",
      "Average of last 10 rewards: -1.0708598997493735\n",
      "Finished episode 4011 after 250944 timesteps\n",
      "Episode reward: -1.06\n",
      "Average of last 10 rewards: -1.0706490000000002\n",
      "Finished episode 4021 after 251404 timesteps\n",
      "Episode reward: -1.013\n",
      "Average of last 10 rewards: -1.0706748129675812\n",
      "Finished episode 4031 after 251788 timesteps\n",
      "Episode reward: -1.049\n",
      "Average of last 10 rewards: -1.070652487562189\n",
      "Finished episode 4041 after 252263 timesteps\n",
      "Episode reward: -1.072\n",
      "Average of last 10 rewards: -1.070655583126551\n",
      "Finished episode 4051 after 252962 timesteps\n",
      "Episode reward: -1.1920000000000002\n",
      "Average of last 10 rewards: -1.0707693069306932\n",
      "Finished episode 4061 after 253509 timesteps\n",
      "Episode reward: -0.714\n",
      "Average of last 10 rewards: -1.0707493827160495\n",
      "Finished episode 4071 after 254239 timesteps\n",
      "Episode reward: -0.13500000000000012\n",
      "Average of last 10 rewards: -1.0705014778325126\n",
      "Finished episode 4081 after 254748 timesteps\n",
      "Episode reward: -1.2300000000000002\n",
      "Average of last 10 rewards: -1.070346683046683\n",
      "INFO:tensorflow:Saving checkpoints for 255292 into ./ppo/model/model.ckpt.\n",
      "Finished episode 4091 after 255340 timesteps\n",
      "Episode reward: -1.033\n",
      "Average of last 10 rewards: -1.0704230392156864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 4101 after 255733 timesteps\n",
      "Episode reward: -1.025\n",
      "Average of last 10 rewards: -1.070319315403423\n",
      "Finished episode 4111 after 256454 timesteps\n",
      "Episode reward: -0.74\n",
      "Average of last 10 rewards: -1.0703082926829268\n",
      "Finished episode 4121 after 256914 timesteps\n",
      "Episode reward: -1.1480000000000001\n",
      "Average of last 10 rewards: -1.0703576642335766\n",
      "Finished episode 4131 after 257592 timesteps\n",
      "Episode reward: -0.742\n",
      "Average of last 10 rewards: -1.07023786407767\n",
      "Finished episode 4141 after 258212 timesteps\n",
      "Episode reward: -1.2000000000000002\n",
      "Average of last 10 rewards: -1.0701249394673122\n",
      "Finished episode 4151 after 258640 timesteps\n",
      "Episode reward: -1.053\n",
      "Average of last 10 rewards: -1.069969082125604\n",
      "Finished episode 4161 after 259303 timesteps\n",
      "Episode reward: -1.068\n",
      "Average of last 10 rewards: -1.0699824096385542\n",
      "Finished episode 4171 after 259821 timesteps\n",
      "Episode reward: -1.053\n",
      "Average of last 10 rewards: -1.0699346153846154\n",
      "Finished episode 4181 after 260393 timesteps\n",
      "Episode reward: -1.044\n",
      "Average of last 10 rewards: -1.0700069544364508\n",
      "Finished episode 4191 after 261025 timesteps\n",
      "Episode reward: -1.059\n",
      "Average of last 10 rewards: -1.0699875598086126\n",
      "Finished episode 4201 after 261505 timesteps\n",
      "Episode reward: -1.043\n",
      "Average of last 10 rewards: -1.0700119331742244\n",
      "Finished episode 4211 after 262108 timesteps\n",
      "Episode reward: -1.047\n",
      "Average of last 10 rewards: -1.070112380952381\n",
      "Finished episode 4221 after 262555 timesteps\n",
      "Episode reward: -1.062\n",
      "Average of last 10 rewards: -1.0700546318289788\n",
      "Finished episode 4231 after 262873 timesteps\n",
      "Episode reward: -1.04\n",
      "Average of last 10 rewards: -1.070039336492891\n",
      "Finished episode 4241 after 263467 timesteps\n",
      "Episode reward: -1.062\n",
      "Average of last 10 rewards: -1.0699673758865247\n",
      "Finished episode 4251 after 263966 timesteps\n",
      "Episode reward: -1.121\n",
      "Average of last 10 rewards: -1.0699233490566038\n",
      "Finished episode 4261 after 265277 timesteps\n",
      "Episode reward: -1.4090000000000003\n",
      "Average of last 10 rewards: -1.069933411764706\n",
      "Finished episode 4271 after 265631 timesteps\n",
      "Episode reward: -1.091\n",
      "Average of last 10 rewards: -1.069944366197183\n",
      "Finished episode 4281 after 266030 timesteps\n",
      "Episode reward: -1.04\n",
      "Average of last 10 rewards: -1.069864637002342\n",
      "Finished episode 4291 after 266629 timesteps\n",
      "Episode reward: -1.072\n",
      "Average of last 10 rewards: -1.0699598130841121\n",
      "Finished episode 4301 after 267253 timesteps\n",
      "Episode reward: -1.243\n",
      "Average of last 10 rewards: -1.0698799533799535\n",
      "Finished episode 4311 after 267760 timesteps\n",
      "Episode reward: -0.665\n",
      "Average of last 10 rewards: -1.0697474418604653\n",
      "INFO:tensorflow:Saving checkpoints for 268099 into ./ppo/model/model.ckpt.\n",
      "Finished episode 4321 after 268401 timesteps\n",
      "Episode reward: -0.657\n",
      "Average of last 10 rewards: -1.0695907192575407\n",
      "Finished episode 4331 after 269002 timesteps\n",
      "Episode reward: -1.1300000000000001\n",
      "Average of last 10 rewards: -1.069660648148148\n",
      "Finished episode 4341 after 269641 timesteps\n",
      "Episode reward: -1.049\n",
      "Average of last 10 rewards: -1.0697422632794458\n",
      "Finished episode 4351 after 270459 timesteps\n",
      "Episode reward: -1.205\n",
      "Average of last 10 rewards: -1.0697276497695853\n",
      "Finished episode 4361 after 271006 timesteps\n",
      "Episode reward: -1.161\n",
      "Average of last 10 rewards: -1.069801379310345\n",
      "Finished episode 4371 after 271493 timesteps\n",
      "Episode reward: -1.119\n",
      "Average of last 10 rewards: -1.0697642201834863\n",
      "Finished episode 4381 after 272219 timesteps\n",
      "Episode reward: -1.149\n",
      "Average of last 10 rewards: -1.0694624713958811\n",
      "Finished episode 4391 after 273361 timesteps\n",
      "Episode reward: -1.2690000000000001\n",
      "Average of last 10 rewards: -1.0692415525114156\n",
      "Finished episode 4401 after 273749 timesteps\n",
      "Episode reward: -1.057\n",
      "Average of last 10 rewards: -1.0692592255125286\n",
      "Finished episode 4411 after 274985 timesteps\n",
      "Episode reward: -1.127\n",
      "Average of last 10 rewards: -1.0695427272727274\n",
      "Finished episode 4421 after 275439 timesteps\n",
      "Episode reward: -1.012\n",
      "Average of last 10 rewards: -1.0695616780045352\n",
      "Finished episode 4431 after 276019 timesteps\n",
      "Episode reward: -1.2100000000000002\n",
      "Average of last 10 rewards: -1.0694622171945702\n",
      "Finished episode 4441 after 276317 timesteps\n",
      "Episode reward: -1.079\n",
      "Average of last 10 rewards: -1.0692785553047406\n",
      "Finished episode 4451 after 276916 timesteps\n",
      "Episode reward: -1.077\n",
      "Average of last 10 rewards: -1.0693272522522523\n",
      "Finished episode 4461 after 277501 timesteps\n",
      "Episode reward: -1.098\n",
      "Average of last 10 rewards: -1.0691696629213483\n",
      "Finished episode 4471 after 278083 timesteps\n",
      "Episode reward: -1.081\n",
      "Average of last 10 rewards: -1.0692011210762333\n",
      "Finished episode 4481 after 278585 timesteps\n",
      "Episode reward: -1.033\n",
      "Average of last 10 rewards: -1.0690561521252797\n",
      "Finished episode 4491 after 279217 timesteps\n",
      "Episode reward: -0.45500000000000007\n",
      "Average of last 10 rewards: -1.0687839285714287\n",
      "Finished episode 4501 after 279953 timesteps\n",
      "Episode reward: -1.062\n",
      "Average of last 10 rewards: -1.0687728285077953\n",
      "Finished episode 4511 after 280608 timesteps\n",
      "Episode reward: -1.075\n",
      "Average of last 10 rewards: -1.0689286666666666\n",
      "INFO:tensorflow:Saving checkpoints for 281024 into ./ppo/model/model.ckpt.\n",
      "Finished episode 4521 after 281025 timesteps\n",
      "Episode reward: -1.131\n",
      "Average of last 10 rewards: -1.0689161862527716\n",
      "Finished episode 4531 after 281675 timesteps\n",
      "Episode reward: -1.2380000000000002\n",
      "Average of last 10 rewards: -1.0690362831858407\n",
      "Finished episode 4541 after 282162 timesteps\n",
      "Episode reward: -1.052\n",
      "Average of last 10 rewards: -1.0690547461368654\n",
      "Finished episode 4551 after 282645 timesteps\n",
      "Episode reward: -1.048\n",
      "Average of last 10 rewards: -1.0690041850220264\n",
      "Finished episode 4561 after 283112 timesteps\n",
      "Episode reward: -1.124\n",
      "Average of last 10 rewards: -1.0688861538461538\n",
      "Finished episode 4571 after 283613 timesteps\n",
      "Episode reward: -1.157\n",
      "Average of last 10 rewards: -1.0688276315789473\n",
      "Finished episode 4581 after 284037 timesteps\n",
      "Episode reward: -1.1720000000000002\n",
      "Average of last 10 rewards: -1.0688623632385121\n",
      "Finished episode 4591 after 284400 timesteps\n",
      "Episode reward: -1.07\n",
      "Average of last 10 rewards: -1.0688508733624456\n",
      "Finished episode 4601 after 284968 timesteps\n",
      "Episode reward: -0.5320000000000001\n",
      "Average of last 10 rewards: -1.0686843137254902\n",
      "Finished episode 4611 after 285615 timesteps\n",
      "Episode reward: -1.039\n",
      "Average of last 10 rewards: -1.0687608695652175\n",
      "Finished episode 4621 after 286019 timesteps\n",
      "Episode reward: -1.038\n",
      "Average of last 10 rewards: -1.0685937093275488\n",
      "Finished episode 4631 after 286547 timesteps\n",
      "Episode reward: -1.1300000000000001\n",
      "Average of last 10 rewards: -1.06855\n",
      "Finished episode 4641 after 286915 timesteps\n",
      "Episode reward: -1.112\n",
      "Average of last 10 rewards: -1.0685509719222464\n",
      "Finished episode 4651 after 287493 timesteps\n",
      "Episode reward: -1.047\n",
      "Average of last 10 rewards: -1.0686553879310343\n",
      "Finished episode 4661 after 288342 timesteps\n",
      "Episode reward: -1.094\n",
      "Average of last 10 rewards: -1.0688144086021505\n",
      "Finished episode 4671 after 288867 timesteps\n",
      "Episode reward: -1.046\n",
      "Average of last 10 rewards: -1.0688811158798284\n",
      "Finished episode 4681 after 289458 timesteps\n",
      "Episode reward: -1.2800000000000002\n",
      "Average of last 10 rewards: -1.0689006423982867\n",
      "Finished episode 4691 after 290189 timesteps\n",
      "Episode reward: -1.054\n",
      "Average of last 10 rewards: -1.069023717948718\n",
      "Finished episode 4701 after 290793 timesteps\n",
      "Episode reward: -1.081\n",
      "Average of last 10 rewards: -1.0691149253731342\n",
      "Finished episode 4711 after 291581 timesteps\n",
      "Episode reward: -1.052\n",
      "Average of last 10 rewards: -1.0692678723404256\n",
      "Finished episode 4721 after 292090 timesteps\n",
      "Episode reward: -1.07\n",
      "Average of last 10 rewards: -1.0693509554140128\n",
      "Finished episode 4731 after 292974 timesteps\n",
      "Episode reward: -1.1920000000000002\n",
      "Average of last 10 rewards: -1.0695313559322035\n",
      "Finished episode 4741 after 293690 timesteps\n",
      "Episode reward: -1.165\n",
      "Average of last 10 rewards: -1.0696424947145877\n",
      "INFO:tensorflow:Saving checkpoints for 293758 into ./ppo/model/model.ckpt.\n",
      "Finished episode 4751 after 294375 timesteps\n",
      "Episode reward: -1.071\n",
      "Average of last 10 rewards: -1.0696721518987342\n",
      "Finished episode 4761 after 294964 timesteps\n",
      "Episode reward: -1.041\n",
      "Average of last 10 rewards: -1.069789263157895\n",
      "Finished episode 4771 after 295665 timesteps\n",
      "Episode reward: -1.028\n",
      "Average of last 10 rewards: -1.0698006302521008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 4781 after 296144 timesteps\n",
      "Episode reward: -1.118\n",
      "Average of last 10 rewards: -1.0698394129979036\n",
      "Finished episode 4791 after 296821 timesteps\n",
      "Episode reward: -1.048\n",
      "Average of last 10 rewards: -1.0699121338912134\n",
      "Finished episode 4801 after 297415 timesteps\n",
      "Episode reward: -1.114\n",
      "Average of last 10 rewards: -1.0697945720250523\n",
      "Finished episode 4811 after 298366 timesteps\n",
      "Episode reward: -1.076\n",
      "Average of last 10 rewards: -1.06993875\n",
      "Finished episode 4821 after 298924 timesteps\n",
      "Episode reward: -1.086\n",
      "Average of last 10 rewards: -1.0699467775467777\n",
      "Finished episode 4831 after 299577 timesteps\n",
      "Episode reward: -1.115\n",
      "Average of last 10 rewards: -1.0698703319502074\n",
      "Finished episode 4841 after 300215 timesteps\n",
      "Episode reward: -1.089\n",
      "Average of last 10 rewards: -1.069813457556936\n",
      "Finished episode 4851 after 300726 timesteps\n",
      "Episode reward: -0.351\n",
      "Average of last 10 rewards: -1.0696491735537192\n",
      "Finished episode 4861 after 301369 timesteps\n",
      "Episode reward: -1.1700000000000002\n",
      "Average of last 10 rewards: -1.0696847422680411\n",
      "Finished episode 4871 after 301909 timesteps\n",
      "Episode reward: -1.071\n",
      "Average of last 10 rewards: -1.0697800411522633\n",
      "Finished episode 4881 after 302374 timesteps\n",
      "Episode reward: -1.1880000000000002\n",
      "Average of last 10 rewards: -1.06977022587269\n",
      "Finished episode 4891 after 303119 timesteps\n",
      "Episode reward: -1.081\n",
      "Average of last 10 rewards: -1.0698106557377047\n",
      "Finished episode 4901 after 303653 timesteps\n",
      "Episode reward: -1.059\n",
      "Average of last 10 rewards: -1.0697862985685074\n",
      "Finished episode 4911 after 304309 timesteps\n",
      "Episode reward: -1.013\n",
      "Average of last 10 rewards: -1.0698302040816325\n",
      "Finished episode 4921 after 305033 timesteps\n",
      "Episode reward: -1.079\n",
      "Average of last 10 rewards: -1.0699633401221995\n",
      "Finished episode 4931 after 305425 timesteps\n",
      "Episode reward: -1.048\n",
      "Average of last 10 rewards: -1.0699865853658537\n",
      "Finished episode 4941 after 305996 timesteps\n",
      "Episode reward: -1.022\n",
      "Average of last 10 rewards: -1.0701075050709938\n",
      "Finished episode 4951 after 306649 timesteps\n",
      "Episode reward: -1.087\n",
      "Average of last 10 rewards: -1.0701787449392712\n",
      "INFO:tensorflow:Saving checkpoints for 306789 into ./ppo/model/model.ckpt.\n",
      "Finished episode 4961 after 307395 timesteps\n",
      "Episode reward: -1.074\n",
      "Average of last 10 rewards: -1.0701624242424244\n",
      "Finished episode 4971 after 307999 timesteps\n",
      "Episode reward: -0.6679999999999999\n",
      "Average of last 10 rewards: -1.070079637096774\n",
      "Finished episode 4981 after 308742 timesteps\n",
      "Episode reward: -1.2180000000000002\n",
      "Average of last 10 rewards: -1.070151106639839\n",
      "Finished episode 4991 after 309418 timesteps\n",
      "Episode reward: -1.3230000000000002\n",
      "Average of last 10 rewards: -1.0702347389558233\n",
      "Finished episode 5001 after 309996 timesteps\n",
      "Episode reward: -0.69\n",
      "Average of last 10 rewards: -1.0701779559118236\n",
      "Finished episode 5011 after 310591 timesteps\n",
      "Episode reward: -1.1800000000000002\n",
      "Average of last 10 rewards: -1.0701756000000002\n",
      "Finished episode 5021 after 311161 timesteps\n",
      "Episode reward: -1.033\n",
      "Average of last 10 rewards: -1.0702221556886227\n",
      "Finished episode 5031 after 311550 timesteps\n",
      "Episode reward: -1.072\n",
      "Average of last 10 rewards: -1.0701577689243027\n",
      "Finished episode 5041 after 312270 timesteps\n",
      "Episode reward: -1.1800000000000002\n",
      "Average of last 10 rewards: -1.070203777335984\n",
      "Finished episode 5051 after 312750 timesteps\n",
      "Episode reward: -1.034\n",
      "Average of last 10 rewards: -1.070167261904762\n",
      "Finished episode 5061 after 313239 timesteps\n",
      "Episode reward: -1.072\n",
      "Average of last 10 rewards: -1.0701287128712873\n",
      "Finished episode 5071 after 313856 timesteps\n",
      "Episode reward: -1.06\n",
      "Average of last 10 rewards: -1.0699563241106718\n",
      "Finished episode 5081 after 314287 timesteps\n",
      "Episode reward: -1.043\n",
      "Average of last 10 rewards: -1.0699990138067061\n",
      "Finished episode 5091 after 314944 timesteps\n",
      "Episode reward: -0.6399999999999999\n",
      "Average of last 10 rewards: -1.0699649606299213\n",
      "Finished episode 5101 after 315557 timesteps\n",
      "Episode reward: -1.066\n",
      "Average of last 10 rewards: -1.0700748526522594\n",
      "Finished episode 5111 after 316248 timesteps\n",
      "Episode reward: -0.42700000000000016\n",
      "Average of last 10 rewards: -1.0698923529411766\n",
      "Finished episode 5121 after 316840 timesteps\n",
      "Episode reward: -1.098\n",
      "Average of last 10 rewards: -1.069847553816047\n",
      "Finished episode 5131 after 317378 timesteps\n",
      "Episode reward: -1.342\n",
      "Average of last 10 rewards: -1.069939453125\n",
      "Finished episode 5141 after 317954 timesteps\n",
      "Episode reward: -1.087\n",
      "Average of last 10 rewards: -1.0700413255360623\n",
      "Finished episode 5151 after 319041 timesteps\n",
      "Episode reward: -1.2040000000000002\n",
      "Average of last 10 rewards: -1.0702429961089497\n",
      "INFO:tensorflow:Saving checkpoints for 319690 into ./ppo/model/model.ckpt.\n",
      "Finished episode 5161 after 319691 timesteps\n",
      "Episode reward: -1.103\n",
      "Average of last 10 rewards: -1.0701819417475729\n",
      "Finished episode 5171 after 320166 timesteps\n",
      "Episode reward: -1.033\n",
      "Average of last 10 rewards: -1.0702358527131783\n",
      "Finished episode 5181 after 320760 timesteps\n",
      "Episode reward: -1.021\n",
      "Average of last 10 rewards: -1.0703317214700194\n",
      "Finished episode 5191 after 321467 timesteps\n",
      "Episode reward: -0.74\n",
      "Average of last 10 rewards: -1.0703625482625483\n",
      "Finished episode 5201 after 322220 timesteps\n",
      "Episode reward: -1.189\n",
      "Average of last 10 rewards: -1.070428323699422\n",
      "Finished episode 5211 after 322984 timesteps\n",
      "Episode reward: -1.203\n",
      "Average of last 10 rewards: -1.0705551923076924\n",
      "Finished episode 5221 after 323628 timesteps\n",
      "Episode reward: -1.123\n",
      "Average of last 10 rewards: -1.0705744721689059\n",
      "Finished episode 5231 after 324053 timesteps\n",
      "Episode reward: -1.078\n",
      "Average of last 10 rewards: -1.0705925287356322\n",
      "Finished episode 5241 after 324650 timesteps\n",
      "Episode reward: -1.2570000000000001\n",
      "Average of last 10 rewards: -1.07067896749522\n",
      "Finished episode 5251 after 325422 timesteps\n",
      "Episode reward: -1.108\n",
      "Average of last 10 rewards: -1.0707954198473284\n",
      "Finished episode 5261 after 326439 timesteps\n",
      "Episode reward: -1.07\n",
      "Average of last 10 rewards: -1.0709508571428572\n",
      "Finished episode 5271 after 326827 timesteps\n",
      "Episode reward: -1.049\n",
      "Average of last 10 rewards: -1.0709471482889734\n",
      "Finished episode 5281 after 327506 timesteps\n",
      "Episode reward: -1.124\n",
      "Average of last 10 rewards: -1.0710548387096774\n",
      "Finished episode 5291 after 327984 timesteps\n",
      "Episode reward: -1.135\n",
      "Average of last 10 rewards: -1.0710969696969697\n",
      "Finished episode 5301 after 328773 timesteps\n",
      "Episode reward: -1.062\n",
      "Average of last 10 rewards: -1.071097920604915\n",
      "Finished episode 5311 after 329454 timesteps\n",
      "Episode reward: -1.11\n",
      "Average of last 10 rewards: -1.0708835849056606\n",
      "Finished episode 5321 after 329924 timesteps\n",
      "Episode reward: -1.064\n",
      "Average of last 10 rewards: -1.0709335216572506\n",
      "Finished episode 5331 after 330348 timesteps\n",
      "Episode reward: -1.049\n",
      "Average of last 10 rewards: -1.070949248120301\n",
      "Finished episode 5341 after 331117 timesteps\n",
      "Episode reward: -1.064\n",
      "Average of last 10 rewards: -1.071050656660413\n",
      "Finished episode 5351 after 331833 timesteps\n",
      "Episode reward: -1.025\n",
      "Average of last 10 rewards: -1.0710837078651687\n",
      "Finished episode 5361 after 332266 timesteps\n",
      "Episode reward: -1.048\n",
      "Average of last 10 rewards: -1.0711282242990654\n",
      "INFO:tensorflow:Saving checkpoints for 332570 into ./ppo/model/model.ckpt.\n",
      "Finished episode 5371 after 332705 timesteps\n",
      "Episode reward: -1.041\n",
      "Average of last 10 rewards: -1.071139365671642\n",
      "Finished episode 5381 after 333280 timesteps\n",
      "Episode reward: -1.5260000000000002\n",
      "Average of last 10 rewards: -1.0711651769087525\n",
      "Finished episode 5391 after 333832 timesteps\n",
      "Episode reward: -1.034\n",
      "Average of last 10 rewards: -1.0712159851301115\n",
      "Finished episode 5401 after 334356 timesteps\n",
      "Episode reward: -1.064\n",
      "Average of last 10 rewards: -1.0712795918367348\n",
      "Finished episode 5411 after 335296 timesteps\n",
      "Episode reward: -1.056\n",
      "Average of last 10 rewards: -1.0713427777777778\n",
      "Finished episode 5421 after 336037 timesteps\n",
      "Episode reward: -1.191\n",
      "Average of last 10 rewards: -1.0714325323475047\n",
      "Finished episode 5431 after 336870 timesteps\n",
      "Episode reward: -0.9060000000000001\n",
      "Average of last 10 rewards: -1.0714464944649447\n",
      "Finished episode 5441 after 337487 timesteps\n",
      "Episode reward: -1.1620000000000001\n",
      "Average of last 10 rewards: -1.0714009208103132\n",
      "Finished episode 5451 after 338001 timesteps\n",
      "Episode reward: -1.063\n",
      "Average of last 10 rewards: -1.071455882352941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 5461 after 338398 timesteps\n",
      "Episode reward: -1.145\n",
      "Average of last 10 rewards: -1.0714357798165137\n",
      "Finished episode 5471 after 338835 timesteps\n",
      "Episode reward: -1.1380000000000001\n",
      "Average of last 10 rewards: -1.0714461538461537\n",
      "Finished episode 5481 after 339519 timesteps\n",
      "Episode reward: -1.043\n",
      "Average of last 10 rewards: -1.0714670932358317\n",
      "Finished episode 5491 after 340161 timesteps\n",
      "Episode reward: -1.081\n",
      "Average of last 10 rewards: -1.0713645985401459\n",
      "Finished episode 5501 after 340856 timesteps\n",
      "Episode reward: -1.127\n",
      "Average of last 10 rewards: -1.0713754098360657\n",
      "Finished episode 5511 after 341479 timesteps\n",
      "Episode reward: -1.088\n",
      "Average of last 10 rewards: -1.0712790909090908\n",
      "Finished episode 5521 after 342263 timesteps\n",
      "Episode reward: -0.29500000000000026\n",
      "Average of last 10 rewards: -1.0711562613430128\n",
      "Finished episode 5531 after 342701 timesteps\n",
      "Episode reward: -1.049\n",
      "Average of last 10 rewards: -1.0711528985507246\n",
      "Finished episode 5541 after 343162 timesteps\n",
      "Episode reward: -1.1520000000000001\n",
      "Average of last 10 rewards: -1.071158047016275\n",
      "Finished episode 5551 after 343769 timesteps\n",
      "Episode reward: -1.149\n",
      "Average of last 10 rewards: -1.0711453068592058\n",
      "Finished episode 5561 after 344232 timesteps\n",
      "Episode reward: -0.7790000000000001\n",
      "Average of last 10 rewards: -1.0710091891891893\n",
      "Finished episode 5571 after 344755 timesteps\n",
      "Episode reward: -1.057\n",
      "Average of last 10 rewards: -1.0710606115107912\n",
      "Finished episode 5581 after 345243 timesteps\n",
      "Episode reward: -1.115\n",
      "Average of last 10 rewards: -1.071084919210054\n",
      "INFO:tensorflow:Saving checkpoints for 345272 into ./ppo/model/model.ckpt.\n",
      "Finished episode 5591 after 345760 timesteps\n",
      "Episode reward: -0.734\n",
      "Average of last 10 rewards: -1.0710010752688173\n",
      "Finished episode 5601 after 346307 timesteps\n",
      "Episode reward: -0.6639999999999999\n",
      "Average of last 10 rewards: -1.0709910554561717\n",
      "Finished episode 5611 after 346982 timesteps\n",
      "Episode reward: -1.098\n",
      "Average of last 10 rewards: -1.07100625\n",
      "Finished episode 5621 after 347698 timesteps\n",
      "Episode reward: -1.036\n",
      "Average of last 10 rewards: -1.070760606060606\n",
      "Finished episode 5631 after 348220 timesteps\n",
      "Episode reward: -1.058\n",
      "Average of last 10 rewards: -1.07071512455516\n",
      "Finished episode 5641 after 348630 timesteps\n",
      "Episode reward: -1.046\n",
      "Average of last 10 rewards: -1.0705687388987568\n",
      "Finished episode 5651 after 349158 timesteps\n",
      "Episode reward: -1.099\n",
      "Average of last 10 rewards: -1.0706085106382979\n",
      "Finished episode 5661 after 349559 timesteps\n",
      "Episode reward: -1.163\n",
      "Average of last 10 rewards: -1.0705982300884958\n",
      "Finished episode 5671 after 349936 timesteps\n",
      "Episode reward: -1.046\n",
      "Average of last 10 rewards: -1.0705171378091873\n",
      "Finished episode 5681 after 350722 timesteps\n",
      "Episode reward: -1.131\n",
      "Average of last 10 rewards: -1.070320987654321\n",
      "Finished episode 5691 after 351249 timesteps\n",
      "Episode reward: -1.061\n",
      "Average of last 10 rewards: -1.0702709507042252\n",
      "Finished episode 5701 after 351886 timesteps\n",
      "Episode reward: -1.2120000000000002\n",
      "Average of last 10 rewards: -1.0703398945518454\n",
      "Finished episode 5711 after 352492 timesteps\n",
      "Episode reward: -1.064\n",
      "Average of last 10 rewards: -1.0702236842105264\n",
      "Finished episode 5721 after 353157 timesteps\n",
      "Episode reward: -1.049\n",
      "Average of last 10 rewards: -1.0701940455341508\n",
      "Finished episode 5731 after 354069 timesteps\n",
      "Episode reward: -1.1640000000000001\n",
      "Average of last 10 rewards: -1.0701181818181817\n",
      "Finished episode 5741 after 354917 timesteps\n",
      "Episode reward: -1.1780000000000002\n",
      "Average of last 10 rewards: -1.0701160558464224\n",
      "Finished episode 5751 after 355791 timesteps\n",
      "Episode reward: -1.3010000000000002\n",
      "Average of last 10 rewards: -1.070214111498258\n",
      "Finished episode 5761 after 356372 timesteps\n",
      "Episode reward: -1.029\n",
      "Average of last 10 rewards: -1.0702706086956522\n",
      "Finished episode 5771 after 357031 timesteps\n",
      "Episode reward: -1.089\n",
      "Average of last 10 rewards: -1.0702692708333335\n",
      "Finished episode 5781 after 357662 timesteps\n",
      "Episode reward: -1.189\n",
      "Average of last 10 rewards: -1.0703592720970536\n",
      "Finished episode 5791 after 358169 timesteps\n",
      "Episode reward: -1.042\n",
      "Average of last 10 rewards: -1.070412456747405\n",
      "INFO:tensorflow:Saving checkpoints for 358221 into ./ppo/model/model.ckpt.\n",
      "Finished episode 5801 after 358761 timesteps\n",
      "Episode reward: -1.104\n",
      "Average of last 10 rewards: -1.0704873920552678\n",
      "Finished episode 5811 after 359344 timesteps\n",
      "Episode reward: -1.067\n",
      "Average of last 10 rewards: -1.070464827586207\n",
      "Finished episode 5821 after 359923 timesteps\n",
      "Episode reward: -1.1340000000000001\n",
      "Average of last 10 rewards: -1.0704462994836488\n",
      "Finished episode 5831 after 360568 timesteps\n",
      "Episode reward: -1.051\n",
      "Average of last 10 rewards: -1.0705407216494847\n",
      "Finished episode 5841 after 361296 timesteps\n",
      "Episode reward: -0.3440000000000001\n",
      "Average of last 10 rewards: -1.0704993138936536\n",
      "Finished episode 5851 after 361936 timesteps\n",
      "Episode reward: -1.073\n",
      "Average of last 10 rewards: -1.0704808219178084\n",
      "Finished episode 5861 after 362567 timesteps\n",
      "Episode reward: -1.026\n",
      "Average of last 10 rewards: -1.0705721367521368\n",
      "Finished episode 5871 after 363612 timesteps\n",
      "Episode reward: -0.29100000000000026\n",
      "Average of last 10 rewards: -1.0705182593856655\n",
      "Finished episode 5881 after 364461 timesteps\n",
      "Episode reward: -1.068\n",
      "Average of last 10 rewards: -1.0706074957410563\n",
      "Finished episode 5891 after 365064 timesteps\n",
      "Episode reward: -1.084\n",
      "Average of last 10 rewards: -1.0705867346938778\n",
      "Finished episode 5901 after 365617 timesteps\n",
      "Episode reward: -1.019\n",
      "Average of last 10 rewards: -1.0705553480475383\n",
      "Finished episode 5911 after 366423 timesteps\n",
      "Episode reward: -0.794\n",
      "Average of last 10 rewards: -1.0704133898305086\n",
      "Finished episode 5921 after 367424 timesteps\n",
      "Episode reward: -1.086\n",
      "Average of last 10 rewards: -1.0704541455160745\n",
      "Finished episode 5931 after 368272 timesteps\n",
      "Episode reward: -1.044\n",
      "Average of last 10 rewards: -1.0704699324324325\n",
      "Finished episode 5941 after 368866 timesteps\n",
      "Episode reward: -1.091\n",
      "Average of last 10 rewards: -1.070533558178752\n",
      "Finished episode 5951 after 369389 timesteps\n",
      "Episode reward: -1.084\n",
      "Average of last 10 rewards: -1.0705599326599327\n",
      "Finished episode 5961 after 370172 timesteps\n",
      "Episode reward: -1.06\n",
      "Average of last 10 rewards: -1.0706268907563026\n",
      "Finished episode 5971 after 370593 timesteps\n",
      "Episode reward: -1.027\n",
      "Average of last 10 rewards: -1.0706260067114095\n",
      "INFO:tensorflow:Saving checkpoints for 371125 into ./ppo/model/model.ckpt.\n",
      "Finished episode 5981 after 371126 timesteps\n",
      "Episode reward: -1.079\n",
      "Average of last 10 rewards: -1.0705959798994975\n",
      "Finished episode 5991 after 371599 timesteps\n",
      "Episode reward: -1.037\n",
      "Average of last 10 rewards: -1.0706389632107023\n",
      "Finished episode 6001 after 372507 timesteps\n",
      "Episode reward: -1.2600000000000002\n",
      "Average of last 10 rewards: -1.070763939899833\n",
      "Finished episode 6011 after 373095 timesteps\n",
      "Episode reward: -1.052\n",
      "Average of last 10 rewards: -1.0706918333333335\n",
      "Finished episode 6021 after 373939 timesteps\n",
      "Episode reward: -1.067\n",
      "Average of last 10 rewards: -1.070695507487521\n",
      "Finished episode 6031 after 374603 timesteps\n",
      "Episode reward: -1.089\n",
      "Average of last 10 rewards: -1.0706524916943523\n",
      "Finished episode 6041 after 374961 timesteps\n",
      "Episode reward: -1.092\n",
      "Average of last 10 rewards: -1.0706587064676618\n",
      "Finished episode 6051 after 375489 timesteps\n",
      "Episode reward: -1.085\n",
      "Average of last 10 rewards: -1.070614569536424\n",
      "Finished episode 6061 after 376025 timesteps\n",
      "Episode reward: -1.1600000000000001\n",
      "Average of last 10 rewards: -1.0706297520661157\n",
      "Finished episode 6071 after 376565 timesteps\n",
      "Episode reward: -1.094\n",
      "Average of last 10 rewards: -1.0706966996699672\n",
      "Finished episode 6081 after 377463 timesteps\n",
      "Episode reward: -0.6579999999999999\n",
      "Average of last 10 rewards: -1.0706792421746294\n",
      "Finished episode 6091 after 378196 timesteps\n",
      "Episode reward: -1.034\n",
      "Average of last 10 rewards: -1.070650657894737\n",
      "Finished episode 6101 after 378888 timesteps\n",
      "Episode reward: -1.098\n",
      "Average of last 10 rewards: -1.070620197044335\n",
      "Finished episode 6111 after 379540 timesteps\n",
      "Episode reward: -1.1980000000000002\n",
      "Average of last 10 rewards: -1.0706918032786885\n",
      "Finished episode 6121 after 379947 timesteps\n",
      "Episode reward: -1.1500000000000001\n",
      "Average of last 10 rewards: -1.0706965630114567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 6131 after 380713 timesteps\n",
      "Episode reward: -1.3940000000000003\n",
      "Average of last 10 rewards: -1.0707071895424836\n",
      "Finished episode 6141 after 381644 timesteps\n",
      "Episode reward: -1.02\n",
      "Average of last 10 rewards: -1.0705977161500817\n",
      "Finished episode 6151 after 382644 timesteps\n",
      "Episode reward: -1.2810000000000001\n",
      "Average of last 10 rewards: -1.0706379478827361\n",
      "Finished episode 6161 after 383271 timesteps\n",
      "Episode reward: -1.052\n",
      "Average of last 10 rewards: -1.070629918699187\n",
      "Finished episode 6171 after 383835 timesteps\n",
      "Episode reward: -1.04\n",
      "Average of last 10 rewards: -1.0705521103896105\n",
      "INFO:tensorflow:Saving checkpoints for 384030 into ./ppo/model/model.ckpt.\n",
      "Finished episode 6181 after 384288 timesteps\n",
      "Episode reward: -1.107\n",
      "Average of last 10 rewards: -1.070569692058347\n",
      "Finished episode 6191 after 384716 timesteps\n",
      "Episode reward: -1.035\n",
      "Average of last 10 rewards: -1.0705692556634305\n",
      "Finished episode 6201 after 385305 timesteps\n",
      "Episode reward: -1.1340000000000001\n",
      "Average of last 10 rewards: -1.0705405492730211\n",
      "Finished episode 6211 after 385797 timesteps\n",
      "Episode reward: -1.149\n",
      "Average of last 10 rewards: -1.070531129032258\n",
      "Finished episode 6221 after 386472 timesteps\n",
      "Episode reward: -1.087\n",
      "Average of last 10 rewards: -1.0706154589371981\n",
      "Finished episode 6231 after 387082 timesteps\n",
      "Episode reward: -1.079\n",
      "Average of last 10 rewards: -1.0706146302250805\n",
      "Finished episode 6241 after 387947 timesteps\n",
      "Episode reward: -0.6930000000000001\n",
      "Average of last 10 rewards: -1.0705823434991977\n",
      "Finished episode 6251 after 388509 timesteps\n",
      "Episode reward: -1.057\n",
      "Average of last 10 rewards: -1.0706091346153848\n",
      "Finished episode 6261 after 389022 timesteps\n",
      "Episode reward: -0.7710000000000001\n",
      "Average of last 10 rewards: -1.07060144\n",
      "Finished episode 6271 after 389588 timesteps\n",
      "Episode reward: -1.116\n",
      "Average of last 10 rewards: -1.07065910543131\n",
      "Finished episode 6281 after 390421 timesteps\n",
      "Episode reward: -1.163\n",
      "Average of last 10 rewards: -1.0707805422647527\n",
      "Finished episode 6291 after 391017 timesteps\n",
      "Episode reward: -1.043\n",
      "Average of last 10 rewards: -1.0708167197452232\n",
      "Finished episode 6301 after 391655 timesteps\n",
      "Episode reward: -1.081\n",
      "Average of last 10 rewards: -1.070593799682035\n",
      "Finished episode 6311 after 392280 timesteps\n",
      "Episode reward: -1.089\n",
      "Average of last 10 rewards: -1.0706395238095239\n",
      "Finished episode 6321 after 393252 timesteps\n",
      "Episode reward: -1.026\n",
      "Average of last 10 rewards: -1.0707725832012678\n",
      "Finished episode 6331 after 393945 timesteps\n",
      "Episode reward: -0.42800000000000005\n",
      "Average of last 10 rewards: -1.0707197784810127\n",
      "Finished episode 6341 after 394312 timesteps\n",
      "Episode reward: -0.6639999999999999\n",
      "Average of last 10 rewards: -1.0705407582938389\n",
      "Finished episode 6351 after 394834 timesteps\n",
      "Episode reward: -1.1680000000000001\n",
      "Average of last 10 rewards: -1.0704913249211356\n",
      "Finished episode 6361 after 395569 timesteps\n",
      "Episode reward: -1.1800000000000002\n",
      "Average of last 10 rewards: -1.0705371653543307\n",
      "Finished episode 6371 after 396090 timesteps\n",
      "Episode reward: -1.048\n",
      "Average of last 10 rewards: -1.0705602201257862\n",
      "Finished episode 6381 after 396553 timesteps\n",
      "Episode reward: -1.048\n",
      "Average of last 10 rewards: -1.070579748822606\n",
      "INFO:tensorflow:Saving checkpoints for 396907 into ./ppo/model/model.ckpt.\n",
      "Finished episode 6391 after 396925 timesteps\n",
      "Episode reward: -0.6930000000000001\n",
      "Average of last 10 rewards: -1.0704608150470218\n",
      "Finished episode 6401 after 397802 timesteps\n",
      "Episode reward: -1.057\n",
      "Average of last 10 rewards: -1.0702621283255087\n",
      "Finished episode 6411 after 398245 timesteps\n",
      "Episode reward: -1.056\n",
      "Average of last 10 rewards: -1.07025765625\n",
      "Finished episode 6421 after 399397 timesteps\n",
      "Episode reward: -1.095\n",
      "Average of last 10 rewards: -1.0704585023400937\n",
      "Finished episode 6431 after 400115 timesteps\n",
      "Episode reward: -1.069\n",
      "Average of last 10 rewards: -1.0704531152647976\n",
      "Finished episode 6441 after 400588 timesteps\n",
      "Episode reward: -1.052\n",
      "Average of last 10 rewards: -1.0704007776049766\n",
      "Finished episode 6451 after 401254 timesteps\n",
      "Episode reward: -1.2800000000000002\n",
      "Average of last 10 rewards: -1.070462888198758\n",
      "Finished episode 6461 after 401803 timesteps\n",
      "Episode reward: -1.08\n",
      "Average of last 10 rewards: -1.0704322480620156\n",
      "Finished episode 6471 after 402192 timesteps\n",
      "Episode reward: -1.046\n",
      "Average of last 10 rewards: -1.0704230650154798\n",
      "Finished episode 6481 after 402741 timesteps\n",
      "Episode reward: -0.7380000000000001\n",
      "Average of last 10 rewards: -1.0703425038639875\n",
      "Finished episode 6491 after 403416 timesteps\n",
      "Episode reward: -1.066\n",
      "Average of last 10 rewards: -1.0703952160493828\n",
      "Finished episode 6501 after 404555 timesteps\n",
      "Episode reward: -1.082\n",
      "Average of last 10 rewards: -1.0703186440677968\n",
      "Finished episode 6511 after 405255 timesteps\n",
      "Episode reward: -1.058\n",
      "Average of last 10 rewards: -1.0702515384615385\n",
      "Finished episode 6521 after 405818 timesteps\n",
      "Episode reward: -1.041\n",
      "Average of last 10 rewards: -1.0702993855606757\n",
      "Finished episode 6531 after 406529 timesteps\n",
      "Episode reward: -1.063\n",
      "Average of last 10 rewards: -1.070320245398773\n",
      "Finished episode 6541 after 407144 timesteps\n",
      "Episode reward: -1.076\n",
      "Average of last 10 rewards: -1.0704039816232773\n",
      "Finished episode 6551 after 407997 timesteps\n",
      "Episode reward: -0.6599999999999999\n",
      "Average of last 10 rewards: -1.0704686544342508\n",
      "Finished episode 6561 after 408709 timesteps\n",
      "Episode reward: -1.1940000000000002\n",
      "Average of last 10 rewards: -1.0705283969465649\n",
      "Finished episode 6571 after 409336 timesteps\n",
      "Episode reward: -1.049\n",
      "Average of last 10 rewards: -1.0705958841463414\n",
      "INFO:tensorflow:Saving checkpoints for 409701 into ./ppo/model/model.ckpt.\n",
      "Finished episode 6581 after 409844 timesteps\n",
      "Episode reward: -1.033\n",
      "Average of last 10 rewards: -1.0706167427701674\n",
      "Finished episode 6591 after 410406 timesteps\n",
      "Episode reward: -1.08\n",
      "Average of last 10 rewards: -1.0706981762917933\n",
      "Finished episode 6601 after 410829 timesteps\n",
      "Episode reward: -1.056\n",
      "Average of last 10 rewards: -1.070635204855842\n",
      "Finished episode 6611 after 411293 timesteps\n",
      "Episode reward: -0.692\n",
      "Average of last 10 rewards: -1.0705798484848486\n",
      "Finished episode 6621 after 411933 timesteps\n",
      "Episode reward: -1.073\n",
      "Average of last 10 rewards: -1.0706475037821481\n",
      "Finished episode 6631 after 412520 timesteps\n",
      "Episode reward: -0.8900000000000001\n",
      "Average of last 10 rewards: -1.0706247734138974\n",
      "Finished episode 6641 after 413171 timesteps\n",
      "Episode reward: -1.04\n",
      "Average of last 10 rewards: -1.0705686274509805\n",
      "Finished episode 6651 after 413913 timesteps\n",
      "Episode reward: -1.093\n",
      "Average of last 10 rewards: -1.0706019578313253\n",
      "Finished episode 6661 after 414555 timesteps\n",
      "Episode reward: -1.0170000000000001\n",
      "Average of last 10 rewards: -1.0706133834586467\n",
      "Finished episode 6671 after 415016 timesteps\n",
      "Episode reward: -1.213\n",
      "Average of last 10 rewards: -1.0706378378378378\n",
      "Finished episode 6681 after 415762 timesteps\n",
      "Episode reward: -1.125\n",
      "Average of last 10 rewards: -1.0706818590704648\n",
      "Finished episode 6691 after 416322 timesteps\n",
      "Episode reward: -1.098\n",
      "Average of last 10 rewards: -1.0707574850299402\n",
      "Finished episode 6701 after 417102 timesteps\n",
      "Episode reward: -1.072\n",
      "Average of last 10 rewards: -1.070877130044843\n",
      "Finished episode 6711 after 417826 timesteps\n",
      "Episode reward: -1.159\n",
      "Average of last 10 rewards: -1.070990447761194\n",
      "Finished episode 6721 after 418425 timesteps\n",
      "Episode reward: -1.1380000000000001\n",
      "Average of last 10 rewards: -1.071047690014903\n",
      "Finished episode 6731 after 419035 timesteps\n",
      "Episode reward: -1.1420000000000001\n",
      "Average of last 10 rewards: -1.0711427083333334\n",
      "Finished episode 6741 after 419473 timesteps\n",
      "Episode reward: -1.1260000000000001\n",
      "Average of last 10 rewards: -1.0711701337295692\n",
      "Finished episode 6751 after 419983 timesteps\n",
      "Episode reward: -1.057\n",
      "Average of last 10 rewards: -1.0712129080118695\n",
      "Finished episode 6761 after 420442 timesteps\n",
      "Episode reward: -1.077\n",
      "Average of last 10 rewards: -1.0711736296296297\n",
      "Finished episode 6771 after 420978 timesteps\n",
      "Episode reward: -1.106\n",
      "Average of last 10 rewards: -1.0712406804733727\n",
      "Finished episode 6781 after 421442 timesteps\n",
      "Episode reward: -1.037\n",
      "Average of last 10 rewards: -1.0711453471196455\n",
      "Finished episode 6791 after 422074 timesteps\n",
      "Episode reward: -1.058\n",
      "Average of last 10 rewards: -1.0710927728613568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 422563 into ./ppo/model/model.ckpt.\n",
      "Finished episode 6801 after 422687 timesteps\n",
      "Episode reward: -1.143\n",
      "Average of last 10 rewards: -1.0711082474226805\n",
      "Finished episode 6811 after 423290 timesteps\n",
      "Episode reward: -0.6799999999999999\n",
      "Average of last 10 rewards: -1.0711307352941177\n",
      "Finished episode 6821 after 423752 timesteps\n",
      "Episode reward: -1.169\n",
      "Average of last 10 rewards: -1.0711189427312777\n",
      "Finished episode 6831 after 424183 timesteps\n",
      "Episode reward: -1.034\n",
      "Average of last 10 rewards: -1.071082991202346\n",
      "Finished episode 6841 after 424741 timesteps\n",
      "Episode reward: -1.137\n",
      "Average of last 10 rewards: -1.071132503660322\n",
      "Finished episode 6851 after 425155 timesteps\n",
      "Episode reward: -1.113\n",
      "Average of last 10 rewards: -1.071106725146199\n",
      "Finished episode 6861 after 425680 timesteps\n",
      "Episode reward: -1.079\n",
      "Average of last 10 rewards: -1.0710982481751827\n",
      "Finished episode 6871 after 426554 timesteps\n",
      "Episode reward: -1.081\n",
      "Average of last 10 rewards: -1.0712362973760934\n",
      "Finished episode 6881 after 426996 timesteps\n",
      "Episode reward: -1.041\n",
      "Average of last 10 rewards: -1.071203347889374\n",
      "Finished episode 6891 after 427782 timesteps\n",
      "Episode reward: -1.157\n",
      "Average of last 10 rewards: -1.0712120639534883\n",
      "Finished episode 6901 after 428333 timesteps\n",
      "Episode reward: -0.6599999999999999\n",
      "Average of last 10 rewards: -1.0711462989840348\n",
      "Finished episode 6911 after 428977 timesteps\n",
      "Episode reward: -1.119\n",
      "Average of last 10 rewards: -1.0710184057971015\n",
      "Finished episode 6921 after 429492 timesteps\n",
      "Episode reward: -1.081\n",
      "Average of last 10 rewards: -1.071102604920405\n",
      "Finished episode 6931 after 430333 timesteps\n",
      "Episode reward: -1.061\n",
      "Average of last 10 rewards: -1.0711921965317919\n",
      "Finished episode 6941 after 430905 timesteps\n",
      "Episode reward: -1.4010000000000002\n",
      "Average of last 10 rewards: -1.0712506493506493\n",
      "Finished episode 6951 after 431507 timesteps\n",
      "Episode reward: -1.051\n",
      "Average of last 10 rewards: -1.0713112391930837\n",
      "Finished episode 6961 after 432120 timesteps\n",
      "Episode reward: -1.032\n",
      "Average of last 10 rewards: -1.0713277697841725\n",
      "Finished episode 6971 after 432780 timesteps\n",
      "Episode reward: -0.7100000000000003\n",
      "Average of last 10 rewards: -1.071231752873563\n",
      "Finished episode 6981 after 433948 timesteps\n",
      "Episode reward: -1.067\n",
      "Average of last 10 rewards: -1.071416068866571\n",
      "Finished episode 6991 after 434561 timesteps\n",
      "Episode reward: -0.7130000000000001\n",
      "Average of last 10 rewards: -1.0714127507163325\n",
      "Finished episode 7001 after 435322 timesteps\n",
      "Episode reward: -1.03\n",
      "Average of last 10 rewards: -1.071352217453505\n",
      "INFO:tensorflow:Saving checkpoints for 435412 into ./ppo/model/model.ckpt.\n",
      "Finished episode 7011 after 436026 timesteps\n",
      "Episode reward: -1.213\n",
      "Average of last 10 rewards: -1.0713825714285714\n",
      "Finished episode 7021 after 436617 timesteps\n",
      "Episode reward: -0.9130000000000003\n",
      "Average of last 10 rewards: -1.0712893009985736\n",
      "Finished episode 7031 after 437465 timesteps\n",
      "Episode reward: -1.203\n",
      "Average of last 10 rewards: -1.071375925925926\n",
      "Finished episode 7041 after 438119 timesteps\n",
      "Episode reward: -1.2750000000000001\n",
      "Average of last 10 rewards: -1.0713836415362732\n",
      "Finished episode 7051 after 438655 timesteps\n",
      "Episode reward: -1.029\n",
      "Average of last 10 rewards: -1.0713688920454545\n",
      "Finished episode 7061 after 439254 timesteps\n",
      "Episode reward: -1.2460000000000002\n",
      "Average of last 10 rewards: -1.0714192907801419\n",
      "Finished episode 7071 after 439791 timesteps\n",
      "Episode reward: -1.028\n",
      "Average of last 10 rewards: -1.071343059490085\n",
      "Finished episode 7081 after 440348 timesteps\n",
      "Episode reward: -1.181\n",
      "Average of last 10 rewards: -1.0713756718528995\n",
      "Finished episode 7091 after 440919 timesteps\n",
      "Episode reward: -1.051\n",
      "Average of last 10 rewards: -1.0713155367231637\n",
      "Finished episode 7101 after 441377 timesteps\n",
      "Episode reward: -1.141\n",
      "Average of last 10 rewards: -1.071335119887165\n",
      "Finished episode 7111 after 441816 timesteps\n",
      "Episode reward: -1.025\n",
      "Average of last 10 rewards: -1.0713433802816903\n",
      "Finished episode 7121 after 442416 timesteps\n",
      "Episode reward: -1.027\n",
      "Average of last 10 rewards: -1.0713933895921237\n",
      "Finished episode 7131 after 443071 timesteps\n",
      "Episode reward: -1.043\n",
      "Average of last 10 rewards: -1.071396488764045\n",
      "Finished episode 7141 after 443882 timesteps\n",
      "Episode reward: -1.5010000000000003\n",
      "Average of last 10 rewards: -1.071528471248247\n",
      "Finished episode 7151 after 444412 timesteps\n",
      "Episode reward: -1.1620000000000001\n",
      "Average of last 10 rewards: -1.07158081232493\n",
      "Finished episode 7161 after 444912 timesteps\n",
      "Episode reward: -1.195\n",
      "Average of last 10 rewards: -1.0716109090909092\n",
      "Finished episode 7171 after 445312 timesteps\n",
      "Episode reward: -1.043\n",
      "Average of last 10 rewards: -1.0716121508379888\n",
      "Finished episode 7181 after 446041 timesteps\n",
      "Episode reward: -1.067\n",
      "Average of last 10 rewards: -1.0716280334728034\n",
      "Finished episode 7191 after 446707 timesteps\n",
      "Episode reward: -1.1420000000000001\n",
      "Average of last 10 rewards: -1.0715785515320335\n",
      "Finished episode 7201 after 447204 timesteps\n",
      "Episode reward: -1.227\n",
      "Average of last 10 rewards: -1.0715467315716274\n",
      "Finished episode 7211 after 447925 timesteps\n",
      "Episode reward: -1.102\n",
      "Average of last 10 rewards: -1.0715722222222224\n",
      "INFO:tensorflow:Saving checkpoints for 448244 into ./ppo/model/model.ckpt.\n",
      "Finished episode 7221 after 448401 timesteps\n",
      "Episode reward: -1.08\n",
      "Average of last 10 rewards: -1.071500138696255\n",
      "Finished episode 7231 after 448887 timesteps\n",
      "Episode reward: -0.708\n",
      "Average of last 10 rewards: -1.0714150969529086\n",
      "Finished episode 7241 after 449306 timesteps\n",
      "Episode reward: -0.679\n",
      "Average of last 10 rewards: -1.0713821576763487\n",
      "Finished episode 7251 after 449739 timesteps\n",
      "Episode reward: -1.175\n",
      "Average of last 10 rewards: -1.0713860497237568\n",
      "Finished episode 7261 after 450289 timesteps\n",
      "Episode reward: -1.068\n",
      "Average of last 10 rewards: -1.071436551724138\n",
      "Finished episode 7271 after 450680 timesteps\n",
      "Episode reward: -1.055\n",
      "Average of last 10 rewards: -1.0712750688705235\n",
      "Finished episode 7281 after 451200 timesteps\n",
      "Episode reward: -1.052\n",
      "Average of last 10 rewards: -1.0712701513067402\n",
      "Finished episode 7291 after 451617 timesteps\n",
      "Episode reward: -1.111\n",
      "Average of last 10 rewards: -1.0712828296703296\n",
      "Finished episode 7301 after 452284 timesteps\n",
      "Episode reward: -1.049\n",
      "Average of last 10 rewards: -1.0712932784636489\n",
      "Finished episode 7311 after 452799 timesteps\n",
      "Episode reward: -1.093\n",
      "Average of last 10 rewards: -1.0712782191780823\n",
      "Finished episode 7321 after 453736 timesteps\n",
      "Episode reward: -1.4170000000000003\n",
      "Average of last 10 rewards: -1.071350752393981\n",
      "Finished episode 7331 after 454508 timesteps\n",
      "Episode reward: -1.048\n",
      "Average of last 10 rewards: -1.0713079234972678\n",
      "Finished episode 7341 after 455135 timesteps\n",
      "Episode reward: -1.101\n",
      "Average of last 10 rewards: -1.0713151432469306\n",
      "Finished episode 7351 after 455764 timesteps\n",
      "Episode reward: -1.033\n",
      "Average of last 10 rewards: -1.0713802452316077\n",
      "Finished episode 7361 after 456414 timesteps\n",
      "Episode reward: -1.057\n",
      "Average of last 10 rewards: -1.0713039455782312\n",
      "Finished episode 7371 after 457190 timesteps\n",
      "Episode reward: -1.033\n",
      "Average of last 10 rewards: -1.071313179347826\n",
      "Finished episode 7381 after 457936 timesteps\n",
      "Episode reward: -1.2140000000000002\n",
      "Average of last 10 rewards: -1.0713484396200814\n",
      "Finished episode 7391 after 458481 timesteps\n",
      "Episode reward: -1.075\n",
      "Average of last 10 rewards: -1.0713527100271003\n",
      "Finished episode 7401 after 458994 timesteps\n",
      "Episode reward: -1.032\n",
      "Average of last 10 rewards: -1.071210554803789\n",
      "Finished episode 7411 after 459636 timesteps\n",
      "Episode reward: -1.037\n",
      "Average of last 10 rewards: -1.0712045945945945\n",
      "Finished episode 7421 after 460176 timesteps\n",
      "Episode reward: -1.057\n",
      "Average of last 10 rewards: -1.0712252361673416\n",
      "Finished episode 7431 after 460731 timesteps\n",
      "Episode reward: -1.1440000000000001\n",
      "Average of last 10 rewards: -1.0712783018867924\n",
      "INFO:tensorflow:Saving checkpoints for 461126 into ./ppo/model/model.ckpt.\n",
      "Finished episode 7441 after 461268 timesteps\n",
      "Episode reward: -1.101\n",
      "Average of last 10 rewards: -1.0713165545087482\n",
      "Finished episode 7451 after 461973 timesteps\n",
      "Episode reward: -1.1420000000000001\n",
      "Average of last 10 rewards: -1.071269623655914\n",
      "Finished episode 7461 after 462552 timesteps\n",
      "Episode reward: -1.016\n",
      "Average of last 10 rewards: -1.071139865771812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 7471 after 463447 timesteps\n",
      "Episode reward: -1.077\n",
      "Average of last 10 rewards: -1.0711231903485254\n",
      "Finished episode 7481 after 464098 timesteps\n",
      "Episode reward: -1.068\n",
      "Average of last 10 rewards: -1.071188219544846\n",
      "Finished episode 7491 after 464915 timesteps\n",
      "Episode reward: -1.059\n",
      "Average of last 10 rewards: -1.0711208556149734\n",
      "Finished episode 7501 after 465277 timesteps\n",
      "Episode reward: -1.04\n",
      "Average of last 10 rewards: -1.0711248331108145\n",
      "Finished episode 7511 after 466211 timesteps\n",
      "Episode reward: -1.219\n",
      "Average of last 10 rewards: -1.071169866666667\n",
      "Finished episode 7521 after 466743 timesteps\n",
      "Episode reward: -1.055\n",
      "Average of last 10 rewards: -1.0712203728362184\n",
      "Finished episode 7531 after 467424 timesteps\n",
      "Episode reward: -1.035\n",
      "Average of last 10 rewards: -1.0713063829787235\n",
      "Finished episode 7541 after 467856 timesteps\n",
      "Episode reward: -1.041\n",
      "Average of last 10 rewards: -1.071272908366534\n",
      "Finished episode 7551 after 468299 timesteps\n",
      "Episode reward: -1.141\n",
      "Average of last 10 rewards: -1.0712575596816978\n",
      "Finished episode 7561 after 469032 timesteps\n",
      "Episode reward: -1.102\n",
      "Average of last 10 rewards: -1.071331523178808\n",
      "Finished episode 7571 after 469574 timesteps\n",
      "Episode reward: -1.157\n",
      "Average of last 10 rewards: -1.0712445767195768\n",
      "Finished episode 7581 after 470378 timesteps\n",
      "Episode reward: -1.09\n",
      "Average of last 10 rewards: -1.0713397622192866\n",
      "Finished episode 7591 after 471573 timesteps\n",
      "Episode reward: -1.3290000000000002\n",
      "Average of last 10 rewards: -1.0715341688654354\n",
      "Finished episode 7601 after 472021 timesteps\n",
      "Episode reward: -1.124\n",
      "Average of last 10 rewards: -1.0715392621870883\n",
      "Finished episode 7611 after 472572 timesteps\n",
      "Episode reward: -1.124\n",
      "Average of last 10 rewards: -1.0714693421052632\n",
      "Finished episode 7621 after 473278 timesteps\n",
      "Episode reward: -1.075\n",
      "Average of last 10 rewards: -1.0715038107752957\n",
      "INFO:tensorflow:Saving checkpoints for 473969 into ./ppo/model/model.ckpt.\n",
      "Finished episode 7631 after 474032 timesteps\n",
      "Episode reward: -1.294\n",
      "Average of last 10 rewards: -1.0716006561679792\n",
      "Finished episode 7641 after 474489 timesteps\n",
      "Episode reward: -1.101\n",
      "Average of last 10 rewards: -1.0716250327653998\n",
      "Finished episode 7651 after 475194 timesteps\n",
      "Episode reward: -1.076\n",
      "Average of last 10 rewards: -1.071644895287958\n",
      "Finished episode 7661 after 475757 timesteps\n",
      "Episode reward: -1.064\n",
      "Average of last 10 rewards: -1.071678562091503\n",
      "Finished episode 7671 after 476379 timesteps\n",
      "Episode reward: -1.065\n",
      "Average of last 10 rewards: -1.0716795039164488\n",
      "Finished episode 7681 after 477154 timesteps\n",
      "Episode reward: -0.5210000000000001\n",
      "Average of last 10 rewards: -1.0715087353324642\n",
      "Finished episode 7691 after 477683 timesteps\n",
      "Episode reward: -1.2480000000000002\n",
      "Average of last 10 rewards: -1.0715701822916666\n",
      "Finished episode 7701 after 478260 timesteps\n",
      "Episode reward: -1.092\n",
      "Average of last 10 rewards: -1.0715429128738623\n",
      "Finished episode 7711 after 478877 timesteps\n",
      "Episode reward: -1.068\n",
      "Average of last 10 rewards: -1.071631948051948\n",
      "Finished episode 7721 after 479460 timesteps\n",
      "Episode reward: -1.137\n",
      "Average of last 10 rewards: -1.0715811932555124\n",
      "Finished episode 7731 after 480084 timesteps\n",
      "Episode reward: -1.1640000000000001\n",
      "Average of last 10 rewards: -1.0715955958549224\n",
      "Finished episode 7741 after 480570 timesteps\n",
      "Episode reward: -1.067\n",
      "Average of last 10 rewards: -1.071627296248383\n",
      "Finished episode 7751 after 481239 timesteps\n",
      "Episode reward: -1.076\n",
      "Average of last 10 rewards: -1.0717033591731264\n",
      "Finished episode 7761 after 481923 timesteps\n",
      "Episode reward: -1.143\n",
      "Average of last 10 rewards: -1.0717770322580646\n",
      "Finished episode 7771 after 482494 timesteps\n",
      "Episode reward: -1.1280000000000001\n",
      "Average of last 10 rewards: -1.0717556701030928\n",
      "Finished episode 7781 after 483514 timesteps\n",
      "Episode reward: -1.2060000000000002\n",
      "Average of last 10 rewards: -1.0718276705276704\n",
      "Finished episode 7791 after 484074 timesteps\n",
      "Episode reward: -0.4960000000000001\n",
      "Average of last 10 rewards: -1.071762082262211\n",
      "Finished episode 7801 after 484956 timesteps\n",
      "Episode reward: -1.1740000000000002\n",
      "Average of last 10 rewards: -1.0717853658536587\n",
      "Finished episode 7811 after 485400 timesteps\n",
      "Episode reward: -1.043\n",
      "Average of last 10 rewards: -1.0717929487179487\n",
      "Finished episode 7821 after 485971 timesteps\n",
      "Episode reward: -1.2440000000000002\n",
      "Average of last 10 rewards: -1.0718404609475032\n",
      "Finished episode 7831 after 486816 timesteps\n",
      "Episode reward: -1.3090000000000002\n",
      "Average of last 10 rewards: -1.0718598465473146\n",
      "INFO:tensorflow:Saving checkpoints for 486918 into ./ppo/model/model.ckpt.\n",
      "Finished episode 7841 after 487314 timesteps\n",
      "Episode reward: -1.043\n",
      "Average of last 10 rewards: -1.0718786717752236\n",
      "Finished episode 7851 after 487847 timesteps\n",
      "Episode reward: -1.044\n",
      "Average of last 10 rewards: -1.0718739795918368\n",
      "Finished episode 7861 after 488469 timesteps\n",
      "Episode reward: -1.1620000000000001\n",
      "Average of last 10 rewards: -1.0718354140127389\n",
      "Finished episode 7871 after 489028 timesteps\n",
      "Episode reward: -0.651\n",
      "Average of last 10 rewards: -1.0718119592875317\n",
      "Finished episode 7881 after 489589 timesteps\n",
      "Episode reward: -1.053\n",
      "Average of last 10 rewards: -1.0718573062261754\n",
      "Finished episode 7891 after 490374 timesteps\n",
      "Episode reward: -1.055\n",
      "Average of last 10 rewards: -1.071897081218274\n",
      "Finished episode 7901 after 491200 timesteps\n",
      "Episode reward: -1.2120000000000002\n",
      "Average of last 10 rewards: -1.07186628643853\n",
      "Finished episode 7911 after 491960 timesteps\n",
      "Episode reward: -1.1560000000000001\n",
      "Average of last 10 rewards: -1.071833670886076\n",
      "Finished episode 7921 after 492380 timesteps\n",
      "Episode reward: -1.061\n",
      "Average of last 10 rewards: -1.0718471554993678\n",
      "Finished episode 7931 after 492894 timesteps\n",
      "Episode reward: -1.096\n",
      "Average of last 10 rewards: -1.0717819444444443\n",
      "Finished episode 7941 after 493474 timesteps\n",
      "Episode reward: -1.3150000000000002\n",
      "Average of last 10 rewards: -1.0717766708701135\n",
      "Finished episode 7951 after 493865 timesteps\n",
      "Episode reward: -1.045\n",
      "Average of last 10 rewards: -1.07176838790932\n",
      "Finished episode 7961 after 494147 timesteps\n",
      "Episode reward: -1.038\n",
      "Average of last 10 rewards: -1.0717392452830188\n",
      "Finished episode 7971 after 494687 timesteps\n",
      "Episode reward: -1.07\n",
      "Average of last 10 rewards: -1.0717566582914575\n",
      "Finished episode 7981 after 495028 timesteps\n",
      "Episode reward: -1.037\n",
      "Average of last 10 rewards: -1.0717452948557091\n",
      "Finished episode 7991 after 495657 timesteps\n",
      "Episode reward: -1.039\n",
      "Average of last 10 rewards: -1.071809523809524\n",
      "Finished episode 8001 after 496091 timesteps\n",
      "Episode reward: -1.1400000000000001\n",
      "Average of last 10 rewards: -1.071825156445557\n",
      "Finished episode 8011 after 496493 timesteps\n",
      "Episode reward: -1.08\n",
      "Average of last 10 rewards: -1.07182075\n",
      "Finished episode 8021 after 497418 timesteps\n",
      "Episode reward: -1.114\n",
      "Average of last 10 rewards: -1.0717836454431962\n",
      "Finished episode 8031 after 498070 timesteps\n",
      "Episode reward: -1.123\n",
      "Average of last 10 rewards: -1.07183042394015\n",
      "Finished episode 8041 after 498937 timesteps\n",
      "Episode reward: -1.1940000000000002\n",
      "Average of last 10 rewards: -1.0719117061021173\n",
      "Finished episode 8051 after 499611 timesteps\n",
      "Episode reward: -1.085\n",
      "Average of last 10 rewards: -1.0719309701492539\n",
      "INFO:tensorflow:Saving checkpoints for 499999 into ./ppo/model/model.ckpt.\n",
      "Finished episode 8061 after 500532 timesteps\n",
      "Episode reward: -1.015\n",
      "Average of last 10 rewards: -1.0718713043478258\n",
      "Finished episode 8071 after 500908 timesteps\n",
      "Episode reward: -1.026\n",
      "Average of last 10 rewards: -1.0718665012406947\n",
      "Finished episode 8081 after 501560 timesteps\n",
      "Episode reward: -1.065\n",
      "Average of last 10 rewards: -1.0718273853779432\n",
      "Finished episode 8091 after 502113 timesteps\n",
      "Episode reward: -1.195\n",
      "Average of last 10 rewards: -1.0718217821782179\n",
      "Finished episode 8101 after 503029 timesteps\n",
      "Episode reward: -1.2680000000000002\n",
      "Average of last 10 rewards: -1.0718551297898642\n",
      "Finished episode 8111 after 503599 timesteps\n",
      "Episode reward: -1.059\n",
      "Average of last 10 rewards: -1.071878148148148\n",
      "Finished episode 8121 after 504570 timesteps\n",
      "Episode reward: -1.1260000000000001\n",
      "Average of last 10 rewards: -1.0719620221948214\n",
      "Finished episode 8131 after 505091 timesteps\n",
      "Episode reward: -1.084\n",
      "Average of last 10 rewards: -1.0719993842364532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 8141 after 505822 timesteps\n",
      "Episode reward: -1.049\n",
      "Average of last 10 rewards: -1.0720680196801968\n",
      "Finished episode 8151 after 506436 timesteps\n",
      "Episode reward: -1.076\n",
      "Average of last 10 rewards: -1.0721159705159706\n",
      "Finished episode 8161 after 506930 timesteps\n",
      "Episode reward: -1.143\n",
      "Average of last 10 rewards: -1.072130920245399\n",
      "Finished episode 8171 after 507548 timesteps\n",
      "Episode reward: -1.04\n",
      "Average of last 10 rewards: -1.0721708333333335\n",
      "Finished episode 8181 after 508330 timesteps\n",
      "Episode reward: -1.096\n",
      "Average of last 10 rewards: -1.0722571603427173\n",
      "Finished episode 8191 after 508908 timesteps\n",
      "Episode reward: -1.1760000000000002\n",
      "Average of last 10 rewards: -1.0722940097799512\n",
      "Finished episode 8201 after 509443 timesteps\n",
      "Episode reward: -1.085\n",
      "Average of last 10 rewards: -1.0723091575091575\n",
      "Finished episode 8211 after 510206 timesteps\n",
      "Episode reward: -1.1920000000000002\n",
      "Average of last 10 rewards: -1.0723219512195121\n",
      "Finished episode 8221 after 510541 timesteps\n",
      "Episode reward: -1.08\n",
      "Average of last 10 rewards: -1.0722233861144945\n",
      "Finished episode 8231 after 511238 timesteps\n",
      "Episode reward: -1.078\n",
      "Average of last 10 rewards: -1.0722996350364964\n",
      "Finished episode 8241 after 511678 timesteps\n",
      "Episode reward: -1.09\n",
      "Average of last 10 rewards: -1.0722652490886997\n",
      "Finished episode 8251 after 512094 timesteps\n",
      "Episode reward: -1.071\n",
      "Average of last 10 rewards: -1.0722756067961163\n",
      "Finished episode 8261 after 512814 timesteps\n",
      "Episode reward: -1.131\n",
      "Average of last 10 rewards: -1.0722839999999998\n",
      "Finished episode 8271 after 513566 timesteps\n",
      "Episode reward: -1.071\n",
      "Average of last 10 rewards: -1.0722726392251816\n",
      "INFO:tensorflow:Saving checkpoints for 513812 into ./ppo/model/model.ckpt.\n",
      "Finished episode 8281 after 514208 timesteps\n",
      "Episode reward: -1.054\n",
      "Average of last 10 rewards: -1.0723204353083433\n",
      "Finished episode 8291 after 514657 timesteps\n",
      "Episode reward: -1.06\n",
      "Average of last 10 rewards: -1.0723474637681158\n",
      "Finished episode 8301 after 515078 timesteps\n",
      "Episode reward: -1.052\n",
      "Average of last 10 rewards: -1.0723628468033775\n",
      "Finished episode 8311 after 515507 timesteps\n",
      "Episode reward: -1.1300000000000001\n",
      "Average of last 10 rewards: -1.0723156626506023\n",
      "Finished episode 8321 after 515907 timesteps\n",
      "Episode reward: -1.053\n",
      "Average of last 10 rewards: -1.0722771359807461\n",
      "Finished episode 8331 after 516240 timesteps\n",
      "Episode reward: -1.076\n",
      "Average of last 10 rewards: -1.072276923076923\n",
      "Finished episode 8341 after 516820 timesteps\n",
      "Episode reward: -1.122\n",
      "Average of last 10 rewards: -1.0722729891956782\n",
      "Finished episode 8351 after 517202 timesteps\n",
      "Episode reward: -1.063\n",
      "Average of last 10 rewards: -1.0722755395683452\n",
      "Finished episode 8361 after 517763 timesteps\n",
      "Episode reward: -1.054\n",
      "Average of last 10 rewards: -1.072214491017964\n",
      "Finished episode 8371 after 518227 timesteps\n",
      "Episode reward: -1.037\n",
      "Average of last 10 rewards: -1.0721749999999999\n",
      "Finished episode 8381 after 518664 timesteps\n",
      "Episode reward: -1.2510000000000001\n",
      "Average of last 10 rewards: -1.07213715651135\n",
      "Finished episode 8391 after 519093 timesteps\n",
      "Episode reward: -1.077\n",
      "Average of last 10 rewards: -1.072095584725537\n",
      "Finished episode 8401 after 519486 timesteps\n",
      "Episode reward: -1.082\n",
      "Average of last 10 rewards: -1.0721001191895114\n",
      "Finished episode 8411 after 520017 timesteps\n",
      "Episode reward: -1.065\n",
      "Average of last 10 rewards: -1.0720814285714284\n",
      "Finished episode 8421 after 520443 timesteps\n",
      "Episode reward: -1.071\n",
      "Average of last 10 rewards: -1.0720884661117716\n",
      "Finished episode 8431 after 520884 timesteps\n",
      "Episode reward: -1.2500000000000002\n",
      "Average of last 10 rewards: -1.072111995249406\n",
      "Finished episode 8441 after 521805 timesteps\n",
      "Episode reward: -1.12\n",
      "Average of last 10 rewards: -1.072164650059312\n",
      "Finished episode 8451 after 522163 timesteps\n",
      "Episode reward: -1.037\n",
      "Average of last 10 rewards: -1.0720611374407583\n",
      "Finished episode 8461 after 522576 timesteps\n",
      "Episode reward: -0.4820000000000001\n",
      "Average of last 10 rewards: -1.0719785798816568\n",
      "Finished episode 8471 after 523156 timesteps\n",
      "Episode reward: -1.045\n",
      "Average of last 10 rewards: -1.0719669030732861\n",
      "Finished episode 8481 after 523677 timesteps\n",
      "Episode reward: -1.067\n",
      "Average of last 10 rewards: -1.0718618654073198\n",
      "Finished episode 8491 after 524279 timesteps\n",
      "Episode reward: -1.054\n",
      "Average of last 10 rewards: -1.0718165094339622\n",
      "Finished episode 8501 after 525020 timesteps\n",
      "Episode reward: -1.096\n",
      "Average of last 10 rewards: -1.0718657243816254\n",
      "Finished episode 8511 after 525589 timesteps\n",
      "Episode reward: -1.05\n",
      "Average of last 10 rewards: -1.0719003529411764\n",
      "Finished episode 8521 after 526108 timesteps\n",
      "Episode reward: -1.062\n",
      "Average of last 10 rewards: -1.0719435957696828\n",
      "Finished episode 8531 after 526829 timesteps\n",
      "Episode reward: -1.043\n",
      "Average of last 10 rewards: -1.0720046948356807\n",
      "Finished episode 8541 after 527199 timesteps\n",
      "Episode reward: -1.073\n",
      "Average of last 10 rewards: -1.0719898007033997\n",
      "INFO:tensorflow:Saving checkpoints for 527673 into ./ppo/model/model.ckpt.\n",
      "Finished episode 8551 after 527937 timesteps\n",
      "Episode reward: -1.147\n",
      "Average of last 10 rewards: -1.071942857142857\n",
      "Finished episode 8561 after 528279 timesteps\n",
      "Episode reward: -1.124\n",
      "Average of last 10 rewards: -1.0719353216374268\n",
      "Finished episode 8571 after 528647 timesteps\n",
      "Episode reward: -1.061\n",
      "Average of last 10 rewards: -1.0719029205607475\n",
      "Finished episode 8581 after 529229 timesteps\n",
      "Episode reward: -1.1280000000000001\n",
      "Average of last 10 rewards: -1.0719630105017504\n",
      "Finished episode 8591 after 529724 timesteps\n",
      "Episode reward: -1.087\n",
      "Average of last 10 rewards: -1.0718937062937064\n",
      "Finished episode 8601 after 530378 timesteps\n",
      "Episode reward: -1.074\n",
      "Average of last 10 rewards: -1.0718414435389987\n",
      "Finished episode 8611 after 531070 timesteps\n",
      "Episode reward: -1.143\n",
      "Average of last 10 rewards: -1.0718647674418602\n",
      "Finished episode 8621 after 531384 timesteps\n",
      "Episode reward: -1.07\n",
      "Average of last 10 rewards: -1.071798838559814\n",
      "Finished episode 8631 after 532138 timesteps\n",
      "Episode reward: -1.048\n",
      "Average of last 10 rewards: -1.0717716937354986\n",
      "Finished episode 8641 after 532725 timesteps\n",
      "Episode reward: -1.075\n",
      "Average of last 10 rewards: -1.0717232908458865\n",
      "Finished episode 8651 after 533547 timesteps\n",
      "Episode reward: -1.102\n",
      "Average of last 10 rewards: -1.0717466435185186\n",
      "Finished episode 8661 after 534125 timesteps\n",
      "Episode reward: -1.1400000000000001\n",
      "Average of last 10 rewards: -1.0717705202312138\n",
      "Finished episode 8671 after 534783 timesteps\n",
      "Episode reward: -1.124\n",
      "Average of last 10 rewards: -1.0717271362586605\n",
      "Finished episode 8681 after 535483 timesteps\n",
      "Episode reward: -1.127\n",
      "Average of last 10 rewards: -1.0717782006920413\n",
      "Finished episode 8691 after 535810 timesteps\n",
      "Episode reward: -1.022\n",
      "Average of last 10 rewards: -1.0717198156682026\n",
      "Finished episode 8701 after 536412 timesteps\n",
      "Episode reward: -1.03\n",
      "Average of last 10 rewards: -1.07175247410817\n",
      "Finished episode 8711 after 536803 timesteps\n",
      "Episode reward: -1.037\n",
      "Average of last 10 rewards: -1.07171\n",
      "Finished episode 8721 after 537547 timesteps\n",
      "Episode reward: -1.047\n",
      "Average of last 10 rewards: -1.0717485648679677\n",
      "Finished episode 8731 after 538042 timesteps\n",
      "Episode reward: -1.054\n",
      "Average of last 10 rewards: -1.0717600917431191\n",
      "Finished episode 8741 after 538729 timesteps\n",
      "Episode reward: -1.102\n",
      "Average of last 10 rewards: -1.0718043528064145\n",
      "Finished episode 8751 after 539283 timesteps\n",
      "Episode reward: -1.044\n",
      "Average of last 10 rewards: -1.0716762013729977\n",
      "Finished episode 8761 after 539798 timesteps\n",
      "Episode reward: -1.049\n",
      "Average of last 10 rewards: -1.0715592\n",
      "Finished episode 8771 after 540360 timesteps\n",
      "Episode reward: -1.04\n",
      "Average of last 10 rewards: -1.0715425799086757\n",
      "Finished episode 8781 after 540762 timesteps\n",
      "Episode reward: -1.151\n",
      "Average of last 10 rewards: -1.0715442417331813\n",
      "Finished episode 8791 after 541272 timesteps\n",
      "Episode reward: -1.025\n",
      "Average of last 10 rewards: -1.071471298405467\n",
      "INFO:tensorflow:Saving checkpoints for 541832 into ./ppo/model/model.ckpt.\n",
      "Finished episode 8801 after 542093 timesteps\n",
      "Episode reward: -1.091\n",
      "Average of last 10 rewards: -1.0715095563139931\n",
      "Finished episode 8811 after 542661 timesteps\n",
      "Episode reward: -1.09\n",
      "Average of last 10 rewards: -1.0715572727272726\n",
      "Finished episode 8821 after 543191 timesteps\n",
      "Episode reward: -1.1340000000000001\n",
      "Average of last 10 rewards: -1.0715116912599318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 8831 after 543915 timesteps\n",
      "Episode reward: -1.081\n",
      "Average of last 10 rewards: -1.071544671201814\n",
      "Finished episode 8841 after 544294 timesteps\n",
      "Episode reward: -1.023\n",
      "Average of last 10 rewards: -1.0715347678369196\n",
      "Finished episode 8851 after 545036 timesteps\n",
      "Episode reward: -1.088\n",
      "Average of last 10 rewards: -1.0714157239819002\n",
      "Finished episode 8861 after 545606 timesteps\n",
      "Episode reward: -1.086\n",
      "Average of last 10 rewards: -1.0714131073446327\n",
      "Finished episode 8871 after 546280 timesteps\n",
      "Episode reward: -1.1340000000000001\n",
      "Average of last 10 rewards: -1.0714628668171557\n",
      "Finished episode 8881 after 547067 timesteps\n",
      "Episode reward: -1.029\n",
      "Average of last 10 rewards: -1.0714880496054113\n",
      "Finished episode 8891 after 547909 timesteps\n",
      "Episode reward: -1.139\n",
      "Average of last 10 rewards: -1.0715280405405405\n",
      "Finished episode 8901 after 548482 timesteps\n",
      "Episode reward: -1.069\n",
      "Average of last 10 rewards: -1.071573453318335\n",
      "Finished episode 8911 after 549225 timesteps\n",
      "Episode reward: -1.2610000000000001\n",
      "Average of last 10 rewards: -1.071645617977528\n",
      "Finished episode 8921 after 549715 timesteps\n",
      "Episode reward: -1.041\n",
      "Average of last 10 rewards: -1.0716237934904602\n",
      "Finished episode 8931 after 550433 timesteps\n",
      "Episode reward: -1.1580000000000001\n",
      "Average of last 10 rewards: -1.0715455156950673\n",
      "Finished episode 8941 after 551099 timesteps\n",
      "Episode reward: -1.047\n",
      "Average of last 10 rewards: -1.0715939529675251\n",
      "Finished episode 8951 after 551596 timesteps\n",
      "Episode reward: -1.041\n",
      "Average of last 10 rewards: -1.0713761744966441\n",
      "Finished episode 8961 after 552163 timesteps\n",
      "Episode reward: -1.137\n",
      "Average of last 10 rewards: -1.0713820111731842\n",
      "Finished episode 8971 after 552948 timesteps\n",
      "Episode reward: -1.229\n",
      "Average of last 10 rewards: -1.071467857142857\n",
      "Finished episode 8981 after 553408 timesteps\n",
      "Episode reward: -0.7590000000000001\n",
      "Average of last 10 rewards: -1.0714329988851727\n",
      "Finished episode 8991 after 553987 timesteps\n",
      "Episode reward: -0.6779999999999999\n",
      "Average of last 10 rewards: -1.0714198218262805\n",
      "Finished episode 9001 after 554606 timesteps\n",
      "Episode reward: -1.03\n",
      "Average of last 10 rewards: -1.0714710789766406\n",
      "Finished episode 9011 after 555186 timesteps\n",
      "Episode reward: -1.207\n",
      "Average of last 10 rewards: -1.0715024444444443\n",
      "INFO:tensorflow:Saving checkpoints for 555498 into ./ppo/model/model.ckpt.\n",
      "Finished episode 9021 after 555873 timesteps\n",
      "Episode reward: -1.08\n",
      "Average of last 10 rewards: -1.071566037735849\n",
      "Finished episode 9031 after 556339 timesteps\n",
      "Episode reward: -0.671\n",
      "Average of last 10 rewards: -1.0715565410199555\n",
      "Finished episode 9041 after 557003 timesteps\n",
      "Episode reward: -1.117\n",
      "Average of last 10 rewards: -1.071598006644518\n",
      "Finished episode 9051 after 557536 timesteps\n",
      "Episode reward: -1.163\n",
      "Average of last 10 rewards: -1.0716150442477876\n",
      "Finished episode 9061 after 558122 timesteps\n",
      "Episode reward: -1.046\n",
      "Average of last 10 rewards: -1.0716411049723755\n",
      "Finished episode 9071 after 558657 timesteps\n",
      "Episode reward: -1.052\n",
      "Average of last 10 rewards: -1.0716701986754966\n",
      "Finished episode 9081 after 559325 timesteps\n",
      "Episode reward: -1.023\n",
      "Average of last 10 rewards: -1.0716753031973538\n",
      "Finished episode 9091 after 560339 timesteps\n",
      "Episode reward: -1.3110000000000002\n",
      "Average of last 10 rewards: -1.0717650881057268\n",
      "Finished episode 9101 after 561198 timesteps\n",
      "Episode reward: -1.108\n",
      "Average of last 10 rewards: -1.0717598459845983\n",
      "Finished episode 9111 after 561984 timesteps\n",
      "Episode reward: -0.9100000000000001\n",
      "Average of last 10 rewards: -1.071790989010989\n",
      "Finished episode 9121 after 562595 timesteps\n",
      "Episode reward: -1.083\n",
      "Average of last 10 rewards: -1.0718478594950602\n",
      "Finished episode 9131 after 563352 timesteps\n",
      "Episode reward: -1.03\n",
      "Average of last 10 rewards: -1.0718252192982454\n",
      "Finished episode 9141 after 563998 timesteps\n",
      "Episode reward: -1.3650000000000002\n",
      "Average of last 10 rewards: -1.071859474260679\n",
      "Finished episode 9151 after 564580 timesteps\n",
      "Episode reward: -1.1500000000000001\n",
      "Average of last 10 rewards: -1.0718615973741792\n",
      "Finished episode 9161 after 565393 timesteps\n",
      "Episode reward: -1.043\n",
      "Average of last 10 rewards: -1.0718697267759563\n",
      "Finished episode 9171 after 565882 timesteps\n",
      "Episode reward: -1.047\n",
      "Average of last 10 rewards: -1.071897270742358\n",
      "Finished episode 9181 after 566679 timesteps\n",
      "Episode reward: -1.2240000000000002\n",
      "Average of last 10 rewards: -1.0719811341330425\n",
      "Finished episode 9191 after 567449 timesteps\n",
      "Episode reward: -1.042\n",
      "Average of last 10 rewards: -1.0719503267973856\n",
      "Finished episode 9201 after 567958 timesteps\n",
      "Episode reward: -1.109\n",
      "Average of last 10 rewards: -1.0719712731229596\n",
      "Finished episode 9211 after 568460 timesteps\n",
      "Episode reward: -1.079\n",
      "Average of last 10 rewards: -1.0719852173913043\n",
      "INFO:tensorflow:Saving checkpoints for 569023 into ./ppo/model/model.ckpt.\n",
      "Finished episode 9221 after 569024 timesteps\n",
      "Episode reward: -1.2970000000000002\n",
      "Average of last 10 rewards: -1.071942345276873\n",
      "Finished episode 9231 after 569765 timesteps\n",
      "Episode reward: -1.047\n",
      "Average of last 10 rewards: -1.0719325379609543\n",
      "Finished episode 9241 after 570244 timesteps\n",
      "Episode reward: -0.6719999999999999\n",
      "Average of last 10 rewards: -1.0718803900325025\n",
      "Finished episode 9251 after 570990 timesteps\n",
      "Episode reward: -1.2960000000000003\n",
      "Average of last 10 rewards: -1.0719482683982684\n",
      "Finished episode 9261 after 571693 timesteps\n",
      "Episode reward: -1.145\n",
      "Average of last 10 rewards: -1.0720233513513513\n",
      "Finished episode 9271 after 572590 timesteps\n",
      "Episode reward: -1.129\n",
      "Average of last 10 rewards: -1.0720199784017277\n",
      "Finished episode 9281 after 573094 timesteps\n",
      "Episode reward: -1.201\n",
      "Average of last 10 rewards: -1.0720527508090616\n",
      "Finished episode 9291 after 573998 timesteps\n",
      "Episode reward: -1.081\n",
      "Average of last 10 rewards: -1.072173922413793\n",
      "Finished episode 9301 after 574410 timesteps\n",
      "Episode reward: -1.078\n",
      "Average of last 10 rewards: -1.072157158234661\n",
      "Finished episode 9311 after 575048 timesteps\n",
      "Episode reward: -1.067\n",
      "Average of last 10 rewards: -1.0721724731182796\n",
      "Finished episode 9321 after 575491 timesteps\n",
      "Episode reward: -1.033\n",
      "Average of last 10 rewards: -1.0721542427497315\n",
      "Finished episode 9331 after 576134 timesteps\n",
      "Episode reward: -1.2240000000000002\n",
      "Average of last 10 rewards: -1.0721309012875535\n",
      "Finished episode 9341 after 576826 timesteps\n",
      "Episode reward: -1.062\n",
      "Average of last 10 rewards: -1.0721972132904607\n",
      "Finished episode 9351 after 577548 timesteps\n",
      "Episode reward: -1.1340000000000001\n",
      "Average of last 10 rewards: -1.0722327623126338\n",
      "Finished episode 9361 after 578320 timesteps\n",
      "Episode reward: -1.055\n",
      "Average of last 10 rewards: -1.0722048128342245\n",
      "Finished episode 9371 after 579032 timesteps\n",
      "Episode reward: -1.149\n",
      "Average of last 10 rewards: -1.0722751068376066\n",
      "Finished episode 9381 after 579569 timesteps\n",
      "Episode reward: -1.031\n",
      "Average of last 10 rewards: -1.0722738527214513\n",
      "Finished episode 9391 after 580260 timesteps\n",
      "Episode reward: -1.088\n",
      "Average of last 10 rewards: -1.072316524520256\n",
      "Finished episode 9401 after 580802 timesteps\n",
      "Episode reward: -1.069\n",
      "Average of last 10 rewards: -1.072361341853035\n",
      "Finished episode 9411 after 581267 timesteps\n",
      "Episode reward: -1.106\n",
      "Average of last 10 rewards: -1.0723477659574467\n",
      "Finished episode 9421 after 581860 timesteps\n",
      "Episode reward: -1.243\n",
      "Average of last 10 rewards: -1.0723844845908608\n",
      "Finished episode 9431 after 582467 timesteps\n",
      "Episode reward: -0.9800000000000002\n",
      "Average of last 10 rewards: -1.0723859872611465\n",
      "INFO:tensorflow:Saving checkpoints for 582821 into ./ppo/model/model.ckpt.\n",
      "Finished episode 9441 after 583252 timesteps\n",
      "Episode reward: -1.057\n",
      "Average of last 10 rewards: -1.0724185577942735\n",
      "Finished episode 9451 after 584058 timesteps\n",
      "Episode reward: -1.119\n",
      "Average of last 10 rewards: -1.0725064618644067\n",
      "Finished episode 9461 after 584756 timesteps\n",
      "Episode reward: -1.1700000000000002\n",
      "Average of last 10 rewards: -1.072551216931217\n",
      "Finished episode 9471 after 585383 timesteps\n",
      "Episode reward: -1.033\n",
      "Average of last 10 rewards: -1.0725542283298097\n",
      "Finished episode 9481 after 585766 timesteps\n",
      "Episode reward: -1.069\n",
      "Average of last 10 rewards: -1.0725105596620907\n",
      "Finished episode 9491 after 586333 timesteps\n",
      "Episode reward: -1.175\n",
      "Average of last 10 rewards: -1.072501371308017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 9501 after 587138 timesteps\n",
      "Episode reward: -0.806\n",
      "Average of last 10 rewards: -1.07244531085353\n",
      "Finished episode 9511 after 587735 timesteps\n",
      "Episode reward: -1.05\n",
      "Average of last 10 rewards: -1.072446\n",
      "Finished episode 9521 after 588200 timesteps\n",
      "Episode reward: -1.111\n",
      "Average of last 10 rewards: -1.0724211356466875\n",
      "Finished episode 9531 after 588856 timesteps\n",
      "Episode reward: -1.282\n",
      "Average of last 10 rewards: -1.0724713235294117\n",
      "Finished episode 9541 after 589448 timesteps\n",
      "Episode reward: -1.1540000000000001\n",
      "Average of last 10 rewards: -1.0724895068205664\n",
      "Finished episode 9551 after 589882 timesteps\n",
      "Episode reward: -1.12\n",
      "Average of last 10 rewards: -1.0724912997903564\n",
      "Finished episode 9561 after 590381 timesteps\n",
      "Episode reward: -0.75\n",
      "Average of last 10 rewards: -1.0724633507853403\n",
      "Finished episode 9571 after 590991 timesteps\n",
      "Episode reward: -1.101\n",
      "Average of last 10 rewards: -1.0723748953974894\n",
      "Finished episode 9581 after 591436 timesteps\n",
      "Episode reward: -1.083\n",
      "Average of last 10 rewards: -1.0722983281086729\n",
      "Finished episode 9591 after 592083 timesteps\n",
      "Episode reward: -1.091\n",
      "Average of last 10 rewards: -1.072182254697286\n",
      "Finished episode 9601 after 592598 timesteps\n",
      "Episode reward: -1.1680000000000001\n",
      "Average of last 10 rewards: -1.072145568300313\n",
      "Finished episode 9611 after 593129 timesteps\n",
      "Episode reward: -1.081\n",
      "Average of last 10 rewards: -1.0721242708333334\n",
      "Finished episode 9621 after 594046 timesteps\n",
      "Episode reward: -1.037\n",
      "Average of last 10 rewards: -1.0721655567117585\n",
      "Finished episode 9631 after 594521 timesteps\n",
      "Episode reward: -1.022\n",
      "Average of last 10 rewards: -1.072185031185031\n",
      "Finished episode 9641 after 595127 timesteps\n",
      "Episode reward: -1.2560000000000002\n",
      "Average of last 10 rewards: -1.0722443406022846\n",
      "Finished episode 9651 after 595640 timesteps\n",
      "Episode reward: -1.046\n",
      "Average of last 10 rewards: -1.0722770746887966\n",
      "Finished episode 9661 after 596240 timesteps\n",
      "Episode reward: -1.103\n",
      "Average of last 10 rewards: -1.0722968911917097\n",
      "INFO:tensorflow:Saving checkpoints for 596660 into ./ppo/model/model.ckpt.\n",
      "Finished episode 9671 after 596877 timesteps\n",
      "Episode reward: -1.2850000000000001\n",
      "Average of last 10 rewards: -1.072286542443064\n",
      "Finished episode 9681 after 597656 timesteps\n",
      "Episode reward: -1.101\n",
      "Average of last 10 rewards: -1.0723570837642191\n",
      "Finished episode 9691 after 598310 timesteps\n",
      "Episode reward: -1.223\n",
      "Average of last 10 rewards: -1.0723510330578512\n",
      "Finished episode 9701 after 598996 timesteps\n",
      "Episode reward: -1.039\n",
      "Average of last 10 rewards: -1.0723169246646027\n",
      "Finished episode 9711 after 599484 timesteps\n",
      "Episode reward: -1.03\n",
      "Average of last 10 rewards: -1.0723377319587628\n",
      "Finished episode 9721 after 599913 timesteps\n",
      "Episode reward: -1.033\n",
      "Average of last 10 rewards: -1.0723486096807415\n",
      "Finished episode 9731 after 600928 timesteps\n",
      "Episode reward: -1.112\n",
      "Average of last 10 rewards: -1.0724591563786008\n",
      "Finished episode 9741 after 601440 timesteps\n",
      "Episode reward: -0.669\n",
      "Average of last 10 rewards: -1.0724039054470709\n",
      "Finished episode 9751 after 602291 timesteps\n",
      "Episode reward: -1.043\n",
      "Average of last 10 rewards: -1.07238613963039\n",
      "Finished episode 9761 after 602940 timesteps\n",
      "Episode reward: -1.121\n",
      "Average of last 10 rewards: -1.0723187692307692\n",
      "Finished episode 9771 after 603707 timesteps\n",
      "Episode reward: -1.088\n",
      "Average of last 10 rewards: -1.0723097336065572\n",
      "Finished episode 9781 after 604336 timesteps\n",
      "Episode reward: -1.074\n",
      "Average of last 10 rewards: -1.0723022517911975\n",
      "Finished episode 9791 after 604944 timesteps\n",
      "Episode reward: -1.046\n",
      "Average of last 10 rewards: -1.0723394683026584\n",
      "Finished episode 9801 after 605453 timesteps\n",
      "Episode reward: -1.092\n",
      "Average of last 10 rewards: -1.0723623084780387\n",
      "Finished episode 9811 after 606398 timesteps\n",
      "Episode reward: -1.127\n",
      "Average of last 10 rewards: -1.0723262244897958\n",
      "Finished episode 9821 after 606861 timesteps\n",
      "Episode reward: -1.083\n",
      "Average of last 10 rewards: -1.0723521916411825\n",
      "Finished episode 9831 after 607359 timesteps\n",
      "Episode reward: -1.049\n",
      "Average of last 10 rewards: -1.0723669042769857\n",
      "Finished episode 9841 after 608034 timesteps\n",
      "Episode reward: -1.05\n",
      "Average of last 10 rewards: -1.072427466937945\n",
      "Finished episode 9851 after 608435 timesteps\n",
      "Episode reward: -1.057\n",
      "Average of last 10 rewards: -1.072389532520325\n",
      "Finished episode 9861 after 608878 timesteps\n",
      "Episode reward: -1.052\n",
      "Average of last 10 rewards: -1.0723221319796954\n",
      "Finished episode 9871 after 609391 timesteps\n",
      "Episode reward: -1.089\n",
      "Average of last 10 rewards: -1.0722773833671397\n",
      "Finished episode 9881 after 609754 timesteps\n",
      "Episode reward: -0.718\n",
      "Average of last 10 rewards: -1.072224924012158\n",
      "INFO:tensorflow:Saving checkpoints for 610253 into ./ppo/model/model.ckpt.\n",
      "Finished episode 9891 after 610254 timesteps\n",
      "Episode reward: -1.048\n",
      "Average of last 10 rewards: -1.0722085020242913\n",
      "Finished episode 9901 after 611129 timesteps\n",
      "Episode reward: -1.1920000000000002\n",
      "Average of last 10 rewards: -1.0722558139534883\n",
      "Finished episode 9911 after 611982 timesteps\n",
      "Episode reward: -1.026\n",
      "Average of last 10 rewards: -1.0722829292929292\n",
      "Finished episode 9921 after 612478 timesteps\n",
      "Episode reward: -1.069\n",
      "Average of last 10 rewards: -1.0722128153380424\n",
      "Finished episode 9931 after 613042 timesteps\n",
      "Episode reward: -1.094\n",
      "Average of last 10 rewards: -1.0722464717741935\n",
      "Finished episode 9941 after 613863 timesteps\n",
      "Episode reward: -1.054\n",
      "Average of last 10 rewards: -1.0721502517623362\n",
      "Finished episode 9951 after 614375 timesteps\n",
      "Episode reward: -1.099\n",
      "Average of last 10 rewards: -1.0721639839034205\n",
      "Finished episode 9961 after 614948 timesteps\n",
      "Episode reward: -0.742\n",
      "Average of last 10 rewards: -1.072168944723618\n",
      "Finished episode 9971 after 615451 timesteps\n",
      "Episode reward: -1.091\n",
      "Average of last 10 rewards: -1.0721076305220882\n",
      "Finished episode 9981 after 615875 timesteps\n",
      "Episode reward: -1.129\n",
      "Average of last 10 rewards: -1.0720660982948846\n",
      "Finished episode 9991 after 616521 timesteps\n",
      "Episode reward: -1.1520000000000001\n",
      "Average of last 10 rewards: -1.072083867735471\n",
      "Finished episode 10001 after 617022 timesteps\n",
      "Episode reward: -1.046\n",
      "Average of last 10 rewards: -1.0720585585585585\n",
      "Finished episode 10011 after 617674 timesteps\n",
      "Episode reward: -1.153\n",
      "Average of last 10 rewards: -1.0721055999999998\n",
      "Finished episode 10021 after 618490 timesteps\n",
      "Episode reward: -1.2320000000000002\n",
      "Average of last 10 rewards: -1.0720794205794206\n",
      "Finished episode 10031 after 619091 timesteps\n",
      "Episode reward: -1.082\n",
      "Average of last 10 rewards: -1.0721301397205587\n",
      "Finished episode 10041 after 619828 timesteps\n",
      "Episode reward: -1.019\n",
      "Average of last 10 rewards: -1.0721379860418743\n",
      "Finished episode 10051 after 620447 timesteps\n",
      "Episode reward: -1.021\n",
      "Average of last 10 rewards: -1.0721232071713145\n",
      "Finished episode 10061 after 621026 timesteps\n",
      "Episode reward: -1.119\n",
      "Average of last 10 rewards: -1.0721555223880597\n",
      "Finished episode 10071 after 621691 timesteps\n",
      "Episode reward: -1.205\n",
      "Average of last 10 rewards: -1.0722013916500994\n",
      "Finished episode 10081 after 622351 timesteps\n",
      "Episode reward: -0.8160000000000001\n",
      "Average of last 10 rewards: -1.0722137040714994\n",
      "Finished episode 10091 after 622980 timesteps\n",
      "Episode reward: -1.058\n",
      "Average of last 10 rewards: -1.0721782738095238\n",
      "Finished episode 10101 after 623912 timesteps\n",
      "Episode reward: -1.2000000000000002\n",
      "Average of last 10 rewards: -1.072217839444995\n",
      "INFO:tensorflow:Saving checkpoints for 624056 into ./ppo/model/model.ckpt.\n",
      "Finished episode 10111 after 624436 timesteps\n",
      "Episode reward: -1.039\n",
      "Average of last 10 rewards: -1.0721664356435643\n",
      "Finished episode 10121 after 625251 timesteps\n",
      "Episode reward: -1.034\n",
      "Average of last 10 rewards: -1.0721966369930762\n",
      "Finished episode 10131 after 626033 timesteps\n",
      "Episode reward: -1.081\n",
      "Average of last 10 rewards: -1.0722603754940712\n",
      "Finished episode 10141 after 626838 timesteps\n",
      "Episode reward: -1.094\n",
      "Average of last 10 rewards: -1.0722823297137216\n",
      "Finished episode 10151 after 627720 timesteps\n",
      "Episode reward: -1.089\n",
      "Average of last 10 rewards: -1.0723538461538462\n",
      "Finished episode 10161 after 628195 timesteps\n",
      "Episode reward: -1.044\n",
      "Average of last 10 rewards: -1.072311921182266\n",
      "Finished episode 10171 after 628911 timesteps\n",
      "Episode reward: -1.4520000000000004\n",
      "Average of last 10 rewards: -1.0723375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 10181 after 629376 timesteps\n",
      "Episode reward: -1.073\n",
      "Average of last 10 rewards: -1.0723455260570305\n",
      "Finished episode 10191 after 629923 timesteps\n",
      "Episode reward: -1.1620000000000001\n",
      "Average of last 10 rewards: -1.0723594302554027\n",
      "Finished episode 10201 after 630635 timesteps\n",
      "Episode reward: -1.086\n",
      "Average of last 10 rewards: -1.0724182531894013\n",
      "Finished episode 10211 after 631286 timesteps\n",
      "Episode reward: -1.121\n",
      "Average of last 10 rewards: -1.0724564705882353\n",
      "Finished episode 10221 after 632210 timesteps\n",
      "Episode reward: -1.0310000000000004\n",
      "Average of last 10 rewards: -1.0724255631733595\n",
      "Finished episode 10231 after 632823 timesteps\n",
      "Episode reward: -1.091\n",
      "Average of last 10 rewards: -1.072434637964775\n",
      "Finished episode 10241 after 633500 timesteps\n",
      "Episode reward: -1.076\n",
      "Average of last 10 rewards: -1.0724802541544476\n",
      "Finished episode 10251 after 633957 timesteps\n",
      "Episode reward: -1.086\n",
      "Average of last 10 rewards: -1.0724829101562499\n",
      "Finished episode 10261 after 634496 timesteps\n",
      "Episode reward: -1.082\n",
      "Average of last 10 rewards: -1.0725099512195122\n",
      "Finished episode 10271 after 635445 timesteps\n",
      "Episode reward: -1.3440000000000003\n",
      "Average of last 10 rewards: -1.072524074074074\n",
      "Finished episode 10281 after 636079 timesteps\n",
      "Episode reward: -1.036\n",
      "Average of last 10 rewards: -1.0725646543330087\n",
      "Finished episode 10291 after 636607 timesteps\n",
      "Episode reward: -1.121\n",
      "Average of last 10 rewards: -1.0725219844357976\n",
      "Finished episode 10301 after 637052 timesteps\n",
      "Episode reward: -1.065\n",
      "Average of last 10 rewards: -1.0724619047619048\n",
      "Finished episode 10311 after 637751 timesteps\n",
      "Episode reward: -1.044\n",
      "Average of last 10 rewards: -1.0724958252427184\n",
      "INFO:tensorflow:Saving checkpoints for 637752 into ./ppo/model/model.ckpt.\n",
      "Finished episode 10321 after 638282 timesteps\n",
      "Episode reward: -1.019\n",
      "Average of last 10 rewards: -1.0725085354025217\n",
      "Finished episode 10331 after 638812 timesteps\n",
      "Episode reward: -1.096\n",
      "Average of last 10 rewards: -1.0724857558139536\n",
      "Finished episode 10341 after 639422 timesteps\n",
      "Episode reward: -1.2300000000000002\n",
      "Average of last 10 rewards: -1.0725089060987416\n",
      "Finished episode 10351 after 639899 timesteps\n",
      "Episode reward: -1.071\n",
      "Average of last 10 rewards: -1.0724745647969052\n",
      "Finished episode 10361 after 640458 timesteps\n",
      "Episode reward: -1.1880000000000002\n",
      "Average of last 10 rewards: -1.0725048309178744\n",
      "Finished episode 10371 after 641044 timesteps\n",
      "Episode reward: -1.035\n",
      "Average of last 10 rewards: -1.0725254826254826\n",
      "Finished episode 10381 after 641585 timesteps\n",
      "Episode reward: -1.012\n",
      "Average of last 10 rewards: -1.07245737704918\n",
      "Finished episode 10391 after 642455 timesteps\n",
      "Episode reward: -1.1400000000000001\n",
      "Average of last 10 rewards: -1.0724504816955684\n",
      "Finished episode 10401 after 643097 timesteps\n",
      "Episode reward: -1.046\n",
      "Average of last 10 rewards: -1.0724813282001924\n",
      "Finished episode 10411 after 643727 timesteps\n",
      "Episode reward: -1.055\n",
      "Average of last 10 rewards: -1.0724842307692306\n",
      "Finished episode 10421 after 644166 timesteps\n",
      "Episode reward: -1.047\n",
      "Average of last 10 rewards: -1.0724927953890488\n",
      "Finished episode 10431 after 644817 timesteps\n",
      "Episode reward: -1.147\n",
      "Average of last 10 rewards: -1.0724417466410747\n",
      "Finished episode 10441 after 645362 timesteps\n",
      "Episode reward: -1.159\n",
      "Average of last 10 rewards: -1.0724297219558965\n",
      "Finished episode 10451 after 646231 timesteps\n",
      "Episode reward: -1.098\n",
      "Average of last 10 rewards: -1.0724293103448277\n",
      "Finished episode 10461 after 646740 timesteps\n",
      "Episode reward: -1.035\n",
      "Average of last 10 rewards: -1.0724371291866028\n",
      "Finished episode 10471 after 647446 timesteps\n",
      "Episode reward: -1.4340000000000002\n",
      "Average of last 10 rewards: -1.0724938814531548\n",
      "Finished episode 10481 after 648190 timesteps\n",
      "Episode reward: -1.037\n",
      "Average of last 10 rewards: -1.072548710601719\n",
      "Finished episode 10491 after 648709 timesteps\n",
      "Episode reward: -1.102\n",
      "Average of last 10 rewards: -1.0724912213740456\n",
      "Finished episode 10501 after 649264 timesteps\n",
      "Episode reward: -1.089\n",
      "Average of last 10 rewards: -1.0725103908484268\n",
      "Finished episode 10511 after 649970 timesteps\n",
      "Episode reward: -1.041\n",
      "Average of last 10 rewards: -1.072524857142857\n",
      "Finished episode 10521 after 650559 timesteps\n",
      "Episode reward: -1.071\n",
      "Average of last 10 rewards: -1.0724460513796383\n",
      "INFO:tensorflow:Saving checkpoints for 651090 into ./ppo/model/model.ckpt.\n",
      "Finished episode 10531 after 651493 timesteps\n",
      "Episode reward: -1.239\n",
      "Average of last 10 rewards: -1.0724139733840303\n",
      "Finished episode 10541 after 652349 timesteps\n",
      "Episode reward: -1.1600000000000001\n",
      "Average of last 10 rewards: -1.0723902184235519\n",
      "Finished episode 10551 after 652832 timesteps\n",
      "Episode reward: -1.033\n",
      "Average of last 10 rewards: -1.0724242884250474\n",
      "Finished episode 10561 after 653249 timesteps\n",
      "Episode reward: -1.059\n",
      "Average of last 10 rewards: -1.0723984834123224\n",
      "Finished episode 10571 after 653925 timesteps\n",
      "Episode reward: -0.756\n",
      "Average of last 10 rewards: -1.0723960227272729\n",
      "Finished episode 10581 after 654389 timesteps\n",
      "Episode reward: -1.106\n",
      "Average of last 10 rewards: -1.0723654683065278\n",
      "Finished episode 10591 after 655079 timesteps\n",
      "Episode reward: -1.12\n",
      "Average of last 10 rewards: -1.0724258034026464\n",
      "Finished episode 10601 after 655808 timesteps\n",
      "Episode reward: -1.109\n",
      "Average of last 10 rewards: -1.0724808309726157\n",
      "Finished episode 10611 after 656375 timesteps\n",
      "Episode reward: -1.1740000000000002\n",
      "Average of last 10 rewards: -1.0725083018867925\n",
      "Finished episode 10621 after 656873 timesteps\n",
      "Episode reward: -1.123\n",
      "Average of last 10 rewards: -1.0724986804901036\n",
      "Finished episode 10631 after 657283 timesteps\n",
      "Episode reward: -1.155\n",
      "Average of last 10 rewards: -1.0724631826741995\n",
      "Finished episode 10641 after 658004 timesteps\n",
      "Episode reward: -1.1360000000000001\n",
      "Average of last 10 rewards: -1.0724406396989652\n",
      "Finished episode 10651 after 658552 timesteps\n",
      "Episode reward: -1.085\n",
      "Average of last 10 rewards: -1.0723490601503758\n",
      "Finished episode 10661 after 659273 timesteps\n",
      "Episode reward: -1.031\n",
      "Average of last 10 rewards: -1.0723989671361502\n",
      "Finished episode 10671 after 659858 timesteps\n",
      "Episode reward: -1.015\n",
      "Average of last 10 rewards: -1.072387898686679\n",
      "Finished episode 10681 after 660455 timesteps\n",
      "Episode reward: -1.057\n",
      "Average of last 10 rewards: -1.07238519212746\n",
      "Finished episode 10691 after 661349 timesteps\n",
      "Episode reward: -1.107\n",
      "Average of last 10 rewards: -1.0723988764044943\n",
      "Finished episode 10701 after 661980 timesteps\n",
      "Episode reward: -1.042\n",
      "Average of last 10 rewards: -1.0724414405986904\n",
      "Finished episode 10711 after 662609 timesteps\n",
      "Episode reward: -1.058\n",
      "Average of last 10 rewards: -1.0724928037383177\n",
      "Finished episode 10721 after 663480 timesteps\n",
      "Episode reward: -0.5920000000000002\n",
      "Average of last 10 rewards: -1.0724958916900094\n",
      "Finished episode 10731 after 663976 timesteps\n",
      "Episode reward: -1.1620000000000001\n",
      "Average of last 10 rewards: -1.072527332089552\n",
      "INFO:tensorflow:Saving checkpoints for 664024 into ./ppo/model/model.ckpt.\n",
      "Finished episode 10741 after 664450 timesteps\n",
      "Episode reward: -1.031\n",
      "Average of last 10 rewards: -1.0725299161230195\n",
      "Finished episode 10751 after 664854 timesteps\n",
      "Episode reward: -1.096\n",
      "Average of last 10 rewards: -1.0724967411545623\n",
      "Finished episode 10761 after 665411 timesteps\n",
      "Episode reward: -1.11\n",
      "Average of last 10 rewards: -1.0725368372093023\n",
      "Finished episode 10771 after 666071 timesteps\n",
      "Episode reward: -1.051\n",
      "Average of last 10 rewards: -1.0724708178438662\n",
      "Finished episode 10781 after 666605 timesteps\n",
      "Episode reward: -1.086\n",
      "Average of last 10 rewards: -1.0725191272051995\n",
      "Finished episode 10791 after 667215 timesteps\n",
      "Episode reward: -1.1920000000000002\n",
      "Average of last 10 rewards: -1.072486085343228\n",
      "Finished episode 10801 after 667616 timesteps\n",
      "Episode reward: -1.042\n",
      "Average of last 10 rewards: -1.0724077849860982\n",
      "Finished episode 10811 after 668181 timesteps\n",
      "Episode reward: -1.118\n",
      "Average of last 10 rewards: -1.0724008333333332\n",
      "Finished episode 10821 after 668737 timesteps\n",
      "Episode reward: -1.053\n",
      "Average of last 10 rewards: -1.07240092506938\n",
      "Finished episode 10831 after 669659 timesteps\n",
      "Episode reward: -1.115\n",
      "Average of last 10 rewards: -1.0724559149722734\n",
      "Finished episode 10841 after 670301 timesteps\n",
      "Episode reward: -1.054\n",
      "Average of last 10 rewards: -1.0724323176361956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 10851 after 670996 timesteps\n",
      "Episode reward: -1.161\n",
      "Average of last 10 rewards: -1.0723689114391144\n",
      "Finished episode 10861 after 671735 timesteps\n",
      "Episode reward: -1.078\n",
      "Average of last 10 rewards: -1.0724280184331796\n",
      "Finished episode 10871 after 672622 timesteps\n",
      "Episode reward: -0.7070000000000001\n",
      "Average of last 10 rewards: -1.072446132596685\n",
      "Finished episode 10881 after 673125 timesteps\n",
      "Episode reward: -1.038\n",
      "Average of last 10 rewards: -1.0724731370745169\n",
      "Finished episode 10891 after 673708 timesteps\n",
      "Episode reward: -1.211\n",
      "Average of last 10 rewards: -1.0724269301470588\n",
      "Finished episode 10901 after 674502 timesteps\n",
      "Episode reward: -1.039\n",
      "Average of last 10 rewards: -1.0724706152433425\n",
      "Finished episode 10911 after 675084 timesteps\n",
      "Episode reward: -1.121\n",
      "Average of last 10 rewards: -1.0724543119266055\n",
      "Finished episode 10921 after 675725 timesteps\n",
      "Episode reward: -1.149\n",
      "Average of last 10 rewards: -1.0724561869844178\n",
      "Finished episode 10931 after 676518 timesteps\n",
      "Episode reward: -0.702\n",
      "Average of last 10 rewards: -1.0723718864468863\n",
      "Finished episode 10941 after 676868 timesteps\n",
      "Episode reward: -1.041\n",
      "Average of last 10 rewards: -1.0723654162854528\n",
      "INFO:tensorflow:Saving checkpoints for 676972 into ./ppo/model/model.ckpt.\n",
      "Finished episode 10951 after 677540 timesteps\n",
      "Episode reward: -1.061\n",
      "Average of last 10 rewards: -1.0723688299817185\n",
      "Finished episode 10961 after 678346 timesteps\n",
      "Episode reward: -1.094\n",
      "Average of last 10 rewards: -1.0723117808219176\n",
      "Finished episode 10971 after 678888 timesteps\n",
      "Episode reward: -1.114\n",
      "Average of last 10 rewards: -1.072327919708029\n",
      "Finished episode 10981 after 679618 timesteps\n",
      "Episode reward: -1.048\n",
      "Average of last 10 rewards: -1.0722967183226981\n",
      "Finished episode 10991 after 680249 timesteps\n",
      "Episode reward: -1.077\n",
      "Average of last 10 rewards: -1.0723024590163934\n",
      "Finished episode 11001 after 680596 timesteps\n",
      "Episode reward: -1.039\n",
      "Average of last 10 rewards: -1.072299636032757\n",
      "Finished episode 11011 after 681049 timesteps\n",
      "Episode reward: -1.062\n",
      "Average of last 10 rewards: -1.072305090909091\n",
      "Finished episode 11021 after 681833 timesteps\n",
      "Episode reward: -1.044\n",
      "Average of last 10 rewards: -1.0723077202543143\n",
      "Finished episode 11031 after 682643 timesteps\n",
      "Episode reward: -1.1720000000000002\n",
      "Average of last 10 rewards: -1.0723147912885662\n",
      "Finished episode 11041 after 683234 timesteps\n",
      "Episode reward: -1.03\n",
      "Average of last 10 rewards: -1.0723140525838621\n",
      "Finished episode 11051 after 683800 timesteps\n",
      "Episode reward: -1.073\n",
      "Average of last 10 rewards: -1.0723364130434783\n",
      "Finished episode 11061 after 684486 timesteps\n",
      "Episode reward: -1.2910000000000001\n",
      "Average of last 10 rewards: -1.0723165610859726\n",
      "Finished episode 11071 after 684933 timesteps\n",
      "Episode reward: -1.033\n",
      "Average of last 10 rewards: -1.0722868896925857\n",
      "Finished episode 11081 after 685411 timesteps\n",
      "Episode reward: -1.25\n",
      "Average of last 10 rewards: -1.072275519421861\n",
      "Finished episode 11091 after 685948 timesteps\n",
      "Episode reward: -1.2770000000000001\n",
      "Average of last 10 rewards: -1.0722769855595669\n",
      "Finished episode 11101 after 686597 timesteps\n",
      "Episode reward: -1.157\n",
      "Average of last 10 rewards: -1.0723193868349863\n",
      "Finished episode 11111 after 687144 timesteps\n",
      "Episode reward: -1.08\n",
      "Average of last 10 rewards: -1.0723002702702702\n",
      "Finished episode 11121 after 687791 timesteps\n",
      "Episode reward: -1.051\n",
      "Average of last 10 rewards: -1.0723354635463545\n",
      "Finished episode 11131 after 688308 timesteps\n",
      "Episode reward: -1.023\n",
      "Average of last 10 rewards: -1.072314928057554\n",
      "Finished episode 11141 after 688662 timesteps\n",
      "Episode reward: -1.133\n",
      "Average of last 10 rewards: -1.0723089847259657\n",
      "Finished episode 11151 after 689280 timesteps\n",
      "Episode reward: -1.032\n",
      "Average of last 10 rewards: -1.0723366247755832\n",
      "Finished episode 11161 after 689809 timesteps\n",
      "Episode reward: -1.1760000000000002\n",
      "Average of last 10 rewards: -1.07234466367713\n",
      "INFO:tensorflow:Saving checkpoints for 689916 into ./ppo/model/model.ckpt.\n",
      "Finished episode 11171 after 690506 timesteps\n",
      "Episode reward: -1.117\n",
      "Average of last 10 rewards: -1.0723581541218639\n",
      "Finished episode 11181 after 690873 timesteps\n",
      "Episode reward: -1.041\n",
      "Average of last 10 rewards: -1.0723240823634734\n",
      "Finished episode 11191 after 691440 timesteps\n",
      "Episode reward: -1.111\n",
      "Average of last 10 rewards: -1.0723161001788908\n",
      "Finished episode 11201 after 692121 timesteps\n",
      "Episode reward: -1.3650000000000002\n",
      "Average of last 10 rewards: -1.072296872207328\n",
      "Finished episode 11211 after 692735 timesteps\n",
      "Episode reward: -1.147\n",
      "Average of last 10 rewards: -1.072286607142857\n",
      "Finished episode 11221 after 693281 timesteps\n",
      "Episode reward: -1.099\n",
      "Average of last 10 rewards: -1.0723167707404102\n",
      "Finished episode 11231 after 694432 timesteps\n",
      "Episode reward: -1.094\n",
      "Average of last 10 rewards: -1.0723889483065954\n",
      "Finished episode 11241 after 694927 timesteps\n",
      "Episode reward: -1.057\n",
      "Average of last 10 rewards: -1.0723375779162956\n",
      "Finished episode 11251 after 695892 timesteps\n",
      "Episode reward: -1.07\n",
      "Average of last 10 rewards: -1.0723883451957297\n",
      "Finished episode 11261 after 696744 timesteps\n",
      "Episode reward: -1.2060000000000002\n",
      "Average of last 10 rewards: -1.072397511111111\n",
      "Finished episode 11271 after 697481 timesteps\n",
      "Episode reward: -1.063\n",
      "Average of last 10 rewards: -1.0724372113676732\n",
      "Finished episode 11281 after 698281 timesteps\n",
      "Episode reward: -1.117\n",
      "Average of last 10 rewards: -1.0724822537710736\n",
      "Finished episode 11291 after 698968 timesteps\n",
      "Episode reward: -1.2880000000000003\n",
      "Average of last 10 rewards: -1.0725082446808512\n",
      "Finished episode 11301 after 699402 timesteps\n",
      "Episode reward: -1.08\n",
      "Average of last 10 rewards: -1.0725170062001772\n",
      "Finished episode 11311 after 699922 timesteps\n",
      "Episode reward: -1.054\n",
      "Average of last 10 rewards: -1.0724945132743362\n",
      "Finished episode 11321 after 700397 timesteps\n",
      "Episode reward: -1.101\n",
      "Average of last 10 rewards: -1.0724881520778073\n",
      "Finished episode 11331 after 701122 timesteps\n",
      "Episode reward: -1.3110000000000002\n",
      "Average of last 10 rewards: -1.0725331272084806\n",
      "Finished episode 11341 after 701694 timesteps\n",
      "Episode reward: -1.2360000000000002\n",
      "Average of last 10 rewards: -1.07256266548985\n",
      "Finished episode 11351 after 702353 timesteps\n",
      "Episode reward: -1.118\n",
      "Average of last 10 rewards: -1.0725911816578482\n",
      "INFO:tensorflow:Saving checkpoints for 702943 into ./ppo/model/model.ckpt.\n",
      "Finished episode 11361 after 703324 timesteps\n",
      "Episode reward: -0.9290000000000002\n",
      "Average of last 10 rewards: -1.0725242290748898\n",
      "Finished episode 11371 after 704144 timesteps\n",
      "Episode reward: -1.095\n",
      "Average of last 10 rewards: -1.0725968309859155\n",
      "Finished episode 11381 after 704671 timesteps\n",
      "Episode reward: -0.679\n",
      "Average of last 10 rewards: -1.0725862796833774\n",
      "Finished episode 11391 after 705238 timesteps\n",
      "Episode reward: -0.687\n",
      "Average of last 10 rewards: -1.0725011423550086\n",
      "Finished episode 11401 after 705927 timesteps\n",
      "Episode reward: -1.022\n",
      "Average of last 10 rewards: -1.0724020193151886\n",
      "Finished episode 11411 after 706675 timesteps\n",
      "Episode reward: -1.063\n",
      "Average of last 10 rewards: -1.0724644736842104\n",
      "Finished episode 11421 after 707530 timesteps\n",
      "Episode reward: -1.137\n",
      "Average of last 10 rewards: -1.0724671340929008\n",
      "Finished episode 11431 after 708068 timesteps\n",
      "Episode reward: -1.122\n",
      "Average of last 10 rewards: -1.07245\n",
      "Finished episode 11441 after 708760 timesteps\n",
      "Episode reward: -1.112\n",
      "Average of last 10 rewards: -1.0724524934383202\n",
      "Finished episode 11451 after 709683 timesteps\n",
      "Episode reward: -1.064\n",
      "Average of last 10 rewards: -1.0725430944055945\n",
      "Finished episode 11461 after 710217 timesteps\n",
      "Episode reward: -1.048\n",
      "Average of last 10 rewards: -1.0725692576419215\n",
      "Finished episode 11471 after 710570 timesteps\n",
      "Episode reward: -1.079\n",
      "Average of last 10 rewards: -1.0725656195462476\n",
      "Finished episode 11481 after 711304 timesteps\n",
      "Episode reward: -1.032\n",
      "Average of last 10 rewards: -1.0725616390584132\n",
      "Finished episode 11491 after 711862 timesteps\n",
      "Episode reward: -1.106\n",
      "Average of last 10 rewards: -1.0725979094076654\n",
      "Finished episode 11501 after 712600 timesteps\n",
      "Episode reward: -1.047\n",
      "Average of last 10 rewards: -1.0725691906005221\n",
      "Finished episode 11511 after 713285 timesteps\n",
      "Episode reward: -1.068\n",
      "Average of last 10 rewards: -1.0725761739130435\n",
      "Finished episode 11521 after 713805 timesteps\n",
      "Episode reward: -0.788\n",
      "Average of last 10 rewards: -1.0725698523023457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 11531 after 714387 timesteps\n",
      "Episode reward: -1.086\n",
      "Average of last 10 rewards: -1.072487673611111\n",
      "Finished episode 11541 after 715014 timesteps\n",
      "Episode reward: -1.068\n",
      "Average of last 10 rewards: -1.0724888117953164\n",
      "Finished episode 11551 after 715741 timesteps\n",
      "Episode reward: -1.03\n",
      "Average of last 10 rewards: -1.0724742634315423\n",
      "INFO:tensorflow:Saving checkpoints for 715969 into ./ppo/model/model.ckpt.\n",
      "Finished episode 11561 after 716221 timesteps\n",
      "Episode reward: -1.106\n",
      "Average of last 10 rewards: -1.0724526406926407\n",
      "Finished episode 11571 after 717160 timesteps\n",
      "Episode reward: -1.051\n",
      "Average of last 10 rewards: -1.0724175605536332\n",
      "Finished episode 11581 after 717962 timesteps\n",
      "Episode reward: -1.109\n",
      "Average of last 10 rewards: -1.0723778738115817\n",
      "Finished episode 11591 after 718395 timesteps\n",
      "Episode reward: -1.147\n",
      "Average of last 10 rewards: -1.072389896373057\n",
      "Finished episode 11601 after 718800 timesteps\n",
      "Episode reward: -1.099\n",
      "Average of last 10 rewards: -1.072404400345125\n",
      "Finished episode 11611 after 719461 timesteps\n",
      "Episode reward: -1.3280000000000003\n",
      "Average of last 10 rewards: -1.0723886206896551\n",
      "Finished episode 11621 after 719936 timesteps\n",
      "Episode reward: -1.067\n",
      "Average of last 10 rewards: -1.0723541774332472\n",
      "Finished episode 11631 after 720656 timesteps\n",
      "Episode reward: -1.036\n",
      "Average of last 10 rewards: -1.0722552495697073\n",
      "Finished episode 11641 after 721176 timesteps\n",
      "Episode reward: -1.097\n",
      "Average of last 10 rewards: -1.07226543422184\n",
      "Finished episode 11651 after 721868 timesteps\n",
      "Episode reward: -1.103\n",
      "Average of last 10 rewards: -1.0723168384879724\n",
      "Finished episode 11661 after 722580 timesteps\n",
      "Episode reward: -1.087\n",
      "Average of last 10 rewards: -1.0723377682403432\n",
      "Finished episode 11671 after 722987 timesteps\n",
      "Episode reward: -1.129\n",
      "Average of last 10 rewards: -1.0723359348198969\n",
      "Finished episode 11681 after 723552 timesteps\n",
      "Episode reward: -1.057\n",
      "Average of last 10 rewards: -1.0722883461868038\n",
      "Finished episode 11691 after 724031 timesteps\n",
      "Episode reward: -1.145\n",
      "Average of last 10 rewards: -1.0723123287671232\n",
      "Finished episode 11701 after 724658 timesteps\n",
      "Episode reward: -1.2990000000000002\n",
      "Average of last 10 rewards: -1.0723591958939263\n",
      "Finished episode 11711 after 725226 timesteps\n",
      "Episode reward: -1.049\n",
      "Average of last 10 rewards: -1.072392735042735\n",
      "Finished episode 11721 after 725752 timesteps\n",
      "Episode reward: -1.078\n",
      "Average of last 10 rewards: -1.072340222032451\n",
      "Finished episode 11731 after 726656 timesteps\n",
      "Episode reward: -1.104\n",
      "Average of last 10 rewards: -1.0723403583617748\n",
      "Finished episode 11741 after 727137 timesteps\n",
      "Episode reward: -1.3860000000000001\n",
      "Average of last 10 rewards: -1.0723241261722078\n",
      "Finished episode 11751 after 727611 timesteps\n",
      "Episode reward: -1.103\n",
      "Average of last 10 rewards: -1.0723490630323678\n",
      "Finished episode 11761 after 728318 timesteps\n",
      "Episode reward: -1.2060000000000002\n",
      "Average of last 10 rewards: -1.072384\n",
      "Finished episode 11771 after 728810 timesteps\n",
      "Episode reward: -1.067\n",
      "Average of last 10 rewards: -1.0723635204081632\n",
      "INFO:tensorflow:Saving checkpoints for 729070 into ./ppo/model/model.ckpt.\n",
      "Finished episode 11781 after 729557 timesteps\n",
      "Episode reward: -1.2200000000000002\n",
      "Average of last 10 rewards: -1.0723844519966015\n",
      "Finished episode 11791 after 730093 timesteps\n",
      "Episode reward: -1.167\n",
      "Average of last 10 rewards: -1.0723673174872665\n",
      "Finished episode 11801 after 730642 timesteps\n",
      "Episode reward: -1.036\n",
      "Average of last 10 rewards: -1.0723233248515691\n",
      "Finished episode 11811 after 731401 timesteps\n",
      "Episode reward: -1.1620000000000001\n",
      "Average of last 10 rewards: -1.0723\n",
      "Finished episode 11821 after 731833 timesteps\n",
      "Episode reward: -1.053\n",
      "Average of last 10 rewards: -1.0722740050804402\n",
      "Finished episode 11831 after 732498 timesteps\n",
      "Episode reward: -1.153\n",
      "Average of last 10 rewards: -1.0723013536379018\n",
      "Finished episode 11841 after 733071 timesteps\n",
      "Episode reward: -1.034\n",
      "Average of last 10 rewards: -1.0722951817413355\n",
      "Finished episode 11851 after 733642 timesteps\n",
      "Episode reward: -1.09\n",
      "Average of last 10 rewards: -1.072293918918919\n",
      "Finished episode 11861 after 734083 timesteps\n",
      "Episode reward: -1.203\n",
      "Average of last 10 rewards: -1.0722698734177214\n",
      "Finished episode 11871 after 734686 timesteps\n",
      "Episode reward: -1.069\n",
      "Average of last 10 rewards: -1.0722658516020236\n",
      "Finished episode 11881 after 735351 timesteps\n",
      "Episode reward: -0.694\n",
      "Average of last 10 rewards: -1.0722482729570344\n",
      "Finished episode 11891 after 735820 timesteps\n",
      "Episode reward: -1.078\n",
      "Average of last 10 rewards: -1.0722242424242423\n",
      "Finished episode 11901 after 736240 timesteps\n",
      "Episode reward: -1.085\n",
      "Average of last 10 rewards: -1.0722384356602186\n",
      "Finished episode 11911 after 736681 timesteps\n",
      "Episode reward: -1.117\n",
      "Average of last 10 rewards: -1.0722110924369748\n",
      "Finished episode 11921 after 737217 timesteps\n",
      "Episode reward: -1.096\n",
      "Average of last 10 rewards: -1.0721869857262805\n",
      "Finished episode 11931 after 738062 timesteps\n",
      "Episode reward: -1.3500000000000005\n",
      "Average of last 10 rewards: -1.0721911073825503\n",
      "Finished episode 11941 after 738723 timesteps\n",
      "Episode reward: -1.1400000000000001\n",
      "Average of last 10 rewards: -1.0722042749371332\n",
      "Finished episode 11951 after 739110 timesteps\n",
      "Episode reward: -1.061\n",
      "Average of last 10 rewards: -1.0722077889447237\n",
      "Finished episode 11961 after 739877 timesteps\n",
      "Episode reward: -0.8820000000000001\n",
      "Average of last 10 rewards: -1.0721938912133893\n",
      "Finished episode 12031 after 745376 timesteps\n",
      "Episode reward: -1.056\n",
      "Average of last 10 rewards: -1.0723227121464225\n",
      "Finished episode 12041 after 745911 timesteps\n",
      "Episode reward: -1.08\n",
      "Average of last 10 rewards: -1.0722758935993348\n",
      "Finished episode 12051 after 746436 timesteps\n",
      "Episode reward: -1.1360000000000001\n",
      "Average of last 10 rewards: -1.0722890365448503\n",
      "Finished episode 12061 after 746967 timesteps\n",
      "Episode reward: -1.054\n",
      "Average of last 10 rewards: -1.0723038174273858\n",
      "Finished episode 12071 after 747464 timesteps\n",
      "Episode reward: -1.102\n",
      "Average of last 10 rewards: -1.0722805140961857\n",
      "Finished episode 12081 after 748090 timesteps\n",
      "Episode reward: -1.039\n",
      "Average of last 10 rewards: -1.0722802816901407\n",
      "Finished episode 12091 after 748663 timesteps\n",
      "Episode reward: -1.2060000000000002\n",
      "Average of last 10 rewards: -1.07230380794702\n",
      "Finished episode 12101 after 749150 timesteps\n",
      "Episode reward: -1.064\n",
      "Average of last 10 rewards: -1.072260711331679\n",
      "Finished episode 12111 after 749814 timesteps\n",
      "Episode reward: -1.3040000000000003\n",
      "Average of last 10 rewards: -1.0723047933884298\n",
      "Finished episode 12121 after 750571 timesteps\n",
      "Episode reward: -1.1680000000000001\n",
      "Average of last 10 rewards: -1.0722787778695293\n",
      "Finished episode 12131 after 751005 timesteps\n",
      "Episode reward: -1.099\n",
      "Average of last 10 rewards: -1.0722945544554454\n",
      "Finished episode 12141 after 751634 timesteps\n",
      "Episode reward: -1.1920000000000002\n",
      "Average of last 10 rewards: -1.0723229183841714\n",
      "Finished episode 12151 after 752119 timesteps\n",
      "Episode reward: -1.06\n",
      "Average of last 10 rewards: -1.0722748764415155\n",
      "Finished episode 12161 after 752593 timesteps\n",
      "Episode reward: -1.1380000000000001\n",
      "Average of last 10 rewards: -1.072254732510288\n",
      "Finished episode 12171 after 753277 timesteps\n",
      "Episode reward: -1.06\n",
      "Average of last 10 rewards: -1.0721970394736842\n",
      "Finished episode 12181 after 753749 timesteps\n",
      "Episode reward: -1.096\n",
      "Average of last 10 rewards: -1.0722096959737057\n",
      "Finished episode 12191 after 754149 timesteps\n",
      "Episode reward: -1.034\n",
      "Average of last 10 rewards: -1.0721449096880131\n",
      "Finished episode 12201 after 754777 timesteps\n",
      "Episode reward: -1.096\n",
      "Average of last 10 rewards: -1.0721401968826907\n",
      "INFO:tensorflow:Saving checkpoints for 755136 into ./ppo/model/model.ckpt.\n",
      "Finished episode 12211 after 755459 timesteps\n",
      "Episode reward: -1.075\n",
      "Average of last 10 rewards: -1.0721459836065574\n",
      "Finished episode 12221 after 755891 timesteps\n",
      "Episode reward: -1.064\n",
      "Average of last 10 rewards: -1.0721514332514333\n",
      "Finished episode 12231 after 756360 timesteps\n",
      "Episode reward: -1.026\n",
      "Average of last 10 rewards: -1.0721585106382978\n",
      "Finished episode 12241 after 756857 timesteps\n",
      "Episode reward: -1.047\n",
      "Average of last 10 rewards: -1.0721313982011447\n",
      "Finished episode 12251 after 757690 timesteps\n",
      "Episode reward: -1.1580000000000001\n",
      "Average of last 10 rewards: -1.0721781862745097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 12261 after 758063 timesteps\n",
      "Episode reward: -1.093\n",
      "Average of last 10 rewards: -1.0721808163265307\n",
      "Finished episode 12271 after 758651 timesteps\n",
      "Episode reward: -1.11\n",
      "Average of last 10 rewards: -1.0722122349102774\n",
      "Finished episode 12281 after 759241 timesteps\n",
      "Episode reward: -0.679\n",
      "Average of last 10 rewards: -1.0722000814995925\n",
      "Finished episode 12291 after 759653 timesteps\n",
      "Episode reward: -1.015\n",
      "Average of last 10 rewards: -1.0722115635179152\n",
      "Finished episode 12301 after 760147 timesteps\n",
      "Episode reward: -1.058\n",
      "Average of last 10 rewards: -1.0721906427990235\n",
      "Finished episode 12311 after 760967 timesteps\n",
      "Episode reward: -0.8710000000000002\n",
      "Average of last 10 rewards: -1.0721734959349594\n",
      "Finished episode 12321 after 761477 timesteps\n",
      "Episode reward: -1.035\n",
      "Average of last 10 rewards: -1.0721838342810721\n",
      "Finished episode 12331 after 762213 timesteps\n",
      "Episode reward: -1.019\n",
      "Average of last 10 rewards: -1.072175081168831\n",
      "Finished episode 12341 after 763207 timesteps\n",
      "Episode reward: -1.1260000000000001\n",
      "Average of last 10 rewards: -1.0721353609083537\n",
      "Finished episode 12351 after 763925 timesteps\n",
      "Episode reward: -1.135\n",
      "Average of last 10 rewards: -1.0721822528363047\n",
      "Finished episode 12361 after 764374 timesteps\n",
      "Episode reward: -1.093\n",
      "Average of last 10 rewards: -1.0721898785425101\n",
      "Finished episode 12371 after 765231 timesteps\n",
      "Episode reward: -1.2200000000000002\n",
      "Average of last 10 rewards: -1.0722135113268607\n",
      "Finished episode 12381 after 765751 timesteps\n",
      "Episode reward: -1.059\n",
      "Average of last 10 rewards: -1.0722013742926435\n",
      "Finished episode 12391 after 766530 timesteps\n",
      "Episode reward: -1.06\n",
      "Average of last 10 rewards: -1.0721821486268175\n",
      "Finished episode 12401 after 767453 timesteps\n",
      "Episode reward: -1.062\n",
      "Average of last 10 rewards: -1.0722627118644068\n",
      "INFO:tensorflow:Saving checkpoints for 768197 into ./ppo/model/model.ckpt.\n",
      "Finished episode 12411 after 768202 timesteps\n",
      "Episode reward: -1.03\n",
      "Average of last 10 rewards: -1.0722435483870967\n",
      "Finished episode 12421 after 768608 timesteps\n",
      "Episode reward: -1.147\n",
      "Average of last 10 rewards: -1.0722135374697823\n",
      "Finished episode 12431 after 769417 timesteps\n",
      "Episode reward: -1.2440000000000002\n",
      "Average of last 10 rewards: -1.0721944444444444\n",
      "Finished episode 12441 after 770123 timesteps\n",
      "Episode reward: -1.1560000000000001\n",
      "Average of last 10 rewards: -1.0722680611423974\n",
      "Finished episode 12451 after 770717 timesteps\n",
      "Episode reward: -1.053\n",
      "Average of last 10 rewards: -1.0722979903536975\n",
      "Finished episode 12461 after 771301 timesteps\n",
      "Episode reward: -1.2950000000000002\n",
      "Average of last 10 rewards: -1.0723325301204818\n",
      "Finished episode 12471 after 772146 timesteps\n",
      "Episode reward: -1.114\n",
      "Average of last 10 rewards: -1.07229430176565\n",
      "Finished episode 12481 after 772740 timesteps\n",
      "Episode reward: -1.119\n",
      "Average of last 10 rewards: -1.072314835605453\n",
      "Finished episode 12491 after 773587 timesteps\n",
      "Episode reward: -1.115\n",
      "Average of last 10 rewards: -1.0723705128205128\n",
      "Finished episode 12501 after 774229 timesteps\n",
      "Episode reward: -1.1680000000000001\n",
      "Average of last 10 rewards: -1.0723652522017613\n",
      "Finished episode 12511 after 774717 timesteps\n",
      "Episode reward: -0.6479999999999999\n",
      "Average of last 10 rewards: -1.0723228\n",
      "Finished episode 12521 after 775454 timesteps\n",
      "Episode reward: -1.029\n",
      "Average of last 10 rewards: -1.072211990407674\n",
      "Finished episode 12531 after 775954 timesteps\n",
      "Episode reward: -1.073\n",
      "Average of last 10 rewards: -1.07219696485623\n",
      "Finished episode 12541 after 776505 timesteps\n",
      "Episode reward: -1.049\n",
      "Average of last 10 rewards: -1.0721772545889865\n",
      "Finished episode 12551 after 777174 timesteps\n",
      "Episode reward: -1.2930000000000001\n",
      "Average of last 10 rewards: -1.0722091706539076\n",
      "Finished episode 12561 after 777874 timesteps\n",
      "Episode reward: -1.045\n",
      "Average of last 10 rewards: -1.0722030278884462\n",
      "Finished episode 12571 after 778392 timesteps\n",
      "Episode reward: -1.071\n",
      "Average of last 10 rewards: -1.0721862261146498\n",
      "Finished episode 12581 after 779317 timesteps\n",
      "Episode reward: -1.1500000000000001\n",
      "Average of last 10 rewards: -1.0722265712012728\n",
      "Finished episode 12591 after 779895 timesteps\n",
      "Episode reward: -1.086\n",
      "Average of last 10 rewards: -1.0721611287758346\n",
      "Finished episode 12601 after 780467 timesteps\n",
      "Episode reward: -1.1580000000000001\n",
      "Average of last 10 rewards: -1.0721961874503574\n",
      "INFO:tensorflow:Saving checkpoints for 781032 into ./ppo/model/model.ckpt.\n",
      "Finished episode 12611 after 781274 timesteps\n",
      "Episode reward: -1.2080000000000002\n",
      "Average of last 10 rewards: -1.072240873015873\n",
      "Finished episode 12621 after 781866 timesteps\n",
      "Episode reward: -1.3970000000000002\n",
      "Average of last 10 rewards: -1.0722514670896115\n",
      "Finished episode 12631 after 782514 timesteps\n",
      "Episode reward: -1.1300000000000001\n",
      "Average of last 10 rewards: -1.0721486529318542\n",
      "Finished episode 12641 after 783141 timesteps\n",
      "Episode reward: -1.023\n",
      "Average of last 10 rewards: -1.0720802850356295\n",
      "Finished episode 12651 after 783888 timesteps\n",
      "Episode reward: -0.7770000000000001\n",
      "Average of last 10 rewards: -1.0721135284810126\n",
      "Finished episode 12661 after 784465 timesteps\n",
      "Episode reward: -1.1260000000000001\n",
      "Average of last 10 rewards: -1.0721072727272727\n",
      "Finished episode 12671 after 785201 timesteps\n",
      "Episode reward: -1.103\n",
      "Average of last 10 rewards: -1.0721055292259083\n",
      "Finished episode 12681 after 785955 timesteps\n",
      "Episode reward: -1.082\n",
      "Average of last 10 rewards: -1.0721579321231254\n",
      "Finished episode 12691 after 786491 timesteps\n",
      "Episode reward: -1.09\n",
      "Average of last 10 rewards: -1.0721890378548895\n",
      "Finished episode 12701 after 787054 timesteps\n",
      "Episode reward: -1.1460000000000001\n",
      "Average of last 10 rewards: -1.0722123719464145\n",
      "Finished episode 12711 after 787607 timesteps\n",
      "Episode reward: -1.058\n",
      "Average of last 10 rewards: -1.0722344881889765\n",
      "Finished episode 12721 after 788152 timesteps\n",
      "Episode reward: -0.7490000000000001\n",
      "Average of last 10 rewards: -1.072226042486231\n",
      "Finished episode 12731 after 788648 timesteps\n",
      "Episode reward: -1.25\n",
      "Average of last 10 rewards: -1.0722407232704403\n",
      "Finished episode 12741 after 789364 timesteps\n",
      "Episode reward: -0.7220000000000001\n",
      "Average of last 10 rewards: -1.0721890023566378\n",
      "Finished episode 12751 after 789820 timesteps\n",
      "Episode reward: -0.786\n",
      "Average of last 10 rewards: -1.072143092621664\n",
      "Finished episode 12761 after 790365 timesteps\n",
      "Episode reward: -1.057\n",
      "Average of last 10 rewards: -1.0720948235294117\n",
      "Finished episode 12771 after 791209 timesteps\n",
      "Episode reward: -1.059\n",
      "Average of last 10 rewards: -1.0720259404388714\n",
      "Finished episode 12781 after 791576 timesteps\n",
      "Episode reward: -1.046\n",
      "Average of last 10 rewards: -1.0720248238057948\n",
      "Finished episode 12791 after 792116 timesteps\n",
      "Episode reward: -1.052\n",
      "Average of last 10 rewards: -1.0720529733959312\n",
      "Finished episode 12801 after 792639 timesteps\n",
      "Episode reward: -1.075\n",
      "Average of last 10 rewards: -1.0720435496481624\n",
      "Finished episode 12811 after 793302 timesteps\n",
      "Episode reward: -0.9860000000000002\n",
      "Average of last 10 rewards: -1.072048125\n",
      "Finished episode 12821 after 793958 timesteps\n",
      "Episode reward: -1.139\n",
      "Average of last 10 rewards: -1.0720910226385636\n",
      "INFO:tensorflow:Saving checkpoints for 794075 into ./ppo/model/model.ckpt.\n",
      "Finished episode 12831 after 794483 timesteps\n",
      "Episode reward: -1.063\n",
      "Average of last 10 rewards: -1.0720816692667707\n",
      "Finished episode 12841 after 794982 timesteps\n",
      "Episode reward: -1.105\n",
      "Average of last 10 rewards: -1.072065003897116\n",
      "Finished episode 12851 after 795622 timesteps\n",
      "Episode reward: -1.045\n",
      "Average of last 10 rewards: -1.0720857476635512\n",
      "Finished episode 12861 after 796174 timesteps\n",
      "Episode reward: -1.1460000000000001\n",
      "Average of last 10 rewards: -1.0720754085603112\n",
      "Finished episode 12871 after 796699 timesteps\n",
      "Episode reward: -1.145\n",
      "Average of last 10 rewards: -1.0720098755832037\n",
      "Finished episode 12881 after 797247 timesteps\n",
      "Episode reward: -1.179\n",
      "Average of last 10 rewards: -1.0719923853923854\n",
      "Finished episode 12891 after 797788 timesteps\n",
      "Episode reward: -1.061\n",
      "Average of last 10 rewards: -1.072007298136646\n",
      "Finished episode 12901 after 798415 timesteps\n",
      "Episode reward: -1.054\n",
      "Average of last 10 rewards: -1.0720173778122575\n",
      "Finished episode 12911 after 798925 timesteps\n",
      "Episode reward: -1.037\n",
      "Average of last 10 rewards: -1.072030465116279\n",
      "Finished episode 12921 after 799917 timesteps\n",
      "Episode reward: -0.5260000000000002\n",
      "Average of last 10 rewards: -1.0719448489542989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 12931 after 800888 timesteps\n",
      "Episode reward: -0.671\n",
      "Average of last 10 rewards: -1.0718631578947369\n",
      "Finished episode 12941 after 801353 timesteps\n",
      "Episode reward: -1.023\n",
      "Average of last 10 rewards: -1.0718372003093581\n",
      "Finished episode 12951 after 801943 timesteps\n",
      "Episode reward: -1.084\n",
      "Average of last 10 rewards: -1.0718374806800617\n",
      "Finished episode 12961 after 802514 timesteps\n",
      "Episode reward: -1.1540000000000001\n",
      "Average of last 10 rewards: -1.0718643243243244\n",
      "Finished episode 12971 after 803069 timesteps\n",
      "Episode reward: -1.045\n",
      "Average of last 10 rewards: -1.071858564814815\n",
      "Finished episode 12981 after 803471 timesteps\n",
      "Episode reward: -1.045\n",
      "Average of last 10 rewards: -1.0718297609868928\n",
      "Finished episode 12991 after 803938 timesteps\n",
      "Episode reward: -1.057\n",
      "Average of last 10 rewards: -1.0718441448382126\n",
      "Finished episode 13001 after 804655 timesteps\n",
      "Episode reward: -1.07\n",
      "Average of last 10 rewards: -1.0718685912240185\n",
      "Finished episode 13011 after 805447 timesteps\n",
      "Episode reward: -1.068\n",
      "Average of last 10 rewards: -1.071877076923077\n",
      "Finished episode 13021 after 805927 timesteps\n",
      "Episode reward: -1.147\n",
      "Average of last 10 rewards: -1.0718628747117602\n",
      "Finished episode 13031 after 806647 timesteps\n",
      "Episode reward: -1.139\n",
      "Average of last 10 rewards: -1.0718960061443932\n",
      "INFO:tensorflow:Saving checkpoints for 807166 into ./ppo/model/model.ckpt.\n",
      "Finished episode 13041 after 807309 timesteps\n",
      "Episode reward: -1.179\n",
      "Average of last 10 rewards: -1.0718353031465848\n",
      "Finished episode 13051 after 807933 timesteps\n",
      "Episode reward: -1.083\n",
      "Average of last 10 rewards: -1.0718606595092024\n",
      "Finished episode 13061 after 808366 timesteps\n",
      "Episode reward: -0.64\n",
      "Average of last 10 rewards: -1.0718269731800765\n",
      "Finished episode 13071 after 809336 timesteps\n",
      "Episode reward: -1.163\n",
      "Average of last 10 rewards: -1.0718410413476263\n",
      "Finished episode 13081 after 810348 timesteps\n",
      "Episode reward: -1.075\n",
      "Average of last 10 rewards: -1.0717537107880644\n",
      "Finished episode 13091 after 810958 timesteps\n",
      "Episode reward: -1.104\n",
      "Average of last 10 rewards: -1.0717549694189603\n",
      "Finished episode 13101 after 811730 timesteps\n",
      "Episode reward: -1.041\n",
      "Average of last 10 rewards: -1.0717376623376622\n",
      "Finished episode 13111 after 812430 timesteps\n",
      "Episode reward: -1.318\n",
      "Average of last 10 rewards: -1.0717281679389312\n",
      "Finished episode 13121 after 812807 timesteps\n",
      "Episode reward: -1.058\n",
      "Average of last 10 rewards: -1.0717289855072463\n",
      "Finished episode 13131 after 813298 timesteps\n",
      "Episode reward: -1.062\n",
      "Average of last 10 rewards: -1.0717101371951219\n",
      "Finished episode 13141 after 813673 timesteps\n",
      "Episode reward: -1.093\n",
      "Average of last 10 rewards: -1.071643183549124\n",
      "Finished episode 13151 after 814296 timesteps\n",
      "Episode reward: -1.079\n",
      "Average of last 10 rewards: -1.0716716133942161\n",
      "Finished episode 13161 after 815178 timesteps\n",
      "Episode reward: -1.0530000000000004\n",
      "Average of last 10 rewards: -1.071680836501901\n",
      "Finished episode 13171 after 815667 timesteps\n",
      "Episode reward: -1.101\n",
      "Average of last 10 rewards: -1.0716641337386017\n",
      "Finished episode 13181 after 816171 timesteps\n",
      "Episode reward: -1.065\n",
      "Average of last 10 rewards: -1.071683826879271\n",
      "Finished episode 13191 after 817040 timesteps\n",
      "Episode reward: -1.074\n",
      "Average of last 10 rewards: -1.0716471927162368\n",
      "Finished episode 13201 after 817831 timesteps\n",
      "Episode reward: -1.072\n",
      "Average of last 10 rewards: -1.0715952994692948\n",
      "Finished episode 13211 after 818695 timesteps\n",
      "Episode reward: -1.057\n",
      "Average of last 10 rewards: -1.071542803030303\n",
      "Finished episode 13221 after 819392 timesteps\n",
      "Episode reward: -1.082\n",
      "Average of last 10 rewards: -1.0715183194549582\n",
      "Finished episode 13231 after 819820 timesteps\n",
      "Episode reward: -1.1840000000000002\n",
      "Average of last 10 rewards: -1.0715193645990921\n",
      "INFO:tensorflow:Saving checkpoints for 820202 into ./ppo/model/model.ckpt.\n",
      "Finished episode 13241 after 820393 timesteps\n",
      "Episode reward: -1.077\n",
      "Average of last 10 rewards: -1.0715275888133031\n",
      "Finished episode 13251 after 821211 timesteps\n",
      "Episode reward: -1.153\n",
      "Average of last 10 rewards: -1.0715808912386706\n",
      "Finished episode 13261 after 821705 timesteps\n",
      "Episode reward: -1.054\n",
      "Average of last 10 rewards: -1.0715694339622641\n",
      "Finished episode 13271 after 822186 timesteps\n",
      "Episode reward: -1.129\n",
      "Average of last 10 rewards: -1.0715589743589744\n",
      "Finished episode 13281 after 822763 timesteps\n",
      "Episode reward: -1.081\n",
      "Average of last 10 rewards: -1.0715845516201958\n",
      "Finished episode 13291 after 823295 timesteps\n",
      "Episode reward: -1.048\n",
      "Average of last 10 rewards: -1.0716063253012047\n",
      "Finished episode 13301 after 823691 timesteps\n",
      "Episode reward: -1.041\n",
      "Average of last 10 rewards: -1.0715519939804363\n",
      "Finished episode 13311 after 824112 timesteps\n",
      "Episode reward: -1.077\n",
      "Average of last 10 rewards: -1.0715203007518797\n",
      "Finished episode 13321 after 824745 timesteps\n",
      "Episode reward: -1.074\n",
      "Average of last 10 rewards: -1.071548309541698\n",
      "Finished episode 13331 after 825371 timesteps\n",
      "Episode reward: -1.1560000000000001\n",
      "Average of last 10 rewards: -1.0715779279279278\n",
      "Finished episode 13341 after 826166 timesteps\n",
      "Episode reward: -1.3640000000000003\n",
      "Average of last 10 rewards: -1.0716402850712679\n",
      "Finished episode 13351 after 826869 timesteps\n",
      "Episode reward: -1.1380000000000001\n",
      "Average of last 10 rewards: -1.0716865817091454\n",
      "Finished episode 13361 after 827274 timesteps\n",
      "Episode reward: -1.05\n",
      "Average of last 10 rewards: -1.0716886142322097\n",
      "Finished episode 13371 after 828044 timesteps\n",
      "Episode reward: -1.326\n",
      "Average of last 10 rewards: -1.071694386227545\n",
      "Finished episode 13381 after 828725 timesteps\n",
      "Episode reward: -1.133\n",
      "Average of last 10 rewards: -1.071688406881077\n",
      "Finished episode 13391 after 829425 timesteps\n",
      "Episode reward: -1.237\n",
      "Average of last 10 rewards: -1.0717166666666667\n",
      "Finished episode 13401 after 829977 timesteps\n",
      "Episode reward: -1.093\n",
      "Average of last 10 rewards: -1.0717174010455564\n",
      "Finished episode 13411 after 830397 timesteps\n",
      "Episode reward: -1.042\n",
      "Average of last 10 rewards: -1.0717257462686567\n",
      "Finished episode 13421 after 831282 timesteps\n",
      "Episode reward: -1.038\n",
      "Average of last 10 rewards: -1.0717572706935123\n",
      "Finished episode 13431 after 832116 timesteps\n",
      "Episode reward: -0.6719999999999999\n",
      "Average of last 10 rewards: -1.0717368852459015\n",
      "Finished episode 13441 after 832627 timesteps\n",
      "Episode reward: -1.112\n",
      "Average of last 10 rewards: -1.071749962769918\n",
      "INFO:tensorflow:Saving checkpoints for 833154 into ./ppo/model/model.ckpt.\n",
      "Finished episode 13451 after 833329 timesteps\n",
      "Episode reward: -1.1420000000000001\n",
      "Average of last 10 rewards: -1.0716984374999998\n",
      "Finished episode 13461 after 833896 timesteps\n",
      "Episode reward: -1.2530000000000001\n",
      "Average of last 10 rewards: -1.0716484014869887\n",
      "Finished episode 13471 after 834307 timesteps\n",
      "Episode reward: -1.039\n",
      "Average of last 10 rewards: -1.0716162704309062\n",
      "Finished episode 13481 after 835321 timesteps\n",
      "Episode reward: -1.197\n",
      "Average of last 10 rewards: -1.0716367483296214\n",
      "Finished episode 13491 after 835850 timesteps\n",
      "Episode reward: -1.075\n",
      "Average of last 10 rewards: -1.0716325667655786\n",
      "Finished episode 13501 after 836792 timesteps\n",
      "Episode reward: -1.03\n",
      "Average of last 10 rewards: -1.071690437361008\n",
      "Finished episode 13511 after 837345 timesteps\n",
      "Episode reward: -1.203\n",
      "Average of last 10 rewards: -1.0717128148148147\n",
      "Finished episode 13521 after 837793 timesteps\n",
      "Episode reward: -1.052\n",
      "Average of last 10 rewards: -1.07170059215396\n",
      "Finished episode 13531 after 838500 timesteps\n",
      "Episode reward: -1.098\n",
      "Average of last 10 rewards: -1.0716195266272188\n",
      "Finished episode 13541 after 839064 timesteps\n",
      "Episode reward: -1.066\n",
      "Average of last 10 rewards: -1.0716471544715447\n",
      "Finished episode 13551 after 839542 timesteps\n",
      "Episode reward: -1.048\n",
      "Average of last 10 rewards: -1.071672008862629\n",
      "Finished episode 13561 after 839876 timesteps\n",
      "Episode reward: -1.067\n",
      "Average of last 10 rewards: -1.0716650922509225\n",
      "Finished episode 13571 after 840469 timesteps\n",
      "Episode reward: -0.6839999999999999\n",
      "Average of last 10 rewards: -1.0716351032448377\n",
      "Finished episode 13581 after 841277 timesteps\n",
      "Episode reward: -1.135\n",
      "Average of last 10 rewards: -1.0716417833456153\n",
      "Finished episode 13591 after 841896 timesteps\n",
      "Episode reward: -1.043\n",
      "Average of last 10 rewards: -1.0716439617083946\n",
      "Finished episode 13601 after 842522 timesteps\n",
      "Episode reward: -1.069\n",
      "Average of last 10 rewards: -1.0716443708609271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 13611 after 843011 timesteps\n",
      "Episode reward: -1.081\n",
      "Average of last 10 rewards: -1.0716211029411764\n",
      "Finished episode 13621 after 843635 timesteps\n",
      "Episode reward: -1.109\n",
      "Average of last 10 rewards: -1.071663335782513\n",
      "Finished episode 13631 after 844234 timesteps\n",
      "Episode reward: -1.06\n",
      "Average of last 10 rewards: -1.0716522026431716\n",
      "Finished episode 13641 after 844729 timesteps\n",
      "Episode reward: -1.1280000000000001\n",
      "Average of last 10 rewards: -1.0716473954512105\n",
      "Finished episode 13651 after 845430 timesteps\n",
      "Episode reward: -1.056\n",
      "Average of last 10 rewards: -1.0716552785923754\n",
      "Finished episode 13661 after 846003 timesteps\n",
      "Episode reward: -1.027\n",
      "Average of last 10 rewards: -1.071658315018315\n",
      "INFO:tensorflow:Saving checkpoints for 846194 into ./ppo/model/model.ckpt.\n",
      "Finished episode 13671 after 846652 timesteps\n",
      "Episode reward: -0.806\n",
      "Average of last 10 rewards: -1.071590336749634\n",
      "Finished episode 13681 after 847218 timesteps\n",
      "Episode reward: -1.081\n",
      "Average of last 10 rewards: -1.0715633504023407\n",
      "Finished episode 13691 after 847824 timesteps\n",
      "Episode reward: -1.1280000000000001\n",
      "Average of last 10 rewards: -1.0715350877192982\n",
      "Finished episode 13701 after 848442 timesteps\n",
      "Episode reward: -1.12\n",
      "Average of last 10 rewards: -1.071543827611395\n",
      "Finished episode 13711 after 849119 timesteps\n",
      "Episode reward: -1.082\n",
      "Average of last 10 rewards: -1.0715265693430656\n",
      "Finished episode 13721 after 849821 timesteps\n",
      "Episode reward: -1.2930000000000001\n",
      "Average of last 10 rewards: -1.0715437636761487\n",
      "Finished episode 13731 after 850492 timesteps\n",
      "Episode reward: -1.077\n",
      "Average of last 10 rewards: -1.0715721574344024\n",
      "Finished episode 13741 after 851009 timesteps\n",
      "Episode reward: -1.063\n",
      "Average of last 10 rewards: -1.071593445010925\n",
      "Finished episode 13751 after 851639 timesteps\n",
      "Episode reward: -1.064\n",
      "Average of last 10 rewards: -1.0716328238719068\n",
      "Finished episode 13761 after 852413 timesteps\n",
      "Episode reward: -1.2060000000000002\n",
      "Average of last 10 rewards: -1.0716604363636362\n",
      "Finished episode 13771 after 853207 timesteps\n",
      "Episode reward: -1.109\n",
      "Average of last 10 rewards: -1.071586191860465\n",
      "Finished episode 13781 after 853655 timesteps\n",
      "Episode reward: -1.056\n",
      "Average of last 10 rewards: -1.071585766158315\n",
      "Finished episode 13791 after 854127 timesteps\n",
      "Episode reward: -1.147\n",
      "Average of last 10 rewards: -1.0715968795355588\n",
      "Finished episode 13801 after 854769 timesteps\n",
      "Episode reward: -1.059\n",
      "Average of last 10 rewards: -1.0715884699057288\n",
      "Finished episode 13811 after 855094 timesteps\n",
      "Episode reward: -1.024\n",
      "Average of last 10 rewards: -1.071526304347826\n",
      "Finished episode 13821 after 855731 timesteps\n",
      "Episode reward: -1.048\n",
      "Average of last 10 rewards: -1.0715452570601014\n",
      "Finished episode 13831 after 856430 timesteps\n",
      "Episode reward: -1.028\n",
      "Average of last 10 rewards: -1.0715452966714905\n",
      "Finished episode 13841 after 856909 timesteps\n",
      "Episode reward: -1.034\n",
      "Average of last 10 rewards: -1.0715305856832973\n",
      "Finished episode 13851 after 857428 timesteps\n",
      "Episode reward: -1.072\n",
      "Average of last 10 rewards: -1.0715157514450866\n",
      "Finished episode 13861 after 857829 timesteps\n",
      "Episode reward: -1.025\n",
      "Average of last 10 rewards: -1.0715137906137184\n",
      "Finished episode 13871 after 858250 timesteps\n",
      "Episode reward: -1.078\n",
      "Average of last 10 rewards: -1.0715143578643578\n",
      "Finished episode 13881 after 859087 timesteps\n",
      "Episode reward: -1.2770000000000001\n",
      "Average of last 10 rewards: -1.0715710165825523\n",
      "INFO:tensorflow:Saving checkpoints for 859276 into ./ppo/model/model.ckpt.\n",
      "Finished episode 13891 after 859936 timesteps\n",
      "Episode reward: -0.6560000000000002\n",
      "Average of last 10 rewards: -1.0715057636887608\n",
      "Finished episode 13901 after 860836 timesteps\n",
      "Episode reward: -1.2890000000000001\n",
      "Average of last 10 rewards: -1.0714544996400288\n",
      "Finished episode 13911 after 861482 timesteps\n",
      "Episode reward: -1.038\n",
      "Average of last 10 rewards: -1.0714758992805755\n",
      "Finished episode 13921 after 862133 timesteps\n",
      "Episode reward: -1.025\n",
      "Average of last 10 rewards: -1.0714524802300502\n",
      "Finished episode 13931 after 863032 timesteps\n",
      "Episode reward: -1.2590000000000001\n",
      "Average of last 10 rewards: -1.071516235632184\n",
      "Finished episode 13941 after 863562 timesteps\n",
      "Episode reward: -1.071\n",
      "Average of last 10 rewards: -1.0715361091170137\n",
      "Finished episode 13951 after 864525 timesteps\n",
      "Episode reward: -1.075\n",
      "Average of last 10 rewards: -1.0715218794835006\n",
      "Finished episode 13961 after 865031 timesteps\n",
      "Episode reward: -1.062\n",
      "Average of last 10 rewards: -1.071508100358423\n",
      "Finished episode 13971 after 865955 timesteps\n",
      "Episode reward: -1.088\n",
      "Average of last 10 rewards: -1.0715515042979942\n",
      "Finished episode 13981 after 866767 timesteps\n",
      "Episode reward: -1.0720000000000003\n",
      "Average of last 10 rewards: -1.0715653543307087\n",
      "Finished episode 13991 after 867356 timesteps\n",
      "Episode reward: -1.048\n",
      "Average of last 10 rewards: -1.0715423462088698\n",
      "Finished episode 14001 after 868110 timesteps\n",
      "Episode reward: -1.068\n",
      "Average of last 10 rewards: -1.0715702644746248\n",
      "Finished episode 14011 after 869271 timesteps\n",
      "Episode reward: -1.3210000000000002\n",
      "Average of last 10 rewards: -1.0716327142857143\n",
      "Finished episode 14021 after 869785 timesteps\n",
      "Episode reward: -0.6799999999999999\n",
      "Average of last 10 rewards: -1.0716174875089222\n",
      "Finished episode 14031 after 870419 timesteps\n",
      "Episode reward: -1.067\n",
      "Average of last 10 rewards: -1.0716468616262482\n",
      "Finished episode 14041 after 870889 timesteps\n",
      "Episode reward: -1.065\n",
      "Average of last 10 rewards: -1.0716404133998574\n",
      "Finished episode 14051 after 871493 timesteps\n",
      "Episode reward: -1.04\n",
      "Average of last 10 rewards: -1.07163198005698\n",
      "INFO:tensorflow:Saving checkpoints for 872269 into ./ppo/model/model.ckpt.\n",
      "Finished episode 14061 after 872406 timesteps\n",
      "Episode reward: -0.81\n",
      "Average of last 10 rewards: -1.071628612099644\n",
      "Finished episode 14071 after 872902 timesteps\n",
      "Episode reward: -1.062\n",
      "Average of last 10 rewards: -1.0716323613086771\n",
      "Finished episode 14081 after 873609 timesteps\n",
      "Episode reward: -1.12\n",
      "Average of last 10 rewards: -1.0716432835820895\n",
      "Finished episode 14091 after 874701 timesteps\n",
      "Episode reward: -1.4430000000000003\n",
      "Average of last 10 rewards: -1.07161171875\n",
      "Finished episode 14101 after 875411 timesteps\n",
      "Episode reward: -1.2940000000000003\n",
      "Average of last 10 rewards: -1.071610361958836\n",
      "Finished episode 14111 after 876363 timesteps\n",
      "Episode reward: -1.099\n",
      "Average of last 10 rewards: -1.0716281560283687\n",
      "Finished episode 14121 after 877243 timesteps\n",
      "Episode reward: -1.1860000000000004\n",
      "Average of last 10 rewards: -1.0716499645641389\n",
      "Finished episode 14131 after 878139 timesteps\n",
      "Episode reward: -1.097\n",
      "Average of last 10 rewards: -1.0717213172804532\n",
      "Finished episode 14141 after 878757 timesteps\n",
      "Episode reward: -1.2320000000000002\n",
      "Average of last 10 rewards: -1.071748195329087\n",
      "Finished episode 14151 after 879338 timesteps\n",
      "Episode reward: -1.052\n",
      "Average of last 10 rewards: -1.0717483026874115\n",
      "Finished episode 14161 after 879902 timesteps\n",
      "Episode reward: -1.063\n",
      "Average of last 10 rewards: -1.0717082685512367\n",
      "Finished episode 14171 after 880687 timesteps\n",
      "Episode reward: -1.055\n",
      "Average of last 10 rewards: -1.071726059322034\n",
      "Finished episode 14181 after 881482 timesteps\n",
      "Episode reward: -1.094\n",
      "Average of last 10 rewards: -1.0717819336626675\n",
      "Finished episode 14191 after 881894 timesteps\n",
      "Episode reward: -1.091\n",
      "Average of last 10 rewards: -1.0717805359661494\n",
      "Finished episode 14201 after 882306 timesteps\n",
      "Episode reward: -1.071\n",
      "Average of last 10 rewards: -1.0717877378435519\n",
      "Finished episode 14211 after 883041 timesteps\n",
      "Episode reward: -0.6659999999999999\n",
      "Average of last 10 rewards: -1.0717145070422536\n",
      "Finished episode 14221 after 883584 timesteps\n",
      "Episode reward: -1.123\n",
      "Average of last 10 rewards: -1.071698099929627\n",
      "Finished episode 14231 after 884544 timesteps\n",
      "Episode reward: 0.03500000000000392\n",
      "Average of last 10 rewards: -1.0716457102672292\n",
      "Finished episode 14241 after 885078 timesteps\n",
      "Episode reward: -1.3530000000000002\n",
      "Average of last 10 rewards: -1.0716546029515108\n",
      "INFO:tensorflow:Saving checkpoints for 885193 into ./ppo/model/model.ckpt.\n",
      "Finished episode 14251 after 885925 timesteps\n",
      "Episode reward: -1.3880000000000003\n",
      "Average of last 10 rewards: -1.071721629213483\n",
      "Finished episode 14261 after 887564 timesteps\n",
      "Episode reward: -1.2040000000000002\n",
      "Average of last 10 rewards: -1.0716722105263157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 14271 after 888044 timesteps\n",
      "Episode reward: -1.061\n",
      "Average of last 10 rewards: -1.0716612201963533\n",
      "Finished episode 14281 after 888451 timesteps\n",
      "Episode reward: -1.087\n",
      "Average of last 10 rewards: -1.0716379117028731\n",
      "Finished episode 14291 after 888967 timesteps\n",
      "Episode reward: -1.053\n",
      "Average of last 10 rewards: -1.0716519607843136\n",
      "Finished episode 14301 after 890125 timesteps\n",
      "Episode reward: -1.1780000000000002\n",
      "Average of last 10 rewards: -1.0717324702589224\n",
      "Finished episode 14311 after 890912 timesteps\n",
      "Episode reward: -1.058\n",
      "Average of last 10 rewards: -1.0717037762237762\n",
      "Finished episode 14321 after 891303 timesteps\n",
      "Episode reward: -0.644\n",
      "Average of last 10 rewards: -1.0716715583508036\n",
      "Finished episode 14331 after 891781 timesteps\n",
      "Episode reward: -1.034\n",
      "Average of last 10 rewards: -1.0716519553072625\n",
      "Finished episode 14341 after 892238 timesteps\n",
      "Episode reward: -1.049\n",
      "Average of last 10 rewards: -1.0716642009769715\n",
      "Finished episode 14351 after 893320 timesteps\n",
      "Episode reward: -1.034\n",
      "Average of last 10 rewards: -1.0717581589958158\n",
      "Finished episode 14361 after 893800 timesteps\n",
      "Episode reward: -1.034\n",
      "Average of last 10 rewards: -1.0717722648083623\n",
      "Finished episode 14371 after 894416 timesteps\n",
      "Episode reward: -1.3400000000000003\n",
      "Average of last 10 rewards: -1.0718034122562674\n",
      "Finished episode 14381 after 895350 timesteps\n",
      "Episode reward: -1.037\n",
      "Average of last 10 rewards: -1.0717792623521225\n",
      "Finished episode 14391 after 895899 timesteps\n",
      "Episode reward: -1.137\n",
      "Average of last 10 rewards: -1.0718026425591098\n",
      "Finished episode 14401 after 896443 timesteps\n",
      "Episode reward: -1.053\n",
      "Average of last 10 rewards: -1.0718311327310632\n",
      "Finished episode 14411 after 897462 timesteps\n",
      "Episode reward: -0.8370000000000004\n",
      "Average of last 10 rewards: -1.0718321527777777\n",
      "Finished episode 14421 after 898045 timesteps\n",
      "Episode reward: -1.083\n",
      "Average of last 10 rewards: -1.071859333795975\n",
      "INFO:tensorflow:Saving checkpoints for 898148 into ./ppo/model/model.ckpt.\n",
      "Finished episode 14431 after 899202 timesteps\n",
      "Episode reward: -1.116\n",
      "Average of last 10 rewards: -1.0718682385575589\n",
      "Finished episode 14441 after 899717 timesteps\n",
      "Episode reward: -1.068\n",
      "Average of last 10 rewards: -1.0718310464310463\n",
      "Finished episode 14451 after 900336 timesteps\n",
      "Episode reward: -1.046\n",
      "Average of last 10 rewards: -1.0718355955678671\n",
      "Finished episode 14461 after 901071 timesteps\n",
      "Episode reward: -1.199\n",
      "Average of last 10 rewards: -1.0718829757785466\n",
      "Finished episode 14471 after 901612 timesteps\n",
      "Episode reward: -1.3400000000000003\n",
      "Average of last 10 rewards: -1.07187918395574\n",
      "Finished episode 14481 after 902233 timesteps\n",
      "Episode reward: -1.107\n",
      "Average of last 10 rewards: -1.0719024187975121\n",
      "Finished episode 14491 after 902969 timesteps\n",
      "Episode reward: -1.077\n",
      "Average of last 10 rewards: -1.0719564917127071\n",
      "Finished episode 14501 after 903560 timesteps\n",
      "Episode reward: -1.1520000000000001\n",
      "Average of last 10 rewards: -1.0719543133195306\n",
      "Finished episode 14511 after 904407 timesteps\n",
      "Episode reward: -1.056\n",
      "Average of last 10 rewards: -1.0719773793103446\n",
      "Finished episode 14521 after 905223 timesteps\n",
      "Episode reward: -1.052\n",
      "Average of last 10 rewards: -1.0719866988283941\n",
      "Finished episode 14531 after 905707 timesteps\n",
      "Episode reward: -1.097\n",
      "Average of last 10 rewards: -1.0719871900826448\n",
      "Finished episode 14541 after 906459 timesteps\n",
      "Episode reward: -1.093\n",
      "Average of last 10 rewards: -1.0719783207157605\n",
      "Finished episode 14551 after 907145 timesteps\n",
      "Episode reward: -1.159\n",
      "Average of last 10 rewards: -1.0719766162310866\n",
      "Finished episode 14561 after 908243 timesteps\n",
      "Episode reward: -1.4690000000000003\n",
      "Average of last 10 rewards: -1.071940412371134\n",
      "Finished episode 14571 after 908756 timesteps\n",
      "Episode reward: -1.063\n",
      "Average of last 10 rewards: -1.0719221153846155\n",
      "Finished episode 14581 after 909229 timesteps\n",
      "Episode reward: -1.044\n",
      "Average of last 10 rewards: -1.0719081674673987\n",
      "Finished episode 14591 after 909950 timesteps\n",
      "Episode reward: -1.249\n",
      "Average of last 10 rewards: -1.0718890946502058\n",
      "Finished episode 14601 after 910298 timesteps\n",
      "Episode reward: -1.08\n",
      "Average of last 10 rewards: -1.0718827278958192\n",
      "Finished episode 14611 after 910751 timesteps\n",
      "Episode reward: -1.038\n",
      "Average of last 10 rewards: -1.0718916438356163\n",
      "Finished episode 14621 after 911265 timesteps\n",
      "Episode reward: -1.1580000000000001\n",
      "Average of last 10 rewards: -1.0719123887748117\n",
      "INFO:tensorflow:Saving checkpoints for 911301 into ./ppo/model/model.ckpt.\n",
      "Finished episode 14631 after 911959 timesteps\n",
      "Episode reward: -0.8160000000000001\n",
      "Average of last 10 rewards: -1.071931121751026\n",
      "Finished episode 14641 after 912575 timesteps\n",
      "Episode reward: -1.09\n",
      "Average of last 10 rewards: -1.0719101845522898\n",
      "Finished episode 14651 after 913172 timesteps\n",
      "Episode reward: -1.1580000000000001\n",
      "Average of last 10 rewards: -1.0719425546448087\n",
      "Finished episode 14661 after 913861 timesteps\n",
      "Episode reward: -1.1280000000000001\n",
      "Average of last 10 rewards: -1.071977337883959\n",
      "Finished episode 14671 after 914390 timesteps\n",
      "Episode reward: -1.1400000000000001\n",
      "Average of last 10 rewards: -1.0719925648021829\n",
      "Finished episode 14681 after 914873 timesteps\n",
      "Episode reward: -1.038\n",
      "Average of last 10 rewards: -1.072007021131561\n",
      "Finished episode 14691 after 915327 timesteps\n",
      "Episode reward: -1.057\n",
      "Average of last 10 rewards: -1.0720139645776565\n",
      "Finished episode 14701 after 916030 timesteps\n",
      "Episode reward: -1.047\n",
      "Average of last 10 rewards: -1.0720318584070796\n",
      "Finished episode 14711 after 916617 timesteps\n",
      "Episode reward: -1.169\n",
      "Average of last 10 rewards: -1.0720619047619047\n",
      "Finished episode 14721 after 917398 timesteps\n",
      "Episode reward: -1.123\n",
      "Average of last 10 rewards: -1.0720578518014956\n",
      "Finished episode 14731 after 918123 timesteps\n",
      "Episode reward: -1.143\n",
      "Average of last 10 rewards: -1.0721087635869564\n",
      "Finished episode 14741 after 918756 timesteps\n",
      "Episode reward: -1.046\n",
      "Average of last 10 rewards: -1.0721444670739986\n",
      "Finished episode 14751 after 919244 timesteps\n",
      "Episode reward: -0.669\n",
      "Average of last 10 rewards: -1.0721358208955223\n",
      "Finished episode 14761 after 919720 timesteps\n",
      "Episode reward: -1.084\n",
      "Average of last 10 rewards: -1.072121627118644\n",
      "Finished episode 14771 after 920147 timesteps\n",
      "Episode reward: -1.1420000000000001\n",
      "Average of last 10 rewards: -1.0721329945799458\n",
      "Finished episode 14781 after 920594 timesteps\n",
      "Episode reward: -1.071\n",
      "Average of last 10 rewards: -1.0721347325660122\n",
      "Finished episode 14791 after 921088 timesteps\n",
      "Episode reward: -1.045\n",
      "Average of last 10 rewards: -1.0720866711772665\n",
      "Finished episode 14801 after 921516 timesteps\n",
      "Episode reward: -1.065\n",
      "Average of last 10 rewards: -1.0720872210953345\n",
      "Finished episode 14811 after 922406 timesteps\n",
      "Episode reward: -1.081\n",
      "Average of last 10 rewards: -1.072102972972973\n",
      "Finished episode 14821 after 922947 timesteps\n",
      "Episode reward: -1.173\n",
      "Average of last 10 rewards: -1.0721027008777853\n",
      "Finished episode 14831 after 923575 timesteps\n",
      "Episode reward: -1.053\n",
      "Average of last 10 rewards: -1.0720968286099863\n",
      "Finished episode 14841 after 923945 timesteps\n",
      "Episode reward: -1.045\n",
      "Average of last 10 rewards: -1.072065745111261\n",
      "INFO:tensorflow:Saving checkpoints for 924319 into ./ppo/model/model.ckpt.\n",
      "Finished episode 14851 after 924460 timesteps\n",
      "Episode reward: -1.039\n",
      "Average of last 10 rewards: -1.0720719676549864\n",
      "Finished episode 14861 after 925463 timesteps\n",
      "Episode reward: -1.089\n",
      "Average of last 10 rewards: -1.0720237037037037\n",
      "Finished episode 14871 after 925891 timesteps\n",
      "Episode reward: -1.028\n",
      "Average of last 10 rewards: -1.0720018169582772\n",
      "Finished episode 14881 after 926387 timesteps\n",
      "Episode reward: -0.7530000000000001\n",
      "Average of last 10 rewards: -1.0719396099529253\n",
      "Finished episode 14891 after 927097 timesteps\n",
      "Episode reward: -1.028\n",
      "Average of last 10 rewards: -1.071941196236559\n",
      "Finished episode 14901 after 927565 timesteps\n",
      "Episode reward: -1.084\n",
      "Average of last 10 rewards: -1.0719006715916721\n",
      "Finished episode 14911 after 928323 timesteps\n",
      "Episode reward: -1.079\n",
      "Average of last 10 rewards: -1.0718864429530202\n",
      "Finished episode 14921 after 928888 timesteps\n",
      "Episode reward: -1.063\n",
      "Average of last 10 rewards: -1.0718691482226692\n",
      "Finished episode 14931 after 929436 timesteps\n",
      "Episode reward: -1.058\n",
      "Average of last 10 rewards: -1.0718983243967828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 14941 after 930104 timesteps\n",
      "Episode reward: -1.055\n",
      "Average of last 10 rewards: -1.0718738111185533\n",
      "Finished episode 14951 after 930776 timesteps\n",
      "Episode reward: -1.2140000000000002\n",
      "Average of last 10 rewards: -1.071913922356091\n",
      "Finished episode 14961 after 931261 timesteps\n",
      "Episode reward: -1.122\n",
      "Average of last 10 rewards: -1.0718985284280935\n",
      "Finished episode 14971 after 931861 timesteps\n",
      "Episode reward: -1.1640000000000001\n",
      "Average of last 10 rewards: -1.071916577540107\n",
      "Finished episode 14981 after 932425 timesteps\n",
      "Episode reward: -1.045\n",
      "Average of last 10 rewards: -1.0718832331329327\n",
      "Finished episode 14991 after 933165 timesteps\n",
      "Episode reward: -1.071\n",
      "Average of last 10 rewards: -1.0719074098798398\n",
      "Finished episode 15001 after 933571 timesteps\n",
      "Episode reward: -1.101\n",
      "Average of last 10 rewards: -1.0719032021347565\n",
      "Finished episode 15011 after 934757 timesteps\n",
      "Episode reward: -1.2520000000000002\n",
      "Average of last 10 rewards: -1.0719222666666666\n",
      "Finished episode 15021 after 935455 timesteps\n",
      "Episode reward: -1.1\n",
      "Average of last 10 rewards: -1.0719534976682212\n",
      "Finished episode 15031 after 936474 timesteps\n",
      "Episode reward: -1.179\n",
      "Average of last 10 rewards: -1.0720362183754995\n",
      "Finished episode 15041 after 936977 timesteps\n",
      "Episode reward: -1.155\n",
      "Average of last 10 rewards: -1.0720236859614105\n",
      "INFO:tensorflow:Saving checkpoints for 937334 into ./ppo/model/model.ckpt.\n",
      "Finished episode 15051 after 937507 timesteps\n",
      "Episode reward: -1.099\n",
      "Average of last 10 rewards: -1.0719413563829787\n",
      "Finished episode 15061 after 938140 timesteps\n",
      "Episode reward: -1.143\n",
      "Average of last 10 rewards: -1.0719794019933555\n",
      "Finished episode 15071 after 938938 timesteps\n",
      "Episode reward: -0.6699999999999999\n",
      "Average of last 10 rewards: -1.0719986055776891\n",
      "Finished episode 15081 after 939516 timesteps\n",
      "Episode reward: -1.03\n",
      "Average of last 10 rewards: -1.0720314532183144\n",
      "Finished episode 15091 after 939996 timesteps\n",
      "Episode reward: -1.063\n",
      "Average of last 10 rewards: -1.0720054376657826\n",
      "Finished episode 15101 after 940557 timesteps\n",
      "Episode reward: -1.137\n",
      "Average of last 10 rewards: -1.0720299536116633\n",
      "Finished episode 15111 after 941204 timesteps\n",
      "Episode reward: -1.054\n",
      "Average of last 10 rewards: -1.0720673509933776\n",
      "Finished episode 15121 after 941918 timesteps\n",
      "Episode reward: -1.3200000000000003\n",
      "Average of last 10 rewards: -1.0721160158835208\n",
      "Finished episode 15131 after 942609 timesteps\n",
      "Episode reward: -1.2200000000000002\n",
      "Average of last 10 rewards: -1.072160648148148\n",
      "Finished episode 15141 after 942972 timesteps\n",
      "Episode reward: -1.062\n",
      "Average of last 10 rewards: -1.0721608063450099\n",
      "Finished episode 15151 after 943479 timesteps\n",
      "Episode reward: -1.101\n",
      "Average of last 10 rewards: -1.0721511228533684\n",
      "Finished episode 15161 after 944136 timesteps\n",
      "Episode reward: -1.057\n",
      "Average of last 10 rewards: -1.072124290429043\n",
      "Finished episode 15171 after 944570 timesteps\n",
      "Episode reward: -1.045\n",
      "Average of last 10 rewards: -1.0721267150395777\n",
      "Finished episode 15181 after 945003 timesteps\n",
      "Episode reward: -1.072\n",
      "Average of last 10 rewards: -1.0721376400791034\n",
      "Finished episode 15191 after 945871 timesteps\n",
      "Episode reward: -1.215\n",
      "Average of last 10 rewards: -1.0721521080368908\n",
      "Finished episode 15201 after 946778 timesteps\n",
      "Episode reward: -1.4160000000000004\n",
      "Average of last 10 rewards: -1.0721471362738644\n",
      "Finished episode 15211 after 947177 timesteps\n",
      "Episode reward: -1.112\n",
      "Average of last 10 rewards: -1.072128552631579\n",
      "Finished episode 15221 after 948046 timesteps\n",
      "Episode reward: -0.6890000000000001\n",
      "Average of last 10 rewards: -1.0720491124260354\n",
      "Finished episode 15231 after 948697 timesteps\n",
      "Episode reward: -1.1560000000000001\n",
      "Average of last 10 rewards: -1.0720643889618922\n",
      "Finished episode 15241 after 949170 timesteps\n",
      "Episode reward: -1.078\n",
      "Average of last 10 rewards: -1.0720710439921208\n",
      "Finished episode 15251 after 949681 timesteps\n",
      "Episode reward: -1.127\n",
      "Average of last 10 rewards: -1.07208031496063\n",
      "Finished episode 15261 after 950113 timesteps\n",
      "Episode reward: -1.109\n",
      "Average of last 10 rewards: -1.0720666885245902\n",
      "INFO:tensorflow:Saving checkpoints for 950408 into ./ppo/model/model.ckpt.\n",
      "Finished episode 15271 after 950514 timesteps\n",
      "Episode reward: -1.043\n",
      "Average of last 10 rewards: -1.07206500655308\n",
      "Finished episode 15281 after 951322 timesteps\n",
      "Episode reward: -1.047\n",
      "Average of last 10 rewards: -1.0720824492468892\n",
      "Finished episode 15291 after 951941 timesteps\n",
      "Episode reward: -1.12\n",
      "Average of last 10 rewards: -1.0720865183246073\n",
      "Finished episode 15301 after 952537 timesteps\n",
      "Episode reward: -1.2730000000000001\n",
      "Average of last 10 rewards: -1.0720928711576194\n",
      "Finished episode 15311 after 953621 timesteps\n",
      "Episode reward: -1.2870000000000001\n",
      "Average of last 10 rewards: -1.0720987581699344\n",
      "Finished episode 15321 after 954101 timesteps\n",
      "Episode reward: -1.104\n",
      "Average of last 10 rewards: -1.072116263879817\n",
      "Finished episode 15331 after 954905 timesteps\n",
      "Episode reward: -1.1580000000000001\n",
      "Average of last 10 rewards: -1.072114817232376\n",
      "Finished episode 15341 after 955495 timesteps\n",
      "Episode reward: -1.063\n",
      "Average of last 10 rewards: -1.0721358773646446\n",
      "Finished episode 15351 after 956022 timesteps\n",
      "Episode reward: -1.054\n",
      "Average of last 10 rewards: -1.0721558018252932\n",
      "Finished episode 15361 after 957031 timesteps\n",
      "Episode reward: -1.108\n",
      "Average of last 10 rewards: -1.0722280130293158\n",
      "Finished episode 15371 after 957634 timesteps\n",
      "Episode reward: -1.127\n",
      "Average of last 10 rewards: -1.072255078125\n",
      "Finished episode 15381 after 958265 timesteps\n",
      "Episode reward: -1.045\n",
      "Average of last 10 rewards: -1.0722666883539362\n",
      "Finished episode 15391 after 958984 timesteps\n",
      "Episode reward: -0.9570000000000002\n",
      "Average of last 10 rewards: -1.0722893368010402\n",
      "Finished episode 15401 after 959654 timesteps\n",
      "Episode reward: -1.145\n",
      "Average of last 10 rewards: -1.0722623131903832\n",
      "Finished episode 15411 after 960300 timesteps\n",
      "Episode reward: -1.031\n",
      "Average of last 10 rewards: -1.0722333766233767\n",
      "Finished episode 15421 after 960999 timesteps\n",
      "Episode reward: -1.1340000000000001\n",
      "Average of last 10 rewards: -1.0722409474367294\n",
      "Finished episode 15431 after 961760 timesteps\n",
      "Episode reward: -1.1820000000000002\n",
      "Average of last 10 rewards: -1.0722769779507133\n",
      "Finished episode 15441 after 963005 timesteps\n",
      "Episode reward: -1.153\n",
      "Average of last 10 rewards: -1.0723296824368114\n",
      "INFO:tensorflow:Saving checkpoints for 963373 into ./ppo/model/model.ckpt.\n",
      "Finished episode 15451 after 963567 timesteps\n",
      "Episode reward: -1.135\n",
      "Average of last 10 rewards: -1.0723090025906736\n",
      "Finished episode 15461 after 964225 timesteps\n",
      "Episode reward: -1.112\n",
      "Average of last 10 rewards: -1.0723341747572817\n",
      "Finished episode 15471 after 964817 timesteps\n",
      "Episode reward: -1.055\n",
      "Average of last 10 rewards: -1.0722986416558862\n",
      "Finished episode 15481 after 965689 timesteps\n",
      "Episode reward: -1.037\n",
      "Average of last 10 rewards: -1.0723250161603102\n",
      "Finished episode 15491 after 966333 timesteps\n",
      "Episode reward: -1.089\n",
      "Average of last 10 rewards: -1.0723378552971576\n",
      "Finished episode 15501 after 966921 timesteps\n",
      "Episode reward: -1.06\n",
      "Average of last 10 rewards: -1.0722783085861847\n",
      "Finished episode 15511 after 967534 timesteps\n",
      "Episode reward: -1.3510000000000002\n",
      "Average of last 10 rewards: -1.0722843870967742\n",
      "Finished episode 15521 after 968317 timesteps\n",
      "Episode reward: -1.069\n",
      "Average of last 10 rewards: -1.0722793681495808\n",
      "Finished episode 15531 after 969065 timesteps\n",
      "Episode reward: -0.692\n",
      "Average of last 10 rewards: -1.0722802835051546\n",
      "Finished episode 15541 after 970024 timesteps\n",
      "Episode reward: -1.2480000000000002\n",
      "Average of last 10 rewards: -1.0722741146168706\n",
      "Finished episode 15551 after 970396 timesteps\n",
      "Episode reward: -1.044\n",
      "Average of last 10 rewards: -1.0722725225225225\n",
      "Finished episode 15561 after 971044 timesteps\n",
      "Episode reward: -1.0390000000000001\n",
      "Average of last 10 rewards: -1.0722579421221865\n",
      "Finished episode 15571 after 971583 timesteps\n",
      "Episode reward: -1.06\n",
      "Average of last 10 rewards: -1.072199614395887\n",
      "Finished episode 15581 after 972174 timesteps\n",
      "Episode reward: -1.09\n",
      "Average of last 10 rewards: -1.072219332048812\n",
      "Finished episode 15591 after 972835 timesteps\n",
      "Episode reward: -1.046\n",
      "Average of last 10 rewards: -1.0722534017971759\n",
      "Finished episode 15601 after 974245 timesteps\n",
      "Episode reward: -1.0970000000000002\n",
      "Average of last 10 rewards: -1.0722557408595252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 15611 after 975227 timesteps\n",
      "Episode reward: -1.057\n",
      "Average of last 10 rewards: -1.0722746153846154\n",
      "Finished episode 15621 after 976245 timesteps\n",
      "Episode reward: -1.085\n",
      "Average of last 10 rewards: -1.072206982703395\n",
      "INFO:tensorflow:Saving checkpoints for 976880 into ./ppo/model/model.ckpt.\n",
      "Finished episode 15631 after 977033 timesteps\n",
      "Episode reward: -1.1580000000000001\n",
      "Average of last 10 rewards: -1.0722095390524968\n",
      "Finished episode 15641 after 977858 timesteps\n",
      "Episode reward: -1.3850000000000002\n",
      "Average of last 10 rewards: -1.0722360204734485\n",
      "Finished episode 15651 after 978498 timesteps\n",
      "Episode reward: -1.084\n",
      "Average of last 10 rewards: -1.072283567774936\n",
      "Finished episode 15661 after 979093 timesteps\n",
      "Episode reward: -0.6619999999999999\n",
      "Average of last 10 rewards: -1.0722305431309904\n",
      "Finished episode 15671 after 980129 timesteps\n",
      "Episode reward: -1.049\n",
      "Average of last 10 rewards: -1.0723049808429117\n",
      "Finished episode 15681 after 980594 timesteps\n",
      "Episode reward: -1.042\n",
      "Average of last 10 rewards: -1.0722966177409061\n",
      "Finished episode 15691 after 981252 timesteps\n",
      "Episode reward: -1.042\n",
      "Average of last 10 rewards: -1.0723260841836735\n",
      "Finished episode 15701 after 981703 timesteps\n",
      "Episode reward: -1.169\n",
      "Average of last 10 rewards: -1.072308667941364\n",
      "Finished episode 15711 after 982283 timesteps\n",
      "Episode reward: -1.057\n",
      "Average of last 10 rewards: -1.0723278343949043\n",
      "Finished episode 15721 after 982712 timesteps\n",
      "Episode reward: -1.032\n",
      "Average of last 10 rewards: -1.072332208784214\n",
      "Finished episode 15731 after 983114 timesteps\n",
      "Episode reward: -1.053\n",
      "Average of last 10 rewards: -1.07230820610687\n",
      "Finished episode 15741 after 983863 timesteps\n",
      "Episode reward: -1.105\n",
      "Average of last 10 rewards: -1.0722632549268913\n",
      "Finished episode 15751 after 984396 timesteps\n",
      "Episode reward: -1.048\n",
      "Average of last 10 rewards: -1.0722784625158832\n",
      "Finished episode 15761 after 984808 timesteps\n",
      "Episode reward: -1.075\n",
      "Average of last 10 rewards: -1.0722533968253969\n",
      "Finished episode 15771 after 985281 timesteps\n",
      "Episode reward: -1.082\n",
      "Average of last 10 rewards: -1.072234771573604\n",
      "Finished episode 15781 after 985987 timesteps\n",
      "Episode reward: -1.045\n",
      "Average of last 10 rewards: -1.07222675967026\n",
      "Finished episode 15791 after 986317 timesteps\n",
      "Episode reward: -1.03\n",
      "Average of last 10 rewards: -1.072216413181242\n",
      "Finished episode 15801 after 986679 timesteps\n",
      "Episode reward: -1.112\n",
      "Average of last 10 rewards: -1.072195060164661\n",
      "Finished episode 15811 after 987029 timesteps\n",
      "Episode reward: -1.091\n",
      "Average of last 10 rewards: -1.0721614556962025\n",
      "Finished episode 15821 after 987770 timesteps\n",
      "Episode reward: -1.024\n",
      "Average of last 10 rewards: -1.0722087286527513\n",
      "Finished episode 15831 after 988641 timesteps\n",
      "Episode reward: -1.231\n",
      "Average of last 10 rewards: -1.0722110619469027\n",
      "Finished episode 15841 after 989637 timesteps\n",
      "Episode reward: -1.4330000000000003\n",
      "Average of last 10 rewards: -1.0722817435249528\n",
      "INFO:tensorflow:Saving checkpoints for 989698 into ./ppo/model/model.ckpt.\n",
      "Finished episode 15851 after 990205 timesteps\n",
      "Episode reward: -1.053\n",
      "Average of last 10 rewards: -1.0722969696969697\n",
      "Finished episode 15861 after 990683 timesteps\n",
      "Episode reward: -1.089\n",
      "Average of last 10 rewards: -1.0723035331230284\n",
      "Finished episode 15871 after 991285 timesteps\n",
      "Episode reward: -1.229\n",
      "Average of last 10 rewards: -1.072264312736444\n",
      "Finished episode 15881 after 991850 timesteps\n",
      "Episode reward: -1.1440000000000001\n",
      "Average of last 10 rewards: -1.0722320100819156\n",
      "Finished episode 15891 after 992502 timesteps\n",
      "Episode reward: -1.031\n",
      "Average of last 10 rewards: -1.0722309193954662\n",
      "Finished episode 15901 after 993165 timesteps\n",
      "Episode reward: -1.108\n",
      "Average of last 10 rewards: -1.0722313404657018\n",
      "Finished episode 15911 after 993799 timesteps\n",
      "Episode reward: -1.2060000000000002\n",
      "Average of last 10 rewards: -1.072228616352201\n",
      "Finished episode 15921 after 994517 timesteps\n",
      "Episode reward: -0.6020000000000002\n",
      "Average of last 10 rewards: -1.0722107479572596\n",
      "Finished episode 15931 after 994952 timesteps\n",
      "Episode reward: -1.085\n",
      "Average of last 10 rewards: -1.072168027638191\n",
      "Finished episode 15941 after 995595 timesteps\n",
      "Episode reward: -0.778\n",
      "Average of last 10 rewards: -1.0721725047080979\n",
      "Finished episode 15951 after 995936 timesteps\n",
      "Episode reward: -1.139\n",
      "Average of last 10 rewards: -1.0721356336260979\n",
      "Finished episode 15961 after 996651 timesteps\n",
      "Episode reward: -1.048\n",
      "Average of last 10 rewards: -1.0721847648902822\n",
      "Finished episode 15971 after 997167 timesteps\n",
      "Episode reward: -1.161\n",
      "Average of last 10 rewards: -1.0721718045112782\n",
      "Finished episode 15981 after 997884 timesteps\n",
      "Episode reward: -1.217\n",
      "Average of last 10 rewards: -1.072211959924859\n",
      "Finished episode 15991 after 998562 timesteps\n",
      "Episode reward: -1.092\n",
      "Average of last 10 rewards: -1.072256445556946\n",
      "Finished episode 16001 after 999033 timesteps\n",
      "Episode reward: -1.07\n",
      "Average of last 10 rewards: -1.0722658536585365\n",
      "Finished episode 16011 after 999703 timesteps\n",
      "Episode reward: -0.8390000000000002\n",
      "Average of last 10 rewards: -1.0722417499999999\n",
      "Finished episode 16021 after 1000127 timesteps\n",
      "Episode reward: -1.135\n",
      "Average of last 10 rewards: -1.0721980637101811\n",
      "Finished episode 16031 after 1000759 timesteps\n",
      "Episode reward: -1.058\n",
      "Average of last 10 rewards: -1.0721759675405742\n",
      "Finished episode 16041 after 1001438 timesteps\n",
      "Episode reward: -1.113\n",
      "Average of last 10 rewards: -1.0722144728633811\n",
      "Finished episode 16051 after 1001942 timesteps\n",
      "Episode reward: -1.081\n",
      "Average of last 10 rewards: -1.0721668952618455\n",
      "INFO:tensorflow:Saving checkpoints for 1002460 into ./ppo/model/model.ckpt.\n",
      "Finished episode 16061 after 1002518 timesteps\n",
      "Episode reward: -1.064\n",
      "Average of last 10 rewards: -1.0721939563862928\n",
      "Finished episode 16071 after 1003228 timesteps\n",
      "Episode reward: -1.065\n",
      "Average of last 10 rewards: -1.0721650684931505\n",
      "Finished episode 16081 after 1003902 timesteps\n",
      "Episode reward: -1.035\n",
      "Average of last 10 rewards: -1.072189359054138\n",
      "Finished episode 16091 after 1004572 timesteps\n",
      "Episode reward: -1.043\n",
      "Average of last 10 rewards: -1.0721432213930346\n",
      "Finished episode 16101 after 1005122 timesteps\n",
      "Episode reward: -1.062\n",
      "Average of last 10 rewards: -1.072161218147918\n",
      "Finished episode 16111 after 1005867 timesteps\n",
      "Episode reward: -1.112\n",
      "Average of last 10 rewards: -1.0721989440993789\n",
      "Finished episode 16121 after 1006336 timesteps\n",
      "Episode reward: -1.042\n",
      "Average of last 10 rewards: -1.072206765983861\n",
      "Finished episode 16131 after 1006746 timesteps\n",
      "Episode reward: -1.095\n",
      "Average of last 10 rewards: -1.072210794044665\n",
      "Finished episode 16141 after 1007324 timesteps\n",
      "Episode reward: -1.084\n",
      "Average of last 10 rewards: -1.0721889646621203\n",
      "Finished episode 16151 after 1007734 timesteps\n",
      "Episode reward: -1.129\n",
      "Average of last 10 rewards: -1.0721660470879801\n",
      "Finished episode 16161 after 1008480 timesteps\n",
      "Episode reward: -1.083\n",
      "Average of last 10 rewards: -1.0722071207430341\n",
      "Finished episode 16171 after 1009165 timesteps\n",
      "Episode reward: -1.042\n",
      "Average of last 10 rewards: -1.0722102103960394\n",
      "Finished episode 16181 after 1009937 timesteps\n",
      "Episode reward: -1.113\n",
      "Average of last 10 rewards: -1.0722460729746444\n",
      "Finished episode 16191 after 1010515 timesteps\n",
      "Episode reward: -1.053\n",
      "Average of last 10 rewards: -1.0722635970333745\n",
      "Finished episode 16201 after 1010971 timesteps\n",
      "Episode reward: -0.6759999999999999\n",
      "Average of last 10 rewards: -1.0722252625077209\n",
      "Finished episode 16211 after 1011386 timesteps\n",
      "Episode reward: -0.6799999999999999\n",
      "Average of last 10 rewards: -1.0721622839506173\n",
      "Finished episode 16221 after 1011978 timesteps\n",
      "Episode reward: -0.8490000000000001\n",
      "Average of last 10 rewards: -1.072106847624923\n",
      "Finished episode 16231 after 1012400 timesteps\n",
      "Episode reward: -1.074\n",
      "Average of last 10 rewards: -1.0720913686806413\n",
      "Finished episode 16241 after 1013002 timesteps\n",
      "Episode reward: -1.049\n",
      "Average of last 10 rewards: -1.0720803450400493\n",
      "Finished episode 16251 after 1013535 timesteps\n",
      "Episode reward: -1.026\n",
      "Average of last 10 rewards: -1.0720508004926108\n",
      "Finished episode 16261 after 1014046 timesteps\n",
      "Episode reward: -1.071\n",
      "Average of last 10 rewards: -1.0720648\n",
      "Finished episode 16271 after 1014553 timesteps\n",
      "Episode reward: -1.085\n",
      "Average of last 10 rewards: -1.0720809963099631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 16281 after 1015155 timesteps\n",
      "Episode reward: -1.081\n",
      "Average of last 10 rewards: -1.0720819913952058\n",
      "INFO:tensorflow:Saving checkpoints for 1015373 into ./ppo/model/model.ckpt.\n",
      "Finished episode 16291 after 1015795 timesteps\n",
      "Episode reward: -1.046\n",
      "Average of last 10 rewards: -1.072105405405405\n",
      "Finished episode 16301 after 1016551 timesteps\n",
      "Episode reward: -1.093\n",
      "Average of last 10 rewards: -1.072141927562922\n",
      "Finished episode 16311 after 1017286 timesteps\n",
      "Episode reward: -1.4060000000000004\n",
      "Average of last 10 rewards: -1.072158773006135\n",
      "Finished episode 16321 after 1017955 timesteps\n",
      "Episode reward: -1.031\n",
      "Average of last 10 rewards: -1.07218467198038\n",
      "Finished episode 16331 after 1018523 timesteps\n",
      "Episode reward: -1.074\n",
      "Average of last 10 rewards: -1.0721792892156863\n",
      "Finished episode 16341 after 1019205 timesteps\n",
      "Episode reward: -1.245\n",
      "Average of last 10 rewards: -1.0721590324556032\n",
      "Finished episode 16351 after 1019867 timesteps\n",
      "Episode reward: -1.089\n",
      "Average of last 10 rewards: -1.0721629130966952\n",
      "Finished episode 16361 after 1020553 timesteps\n",
      "Episode reward: -0.4730000000000001\n",
      "Average of last 10 rewards: -1.0721559021406728\n",
      "Finished episode 16371 after 1021085 timesteps\n",
      "Episode reward: -0.748\n",
      "Average of last 10 rewards: -1.0721300122249389\n",
      "Finished episode 16381 after 1021615 timesteps\n",
      "Episode reward: -1.043\n",
      "Average of last 10 rewards: -1.0720920586438607\n",
      "Finished episode 16391 after 1022201 timesteps\n",
      "Episode reward: -1.062\n",
      "Average of last 10 rewards: -1.0721088522588522\n",
      "Finished episode 16401 after 1022868 timesteps\n",
      "Episode reward: -1.029\n",
      "Average of last 10 rewards: -1.072075350823673\n",
      "Finished episode 16411 after 1023626 timesteps\n",
      "Episode reward: -0.9310000000000002\n",
      "Average of last 10 rewards: -1.0720719512195123\n",
      "Finished episode 16421 after 1024286 timesteps\n",
      "Episode reward: -1.213\n",
      "Average of last 10 rewards: -1.0721010359536867\n",
      "Finished episode 16431 after 1024797 timesteps\n",
      "Episode reward: -1.052\n",
      "Average of last 10 rewards: -1.0720833739342266\n",
      "Finished episode 16441 after 1025492 timesteps\n",
      "Episode reward: -1.109\n",
      "Average of last 10 rewards: -1.0720655508216677\n",
      "Finished episode 16451 after 1026112 timesteps\n",
      "Episode reward: -1.093\n",
      "Average of last 10 rewards: -1.0720523722627737\n",
      "Finished episode 16461 after 1026893 timesteps\n",
      "Episode reward: -0.6599999999999999\n",
      "Average of last 10 rewards: -1.07201452887538\n",
      "Finished episode 16471 after 1027308 timesteps\n",
      "Episode reward: -0.7450000000000001\n",
      "Average of last 10 rewards: -1.0720003645200487\n",
      "Finished episode 16481 after 1028229 timesteps\n",
      "Episode reward: -0.8560000000000001\n",
      "Average of last 10 rewards: -1.0719818457802066\n",
      "INFO:tensorflow:Saving checkpoints for 1028602 into ./ppo/model/model.ckpt.\n",
      "Finished episode 16491 after 1028746 timesteps\n",
      "Episode reward: -1.113\n",
      "Average of last 10 rewards: -1.0719733009708736\n",
      "Finished episode 16501 after 1029480 timesteps\n",
      "Episode reward: -1.3890000000000002\n",
      "Average of last 10 rewards: -1.0719844147968467\n",
      "Finished episode 16511 after 1030109 timesteps\n",
      "Episode reward: -1.0100000000000002\n",
      "Average of last 10 rewards: -1.071982\n",
      "Finished episode 16521 after 1030615 timesteps\n",
      "Episode reward: -1.056\n",
      "Average of last 10 rewards: -1.0719957601453665\n",
      "Finished episode 16531 after 1031108 timesteps\n",
      "Episode reward: -1.087\n",
      "Average of last 10 rewards: -1.0719525423728813\n",
      "Finished episode 16541 after 1031654 timesteps\n",
      "Episode reward: -1.2550000000000001\n",
      "Average of last 10 rewards: -1.071949969751966\n",
      "Finished episode 16551 after 1032394 timesteps\n",
      "Episode reward: -1.155\n",
      "Average of last 10 rewards: -1.0719377871825877\n",
      "Finished episode 16561 after 1033026 timesteps\n",
      "Episode reward: -1.036\n",
      "Average of last 10 rewards: -1.0719538972809668\n",
      "Finished episode 16571 after 1033748 timesteps\n",
      "Episode reward: -0.744\n",
      "Average of last 10 rewards: -1.0719655797101448\n",
      "Finished episode 16581 after 1034334 timesteps\n",
      "Episode reward: -1.3190000000000002\n",
      "Average of last 10 rewards: -1.071966867833434\n",
      "Finished episode 16591 after 1034890 timesteps\n",
      "Episode reward: -1.1780000000000002\n",
      "Average of last 10 rewards: -1.0719597708082027\n",
      "Finished episode 16601 after 1035278 timesteps\n",
      "Episode reward: -1.04\n",
      "Average of last 10 rewards: -1.0719332730560578\n",
      "Finished episode 16611 after 1035784 timesteps\n",
      "Episode reward: -1.037\n",
      "Average of last 10 rewards: -1.071947590361446\n",
      "Finished episode 16621 after 1036294 timesteps\n",
      "Episode reward: -1.3150000000000002\n",
      "Average of last 10 rewards: -1.0718941601444913\n",
      "Finished episode 16631 after 1036895 timesteps\n",
      "Episode reward: -1.139\n",
      "Average of last 10 rewards: -1.071915102286402\n",
      "Finished episode 16641 after 1037321 timesteps\n",
      "Episode reward: -1.195\n",
      "Average of last 10 rewards: -1.0719221286831029\n",
      "Finished episode 16651 after 1037876 timesteps\n",
      "Episode reward: -0.72\n",
      "Average of last 10 rewards: -1.0718890625\n",
      "Finished episode 16661 after 1038578 timesteps\n",
      "Episode reward: -1.1860000000000002\n",
      "Average of last 10 rewards: -1.0719212612612614\n",
      "Finished episode 16671 after 1039142 timesteps\n",
      "Episode reward: -1.065\n",
      "Average of last 10 rewards: -1.071938475390156\n",
      "Finished episode 16681 after 1039704 timesteps\n",
      "Episode reward: -1.048\n",
      "Average of last 10 rewards: -1.071935572885423\n",
      "Finished episode 16691 after 1040254 timesteps\n",
      "Episode reward: -1.096\n",
      "Average of last 10 rewards: -1.0719666067146285\n",
      "Finished episode 16701 after 1040974 timesteps\n",
      "Episode reward: -1.067\n",
      "Average of last 10 rewards: -1.071994907130018\n",
      "INFO:tensorflow:Saving checkpoints for 1041602 into ./ppo/model/model.ckpt.\n",
      "Finished episode 16711 after 1041646 timesteps\n",
      "Episode reward: -1.185\n",
      "Average of last 10 rewards: -1.0720122155688623\n",
      "Finished episode 16721 after 1042322 timesteps\n",
      "Episode reward: -1.1\n",
      "Average of last 10 rewards: -1.0720485936564932\n",
      "Finished episode 16731 after 1043019 timesteps\n",
      "Episode reward: -1.07\n",
      "Average of last 10 rewards: -1.0720624401913876\n",
      "Finished episode 16741 after 1043802 timesteps\n",
      "Episode reward: -0.6639999999999999\n",
      "Average of last 10 rewards: -1.0720907352062166\n",
      "Finished episode 16751 after 1044383 timesteps\n",
      "Episode reward: -1.091\n",
      "Average of last 10 rewards: -1.072091218637993\n",
      "Finished episode 16761 after 1044763 timesteps\n",
      "Episode reward: -1.07\n",
      "Average of last 10 rewards: -1.0720933134328359\n",
      "Finished episode 16771 after 1045449 timesteps\n",
      "Episode reward: -1.08\n",
      "Average of last 10 rewards: -1.0721365155131266\n",
      "Finished episode 16781 after 1046226 timesteps\n",
      "Episode reward: -1.027\n",
      "Average of last 10 rewards: -1.0721434108527133\n",
      "Finished episode 16791 after 1046874 timesteps\n",
      "Episode reward: -1.139\n",
      "Average of last 10 rewards: -1.072119010727056\n",
      "Finished episode 16801 after 1047873 timesteps\n",
      "Episode reward: -1.1940000000000002\n",
      "Average of last 10 rewards: -1.0721497319833235\n",
      "Finished episode 16811 after 1048675 timesteps\n",
      "Episode reward: -0.7090000000000001\n",
      "Average of last 10 rewards: -1.0721621428571428\n",
      "Finished episode 16821 after 1049452 timesteps\n",
      "Episode reward: -1.092\n",
      "Average of last 10 rewards: -1.072179714455681\n",
      "Finished episode 16831 after 1050064 timesteps\n",
      "Episode reward: -1.05\n",
      "Average of last 10 rewards: -1.0721356123662307\n",
      "Finished episode 16841 after 1050789 timesteps\n",
      "Episode reward: -1.115\n",
      "Average of last 10 rewards: -1.0721717171717173\n",
      "Finished episode 16851 after 1051231 timesteps\n",
      "Episode reward: -1.045\n",
      "Average of last 10 rewards: -1.0721703087885988\n",
      "Finished episode 16861 after 1051823 timesteps\n",
      "Episode reward: -1.05\n",
      "Average of last 10 rewards: -1.0721237982195846\n",
      "Finished episode 16871 after 1052921 timesteps\n",
      "Episode reward: -1.209\n",
      "Average of last 10 rewards: -1.07215871886121\n",
      "Finished episode 16881 after 1053679 timesteps\n",
      "Episode reward: -1.036\n",
      "Average of last 10 rewards: -1.0721976289270896\n",
      "Finished episode 16891 after 1054202 timesteps\n",
      "Episode reward: -1.098\n",
      "Average of last 10 rewards: -1.0721889218009477\n",
      "INFO:tensorflow:Saving checkpoints for 1054555 into ./ppo/model/model.ckpt.\n",
      "Finished episode 16901 after 1054770 timesteps\n",
      "Episode reward: -1.114\n",
      "Average of last 10 rewards: -1.0721857312018948\n",
      "Finished episode 16911 after 1055452 timesteps\n",
      "Episode reward: -1.249\n",
      "Average of last 10 rewards: -1.0721937278106508\n",
      "Finished episode 16921 after 1056009 timesteps\n",
      "Episode reward: -1.034\n",
      "Average of last 10 rewards: -1.0722116499112953\n",
      "Finished episode 16931 after 1056905 timesteps\n",
      "Episode reward: -1.3400000000000003\n",
      "Average of last 10 rewards: -1.0722736406619384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 16941 after 1057420 timesteps\n",
      "Episode reward: -1.077\n",
      "Average of last 10 rewards: -1.07228812758417\n",
      "Finished episode 16951 after 1058271 timesteps\n",
      "Episode reward: -1.062\n",
      "Average of last 10 rewards: -1.0723133412042503\n",
      "Finished episode 16961 after 1059154 timesteps\n",
      "Episode reward: -1.067\n",
      "Average of last 10 rewards: -1.0722945132743362\n",
      "Finished episode 16971 after 1059576 timesteps\n",
      "Episode reward: -1.05\n",
      "Average of last 10 rewards: -1.0722972287735848\n",
      "Finished episode 16981 after 1060382 timesteps\n",
      "Episode reward: -1.2650000000000001\n",
      "Average of last 10 rewards: -1.0723166175604006\n",
      "Finished episode 16991 after 1061048 timesteps\n",
      "Episode reward: -1.2400000000000002\n",
      "Average of last 10 rewards: -1.0723025323910482\n",
      "Finished episode 17001 after 1061706 timesteps\n",
      "Episode reward: -1.1660000000000001\n",
      "Average of last 10 rewards: -1.0723335491465569\n",
      "Finished episode 17011 after 1062615 timesteps\n",
      "Episode reward: -0.6620000000000001\n",
      "Average of last 10 rewards: -1.0723377647058825\n",
      "Finished episode 17021 after 1063276 timesteps\n",
      "Episode reward: -1.1620000000000001\n",
      "Average of last 10 rewards: -1.0723420340975898\n",
      "Finished episode 17031 after 1063860 timesteps\n",
      "Episode reward: -1.057\n",
      "Average of last 10 rewards: -1.0723165687426557\n",
      "Finished episode 17041 after 1064238 timesteps\n",
      "Episode reward: -1.049\n",
      "Average of last 10 rewards: -1.072315913094539\n",
      "Finished episode 17051 after 1064942 timesteps\n",
      "Episode reward: -1.057\n",
      "Average of last 10 rewards: -1.0723264084507043\n",
      "Finished episode 17061 after 1065528 timesteps\n",
      "Episode reward: -1.069\n",
      "Average of last 10 rewards: -1.0722994134897361\n",
      "Finished episode 17071 after 1066061 timesteps\n",
      "Episode reward: -0.7110000000000001\n",
      "Average of last 10 rewards: -1.0722989449003517\n",
      "Finished episode 17081 after 1067139 timesteps\n",
      "Episode reward: -0.9100000000000001\n",
      "Average of last 10 rewards: -1.0723124194493263\n",
      "Finished episode 17091 after 1067692 timesteps\n",
      "Episode reward: -1.085\n",
      "Average of last 10 rewards: -1.0723029274004685\n",
      "INFO:tensorflow:Saving checkpoints for 1067766 into ./ppo/model/model.ckpt.\n",
      "Finished episode 17101 after 1068507 timesteps\n",
      "Episode reward: -1.239\n",
      "Average of last 10 rewards: -1.0723263311878293\n",
      "Finished episode 17111 after 1069122 timesteps\n",
      "Episode reward: -1.2560000000000002\n",
      "Average of last 10 rewards: -1.0723541520467836\n",
      "Finished episode 17121 after 1070079 timesteps\n",
      "Episode reward: -1.233\n",
      "Average of last 10 rewards: -1.072381414377557\n",
      "Finished episode 17131 after 1070971 timesteps\n",
      "Episode reward: -1.153\n",
      "Average of last 10 rewards: -1.0724391355140186\n",
      "Finished episode 17141 after 1071505 timesteps\n",
      "Episode reward: -1.059\n",
      "Average of last 10 rewards: -1.0724432574430824\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and run the environment for 5 episodes.\n",
    "wrapped_env = WrappedEnv(env, False)\n",
    "runner = Runner(agent=agent, environment=wrapped_env)\n",
    "runner.run(num_episodes=num_episodes, episode_finished=episode_finished, max_episode_timesteps=env._max_steps)\n",
    "print(\"Stats: \", runner.episode_rewards, runner.episode_timesteps, runner.episode_times)\n",
    "\n",
    "try:\n",
    "    runner.close()\n",
    "except AttributeError as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
