{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import multiprocessing\n",
    "from queue import Empty\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import pommerman\n",
    "from pommerman.agents import BaseAgent, SimpleAgent\n",
    "from pommerman import constants\n",
    "from pommerman.constants import BOARD_SIZE\n",
    "\n",
    "from keras.models import Model, model_from_json\n",
    "from keras.layers import Input, Conv2D, Flatten, Dense\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_AGENTS = 4\n",
    "NUM_ACTIONS = len(constants.Action)\n",
    "\n",
    "# environmental parameters\n",
    "LOAD_MODEL = './dagger/model/il_go_3res_block/model.h4'\n",
    "LOGDIR = './mcts/pretrained_go_3blocks/'\n",
    "RENDER = False\n",
    "NUM_EPISODES = 1000000 \n",
    "SAVE_INTERVAL = 20 \n",
    "\n",
    "# queue params\n",
    "QUEUE_LENGTH = 10\n",
    "QUEUE_TIMEOUT = 1\n",
    "\n",
    "# runner params\n",
    "NUM_RUNNERS = 16\n",
    "MAX_STEPS = constants.MAX_STEPS\n",
    "\n",
    "# trainer params\n",
    "BATCH_SIZE = 2048\n",
    "NUM_EPOCHS = 100\n",
    "RESET_NETWORK = False\n",
    "\n",
    "# MCTS params\n",
    "MCTS_C_PUCT            = 1.0\n",
    "MCTS_DIRICHLET_EPSILON = 0.25 \n",
    "MCTS_ITERS             = 300\n",
    "MCTS_DIRICHLET_ALPHA   = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTSNode(object):\n",
    "    def __init__(self, p):\n",
    "        # values for 6 actions\n",
    "        self.Q = np.zeros(NUM_ACTIONS)\n",
    "        self.W = np.zeros(NUM_ACTIONS)\n",
    "        self.N = np.zeros(NUM_ACTIONS)\n",
    "        assert p.shape == (NUM_ACTIONS,)\n",
    "        self.P = p\n",
    "\n",
    "    def action(self):\n",
    "        U = MCTS_C_PUCT * self.P * np.sqrt(np.sum(self.N)) / (1 + self.N)\n",
    "        # TODO: use random tie-breaking for equal values\n",
    "        return np.argmax(self.Q + U)\n",
    "\n",
    "    def update(self, action, reward):\n",
    "        self.W[action] += reward\n",
    "        self.N[action] += 1\n",
    "        self.Q[action] = self.W[action] / self.N[action]\n",
    "\n",
    "    def probs(self, temperature=1):\n",
    "        if temperature == 0:\n",
    "            p = np.zeros(NUM_ACTIONS)\n",
    "            p[np.argmax(self.N)] = 1\n",
    "            return p\n",
    "        else:\n",
    "            Nt = self.N ** (1.0 / temperature)\n",
    "            return Nt / np.sum(Nt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTSAgent(BaseAgent):\n",
    "    def __init__(self, model_file=None, train=False, agent_id=0):\n",
    "        super().__init__()\n",
    "        self.agent_id = agent_id\n",
    "\n",
    "        if train:\n",
    "            self.env = self.make_env()\n",
    "\n",
    "        if model_file is None:\n",
    "            self.model = make_model()\n",
    "        else:\n",
    "            self.model = load_model(model_file)\n",
    "\n",
    "        self.reset_tree()\n",
    "\n",
    "    def make_env(self):\n",
    "        agents = []\n",
    "        for agent_id in range(NUM_AGENTS):\n",
    "            if agent_id == self.agent_id:\n",
    "                agents.append(self)\n",
    "            else:\n",
    "                agents.append(SimpleAgent())\n",
    "\n",
    "        return pommerman.make('PommeFFACompetition-v0', agents)\n",
    "\n",
    "    def reset_tree(self):\n",
    "        self.tree = {}\n",
    "        # for statistics\n",
    "        self.hit_probs = []\n",
    "        self.avg_lengths = []\n",
    "        self.entropies = []\n",
    "\n",
    "    def observation_to_features(self, obs):\n",
    "        shape = (BOARD_SIZE, BOARD_SIZE, 1)\n",
    "\n",
    "        def get_matrix(dict, key):\n",
    "            res = dict[key]\n",
    "            return res.reshape(shape).astype(np.float32)\n",
    "\n",
    "        def get_map(board, item):\n",
    "            map = np.zeros(shape)\n",
    "            map[board == item] = 1\n",
    "            return map\n",
    "\n",
    "        board = get_matrix(obs, 'board')\n",
    "\n",
    "        # TODO: probably not needed Passage = 0\n",
    "        rigid_map = get_map(board, 1)               # Rigid = 1\n",
    "        wood_map = get_map(board, 2)                # Wood = 2\n",
    "        bomb_map = get_map(board, 3)                # Bomb = 3\n",
    "        flames_map = get_map(board, 4)              # Flames = 4\n",
    "        fog_map = get_map(board, 5)                 # TODO: not used for first two stages Fog = 5\n",
    "        extra_bomb_map = get_map(board, 6)          # ExtraBomb = 6\n",
    "        incr_range_map = get_map(board, 7)          # IncrRange = 7\n",
    "        kick_map = get_map(board, 8)                # Kick = 8\n",
    "        skull_map = get_map(board, 9)               # Skull = 9\n",
    "\n",
    "        position = obs[\"position\"]\n",
    "        my_position = np.zeros(shape)\n",
    "        my_position[position[0], position[1], 0] = 1\n",
    "\n",
    "        team_mates = get_map(board, obs[\"teammate\"].value) # TODO during documentation it should be an array\n",
    "\n",
    "        enemies = np.zeros(shape)\n",
    "        for enemy in obs[\"enemies\"]:\n",
    "            enemies[board == enemy.value] = 1\n",
    "\n",
    "        bomb_blast_strength = get_matrix(obs, 'bomb_blast_strength')\n",
    "        bomb_life = get_matrix(obs, 'bomb_life')\n",
    "\n",
    "        ammo = np.full((BOARD_SIZE, BOARD_SIZE, 1), obs[\"ammo\"])\n",
    "        blast_strength = np.full((BOARD_SIZE, BOARD_SIZE, 1), obs[\"blast_strength\"])\n",
    "        can_kick = np.full((BOARD_SIZE, BOARD_SIZE, 1), int(obs[\"can_kick\"]))\n",
    "\n",
    "        obs = np.concatenate([my_position, enemies, team_mates, rigid_map,\n",
    "                              wood_map, bomb_map, flames_map,\n",
    "                              fog_map, extra_bomb_map, incr_range_map,\n",
    "                              kick_map, skull_map, bomb_blast_strength,\n",
    "                              bomb_life, ammo, blast_strength, can_kick], axis=2)\n",
    "        return obs \n",
    "\n",
    "    def search(self, root, num_iters, temperature=1):\n",
    "        # remember current game state\n",
    "        self.env._init_game_state = root\n",
    "        root = str(self.env.get_json_info())\n",
    "\n",
    "        # for statistics\n",
    "        hits = 0\n",
    "        misses = 0\n",
    "        total_length = 0\n",
    "        for i in range(num_iters):\n",
    "            # restore game state to root node\n",
    "            obs = self.env.reset()\n",
    "            #print('\\rStep %d: iteration %d' % (self.env._step_count, i + 1), end=' ')\n",
    "            # serialize game state\n",
    "            state = str(self.env.get_json_info())\n",
    "\n",
    "            trace = []\n",
    "            done = False\n",
    "            while not done:\n",
    "                #print(board)\n",
    "                if state in self.tree:\n",
    "                    node = self.tree[state]\n",
    "                    # choose actions based on Q + U\n",
    "                    action = node.action()\n",
    "                    trace.append((node, action))\n",
    "                    #print(\"Action from tree:\", constants.Action(action).name)\n",
    "                    hits += 1\n",
    "                else:\n",
    "                    # initialize action probabilities with policy network\n",
    "                    feats = self.observation_to_features(obs[self.agent_id])\n",
    "                    feats = feats[np.newaxis, ...]\n",
    "                    probs, values = self.model.predict(feats)\n",
    "                    probs = probs[0]\n",
    "                    values = values[0]\n",
    "\n",
    "                    # add Dirichlet noise to root node for added exploration\n",
    "                    if len(trace) == 0:\n",
    "                        noise = np.random.dirichlet([MCTS_DIRICHLET_ALPHA] * len(probs))\n",
    "                        probs = (1 - MCTS_DIRICHLET_EPSILON) * probs + MCTS_DIRICHLET_EPSILON * noise\n",
    "\n",
    "                    # add new node to the tree\n",
    "                    self.tree[state] = MCTSNode(probs)\n",
    "\n",
    "                    # get a reward\n",
    "                    rewards = self.env._get_rewards()\n",
    "                    reward = rewards[self.agent_id]\n",
    "\n",
    "                    misses += 1\n",
    "                    #print(\"Leaf node\")\n",
    "                    # stop at leaf node\n",
    "                    break\n",
    "\n",
    "                # ensure we are not called recursively\n",
    "                assert self.env.training_agent == self.agent_id\n",
    "                # make other agents act\n",
    "                actions = self.env.act(obs)\n",
    "                # add my action to list of actions\n",
    "                actions.insert(self.agent_id, action)\n",
    "                # step environment forward\n",
    "                try:\n",
    "                    obs, rewards, done, info = self.env.step(actions)\n",
    "                except:\n",
    "                    continue\n",
    "                reward = rewards[self.agent_id]\n",
    "\n",
    "                state = str(self.env.get_json_info())\n",
    "\n",
    "            total_length += len(trace)\n",
    "\n",
    "            #print(\"Finished rollout, length:\", len(trace))\n",
    "            #print(\"Backpropagating rewards:\", rewards)\n",
    "\n",
    "            # update tree nodes with rollout results\n",
    "            for node, action in trace:\n",
    "                node.update(action, reward)\n",
    "\n",
    "            #print(\"Root Q:\")\n",
    "            #print(self.tree[root].Q)\n",
    "            #print(\"Root N:\")\n",
    "            #print(self.tree[root].N)\n",
    "\n",
    "        #print(\"(tree hits: %0.2f, avg. len: %0.2f, tree size: %d)\" % (hits / (hits + misses), total_length / num_iters, len(self.tree)))\n",
    "        self.hit_probs.append(hits / (hits + misses))\n",
    "        self.avg_lengths.append(total_length / num_iters)\n",
    "\n",
    "        # reset env back where we were\n",
    "        self.env.set_json_info()\n",
    "        self.env._init_game_state = None\n",
    "        # return action probabilities\n",
    "        pi = self.tree[root].probs(temperature)\n",
    "        idx = (pi != 0)\n",
    "        self.entropies.append(-np.sum(pi[idx] * np.log(pi[idx])))\n",
    "        return pi\n",
    "\n",
    "    def rollout(self, shared_buffer, finished):\n",
    "        # reset search tree in the beginning of each rollout\n",
    "        self.reset_tree()\n",
    "\n",
    "        # guarantees that we are not called recursively\n",
    "        # and episode ends when this agent dies\n",
    "        self.env.training_agent = self.agent_id\n",
    "        obs = self.env.reset()\n",
    "\n",
    "        trace = []\n",
    "        done = False\n",
    "        while not done and not finished.value:\n",
    "            if RENDER:\n",
    "                self.env.render()\n",
    "\n",
    "            # copy weights from trainer\n",
    "            self.model.set_weights(pickle.loads(shared_buffer.raw))\n",
    "\n",
    "            # use temperature 1 for first 30 steps and temperature 0 afterwards\n",
    "            temp = 1 #if self.env._step_count < 30 else 0\n",
    "            # TODO: only works when agent has access to the env\n",
    "            root = self.env.get_json_info()\n",
    "            # do Monte-Carlo tree search\n",
    "            pi = self.search(root, MCTS_ITERS, temp)\n",
    "            # sample action from probabilities\n",
    "            action = np.random.choice(NUM_ACTIONS, p=pi)\n",
    "            # record observations and action probabilities\n",
    "            feats = self.observation_to_features(obs[self.agent_id])\n",
    "            trace.append((feats, pi))\n",
    "\n",
    "            # ensure we are not called recursively\n",
    "            assert self.env.training_agent == self.agent_id\n",
    "            # make other agents act\n",
    "            actions = self.env.act(obs)\n",
    "            # add my action to list of actions\n",
    "            actions.insert(self.agent_id, action)\n",
    "            # step environment\n",
    "            try:\n",
    "                obs, rewards, done, info = self.env.step(actions)\n",
    "            except:\n",
    "                continue\n",
    "            assert self == self.env._agents[self.agent_id]\n",
    "            print(\"Agent:\", self.agent_id, \"Step:\", self.env._step_count, \"Actions:\", [constants.Action(a).name for a in actions], \"Entropy: %.2f\" % self.entropies[-1], \"Rewards:\", rewards, \"Done:\", done)\n",
    "\n",
    "            #print(\"Rollout finished:\", finished.value)\n",
    "\n",
    "        reward = rewards[self.agent_id]\n",
    "        #print(\"Agent:\", self.agent_id, \"Reward:\", reward, \"Len trace:\", len(trace))\n",
    "        return trace, reward, rewards\n",
    "\n",
    "    def act(self, obs, action_space):\n",
    "        feats = self.observation_to_features(obs)\n",
    "        feats = feats[np.newaxis, ...]\n",
    "        probs, values = self.model.predict(feats)\n",
    "        probs = probs[0]\n",
    "        return np.argmax(probs)\n",
    "        # sample action from probabilities\n",
    "        #return np.random.choice(NUM_ACTIONS, p=pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    def __init__(self, size=100000):\n",
    "        self.observations = np.empty((size, constants.BOARD_SIZE, constants.BOARD_SIZE, 17))\n",
    "        self.action_probs = np.empty((size, NUM_ACTIONS))\n",
    "        self.state_values = np.empty((size,))\n",
    "        self.size = size\n",
    "        self.current = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def add_sample(self, obs, pi, z):\n",
    "        self.observations[self.current] = obs\n",
    "        self.action_probs[self.current] = pi\n",
    "        self.state_values[self.current] = z\n",
    "        self.current = (self.current + 1) % self.size\n",
    "        if self.count < self.size:\n",
    "            self.count += 1\n",
    "\n",
    "    def dataset(self):\n",
    "        return self.observations[:self.count], self.action_probs[:self.count], self.state_values[:self.count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_res_block(self, input):\n",
    "    # Res block 1        \n",
    "    x = Convolution2D(256, 3, padding='same')(input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Convolution2D(256, 3, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Add()([input, x])\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def make_model():\n",
    "    inp = Input(input_shape)\n",
    "    x = Convolution2D(256, 3, padding='same')(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # Ten residual blocks\n",
    "    for i in range(3):\n",
    "        x = get_res_block(x)\n",
    "\n",
    "    # Output block\n",
    "    # Should be 2 filters\n",
    "    x = Convolution2D(4, 1, padding='same')(x)\n",
    "    x = BatchNormalization()(x)   \n",
    "    x = Activation('relu')(x)\n",
    "    x = Flatten()(x)\n",
    "    out = Dense(actions, activation='softmax')(x)\n",
    "    model = Model(inputs = inp, outputs=out)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_model(model_path):\n",
    "    model = keras.models.load_model(LOAD_MODEL)\n",
    "    inp = model.layers[0]\n",
    "    probabilities = model.layers[-1].output\n",
    "    h = model.layers[-2]\n",
    "    reward = Dense(128, activation='relu', name='reward_1')(h.output)\n",
    "    reward = Dense(1, activation='tanh', name='reward_out')(reward)\n",
    "    model = Model(inp.input, [probabilities, reward])\n",
    "    model.compile(optimizer='adam', loss=['categorical_crossentropy', 'mse'])\n",
    "    return model\n",
    "\n",
    "def init_tensorflow():\n",
    "    # make sure TF does not allocate all memory\n",
    "    # NB! this needs to be done also in subprocesses!\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    K.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runner(id, model_file, shared_buffer, fifo, finished):\n",
    "    # initialize tensorflow\n",
    "    init_tensorflow()\n",
    "    # make sure agents play at all positions\n",
    "    agent_id = id % NUM_AGENTS\n",
    "    agent = MCTSAgent(model_file, train=True, agent_id=agent_id)\n",
    "\n",
    "    while not finished.value:\n",
    "        # do rollout\n",
    "        trace, reward, rewards = agent.rollout(shared_buffer, finished)\n",
    "        # don't put last trace into fifo\n",
    "        if finished.value:\n",
    "            break\n",
    "        # add data samples to training set\n",
    "        fifo.put((trace, reward, rewards, agent_id, agent.hit_probs, agent.avg_lengths, len(agent.tree), agent.entropies))\n",
    "        #print(\"Runner finished:\", finished.value)\n",
    "\n",
    "    #print(\"Runner done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(num_episodes, fifos, shared_buffer, model, memory, writer):\n",
    "    early_stopping = EarlyStopping(monitor='loss', min_delta=0.003, patience=5, verbose=1, mode='auto')\n",
    "    callbacks = [early_stopping]\n",
    "    while num_episodes < NUM_EPISODES:\n",
    "        while True:\n",
    "            # pick random fifo (agent)\n",
    "            fifo = random.choice(fifos)\n",
    "            try:\n",
    "                # wait for a new trajectory and statistics\n",
    "                trace, reward, rewards, agent_id, hit_probs, avg_lengths, tree_size, entropies = \\\n",
    "                    fifo.get(timeout=QUEUE_TIMEOUT)\n",
    "                # break out of the infinite loop\n",
    "                break\n",
    "            except Empty:\n",
    "                # just ignore empty fifos\n",
    "                pass\n",
    "\n",
    "        num_episodes += 1\n",
    "\n",
    "        # add samples to replay memory\n",
    "        # TODO: add_batch would be more efficient?\n",
    "        for obs, pi in trace:\n",
    "            memory.add_sample(obs, pi, reward)\n",
    "\n",
    "        add_summary(writer, \"tree/size\", tree_size, num_episodes)\n",
    "        add_summary(writer, \"tree/mean_hit_prob\", float(np.mean(hit_probs)), num_episodes)\n",
    "        add_summary(writer, \"tree/mean_rollout_len\", float(np.mean(avg_lengths)), num_episodes)\n",
    "        add_histogram(writer, \"tree/hit_probability\", hit_probs, num_episodes)\n",
    "        add_histogram(writer, \"tree/rollout_length\", avg_lengths, num_episodes)\n",
    "        add_histogram(writer, \"tree/entropies\", entropies, num_episodes)\n",
    "        add_summary(writer, \"episode/mean_entropy\", float(np.mean(entropies)), num_episodes)\n",
    "        add_summary(writer, \"episode/reward\", reward, num_episodes)\n",
    "        add_summary(writer, \"episode/length\", len(trace), num_episodes)\n",
    "        add_summary(writer, \"rewards/agent_id\", agent_id, num_episodes)\n",
    "        for i in range(len(rewards)):\n",
    "            add_summary(writer, \"rewards/agent%d\" % i, rewards[i], num_episodes)\n",
    "        add_summary(writer, \"replay_memory/size\", memory.size, num_episodes)\n",
    "        add_summary(writer, \"replay_memory/count\", memory.count, num_episodes)\n",
    "        add_summary(writer, \"replay_memory/current\", memory.current, num_episodes)\n",
    "\n",
    "        #print(\"Replay memory size: %d, count: %d, current: %d\" % (memory.size, memory.count, memory.current))\n",
    "        X, y, z = memory.dataset()\n",
    "        assert len(X) != 0\n",
    "\n",
    "        # reset weights?\n",
    "        if RESET_NETWORK:\n",
    "            #model.set_weights(init_weights)\n",
    "            model = model_from_json(model.to_json())\n",
    "            model.compile(optimizer='adam', loss=['categorical_crossentropy', 'mse'])\n",
    "        # train for limited epochs to avoid overfitting?\n",
    "        history = model.fit(X, [y, z], batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, callbacks=callbacks)\n",
    "        # log loss values\n",
    "        for k, v in history.history.items():\n",
    "            add_summary(writer, \"training/\" + k, v[-1], num_episodes)\n",
    "        # shared weights with runners\n",
    "        shared_buffer.raw = pickle.dumps(model.get_weights(), pickle.HIGHEST_PROTOCOL)\n",
    "        # save weights\n",
    "        if num_episodes % SAVE_INTERVAL == 0:\n",
    "            model.save(os.path.join(LOGDIR, \"model_%d.hdf5\" % num_episodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":    \n",
    "    global runner\n",
    "    memory = ReplayMemory()\n",
    "    if not os.path.isdir(LOGDIR):\n",
    "        os.makedirs(LOGDIR)\n",
    "\n",
    "    # use spawn method for starting subprocesses\n",
    "    # this seems to be more compatible with TensorFlow?\n",
    "    ctx = multiprocessing.get_context('spawn')\n",
    "\n",
    "    # check for commandline argument or previous model file\n",
    "    num_episodes = 0\n",
    "    model_file = None\n",
    "    if LOAD_MODEL:\n",
    "        model_file = LOAD_MODEL\n",
    "    else:\n",
    "        files = glob.glob(os.path.join(LOGDIR, \"model_*.hdf5\"))\n",
    "        if files:\n",
    "            model_file = max(files, key=lambda f: int(re.search(r'_(\\d+).hdf5', f).group(1)))\n",
    "            # set start timestep from file name when continuing previous session\n",
    "            num_episodes = int(re.search(r'_(\\d+).hdf5', model_file).group(1))\n",
    "            print(\"Setting start episode to %d\" % num_episodes)\n",
    "\n",
    "    # load saved model\n",
    "    init_tensorflow()\n",
    "    if model_file:\n",
    "        print(\"Loading model:\", model_file)\n",
    "        model = load_model(model_file)\n",
    "    else:\n",
    "        print(\"Initializing new model\")\n",
    "        model = make_model()\n",
    "    model.summary()\n",
    "\n",
    "    # create shared buffer for sharing weights\n",
    "    print(\"Creating shared memory for model\")\n",
    "    init_weights = model.get_weights()\n",
    "    blob = pickle.dumps(init_weights, pickle.HIGHEST_PROTOCOL)\n",
    "    shared_buffer = ctx.Array('c', len(blob))\n",
    "    shared_buffer.raw = blob\n",
    "\n",
    "    # create boolean to signal end\n",
    "    finished = ctx.Value('i', 0)\n",
    "\n",
    "    # create fifos and processes for all runners\n",
    "    print(\"Creating child processes\")\n",
    "    fifos = []\n",
    "    for i in range(NUM_RUNNERS):\n",
    "        fifo = ctx.Queue(QUEUE_LENGTH)\n",
    "        fifos.append(fifo)\n",
    "        process = ctx.Process(target=runner, args=(i, model_file, shared_buffer, fifo, finished))\n",
    "        process.start()\n",
    "\n",
    "    from tensorboard_utils import create_summary_writer, add_summary, add_histogram\n",
    "    writer = create_summary_writer(LOGDIR)\n",
    "\n",
    "    # do training in main process\n",
    "    print(\"Starting training in main process\")\n",
    "    trainer(num_episodes, fifos, shared_buffer, model, memory, writer)\n",
    "    finished.value = 1\n",
    "    print(\"Finishing\")\n",
    "    # empty queues until all child processes have exited\n",
    "    while len(multiprocessing.active_children()) > 0:\n",
    "        for i, fifo in enumerate(fifos):\n",
    "            if not fifo.empty():\n",
    "                fifo.get_nowait()\n",
    "    print(\"All done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs/hpchome/anton95/.conda/envs/pommerman/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/gpfs/hpchome/anton95/.conda/envs/pommerman/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "Import error! You will not be able to render --> Cannot connect to \"None\"\n",
      "2018-05-27 15:41:53.125616: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-05-27 15:41:56.781498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
      "pciBusID: 0000:09:00.0\n",
      "totalMemory: 15.89GiB freeMemory: 15.61GiB\n",
      "2018-05-27 15:41:56.781587: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:09:00.0, compute capability: 6.0)\n",
      "Loading model: ./dagger/model/il_go_3res_block/model.h4\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 11, 11, 17)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 11, 11, 256)  39424       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 11, 11, 256)  1024        conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 11, 11, 256)  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 11, 11, 256)  590080      activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 11, 11, 256)  1024        conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 11, 11, 256)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 11, 11, 256)  590080      activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 11, 11, 256)  1024        conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 11, 11, 256)  0           activation_1[0][0]               \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 11, 11, 256)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 11, 11, 256)  590080      activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 11, 11, 256)  1024        conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 11, 11, 256)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 11, 11, 256)  590080      activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 11, 11, 256)  1024        conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 11, 11, 256)  0           activation_3[0][0]               \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 11, 11, 256)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 11, 11, 256)  590080      activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 11, 11, 256)  1024        conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 11, 11, 256)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 11, 11, 256)  590080      activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 11, 11, 256)  1024        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 11, 11, 256)  0           activation_5[0][0]               \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 11, 11, 256)  0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 11, 11, 4)    1028        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 11, 11, 4)    16          conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 11, 11, 4)    0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 484)          0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reward_1 (Dense)                (None, 128)          62080       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 6)            2910        flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reward_out (Dense)              (None, 1)            129         reward_1[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 3,653,235\n",
      "Trainable params: 3,649,643\n",
      "Non-trainable params: 3,592\n",
      "__________________________________________________________________________________________________\n",
      "Creating shared memory for model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating child processes\n",
      "Starting training in main process\n",
      "/gpfs/hpchome/anton95/.conda/envs/pommerman/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/gpfs/hpchome/anton95/.conda/envs/pommerman/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/gpfs/hpchome/anton95/.conda/envs/pommerman/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/gpfs/hpchome/anton95/.conda/envs/pommerman/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/gpfs/hpchome/anton95/.conda/envs/pommerman/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/gpfs/hpchome/anton95/.conda/envs/pommerman/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/gpfs/hpchome/anton95/.conda/envs/pommerman/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/gpfs/hpchome/anton95/.conda/envs/pommerman/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/gpfs/hpchome/anton95/.conda/envs/pommerman/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/gpfs/hpchome/anton95/.conda/envs/pommerman/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/gpfs/hpchome/anton95/.conda/envs/pommerman/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/gpfs/hpchome/anton95/.conda/envs/pommerman/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "Using TensorFlow backend.\n",
      "Using TensorFlow backend.\n",
      "Using TensorFlow backend.\n",
      "Using TensorFlow backend.\n",
      "Using TensorFlow backend.\n",
      "Using TensorFlow backend.\n",
      "Using TensorFlow backend.\n",
      "Using TensorFlow backend.\n",
      "Using TensorFlow backend.\n",
      "Using TensorFlow backend.\n",
      "Using TensorFlow backend.\n",
      "/gpfs/hpchome/anton95/.conda/envs/pommerman/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/gpfs/hpchome/anton95/.conda/envs/pommerman/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/gpfs/hpchome/anton95/.conda/envs/pommerman/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/gpfs/hpchome/anton95/.conda/envs/pommerman/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/gpfs/hpchome/anton95/.conda/envs/pommerman/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/gpfs/hpchome/anton95/.conda/envs/pommerman/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/gpfs/hpchome/anton95/.conda/envs/pommerman/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/gpfs/hpchome/anton95/.conda/envs/pommerman/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/gpfs/hpchome/anton95/.conda/envs/pommerman/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/gpfs/hpchome/anton95/.conda/envs/pommerman/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/gpfs/hpchome/anton95/.conda/envs/pommerman/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/gpfs/hpchome/anton95/.conda/envs/pommerman/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "Import error! You will not be able to render --> Cannot connect to \"None\"\n",
      "Import error! You will not be able to render --> Cannot connect to \"None\"\n",
      "2018-05-27 15:42:09.501558: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-05-27 15:42:09.502282: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "Import error! You will not be able to render --> Cannot connect to \"None\"\n",
      "Import error! You will not be able to render --> Cannot connect to \"None\"\n",
      "2018-05-27 15:42:09.513553: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-05-27 15:42:09.519006: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "Import error! You will not be able to render --> Cannot connect to \"None\"\n",
      "Import error! You will not be able to render --> Cannot connect to \"None\"\n",
      "Import error! You will not be able to render --> Cannot connect to \"None\"\n",
      "2018-05-27 15:42:09.528707: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "Import error! You will not be able to render --> Cannot connect to \"None\"\n",
      "Import error! You will not be able to render --> Cannot connect to \"None\"\n",
      "Import error! You will not be able to render --> Cannot connect to \"None\"\n",
      "2018-05-27 15:42:09.533173: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-05-27 15:42:09.533571: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "Import error! You will not be able to render --> Cannot connect to \"None\"\n",
      "2018-05-27 15:42:09.535851: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-05-27 15:42:09.535916: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-05-27 15:42:09.535995: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import error! You will not be able to render --> Cannot connect to \"None\"\n",
      "2018-05-27 15:42:09.541094: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-05-27 15:42:09.546185: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2018-05-27 15:42:17.746753: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
      "pciBusID: 0000:09:00.0\n",
      "totalMemory: 15.89GiB freeMemory: 14.76GiB\n",
      "2018-05-27 15:42:17.746994: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:09:00.0, compute capability: 6.0)\n",
      "2018-05-27 15:42:17.754920: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
      "pciBusID: 0000:09:00.0\n",
      "totalMemory: 15.89GiB freeMemory: 14.49GiB\n",
      "2018-05-27 15:42:17.755081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:09:00.0, compute capability: 6.0)\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "2018-05-27 15:42:17.792527: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
      "pciBusID: 0000:09:00.0\n",
      "totalMemory: 15.89GiB freeMemory: 13.79GiB\n",
      "2018-05-27 15:42:17.792638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:09:00.0, compute capability: 6.0)\n",
      "2018-05-27 15:42:17.796800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
      "pciBusID: 0000:09:00.0\n",
      "totalMemory: 15.89GiB freeMemory: 13.79GiB\n",
      "2018-05-27 15:42:17.796868: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:09:00.0, compute capability: 6.0)\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "2018-05-27 15:42:17.805851: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
      "pciBusID: 0000:09:00.0\n",
      "totalMemory: 15.89GiB freeMemory: 13.79GiB\n",
      "2018-05-27 15:42:17.805929: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:09:00.0, compute capability: 6.0)\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "2018-05-27 15:42:17.815851: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
      "pciBusID: 0000:09:00.0\n",
      "totalMemory: 15.89GiB freeMemory: 13.09GiB\n",
      "2018-05-27 15:42:17.815934: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:09:00.0, compute capability: 6.0)\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "2018-05-27 15:42:17.817770: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
      "pciBusID: 0000:09:00.0\n",
      "totalMemory: 15.89GiB freeMemory: 12.86GiB\n",
      "2018-05-27 15:42:17.817824: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:09:00.0, compute capability: 6.0)\n",
      "2018-05-27 15:42:17.820725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
      "pciBusID: 0000:09:00.0\n",
      "totalMemory: 15.89GiB freeMemory: 12.86GiB\n",
      "2018-05-27 15:42:17.820783: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:09:00.0, compute capability: 6.0)\n",
      "2018-05-27 15:42:17.824740: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
      "pciBusID: 0000:09:00.0\n",
      "totalMemory: 15.89GiB freeMemory: 12.39GiB\n",
      "2018-05-27 15:42:17.824800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:09:00.0, compute capability: 6.0)\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "2018-05-27 15:42:17.829099: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
      "pciBusID: 0000:09:00.0\n",
      "totalMemory: 15.89GiB freeMemory: 12.16GiB\n",
      "2018-05-27 15:42:17.829156: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:09:00.0, compute capability: 6.0)\n",
      "2018-05-27 15:42:17.831115: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
      "pciBusID: 0000:09:00.0\n",
      "totalMemory: 15.89GiB freeMemory: 12.16GiB\n",
      "2018-05-27 15:42:17.831170: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:09:00.0, compute capability: 6.0)\n",
      "2018-05-27 15:42:17.831288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
      "pciBusID: 0000:09:00.0\n",
      "totalMemory: 15.89GiB freeMemory: 12.16GiB\n",
      "2018-05-27 15:42:17.831346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:09:00.0, compute capability: 6.0)\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Train on 9 samples, validate on 2 samples\n",
      "Epoch 1/100\n",
      "9/9 [==============================] - 5s 597ms/step - loss: 15.3402 - dense_1_loss: 15.2540 - reward_out_loss: 0.0862 - val_loss: 5.5010 - val_dense_1_loss: 5.5010 - val_reward_out_loss: 5.8013e-05\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 3.6905 - dense_1_loss: 3.6793 - reward_out_loss: 0.0113 - val_loss: 4.8338 - val_dense_1_loss: 4.8338 - val_reward_out_loss: 3.4104e-05\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3.2253 - dense_1_loss: 3.2201 - reward_out_loss: 0.0052 - val_loss: 10.8375 - val_dense_1_loss: 10.8375 - val_reward_out_loss: 1.4080e-05\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0209 - dense_1_loss: 0.0158 - reward_out_loss: 0.0051 - val_loss: 12.8033 - val_dense_1_loss: 12.8033 - val_reward_out_loss: 5.5339e-06\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0748 - dense_1_loss: 0.0713 - reward_out_loss: 0.0035 - val_loss: 13.7863 - val_dense_1_loss: 13.7862 - val_reward_out_loss: 2.5181e-06\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.0191 - dense_1_loss: 0.0179 - reward_out_loss: 0.0012 - val_loss: 14.3120 - val_dense_1_loss: 14.3120 - val_reward_out_loss: 1.2480e-06\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0033 - dense_1_loss: 0.0029 - reward_out_loss: 3.5595e-04 - val_loss: 14.5595 - val_dense_1_loss: 14.5595 - val_reward_out_loss: 5.8466e-07\n",
      "Epoch 00007: early stopping\n",
      "Train on 19 samples, validate on 3 samples\n",
      "Epoch 1/100\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.2404 - dense_1_loss: 2.2396 - reward_out_loss: 8.7122e-04 - val_loss: 2.7587 - val_dense_1_loss: 2.7584 - val_reward_out_loss: 2.5997e-04\n",
      "Epoch 2/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 1.0593 - dense_1_loss: 1.0586 - reward_out_loss: 6.6372e-04 - val_loss: 1.9359 - val_dense_1_loss: 1.9358 - val_reward_out_loss: 1.2167e-04\n",
      "Epoch 3/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3315 - dense_1_loss: 0.3311 - reward_out_loss: 4.6749e-04 - val_loss: 1.3485 - val_dense_1_loss: 1.3484 - val_reward_out_loss: 5.1216e-05\n",
      "Epoch 4/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0339 - dense_1_loss: 0.0336 - reward_out_loss: 2.9728e-04 - val_loss: 0.9451 - val_dense_1_loss: 0.9451 - val_reward_out_loss: 2.2701e-05\n",
      "Epoch 5/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0130 - dense_1_loss: 0.0128 - reward_out_loss: 1.7353e-04 - val_loss: 0.6535 - val_dense_1_loss: 0.6535 - val_reward_out_loss: 1.0830e-05\n",
      "Epoch 6/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0056 - dense_1_loss: 0.0055 - reward_out_loss: 1.0168e-04 - val_loss: 0.4422 - val_dense_1_loss: 0.4422 - val_reward_out_loss: 5.7141e-06\n",
      "Epoch 7/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0038 - dense_1_loss: 0.0038 - reward_out_loss: 6.4003e-05 - val_loss: 0.2939 - val_dense_1_loss: 0.2939 - val_reward_out_loss: 3.2779e-06\n",
      "Epoch 8/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0062 - dense_1_loss: 0.0061 - reward_out_loss: 4.1548e-05 - val_loss: 0.1929 - val_dense_1_loss: 0.1929 - val_reward_out_loss: 1.9916e-06\n",
      "Epoch 9/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0266 - dense_1_loss: 0.0266 - reward_out_loss: 2.7541e-05 - val_loss: 0.1266 - val_dense_1_loss: 0.1266 - val_reward_out_loss: 1.3292e-06\n",
      "Epoch 10/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0350 - dense_1_loss: 0.0350 - reward_out_loss: 1.8647e-05 - val_loss: 0.0832 - val_dense_1_loss: 0.0832 - val_reward_out_loss: 9.6106e-07\n",
      "Epoch 11/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0115 - dense_1_loss: 0.0115 - reward_out_loss: 1.2741e-05 - val_loss: 0.0577 - val_dense_1_loss: 0.0577 - val_reward_out_loss: 7.4299e-07\n",
      "Epoch 12/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0032 - dense_1_loss: 0.0032 - reward_out_loss: 8.8264e-06 - val_loss: 0.0415 - val_dense_1_loss: 0.0415 - val_reward_out_loss: 6.0086e-07\n",
      "Epoch 13/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0016 - dense_1_loss: 0.0016 - reward_out_loss: 6.2550e-06 - val_loss: 0.0313 - val_dense_1_loss: 0.0313 - val_reward_out_loss: 4.9679e-07\n",
      "Epoch 14/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0011 - dense_1_loss: 0.0011 - reward_out_loss: 4.4741e-06 - val_loss: 0.0246 - val_dense_1_loss: 0.0246 - val_reward_out_loss: 4.1708e-07\n",
      "Epoch 15/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 8.8086e-04 - dense_1_loss: 8.7763e-04 - reward_out_loss: 3.2282e-06 - val_loss: 0.0201 - val_dense_1_loss: 0.0201 - val_reward_out_loss: 3.6080e-07\n",
      "Epoch 16/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 7.4510e-04 - dense_1_loss: 7.4273e-04 - reward_out_loss: 2.3710e-06 - val_loss: 0.0171 - val_dense_1_loss: 0.0171 - val_reward_out_loss: 3.1796e-07\n",
      "Epoch 17/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 6.4249e-04 - dense_1_loss: 6.4072e-04 - reward_out_loss: 1.7619e-06 - val_loss: 0.0148 - val_dense_1_loss: 0.0148 - val_reward_out_loss: 2.8580e-07\n",
      "Epoch 18/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 5.6183e-04 - dense_1_loss: 5.6050e-04 - reward_out_loss: 1.3272e-06 - val_loss: 0.0131 - val_dense_1_loss: 0.0131 - val_reward_out_loss: 2.5915e-07\n",
      "Epoch 19/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 4.9400e-04 - dense_1_loss: 4.9299e-04 - reward_out_loss: 1.0126e-06 - val_loss: 0.0118 - val_dense_1_loss: 0.0118 - val_reward_out_loss: 2.3718e-07\n",
      "Epoch 20/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 4.3686e-04 - dense_1_loss: 4.3607e-04 - reward_out_loss: 7.8400e-07 - val_loss: 0.0108 - val_dense_1_loss: 0.0108 - val_reward_out_loss: 2.2022e-07\n",
      "Epoch 21/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 3.8840e-04 - dense_1_loss: 3.8778e-04 - reward_out_loss: 6.1456e-07 - val_loss: 0.0101 - val_dense_1_loss: 0.0101 - val_reward_out_loss: 2.0748e-07\n",
      "Epoch 22/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 3.4721e-04 - dense_1_loss: 3.4672e-04 - reward_out_loss: 4.8579e-07 - val_loss: 0.0095 - val_dense_1_loss: 0.0095 - val_reward_out_loss: 1.9677e-07\n",
      "Epoch 23/100\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 3.1264e-04 - dense_1_loss: 3.1225e-04 - reward_out_loss: 3.8987e-07 - val_loss: 0.0090 - val_dense_1_loss: 0.0090 - val_reward_out_loss: 1.8798e-07\n",
      "Epoch 24/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 2.8308e-04 - dense_1_loss: 2.8276e-04 - reward_out_loss: 3.1704e-07 - val_loss: 0.0086 - val_dense_1_loss: 0.0086 - val_reward_out_loss: 1.8167e-07\n",
      "Epoch 25/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 2.5801e-04 - dense_1_loss: 2.5775e-04 - reward_out_loss: 2.6141e-07 - val_loss: 0.0083 - val_dense_1_loss: 0.0083 - val_reward_out_loss: 1.7661e-07\n",
      "Epoch 26/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 2.3634e-04 - dense_1_loss: 2.3612e-04 - reward_out_loss: 2.1818e-07 - val_loss: 0.0081 - val_dense_1_loss: 0.0081 - val_reward_out_loss: 1.7264e-07\n",
      "Epoch 27/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 2.1787e-04 - dense_1_loss: 2.1769e-04 - reward_out_loss: 1.8422e-07 - val_loss: 0.0079 - val_dense_1_loss: 0.0079 - val_reward_out_loss: 1.6986e-07\n",
      "Epoch 28/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 2.0166e-04 - dense_1_loss: 2.0151e-04 - reward_out_loss: 1.5717e-07 - val_loss: 0.0078 - val_dense_1_loss: 0.0078 - val_reward_out_loss: 1.6818e-07\n",
      "Epoch 29/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 1.8744e-04 - dense_1_loss: 1.8730e-04 - reward_out_loss: 1.3563e-07 - val_loss: 0.0078 - val_dense_1_loss: 0.0078 - val_reward_out_loss: 1.6737e-07\n",
      "Epoch 30/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 1.7505e-04 - dense_1_loss: 1.7494e-04 - reward_out_loss: 1.1815e-07 - val_loss: 0.0077 - val_dense_1_loss: 0.0077 - val_reward_out_loss: 1.6715e-07\n",
      "Epoch 31/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 1.6408e-04 - dense_1_loss: 1.6397e-04 - reward_out_loss: 1.0395e-07 - val_loss: 0.0077 - val_dense_1_loss: 0.0077 - val_reward_out_loss: 1.6773e-07\n",
      "Epoch 32/100\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 1.5435e-04 - dense_1_loss: 1.5426e-04 - reward_out_loss: 9.2342e-08 - val_loss: 0.0077 - val_dense_1_loss: 0.0077 - val_reward_out_loss: 1.6904e-07\n",
      "Epoch 00032: early stopping\n",
      "Train on 29 samples, validate on 4 samples\n",
      "Epoch 1/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 1.1869 - dense_1_loss: 1.1869 - reward_out_loss: 5.4977e-07 - val_loss: 0.5269 - val_dense_1_loss: 0.5269 - val_reward_out_loss: 3.5522e-08\n",
      "Epoch 2/100\n",
      "29/29 [==============================] - 0s 777us/step - loss: 0.9379 - dense_1_loss: 0.9379 - reward_out_loss: 3.9317e-07 - val_loss: 0.1502 - val_dense_1_loss: 0.1502 - val_reward_out_loss: 3.5234e-09\n",
      "Epoch 3/100\n",
      "29/29 [==============================] - 0s 855us/step - loss: 0.3664 - dense_1_loss: 0.3664 - reward_out_loss: 2.4928e-07 - val_loss: 0.0141 - val_dense_1_loss: 0.0141 - val_reward_out_loss: 1.0282e-10\n",
      "Epoch 4/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0373 - dense_1_loss: 0.0373 - reward_out_loss: 1.5378e-07 - val_loss: 0.0015 - val_dense_1_loss: 0.0015 - val_reward_out_loss: 4.6079e-12\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 915us/step - loss: 0.2201 - dense_1_loss: 0.2201 - reward_out_loss: 1.2759e-07 - val_loss: 5.2710e-04 - val_dense_1_loss: 5.2710e-04 - val_reward_out_loss: 9.6811e-13\n",
      "Epoch 6/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0023 - dense_1_loss: 0.0023 - reward_out_loss: 1.2647e-07 - val_loss: 2.1295e-04 - val_dense_1_loss: 2.1295e-04 - val_reward_out_loss: 2.5668e-13\n",
      "Epoch 7/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0021 - dense_1_loss: 0.0021 - reward_out_loss: 1.2328e-07 - val_loss: 9.6986e-05 - val_dense_1_loss: 9.6986e-05 - val_reward_out_loss: 7.1942e-14\n",
      "Epoch 8/100\n",
      "29/29 [==============================] - 0s 901us/step - loss: 0.0020 - dense_1_loss: 0.0020 - reward_out_loss: 1.1801e-07 - val_loss: 4.6613e-05 - val_dense_1_loss: 4.6613e-05 - val_reward_out_loss: 2.2204e-14\n",
      "Epoch 9/100\n",
      "29/29 [==============================] - 0s 908us/step - loss: 0.0018 - dense_1_loss: 0.0018 - reward_out_loss: 1.1162e-07 - val_loss: 2.3500e-05 - val_dense_1_loss: 2.3500e-05 - val_reward_out_loss: 7.9936e-15\n",
      "Epoch 10/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0017 - dense_1_loss: 0.0017 - reward_out_loss: 1.0405e-07 - val_loss: 1.3441e-05 - val_dense_1_loss: 1.3441e-05 - val_reward_out_loss: 3.5527e-15\n",
      "Epoch 11/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0022 - dense_1_loss: 0.0022 - reward_out_loss: 9.5792e-08 - val_loss: 8.3746e-06 - val_dense_1_loss: 8.3746e-06 - val_reward_out_loss: 8.8818e-16\n",
      "Epoch 00011: early stopping\n",
      "Train on 39 samples, validate on 5 samples\n",
      "Epoch 1/100\n",
      "39/39 [==============================] - 0s 1ms/step - loss: 0.5421 - dense_1_loss: 0.5421 - reward_out_loss: 1.5470e-06 - val_loss: 2.0151 - val_dense_1_loss: 2.0151 - val_reward_out_loss: 1.1771e-05\n",
      "Epoch 2/100\n",
      "39/39 [==============================] - 0s 821us/step - loss: 0.1373 - dense_1_loss: 0.1373 - reward_out_loss: 1.2061e-06 - val_loss: 0.9776 - val_dense_1_loss: 0.9776 - val_reward_out_loss: 1.0019e-05\n",
      "Epoch 3/100\n",
      "39/39 [==============================] - 0s 745us/step - loss: 0.0294 - dense_1_loss: 0.0294 - reward_out_loss: 8.2634e-07 - val_loss: 0.4311 - val_dense_1_loss: 0.4311 - val_reward_out_loss: 7.8835e-06\n",
      "Epoch 4/100\n",
      "39/39 [==============================] - 0s 865us/step - loss: 0.0186 - dense_1_loss: 0.0186 - reward_out_loss: 5.3730e-07 - val_loss: 0.2674 - val_dense_1_loss: 0.2674 - val_reward_out_loss: 6.1780e-06\n",
      "Epoch 5/100\n",
      "39/39 [==============================] - 0s 762us/step - loss: 0.0442 - dense_1_loss: 0.0442 - reward_out_loss: 3.5187e-07 - val_loss: 0.1947 - val_dense_1_loss: 0.1947 - val_reward_out_loss: 5.4739e-06\n",
      "Epoch 6/100\n",
      "39/39 [==============================] - 0s 804us/step - loss: 0.0052 - dense_1_loss: 0.0052 - reward_out_loss: 2.6313e-07 - val_loss: 0.1458 - val_dense_1_loss: 0.1458 - val_reward_out_loss: 4.8827e-06\n",
      "Epoch 7/100\n",
      "39/39 [==============================] - 0s 808us/step - loss: 8.0594e-04 - dense_1_loss: 8.0575e-04 - reward_out_loss: 1.8973e-07 - val_loss: 0.1114 - val_dense_1_loss: 0.1114 - val_reward_out_loss: 4.3737e-06\n",
      "Epoch 8/100\n",
      "39/39 [==============================] - 0s 947us/step - loss: 4.2417e-04 - dense_1_loss: 4.2404e-04 - reward_out_loss: 1.3178e-07 - val_loss: 0.0858 - val_dense_1_loss: 0.0858 - val_reward_out_loss: 3.9474e-06\n",
      "Epoch 9/100\n",
      "39/39 [==============================] - 0s 812us/step - loss: 3.0046e-04 - dense_1_loss: 3.0037e-04 - reward_out_loss: 9.2528e-08 - val_loss: 0.0676 - val_dense_1_loss: 0.0676 - val_reward_out_loss: 3.6066e-06\n",
      "Epoch 10/100\n",
      "39/39 [==============================] - 0s 765us/step - loss: 2.3491e-04 - dense_1_loss: 2.3485e-04 - reward_out_loss: 6.5910e-08 - val_loss: 0.0548 - val_dense_1_loss: 0.0548 - val_reward_out_loss: 3.3361e-06\n",
      "Epoch 11/100\n",
      "39/39 [==============================] - 0s 847us/step - loss: 1.9460e-04 - dense_1_loss: 1.9455e-04 - reward_out_loss: 4.8528e-08 - val_loss: 0.0452 - val_dense_1_loss: 0.0452 - val_reward_out_loss: 3.1184e-06\n",
      "Epoch 12/100\n",
      "39/39 [==============================] - 0s 804us/step - loss: 1.6743e-04 - dense_1_loss: 1.6739e-04 - reward_out_loss: 3.6831e-08 - val_loss: 0.0378 - val_dense_1_loss: 0.0378 - val_reward_out_loss: 2.9387e-06\n",
      "Epoch 13/100\n",
      "39/39 [==============================] - 0s 764us/step - loss: 1.4772e-04 - dense_1_loss: 1.4770e-04 - reward_out_loss: 2.8952e-08 - val_loss: 0.0321 - val_dense_1_loss: 0.0321 - val_reward_out_loss: 2.7861e-06\n",
      "Epoch 14/100\n",
      "39/39 [==============================] - 0s 846us/step - loss: 1.3287e-04 - dense_1_loss: 1.3285e-04 - reward_out_loss: 2.3604e-08 - val_loss: 0.0277 - val_dense_1_loss: 0.0277 - val_reward_out_loss: 2.6671e-06\n",
      "Epoch 15/100\n",
      "39/39 [==============================] - 0s 761us/step - loss: 1.2158e-04 - dense_1_loss: 1.2156e-04 - reward_out_loss: 1.9773e-08 - val_loss: 0.0244 - val_dense_1_loss: 0.0244 - val_reward_out_loss: 2.5706e-06\n",
      "Epoch 16/100\n",
      "39/39 [==============================] - 0s 797us/step - loss: 1.1897e-04 - dense_1_loss: 1.1896e-04 - reward_out_loss: 1.6979e-08 - val_loss: 0.0216 - val_dense_1_loss: 0.0216 - val_reward_out_loss: 2.4924e-06\n",
      "Epoch 17/100\n",
      "39/39 [==============================] - 0s 766us/step - loss: 2.2363e-04 - dense_1_loss: 2.2362e-04 - reward_out_loss: 1.4865e-08 - val_loss: 0.0194 - val_dense_1_loss: 0.0194 - val_reward_out_loss: 2.4318e-06\n",
      "Epoch 18/100\n",
      "39/39 [==============================] - 0s 750us/step - loss: 0.0013 - dense_1_loss: 0.0013 - reward_out_loss: 1.3253e-08 - val_loss: 0.0176 - val_dense_1_loss: 0.0176 - val_reward_out_loss: 2.3931e-06\n",
      "Epoch 19/100\n",
      "39/39 [==============================] - 0s 891us/step - loss: 0.0020 - dense_1_loss: 0.0020 - reward_out_loss: 1.2089e-08 - val_loss: 0.0161 - val_dense_1_loss: 0.0161 - val_reward_out_loss: 2.3831e-06\n",
      "Epoch 20/100\n",
      "39/39 [==============================] - 0s 822us/step - loss: 3.1877e-04 - dense_1_loss: 3.1876e-04 - reward_out_loss: 1.1335e-08 - val_loss: 0.0149 - val_dense_1_loss: 0.0149 - val_reward_out_loss: 2.3821e-06\n",
      "Epoch 21/100\n",
      "39/39 [==============================] - 0s 770us/step - loss: 1.1413e-04 - dense_1_loss: 1.1412e-04 - reward_out_loss: 1.0746e-08 - val_loss: 0.0139 - val_dense_1_loss: 0.0139 - val_reward_out_loss: 2.3907e-06\n",
      "Epoch 22/100\n",
      "39/39 [==============================] - 0s 883us/step - loss: 9.1190e-05 - dense_1_loss: 9.1179e-05 - reward_out_loss: 1.0269e-08 - val_loss: 0.0130 - val_dense_1_loss: 0.0130 - val_reward_out_loss: 2.4058e-06\n",
      "Epoch 23/100\n",
      "39/39 [==============================] - 0s 791us/step - loss: 8.6965e-05 - dense_1_loss: 8.6955e-05 - reward_out_loss: 9.8783e-09 - val_loss: 0.0123 - val_dense_1_loss: 0.0123 - val_reward_out_loss: 2.4274e-06\n",
      "Epoch 24/100\n",
      "39/39 [==============================] - 0s 736us/step - loss: 8.5100e-05 - dense_1_loss: 8.5091e-05 - reward_out_loss: 9.5485e-09 - val_loss: 0.0117 - val_dense_1_loss: 0.0117 - val_reward_out_loss: 2.4522e-06\n",
      "Epoch 25/100\n",
      "39/39 [==============================] - 0s 748us/step - loss: 8.3670e-05 - dense_1_loss: 8.3661e-05 - reward_out_loss: 9.2706e-09 - val_loss: 0.0112 - val_dense_1_loss: 0.0112 - val_reward_out_loss: 2.4811e-06\n",
      "Epoch 26/100\n",
      "39/39 [==============================] - 0s 762us/step - loss: 8.2377e-05 - dense_1_loss: 8.2368e-05 - reward_out_loss: 9.0372e-09 - val_loss: 0.0107 - val_dense_1_loss: 0.0107 - val_reward_out_loss: 2.5133e-06\n",
      "Epoch 27/100\n",
      "39/39 [==============================] - 0s 825us/step - loss: 8.1167e-05 - dense_1_loss: 8.1158e-05 - reward_out_loss: 8.8361e-09 - val_loss: 0.0104 - val_dense_1_loss: 0.0103 - val_reward_out_loss: 2.5505e-06\n",
      "Epoch 28/100\n",
      "39/39 [==============================] - 0s 792us/step - loss: 8.0021e-05 - dense_1_loss: 8.0012e-05 - reward_out_loss: 8.6645e-09 - val_loss: 0.0100 - val_dense_1_loss: 0.0100 - val_reward_out_loss: 2.5906e-06\n",
      "Epoch 29/100\n",
      "39/39 [==============================] - 0s 842us/step - loss: 7.8912e-05 - dense_1_loss: 7.8904e-05 - reward_out_loss: 8.5140e-09 - val_loss: 0.0097 - val_dense_1_loss: 0.0097 - val_reward_out_loss: 2.6352e-06\n",
      "Epoch 30/100\n",
      "39/39 [==============================] - 0s 820us/step - loss: 7.7840e-05 - dense_1_loss: 7.7831e-05 - reward_out_loss: 8.3793e-09 - val_loss: 0.0095 - val_dense_1_loss: 0.0095 - val_reward_out_loss: 2.6821e-06\n",
      "Epoch 31/100\n",
      "39/39 [==============================] - 0s 749us/step - loss: 7.6812e-05 - dense_1_loss: 7.6804e-05 - reward_out_loss: 8.2600e-09 - val_loss: 0.0093 - val_dense_1_loss: 0.0093 - val_reward_out_loss: 2.7302e-06\n",
      "Epoch 32/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - 0s 753us/step - loss: 7.5805e-05 - dense_1_loss: 7.5797e-05 - reward_out_loss: 8.1519e-09 - val_loss: 0.0092 - val_dense_1_loss: 0.0092 - val_reward_out_loss: 2.7811e-06\n",
      "Epoch 33/100\n",
      "39/39 [==============================] - 0s 739us/step - loss: 7.4834e-05 - dense_1_loss: 7.4826e-05 - reward_out_loss: 8.0545e-09 - val_loss: 0.0090 - val_dense_1_loss: 0.0090 - val_reward_out_loss: 2.8344e-06\n",
      "Epoch 00033: early stopping\n",
      "Train on 49 samples, validate on 6 samples\n",
      "Epoch 1/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.9045 - dense_1_loss: 0.9045 - reward_out_loss: 2.4935e-07 - val_loss: 2.6647 - val_dense_1_loss: 2.6647 - val_reward_out_loss: 8.4842e-08\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 699us/step - loss: 0.1644 - dense_1_loss: 0.1644 - reward_out_loss: 2.6888e-07 - val_loss: 2.4781 - val_dense_1_loss: 2.4781 - val_reward_out_loss: 7.5106e-08\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s 703us/step - loss: 0.0049 - dense_1_loss: 0.0049 - reward_out_loss: 2.5319e-07 - val_loss: 2.6239 - val_dense_1_loss: 2.6239 - val_reward_out_loss: 5.5588e-08\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 650us/step - loss: 0.0014 - dense_1_loss: 0.0014 - reward_out_loss: 2.3720e-07 - val_loss: 2.8719 - val_dense_1_loss: 2.8719 - val_reward_out_loss: 3.3765e-08\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 0s 652us/step - loss: 9.0647e-04 - dense_1_loss: 9.0625e-04 - reward_out_loss: 2.2074e-07 - val_loss: 3.2029 - val_dense_1_loss: 3.2029 - val_reward_out_loss: 1.9017e-08\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - 0s 652us/step - loss: 8.3836e-04 - dense_1_loss: 8.3815e-04 - reward_out_loss: 2.0398e-07 - val_loss: 3.5912 - val_dense_1_loss: 3.5912 - val_reward_out_loss: 1.1067e-08\n",
      "Epoch 7/100\n",
      "49/49 [==============================] - 0s 650us/step - loss: 8.2945e-04 - dense_1_loss: 8.2926e-04 - reward_out_loss: 1.8800e-07 - val_loss: 3.9956 - val_dense_1_loss: 3.9956 - val_reward_out_loss: 6.7358e-09\n",
      "Epoch 00007: early stopping\n",
      "Train on 61 samples, validate on 7 samples\n",
      "Epoch 1/100\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.7199 - dense_1_loss: 0.7199 - reward_out_loss: 4.0598e-07 - val_loss: 3.7435 - val_dense_1_loss: 3.7435 - val_reward_out_loss: 4.0934e-06\n",
      "Epoch 2/100\n",
      "61/61 [==============================] - 0s 503us/step - loss: 0.3605 - dense_1_loss: 0.3605 - reward_out_loss: 2.4045e-07 - val_loss: 3.2165 - val_dense_1_loss: 3.2165 - val_reward_out_loss: 2.6436e-06\n",
      "Epoch 3/100\n",
      "61/61 [==============================] - 0s 577us/step - loss: 0.2669 - dense_1_loss: 0.2669 - reward_out_loss: 1.4421e-07 - val_loss: 2.7302 - val_dense_1_loss: 2.7302 - val_reward_out_loss: 1.7091e-06\n",
      "Epoch 4/100\n",
      "61/61 [==============================] - 0s 519us/step - loss: 0.2650 - dense_1_loss: 0.2650 - reward_out_loss: 9.6860e-08 - val_loss: 2.2985 - val_dense_1_loss: 2.2985 - val_reward_out_loss: 1.1214e-06\n",
      "Epoch 5/100\n",
      "61/61 [==============================] - 0s 612us/step - loss: 0.2655 - dense_1_loss: 0.2655 - reward_out_loss: 7.1034e-08 - val_loss: 1.9280 - val_dense_1_loss: 1.9280 - val_reward_out_loss: 7.5407e-07\n",
      "Epoch 6/100\n",
      "61/61 [==============================] - 0s 645us/step - loss: 0.2805 - dense_1_loss: 0.2805 - reward_out_loss: 5.5900e-08 - val_loss: 1.6089 - val_dense_1_loss: 1.6089 - val_reward_out_loss: 5.2951e-07\n",
      "Epoch 7/100\n",
      "61/61 [==============================] - 0s 542us/step - loss: 0.2804 - dense_1_loss: 0.2804 - reward_out_loss: 4.7357e-08 - val_loss: 1.3390 - val_dense_1_loss: 1.3390 - val_reward_out_loss: 3.8587e-07\n",
      "Epoch 8/100\n",
      "61/61 [==============================] - 0s 541us/step - loss: 0.2663 - dense_1_loss: 0.2663 - reward_out_loss: 4.2575e-08 - val_loss: 1.1111 - val_dense_1_loss: 1.1111 - val_reward_out_loss: 2.8714e-07\n",
      "Epoch 9/100\n",
      "61/61 [==============================] - 0s 636us/step - loss: 0.2645 - dense_1_loss: 0.2645 - reward_out_loss: 3.9192e-08 - val_loss: 0.9260 - val_dense_1_loss: 0.9260 - val_reward_out_loss: 2.1517e-07\n",
      "Epoch 10/100\n",
      "61/61 [==============================] - 0s 607us/step - loss: 0.2644 - dense_1_loss: 0.2644 - reward_out_loss: 3.6504e-08 - val_loss: 0.7769 - val_dense_1_loss: 0.7769 - val_reward_out_loss: 1.6504e-07\n",
      "Epoch 11/100\n",
      "61/61 [==============================] - 0s 563us/step - loss: 0.2644 - dense_1_loss: 0.2644 - reward_out_loss: 3.4325e-08 - val_loss: 0.6553 - val_dense_1_loss: 0.6553 - val_reward_out_loss: 1.2945e-07\n",
      "Epoch 12/100\n",
      "61/61 [==============================] - 0s 586us/step - loss: 0.2644 - dense_1_loss: 0.2644 - reward_out_loss: 3.2524e-08 - val_loss: 0.5551 - val_dense_1_loss: 0.5551 - val_reward_out_loss: 1.0349e-07\n",
      "Epoch 13/100\n",
      "61/61 [==============================] - 0s 719us/step - loss: 0.2644 - dense_1_loss: 0.2644 - reward_out_loss: 3.0979e-08 - val_loss: 0.4718 - val_dense_1_loss: 0.4718 - val_reward_out_loss: 8.4720e-08\n",
      "Epoch 14/100\n",
      "61/61 [==============================] - 0s 637us/step - loss: 0.2644 - dense_1_loss: 0.2644 - reward_out_loss: 2.9623e-08 - val_loss: 0.4032 - val_dense_1_loss: 0.4032 - val_reward_out_loss: 7.0828e-08\n",
      "Epoch 15/100\n",
      "61/61 [==============================] - 0s 559us/step - loss: 0.2644 - dense_1_loss: 0.2644 - reward_out_loss: 2.8397e-08 - val_loss: 0.3479 - val_dense_1_loss: 0.3479 - val_reward_out_loss: 6.0357e-08\n",
      "Epoch 16/100\n",
      "61/61 [==============================] - 0s 646us/step - loss: 0.2644 - dense_1_loss: 0.2644 - reward_out_loss: 2.7272e-08 - val_loss: 0.3040 - val_dense_1_loss: 0.3040 - val_reward_out_loss: 5.2057e-08\n",
      "Epoch 17/100\n",
      "61/61 [==============================] - 0s 644us/step - loss: 0.2644 - dense_1_loss: 0.2644 - reward_out_loss: 2.6235e-08 - val_loss: 0.2684 - val_dense_1_loss: 0.2684 - val_reward_out_loss: 4.5655e-08\n",
      "Epoch 18/100\n",
      "61/61 [==============================] - 0s 548us/step - loss: 0.2644 - dense_1_loss: 0.2644 - reward_out_loss: 2.5271e-08 - val_loss: 0.2396 - val_dense_1_loss: 0.2396 - val_reward_out_loss: 4.0544e-08\n",
      "Epoch 19/100\n",
      "61/61 [==============================] - 0s 569us/step - loss: 0.2644 - dense_1_loss: 0.2644 - reward_out_loss: 2.4383e-08 - val_loss: 0.2169 - val_dense_1_loss: 0.2169 - val_reward_out_loss: 3.6469e-08\n",
      "Epoch 20/100\n",
      "61/61 [==============================] - 0s 607us/step - loss: 0.2644 - dense_1_loss: 0.2644 - reward_out_loss: 2.3556e-08 - val_loss: 0.1988 - val_dense_1_loss: 0.1988 - val_reward_out_loss: 3.3168e-08\n",
      "Epoch 21/100\n",
      "61/61 [==============================] - 0s 568us/step - loss: 0.2644 - dense_1_loss: 0.2644 - reward_out_loss: 2.2790e-08 - val_loss: 0.1841 - val_dense_1_loss: 0.1841 - val_reward_out_loss: 3.0489e-08\n",
      "Epoch 22/100\n",
      "61/61 [==============================] - 0s 618us/step - loss: 0.2644 - dense_1_loss: 0.2644 - reward_out_loss: 2.2042e-08 - val_loss: 0.1722 - val_dense_1_loss: 0.1722 - val_reward_out_loss: 2.8292e-08\n",
      "Epoch 23/100\n",
      "61/61 [==============================] - 0s 597us/step - loss: 0.2644 - dense_1_loss: 0.2644 - reward_out_loss: 2.1341e-08 - val_loss: 0.1624 - val_dense_1_loss: 0.1624 - val_reward_out_loss: 2.6422e-08\n",
      "Epoch 24/100\n",
      "61/61 [==============================] - 0s 605us/step - loss: 0.2643 - dense_1_loss: 0.2643 - reward_out_loss: 2.0689e-08 - val_loss: 0.1547 - val_dense_1_loss: 0.1547 - val_reward_out_loss: 2.4850e-08\n",
      "Epoch 25/100\n",
      "61/61 [==============================] - 0s 560us/step - loss: 0.2643 - dense_1_loss: 0.2643 - reward_out_loss: 2.0081e-08 - val_loss: 0.1484 - val_dense_1_loss: 0.1484 - val_reward_out_loss: 2.3503e-08\n",
      "Epoch 26/100\n",
      "61/61 [==============================] - 0s 558us/step - loss: 0.2643 - dense_1_loss: 0.2643 - reward_out_loss: 1.9501e-08 - val_loss: 0.1434 - val_dense_1_loss: 0.1434 - val_reward_out_loss: 2.2349e-08\n",
      "Epoch 27/100\n",
      "61/61 [==============================] - 0s 608us/step - loss: 0.2643 - dense_1_loss: 0.2643 - reward_out_loss: 1.8957e-08 - val_loss: 0.1393 - val_dense_1_loss: 0.1393 - val_reward_out_loss: 2.1336e-08\n",
      "Epoch 28/100\n",
      "61/61 [==============================] - 0s 603us/step - loss: 0.2643 - dense_1_loss: 0.2643 - reward_out_loss: 1.8441e-08 - val_loss: 0.1360 - val_dense_1_loss: 0.1360 - val_reward_out_loss: 2.0467e-08\n",
      "Epoch 29/100\n",
      "61/61 [==============================] - 0s 647us/step - loss: 0.2643 - dense_1_loss: 0.2643 - reward_out_loss: 1.7957e-08 - val_loss: 0.1333 - val_dense_1_loss: 0.1333 - val_reward_out_loss: 1.9713e-08\n",
      "Epoch 30/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 0s 622us/step - loss: 0.2643 - dense_1_loss: 0.2643 - reward_out_loss: 1.7501e-08 - val_loss: 0.1312 - val_dense_1_loss: 0.1312 - val_reward_out_loss: 1.9063e-08\n",
      "Epoch 31/100\n",
      "61/61 [==============================] - 0s 576us/step - loss: 0.2643 - dense_1_loss: 0.2643 - reward_out_loss: 1.7072e-08 - val_loss: 0.1297 - val_dense_1_loss: 0.1297 - val_reward_out_loss: 1.8482e-08\n",
      "Epoch 32/100\n",
      "61/61 [==============================] - 0s 574us/step - loss: 0.2643 - dense_1_loss: 0.2643 - reward_out_loss: 1.6671e-08 - val_loss: 0.1285 - val_dense_1_loss: 0.1285 - val_reward_out_loss: 1.7960e-08\n",
      "Epoch 33/100\n",
      "61/61 [==============================] - 0s 582us/step - loss: 0.2643 - dense_1_loss: 0.2643 - reward_out_loss: 1.6288e-08 - val_loss: 0.1278 - val_dense_1_loss: 0.1278 - val_reward_out_loss: 1.7493e-08\n",
      "Epoch 34/100\n",
      "61/61 [==============================] - 0s 576us/step - loss: 0.2643 - dense_1_loss: 0.2643 - reward_out_loss: 1.5925e-08 - val_loss: 0.1275 - val_dense_1_loss: 0.1275 - val_reward_out_loss: 1.7075e-08\n",
      "Epoch 35/100\n",
      "61/61 [==============================] - 0s 615us/step - loss: 0.2643 - dense_1_loss: 0.2643 - reward_out_loss: 1.5580e-08 - val_loss: 0.1274 - val_dense_1_loss: 0.1274 - val_reward_out_loss: 1.6703e-08\n",
      "Epoch 36/100\n",
      "61/61 [==============================] - 0s 566us/step - loss: 0.2643 - dense_1_loss: 0.2643 - reward_out_loss: 1.5251e-08 - val_loss: 0.1275 - val_dense_1_loss: 0.1275 - val_reward_out_loss: 1.6379e-08\n",
      "Epoch 37/100\n",
      "61/61 [==============================] - 0s 577us/step - loss: 0.2643 - dense_1_loss: 0.2643 - reward_out_loss: 1.4940e-08 - val_loss: 0.1279 - val_dense_1_loss: 0.1279 - val_reward_out_loss: 1.6069e-08\n",
      "Epoch 38/100\n",
      "61/61 [==============================] - 0s 553us/step - loss: 0.2643 - dense_1_loss: 0.2643 - reward_out_loss: 1.4643e-08 - val_loss: 0.1285 - val_dense_1_loss: 0.1285 - val_reward_out_loss: 1.5790e-08\n",
      "Epoch 39/100\n",
      "61/61 [==============================] - 0s 631us/step - loss: 0.2643 - dense_1_loss: 0.2643 - reward_out_loss: 1.4360e-08 - val_loss: 0.1293 - val_dense_1_loss: 0.1293 - val_reward_out_loss: 1.5560e-08\n",
      "Epoch 00039: early stopping\n",
      "Train on 72 samples, validate on 9 samples\n",
      "Epoch 1/100\n",
      "72/72 [==============================] - 0s 1000us/step - loss: 0.4891 - dense_1_loss: 0.4891 - reward_out_loss: 2.1773e-08 - val_loss: 0.7080 - val_dense_1_loss: 0.7080 - val_reward_out_loss: 3.3138e-07\n",
      "Epoch 2/100\n",
      "72/72 [==============================] - 0s 582us/step - loss: 0.2566 - dense_1_loss: 0.2566 - reward_out_loss: 1.4889e-08 - val_loss: 0.7444 - val_dense_1_loss: 0.7444 - val_reward_out_loss: 2.6385e-07\n",
      "Epoch 3/100\n",
      "72/72 [==============================] - 0s 490us/step - loss: 0.2361 - dense_1_loss: 0.2361 - reward_out_loss: 9.7686e-09 - val_loss: 0.7738 - val_dense_1_loss: 0.7738 - val_reward_out_loss: 2.0707e-07\n",
      "Epoch 4/100\n",
      "72/72 [==============================] - 0s 502us/step - loss: 0.2313 - dense_1_loss: 0.2313 - reward_out_loss: 7.2178e-09 - val_loss: 0.7885 - val_dense_1_loss: 0.7885 - val_reward_out_loss: 1.6054e-07\n",
      "Epoch 5/100\n",
      "72/72 [==============================] - 0s 580us/step - loss: 0.2256 - dense_1_loss: 0.2256 - reward_out_loss: 5.8801e-09 - val_loss: 0.8046 - val_dense_1_loss: 0.8046 - val_reward_out_loss: 1.2433e-07\n",
      "Epoch 6/100\n",
      "72/72 [==============================] - 0s 516us/step - loss: 0.2248 - dense_1_loss: 0.2248 - reward_out_loss: 5.1775e-09 - val_loss: 0.8221 - val_dense_1_loss: 0.8221 - val_reward_out_loss: 9.6946e-08\n",
      "Epoch 00006: early stopping\n",
      "Train on 84 samples, validate on 10 samples\n",
      "Epoch 1/100\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.5026 - dense_1_loss: 0.5026 - reward_out_loss: 4.2914e-09 - val_loss: 2.4220 - val_dense_1_loss: 2.4220 - val_reward_out_loss: 6.1403e-10\n",
      "Epoch 2/100\n",
      "84/84 [==============================] - 0s 539us/step - loss: 0.4095 - dense_1_loss: 0.4095 - reward_out_loss: 4.1850e-09 - val_loss: 2.4440 - val_dense_1_loss: 2.4440 - val_reward_out_loss: 5.9540e-10\n",
      "Epoch 3/100\n",
      "84/84 [==============================] - 0s 517us/step - loss: 0.3859 - dense_1_loss: 0.3859 - reward_out_loss: 4.0825e-09 - val_loss: 2.4599 - val_dense_1_loss: 2.4599 - val_reward_out_loss: 5.7025e-10\n",
      "Epoch 4/100\n",
      "84/84 [==============================] - 0s 512us/step - loss: 0.3850 - dense_1_loss: 0.3850 - reward_out_loss: 4.0168e-09 - val_loss: 2.4749 - val_dense_1_loss: 2.4749 - val_reward_out_loss: 5.4279e-10\n",
      "Epoch 5/100\n",
      "84/84 [==============================] - 0s 492us/step - loss: 0.3821 - dense_1_loss: 0.3821 - reward_out_loss: 3.9808e-09 - val_loss: 2.5142 - val_dense_1_loss: 2.5142 - val_reward_out_loss: 5.4045e-10\n",
      "Epoch 6/100\n",
      "84/84 [==============================] - 0s 467us/step - loss: 0.3023 - dense_1_loss: 0.3023 - reward_out_loss: 4.0791e-09 - val_loss: 2.8423 - val_dense_1_loss: 2.8423 - val_reward_out_loss: 5.5727e-10\n",
      "Epoch 00006: early stopping\n",
      "Train on 94 samples, validate on 11 samples\n",
      "Epoch 1/100\n",
      "94/94 [==============================] - 0s 996us/step - loss: 0.5655 - dense_1_loss: 0.5655 - reward_out_loss: 2.5306e-09 - val_loss: 2.4505 - val_dense_1_loss: 2.2977 - val_reward_out_loss: 0.1528\n",
      "Epoch 2/100\n",
      "94/94 [==============================] - 0s 484us/step - loss: 0.3657 - dense_1_loss: 0.3657 - reward_out_loss: 2.2346e-09 - val_loss: 2.4293 - val_dense_1_loss: 2.2800 - val_reward_out_loss: 0.1493\n",
      "Epoch 3/100\n",
      "94/94 [==============================] - 0s 504us/step - loss: 0.2275 - dense_1_loss: 0.2275 - reward_out_loss: 1.9018e-09 - val_loss: 2.4057 - val_dense_1_loss: 2.2616 - val_reward_out_loss: 0.1441\n",
      "Epoch 4/100\n",
      "94/94 [==============================] - 0s 463us/step - loss: 0.1727 - dense_1_loss: 0.1727 - reward_out_loss: 1.6960e-09 - val_loss: 2.3834 - val_dense_1_loss: 2.2444 - val_reward_out_loss: 0.1389\n",
      "Epoch 5/100\n",
      "94/94 [==============================] - 0s 425us/step - loss: 0.1725 - dense_1_loss: 0.1725 - reward_out_loss: 1.6076e-09 - val_loss: 2.3627 - val_dense_1_loss: 2.2284 - val_reward_out_loss: 0.1343\n",
      "Epoch 6/100\n",
      "94/94 [==============================] - 0s 412us/step - loss: 0.1981 - dense_1_loss: 0.1981 - reward_out_loss: 1.5833e-09 - val_loss: 2.3464 - val_dense_1_loss: 2.2155 - val_reward_out_loss: 0.1309\n",
      "Epoch 7/100\n",
      "94/94 [==============================] - 0s 431us/step - loss: 0.1854 - dense_1_loss: 0.1854 - reward_out_loss: 1.6081e-09 - val_loss: 2.3323 - val_dense_1_loss: 2.2038 - val_reward_out_loss: 0.1285\n",
      "Epoch 8/100\n",
      "94/94 [==============================] - 0s 440us/step - loss: 0.1718 - dense_1_loss: 0.1718 - reward_out_loss: 1.6339e-09 - val_loss: 2.3200 - val_dense_1_loss: 2.1936 - val_reward_out_loss: 0.1264\n",
      "Epoch 9/100\n",
      "94/94 [==============================] - 0s 438us/step - loss: 0.1717 - dense_1_loss: 0.1717 - reward_out_loss: 1.6604e-09 - val_loss: 2.3095 - val_dense_1_loss: 2.1852 - val_reward_out_loss: 0.1243\n",
      "Epoch 10/100\n",
      "94/94 [==============================] - 0s 446us/step - loss: 0.1716 - dense_1_loss: 0.1716 - reward_out_loss: 1.6964e-09 - val_loss: 2.2996 - val_dense_1_loss: 2.1772 - val_reward_out_loss: 0.1223\n",
      "Epoch 11/100\n",
      "94/94 [==============================] - 0s 454us/step - loss: 0.1716 - dense_1_loss: 0.1716 - reward_out_loss: 1.7357e-09 - val_loss: 2.2905 - val_dense_1_loss: 2.1700 - val_reward_out_loss: 0.1204\n",
      "Epoch 12/100\n",
      "94/94 [==============================] - 0s 473us/step - loss: 0.1716 - dense_1_loss: 0.1716 - reward_out_loss: 1.7749e-09 - val_loss: 2.2823 - val_dense_1_loss: 2.1636 - val_reward_out_loss: 0.1187\n",
      "Epoch 13/100\n",
      "94/94 [==============================] - 0s 543us/step - loss: 0.1716 - dense_1_loss: 0.1716 - reward_out_loss: 1.8113e-09 - val_loss: 2.2749 - val_dense_1_loss: 2.1578 - val_reward_out_loss: 0.1172\n",
      "Epoch 14/100\n",
      "94/94 [==============================] - 0s 442us/step - loss: 0.1715 - dense_1_loss: 0.1715 - reward_out_loss: 1.8467e-09 - val_loss: 2.2683 - val_dense_1_loss: 2.1525 - val_reward_out_loss: 0.1157\n",
      "Epoch 15/100\n",
      "94/94 [==============================] - 0s 445us/step - loss: 0.1715 - dense_1_loss: 0.1715 - reward_out_loss: 1.8820e-09 - val_loss: 2.2623 - val_dense_1_loss: 2.1480 - val_reward_out_loss: 0.1143\n",
      "Epoch 16/100\n",
      "94/94 [==============================] - 0s 480us/step - loss: 0.1715 - dense_1_loss: 0.1715 - reward_out_loss: 1.9109e-09 - val_loss: 2.2569 - val_dense_1_loss: 2.1439 - val_reward_out_loss: 0.1130\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 0s 453us/step - loss: 0.1715 - dense_1_loss: 0.1715 - reward_out_loss: 1.9373e-09 - val_loss: 2.2512 - val_dense_1_loss: 2.1394 - val_reward_out_loss: 0.1118\n",
      "Epoch 18/100\n",
      "94/94 [==============================] - 0s 514us/step - loss: 0.1715 - dense_1_loss: 0.1715 - reward_out_loss: 1.9617e-09 - val_loss: 2.2462 - val_dense_1_loss: 2.1355 - val_reward_out_loss: 0.1106\n",
      "Epoch 19/100\n",
      "94/94 [==============================] - 0s 464us/step - loss: 0.1715 - dense_1_loss: 0.1715 - reward_out_loss: 1.9855e-09 - val_loss: 2.2414 - val_dense_1_loss: 2.1319 - val_reward_out_loss: 0.1095\n",
      "Epoch 20/100\n",
      "94/94 [==============================] - 0s 435us/step - loss: 0.1715 - dense_1_loss: 0.1715 - reward_out_loss: 2.0056e-09 - val_loss: 2.2368 - val_dense_1_loss: 2.1283 - val_reward_out_loss: 0.1085\n",
      "Epoch 21/100\n",
      "94/94 [==============================] - 0s 404us/step - loss: 0.1715 - dense_1_loss: 0.1715 - reward_out_loss: 2.0239e-09 - val_loss: 2.2320 - val_dense_1_loss: 2.1246 - val_reward_out_loss: 0.1074\n",
      "Epoch 22/100\n",
      "94/94 [==============================] - 0s 454us/step - loss: 0.1715 - dense_1_loss: 0.1715 - reward_out_loss: 2.0400e-09 - val_loss: 2.2277 - val_dense_1_loss: 2.1212 - val_reward_out_loss: 0.1064\n",
      "Epoch 23/100\n",
      "94/94 [==============================] - 0s 479us/step - loss: 0.1715 - dense_1_loss: 0.1715 - reward_out_loss: 2.0530e-09 - val_loss: 2.2237 - val_dense_1_loss: 2.1182 - val_reward_out_loss: 0.1055\n",
      "Epoch 24/100\n",
      "94/94 [==============================] - 0s 449us/step - loss: 0.1715 - dense_1_loss: 0.1715 - reward_out_loss: 2.0660e-09 - val_loss: 2.2203 - val_dense_1_loss: 2.1157 - val_reward_out_loss: 0.1045\n",
      "Epoch 25/100\n",
      "94/94 [==============================] - 0s 443us/step - loss: 0.1715 - dense_1_loss: 0.1715 - reward_out_loss: 2.0768e-09 - val_loss: 2.2170 - val_dense_1_loss: 2.1133 - val_reward_out_loss: 0.1037\n",
      "Epoch 26/100\n",
      "94/94 [==============================] - 0s 490us/step - loss: 0.1715 - dense_1_loss: 0.1715 - reward_out_loss: 2.0869e-09 - val_loss: 2.2137 - val_dense_1_loss: 2.1108 - val_reward_out_loss: 0.1028\n",
      "Epoch 27/100\n",
      "94/94 [==============================] - 0s 460us/step - loss: 0.1715 - dense_1_loss: 0.1715 - reward_out_loss: 2.0954e-09 - val_loss: 2.2105 - val_dense_1_loss: 2.1085 - val_reward_out_loss: 0.1021\n",
      "Epoch 28/100\n",
      "94/94 [==============================] - 0s 482us/step - loss: 0.1708 - dense_1_loss: 0.1708 - reward_out_loss: 2.1024e-09 - val_loss: 2.2124 - val_dense_1_loss: 2.1098 - val_reward_out_loss: 0.1027\n",
      "Epoch 29/100\n",
      "94/94 [==============================] - 0s 492us/step - loss: 2.2794e-04 - dense_1_loss: 2.2793e-04 - reward_out_loss: 2.1251e-09 - val_loss: 2.2154 - val_dense_1_loss: 2.1122 - val_reward_out_loss: 0.1032\n",
      "Epoch 30/100\n",
      "94/94 [==============================] - 0s 507us/step - loss: 3.0745e-05 - dense_1_loss: 3.0742e-05 - reward_out_loss: 2.1595e-09 - val_loss: 2.2174 - val_dense_1_loss: 2.1139 - val_reward_out_loss: 0.1035\n",
      "Epoch 31/100\n",
      "94/94 [==============================] - 0s 445us/step - loss: 3.1232e-05 - dense_1_loss: 3.1230e-05 - reward_out_loss: 2.2019e-09 - val_loss: 2.2194 - val_dense_1_loss: 2.1157 - val_reward_out_loss: 0.1036\n",
      "Epoch 32/100\n",
      "94/94 [==============================] - 0s 441us/step - loss: 3.1789e-05 - dense_1_loss: 3.1787e-05 - reward_out_loss: 2.2477e-09 - val_loss: 2.2215 - val_dense_1_loss: 2.1177 - val_reward_out_loss: 0.1037\n",
      "Epoch 00032: early stopping\n",
      "Train on 104 samples, validate on 12 samples\n",
      "Epoch 1/100\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1720 - dense_1_loss: 0.1694 - reward_out_loss: 0.0026 - val_loss: 1.1220 - val_dense_1_loss: 1.1025 - val_reward_out_loss: 0.0194\n",
      "Epoch 2/100\n",
      "104/104 [==============================] - 0s 507us/step - loss: 0.1384 - dense_1_loss: 0.1367 - reward_out_loss: 0.0017 - val_loss: 1.1230 - val_dense_1_loss: 1.1074 - val_reward_out_loss: 0.0155\n",
      "Epoch 3/100\n",
      "104/104 [==============================] - 0s 445us/step - loss: 0.0919 - dense_1_loss: 0.0911 - reward_out_loss: 7.9603e-04 - val_loss: 1.1211 - val_dense_1_loss: 1.1101 - val_reward_out_loss: 0.0110\n",
      "Epoch 4/100\n",
      "104/104 [==============================] - 0s 446us/step - loss: 0.0561 - dense_1_loss: 0.0558 - reward_out_loss: 2.7177e-04 - val_loss: 1.1107 - val_dense_1_loss: 1.1039 - val_reward_out_loss: 0.0068\n",
      "Epoch 5/100\n",
      "104/104 [==============================] - 0s 492us/step - loss: 0.0328 - dense_1_loss: 0.0327 - reward_out_loss: 7.0705e-05 - val_loss: 1.0967 - val_dense_1_loss: 1.0928 - val_reward_out_loss: 0.0039\n",
      "Epoch 6/100\n",
      "104/104 [==============================] - 0s 448us/step - loss: 0.0183 - dense_1_loss: 0.0183 - reward_out_loss: 1.6229e-05 - val_loss: 1.0765 - val_dense_1_loss: 1.0745 - val_reward_out_loss: 0.0020\n",
      "Epoch 7/100\n",
      "104/104 [==============================] - 0s 446us/step - loss: 0.0100 - dense_1_loss: 0.0100 - reward_out_loss: 3.4849e-06 - val_loss: 1.0534 - val_dense_1_loss: 1.0525 - val_reward_out_loss: 8.9248e-04\n",
      "Epoch 8/100\n",
      "104/104 [==============================] - 0s 498us/step - loss: 0.0053 - dense_1_loss: 0.0053 - reward_out_loss: 7.0733e-07 - val_loss: 1.0331 - val_dense_1_loss: 1.0327 - val_reward_out_loss: 3.8199e-04\n",
      "Epoch 9/100\n",
      "104/104 [==============================] - 0s 684us/step - loss: 0.0027 - dense_1_loss: 0.0027 - reward_out_loss: 1.2926e-07 - val_loss: 1.0161 - val_dense_1_loss: 1.0159 - val_reward_out_loss: 1.6078e-04\n",
      "Epoch 10/100\n",
      "104/104 [==============================] - 0s 518us/step - loss: 0.0013 - dense_1_loss: 0.0013 - reward_out_loss: 2.2660e-08 - val_loss: 1.0027 - val_dense_1_loss: 1.0027 - val_reward_out_loss: 6.9732e-05\n",
      "Epoch 11/100\n",
      "104/104 [==============================] - 0s 435us/step - loss: 6.5132e-04 - dense_1_loss: 6.5132e-04 - reward_out_loss: 3.9956e-09 - val_loss: 0.9943 - val_dense_1_loss: 0.9943 - val_reward_out_loss: 3.1262e-05\n",
      "Epoch 12/100\n",
      "104/104 [==============================] - 0s 467us/step - loss: 3.9312e-04 - dense_1_loss: 3.9312e-04 - reward_out_loss: 8.2279e-10 - val_loss: 0.9894 - val_dense_1_loss: 0.9894 - val_reward_out_loss: 1.4561e-05\n",
      "Epoch 13/100\n",
      "104/104 [==============================] - 0s 480us/step - loss: 3.0117e-04 - dense_1_loss: 3.0116e-04 - reward_out_loss: 2.2839e-10 - val_loss: 0.9863 - val_dense_1_loss: 0.9863 - val_reward_out_loss: 7.0011e-06\n",
      "Epoch 14/100\n",
      "104/104 [==============================] - 0s 442us/step - loss: 2.7048e-04 - dense_1_loss: 2.7048e-04 - reward_out_loss: 1.0713e-10 - val_loss: 0.9855 - val_dense_1_loss: 0.9855 - val_reward_out_loss: 3.5137e-06\n",
      "Epoch 15/100\n",
      "104/104 [==============================] - 0s 456us/step - loss: 2.4056e-04 - dense_1_loss: 2.4056e-04 - reward_out_loss: 7.4046e-11 - val_loss: 0.9868 - val_dense_1_loss: 0.9868 - val_reward_out_loss: 1.8197e-06\n",
      "Epoch 16/100\n",
      "104/104 [==============================] - 0s 428us/step - loss: 1.9674e-04 - dense_1_loss: 1.9674e-04 - reward_out_loss: 6.0693e-11 - val_loss: 0.9902 - val_dense_1_loss: 0.9902 - val_reward_out_loss: 9.7895e-07\n",
      "Epoch 17/100\n",
      "104/104 [==============================] - 0s 468us/step - loss: 1.4789e-04 - dense_1_loss: 1.4789e-04 - reward_out_loss: 5.2851e-11 - val_loss: 0.9950 - val_dense_1_loss: 0.9950 - val_reward_out_loss: 5.5239e-07\n",
      "Epoch 18/100\n",
      "104/104 [==============================] - 0s 451us/step - loss: 1.0623e-04 - dense_1_loss: 1.0623e-04 - reward_out_loss: 4.7007e-11 - val_loss: 1.0006 - val_dense_1_loss: 1.0006 - val_reward_out_loss: 3.2333e-07\n",
      "Epoch 00018: early stopping\n",
      "Train on 114 samples, validate on 13 samples\n",
      "Epoch 1/100\n",
      "114/114 [==============================] - 0s 1ms/step - loss: 0.1291 - dense_1_loss: 0.1291 - reward_out_loss: 3.8244e-11 - val_loss: 0.0863 - val_dense_1_loss: 0.0863 - val_reward_out_loss: 4.4193e-12\n",
      "Epoch 2/100\n",
      "114/114 [==============================] - 0s 467us/step - loss: 0.0688 - dense_1_loss: 0.0688 - reward_out_loss: 3.1237e-11 - val_loss: 0.0625 - val_dense_1_loss: 0.0625 - val_reward_out_loss: 4.2848e-12\n",
      "Epoch 3/100\n",
      "114/114 [==============================] - 0s 426us/step - loss: 0.0022 - dense_1_loss: 0.0022 - reward_out_loss: 2.8541e-11 - val_loss: 0.0444 - val_dense_1_loss: 0.0444 - val_reward_out_loss: 4.2179e-12\n",
      "Epoch 4/100\n",
      "114/114 [==============================] - 0s 438us/step - loss: 2.9802e-04 - dense_1_loss: 2.9802e-04 - reward_out_loss: 2.7132e-11 - val_loss: 0.0330 - val_dense_1_loss: 0.0330 - val_reward_out_loss: 4.1515e-12\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114/114 [==============================] - 0s 419us/step - loss: 0.0018 - dense_1_loss: 0.0018 - reward_out_loss: 2.6252e-11 - val_loss: 0.0266 - val_dense_1_loss: 0.0266 - val_reward_out_loss: 3.9569e-12\n",
      "Epoch 6/100\n",
      "114/114 [==============================] - 0s 532us/step - loss: 0.0122 - dense_1_loss: 0.0122 - reward_out_loss: 2.5777e-11 - val_loss: 0.0241 - val_dense_1_loss: 0.0241 - val_reward_out_loss: 3.6243e-12\n",
      "Epoch 7/100\n",
      "114/114 [==============================] - 0s 455us/step - loss: 0.0147 - dense_1_loss: 0.0147 - reward_out_loss: 2.5689e-11 - val_loss: 0.0237 - val_dense_1_loss: 0.0237 - val_reward_out_loss: 3.2382e-12\n",
      "Epoch 8/100\n",
      "114/114 [==============================] - 0s 522us/step - loss: 0.0049 - dense_1_loss: 0.0049 - reward_out_loss: 2.6101e-11 - val_loss: 0.0240 - val_dense_1_loss: 0.0240 - val_reward_out_loss: 2.8280e-12\n",
      "Epoch 9/100\n",
      "114/114 [==============================] - 0s 442us/step - loss: 6.1877e-04 - dense_1_loss: 6.1877e-04 - reward_out_loss: 2.6683e-11 - val_loss: 0.0244 - val_dense_1_loss: 0.0244 - val_reward_out_loss: 2.4888e-12\n",
      "Epoch 10/100\n",
      "114/114 [==============================] - 0s 538us/step - loss: 1.4406e-04 - dense_1_loss: 1.4406e-04 - reward_out_loss: 2.7187e-11 - val_loss: 0.0248 - val_dense_1_loss: 0.0248 - val_reward_out_loss: 2.1789e-12\n",
      "Epoch 11/100\n",
      "114/114 [==============================] - 0s 398us/step - loss: 7.9741e-05 - dense_1_loss: 7.9741e-05 - reward_out_loss: 2.7617e-11 - val_loss: 0.0254 - val_dense_1_loss: 0.0254 - val_reward_out_loss: 1.9370e-12\n",
      "Epoch 00011: early stopping\n",
      "Train on 137 samples, validate on 16 samples\n",
      "Epoch 1/100\n",
      "137/137 [==============================] - 0s 967us/step - loss: 0.1483 - dense_1_loss: 0.1483 - reward_out_loss: 1.3303e-10 - val_loss: 0.6646 - val_dense_1_loss: 0.6641 - val_reward_out_loss: 5.2377e-04\n",
      "Epoch 2/100\n",
      "137/137 [==============================] - 0s 440us/step - loss: 0.0676 - dense_1_loss: 0.0676 - reward_out_loss: 5.9291e-10 - val_loss: 0.6876 - val_dense_1_loss: 0.6871 - val_reward_out_loss: 5.0155e-04\n",
      "Epoch 3/100\n",
      "137/137 [==============================] - 0s 424us/step - loss: 0.0206 - dense_1_loss: 0.0206 - reward_out_loss: 5.1566e-09 - val_loss: 0.7156 - val_dense_1_loss: 0.7151 - val_reward_out_loss: 5.1271e-04\n",
      "Epoch 4/100\n",
      "137/137 [==============================] - 0s 428us/step - loss: 0.0129 - dense_1_loss: 0.0129 - reward_out_loss: 3.1097e-08 - val_loss: 0.7476 - val_dense_1_loss: 0.7471 - val_reward_out_loss: 5.3291e-04\n",
      "Epoch 5/100\n",
      "137/137 [==============================] - 0s 467us/step - loss: 0.0127 - dense_1_loss: 0.0127 - reward_out_loss: 1.0957e-07 - val_loss: 0.7768 - val_dense_1_loss: 0.7763 - val_reward_out_loss: 5.4919e-04\n",
      "Epoch 6/100\n",
      "137/137 [==============================] - 0s 411us/step - loss: 0.0128 - dense_1_loss: 0.0128 - reward_out_loss: 2.6663e-07 - val_loss: 0.8043 - val_dense_1_loss: 0.8038 - val_reward_out_loss: 5.6045e-04\n",
      "Epoch 00006: early stopping\n",
      "Train on 170 samples, validate on 19 samples\n",
      "Epoch 1/100\n",
      "170/170 [==============================] - 0s 870us/step - loss: 0.3243 - dense_1_loss: 0.3243 - reward_out_loss: 3.6583e-06 - val_loss: 0.2819 - val_dense_1_loss: 0.2809 - val_reward_out_loss: 9.5068e-04\n",
      "Epoch 2/100\n",
      "170/170 [==============================] - 0s 423us/step - loss: 0.1261 - dense_1_loss: 0.1261 - reward_out_loss: 6.2731e-06 - val_loss: 0.2861 - val_dense_1_loss: 0.2852 - val_reward_out_loss: 8.6569e-04\n",
      "Epoch 3/100\n",
      "170/170 [==============================] - 0s 370us/step - loss: 0.0784 - dense_1_loss: 0.0784 - reward_out_loss: 8.2685e-06 - val_loss: 0.2976 - val_dense_1_loss: 0.2969 - val_reward_out_loss: 7.8335e-04\n",
      "Epoch 4/100\n",
      "170/170 [==============================] - 0s 382us/step - loss: 0.0581 - dense_1_loss: 0.0581 - reward_out_loss: 7.9295e-06 - val_loss: 0.3040 - val_dense_1_loss: 0.3033 - val_reward_out_loss: 7.2078e-04\n",
      "Epoch 5/100\n",
      "170/170 [==============================] - 0s 384us/step - loss: 0.0352 - dense_1_loss: 0.0352 - reward_out_loss: 5.3716e-06 - val_loss: 0.3220 - val_dense_1_loss: 0.3213 - val_reward_out_loss: 6.6282e-04\n",
      "Epoch 6/100\n",
      "170/170 [==============================] - 0s 401us/step - loss: 0.0220 - dense_1_loss: 0.0220 - reward_out_loss: 3.1261e-06 - val_loss: 0.3547 - val_dense_1_loss: 0.3541 - val_reward_out_loss: 5.9240e-04\n",
      "Epoch 00006: early stopping\n",
      "Train on 207 samples, validate on 24 samples\n",
      "Epoch 1/100\n",
      "207/207 [==============================] - 0s 860us/step - loss: 0.6907 - dense_1_loss: 0.6907 - reward_out_loss: 1.0643e-06 - val_loss: 2.4678 - val_dense_1_loss: 2.4678 - val_reward_out_loss: 1.0367e-07\n",
      "Epoch 2/100\n",
      "207/207 [==============================] - 0s 350us/step - loss: 0.4159 - dense_1_loss: 0.4159 - reward_out_loss: 8.4480e-07 - val_loss: 2.4640 - val_dense_1_loss: 2.4640 - val_reward_out_loss: 5.5372e-08\n",
      "Epoch 3/100\n",
      "207/207 [==============================] - 0s 371us/step - loss: 0.2648 - dense_1_loss: 0.2648 - reward_out_loss: 5.7546e-07 - val_loss: 2.4845 - val_dense_1_loss: 2.4845 - val_reward_out_loss: 3.9404e-08\n",
      "Epoch 4/100\n",
      "207/207 [==============================] - 0s 353us/step - loss: 0.2700 - dense_1_loss: 0.2700 - reward_out_loss: 3.8492e-07 - val_loss: 2.5122 - val_dense_1_loss: 2.5122 - val_reward_out_loss: 3.3412e-08\n",
      "Epoch 5/100\n",
      "207/207 [==============================] - 0s 371us/step - loss: 0.2724 - dense_1_loss: 0.2724 - reward_out_loss: 2.8597e-07 - val_loss: 2.5433 - val_dense_1_loss: 2.5433 - val_reward_out_loss: 3.1136e-08\n",
      "Epoch 6/100\n",
      "207/207 [==============================] - 0s 355us/step - loss: 0.2606 - dense_1_loss: 0.2606 - reward_out_loss: 2.3192e-07 - val_loss: 2.5821 - val_dense_1_loss: 2.5821 - val_reward_out_loss: 2.8182e-08\n",
      "Epoch 7/100\n",
      "207/207 [==============================] - 0s 348us/step - loss: 0.1786 - dense_1_loss: 0.1786 - reward_out_loss: 1.8364e-07 - val_loss: 2.6257 - val_dense_1_loss: 2.6257 - val_reward_out_loss: 2.4569e-08\n",
      "Epoch 00007: early stopping\n",
      "Train on 242 samples, validate on 27 samples\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 795us/step - loss: 0.5024 - dense_1_loss: 0.5024 - reward_out_loss: 1.1439e-07 - val_loss: 0.3955 - val_dense_1_loss: 0.3949 - val_reward_out_loss: 5.9180e-04\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 349us/step - loss: 0.4304 - dense_1_loss: 0.4304 - reward_out_loss: 1.0030e-07 - val_loss: 0.4025 - val_dense_1_loss: 0.4019 - val_reward_out_loss: 6.0396e-04\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 359us/step - loss: 0.3952 - dense_1_loss: 0.3952 - reward_out_loss: 9.5255e-08 - val_loss: 0.4111 - val_dense_1_loss: 0.4105 - val_reward_out_loss: 6.1468e-04\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 340us/step - loss: 0.3744 - dense_1_loss: 0.3744 - reward_out_loss: 9.3941e-08 - val_loss: 0.4191 - val_dense_1_loss: 0.4184 - val_reward_out_loss: 6.1727e-04\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 338us/step - loss: 0.3630 - dense_1_loss: 0.3630 - reward_out_loss: 9.4180e-08 - val_loss: 0.4258 - val_dense_1_loss: 0.4252 - val_reward_out_loss: 6.0814e-04\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 366us/step - loss: 0.3536 - dense_1_loss: 0.3536 - reward_out_loss: 9.4439e-08 - val_loss: 0.4327 - val_dense_1_loss: 0.4322 - val_reward_out_loss: 5.7820e-04\n",
      "Epoch 00006: early stopping\n",
      "Train on 253 samples, validate on 29 samples\n",
      "Epoch 1/100\n",
      "253/253 [==============================] - 0s 783us/step - loss: 0.3272 - dense_1_loss: 0.3272 - reward_out_loss: 3.3964e-07 - val_loss: 0.2377 - val_dense_1_loss: 0.2375 - val_reward_out_loss: 2.0248e-04\n",
      "Epoch 2/100\n",
      "253/253 [==============================] - 0s 346us/step - loss: 0.2844 - dense_1_loss: 0.2844 - reward_out_loss: 2.6999e-07 - val_loss: 0.2556 - val_dense_1_loss: 0.2554 - val_reward_out_loss: 1.8049e-04\n",
      "Epoch 3/100\n",
      "253/253 [==============================] - 0s 332us/step - loss: 0.2167 - dense_1_loss: 0.2167 - reward_out_loss: 1.9038e-07 - val_loss: 0.2803 - val_dense_1_loss: 0.2801 - val_reward_out_loss: 1.5944e-04\n",
      "Epoch 4/100\n",
      "253/253 [==============================] - 0s 367us/step - loss: 0.2074 - dense_1_loss: 0.2074 - reward_out_loss: 1.3385e-07 - val_loss: 0.3097 - val_dense_1_loss: 0.3095 - val_reward_out_loss: 1.4089e-04\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253/253 [==============================] - 0s 336us/step - loss: 0.2024 - dense_1_loss: 0.2024 - reward_out_loss: 9.2309e-08 - val_loss: 0.3403 - val_dense_1_loss: 0.3401 - val_reward_out_loss: 1.2485e-04\n",
      "Epoch 6/100\n",
      "253/253 [==============================] - 0s 356us/step - loss: 0.2009 - dense_1_loss: 0.2009 - reward_out_loss: 6.3625e-08 - val_loss: 0.3697 - val_dense_1_loss: 0.3696 - val_reward_out_loss: 1.1126e-04\n",
      "Epoch 00006: early stopping\n",
      "Train on 274 samples, validate on 31 samples\n",
      "Epoch 1/100\n",
      "274/274 [==============================] - 0s 837us/step - loss: 0.2272 - dense_1_loss: 0.2272 - reward_out_loss: 3.5259e-08 - val_loss: 1.0382 - val_dense_1_loss: 1.0382 - val_reward_out_loss: 1.7866e-10\n",
      "Epoch 2/100\n",
      "274/274 [==============================] - 0s 340us/step - loss: 0.2077 - dense_1_loss: 0.2077 - reward_out_loss: 2.5155e-08 - val_loss: 1.0526 - val_dense_1_loss: 1.0526 - val_reward_out_loss: 1.4704e-10\n",
      "Epoch 3/100\n",
      "274/274 [==============================] - 0s 331us/step - loss: 0.1929 - dense_1_loss: 0.1929 - reward_out_loss: 1.8062e-08 - val_loss: 1.0637 - val_dense_1_loss: 1.0637 - val_reward_out_loss: 1.2212e-10\n",
      "Epoch 4/100\n",
      "274/274 [==============================] - 0s 326us/step - loss: 0.1861 - dense_1_loss: 0.1861 - reward_out_loss: 1.2996e-08 - val_loss: 1.0725 - val_dense_1_loss: 1.0725 - val_reward_out_loss: 1.0349e-10\n",
      "Epoch 5/100\n",
      "274/274 [==============================] - 0s 329us/step - loss: 0.1833 - dense_1_loss: 0.1833 - reward_out_loss: 9.4607e-09 - val_loss: 1.0786 - val_dense_1_loss: 1.0786 - val_reward_out_loss: 8.8136e-11\n",
      "Epoch 6/100\n",
      "274/274 [==============================] - 0s 337us/step - loss: 0.1824 - dense_1_loss: 0.1824 - reward_out_loss: 6.7722e-09 - val_loss: 1.0822 - val_dense_1_loss: 1.0822 - val_reward_out_loss: 7.5086e-11\n",
      "Epoch 00006: early stopping\n",
      "Train on 284 samples, validate on 32 samples\n",
      "Epoch 1/100\n",
      "284/284 [==============================] - 0s 729us/step - loss: 0.1855 - dense_1_loss: 0.1855 - reward_out_loss: 3.3044e-09 - val_loss: 1.0216 - val_dense_1_loss: 1.0216 - val_reward_out_loss: 5.4038e-11\n",
      "Epoch 2/100\n",
      "284/284 [==============================] - 0s 340us/step - loss: 0.1752 - dense_1_loss: 0.1752 - reward_out_loss: 2.3227e-09 - val_loss: 1.0137 - val_dense_1_loss: 1.0137 - val_reward_out_loss: 4.3912e-11\n",
      "Epoch 3/100\n",
      "284/284 [==============================] - 0s 327us/step - loss: 0.1737 - dense_1_loss: 0.1737 - reward_out_loss: 1.6690e-09 - val_loss: 1.0092 - val_dense_1_loss: 1.0092 - val_reward_out_loss: 3.5506e-11\n",
      "Epoch 4/100\n",
      "284/284 [==============================] - 0s 320us/step - loss: 0.1742 - dense_1_loss: 0.1742 - reward_out_loss: 1.2294e-09 - val_loss: 1.0078 - val_dense_1_loss: 1.0078 - val_reward_out_loss: 2.8752e-11\n",
      "Epoch 5/100\n",
      "284/284 [==============================] - 0s 319us/step - loss: 0.1740 - dense_1_loss: 0.1740 - reward_out_loss: 9.2132e-10 - val_loss: 1.0099 - val_dense_1_loss: 1.0099 - val_reward_out_loss: 2.3127e-11\n",
      "Epoch 6/100\n",
      "284/284 [==============================] - 0s 338us/step - loss: 0.1731 - dense_1_loss: 0.1731 - reward_out_loss: 7.0077e-10 - val_loss: 1.0148 - val_dense_1_loss: 1.0148 - val_reward_out_loss: 1.8515e-11\n",
      "Epoch 7/100\n",
      "284/284 [==============================] - 0s 336us/step - loss: 0.1728 - dense_1_loss: 0.1728 - reward_out_loss: 5.4627e-10 - val_loss: 1.0219 - val_dense_1_loss: 1.0219 - val_reward_out_loss: 1.4800e-11\n",
      "Epoch 8/100\n",
      "284/284 [==============================] - 0s 309us/step - loss: 0.1724 - dense_1_loss: 0.1724 - reward_out_loss: 4.3632e-10 - val_loss: 1.0307 - val_dense_1_loss: 1.0307 - val_reward_out_loss: 1.1882e-11\n",
      "Epoch 9/100\n",
      "284/284 [==============================] - 0s 335us/step - loss: 0.1721 - dense_1_loss: 0.1721 - reward_out_loss: 3.5536e-10 - val_loss: 1.0404 - val_dense_1_loss: 1.0404 - val_reward_out_loss: 9.5367e-12\n",
      "Epoch 00009: early stopping\n",
      "Train on 294 samples, validate on 33 samples\n",
      "Epoch 1/100\n",
      "294/294 [==============================] - 0s 790us/step - loss: 0.2583 - dense_1_loss: 0.2583 - reward_out_loss: 1.9092e-10 - val_loss: 0.3511 - val_dense_1_loss: 0.3511 - val_reward_out_loss: 9.1783e-12\n",
      "Epoch 2/100\n",
      "294/294 [==============================] - 0s 343us/step - loss: 0.1745 - dense_1_loss: 0.1745 - reward_out_loss: 1.4152e-10 - val_loss: 0.3719 - val_dense_1_loss: 0.3719 - val_reward_out_loss: 8.9645e-12\n",
      "Epoch 3/100\n",
      "294/294 [==============================] - 0s 351us/step - loss: 0.1726 - dense_1_loss: 0.1726 - reward_out_loss: 1.0901e-10 - val_loss: 0.3905 - val_dense_1_loss: 0.3905 - val_reward_out_loss: 7.9847e-12\n",
      "Epoch 4/100\n",
      "294/294 [==============================] - 0s 349us/step - loss: 0.1685 - dense_1_loss: 0.1685 - reward_out_loss: 8.5812e-11 - val_loss: 0.4036 - val_dense_1_loss: 0.4036 - val_reward_out_loss: 6.7313e-12\n",
      "Epoch 5/100\n",
      "294/294 [==============================] - 0s 328us/step - loss: 0.1668 - dense_1_loss: 0.1668 - reward_out_loss: 6.9689e-11 - val_loss: 0.4119 - val_dense_1_loss: 0.4119 - val_reward_out_loss: 5.5922e-12\n",
      "Epoch 6/100\n",
      "294/294 [==============================] - 0s 321us/step - loss: 0.1666 - dense_1_loss: 0.1666 - reward_out_loss: 5.8514e-11 - val_loss: 0.4173 - val_dense_1_loss: 0.4173 - val_reward_out_loss: 4.6858e-12\n",
      "Epoch 00006: early stopping\n",
      "Train on 306 samples, validate on 34 samples\n",
      "Epoch 1/100\n",
      "306/306 [==============================] - 0s 796us/step - loss: 0.2101 - dense_1_loss: 0.2101 - reward_out_loss: 4.4197e-11 - val_loss: 0.0906 - val_dense_1_loss: 0.0906 - val_reward_out_loss: 2.2570e-14\n",
      "Epoch 2/100\n",
      "306/306 [==============================] - 0s 338us/step - loss: 0.1618 - dense_1_loss: 0.1618 - reward_out_loss: 4.0351e-11 - val_loss: 0.0889 - val_dense_1_loss: 0.0889 - val_reward_out_loss: 1.8182e-14\n",
      "Epoch 3/100\n",
      "306/306 [==============================] - 0s 327us/step - loss: 0.1595 - dense_1_loss: 0.1595 - reward_out_loss: 3.7381e-11 - val_loss: 0.0873 - val_dense_1_loss: 0.0873 - val_reward_out_loss: 1.4106e-14\n",
      "Epoch 4/100\n",
      "306/306 [==============================] - 0s 333us/step - loss: 0.1597 - dense_1_loss: 0.1597 - reward_out_loss: 3.4974e-11 - val_loss: 0.0859 - val_dense_1_loss: 0.0859 - val_reward_out_loss: 1.2330e-14\n",
      "Epoch 5/100\n",
      "306/306 [==============================] - 0s 320us/step - loss: 0.1613 - dense_1_loss: 0.1613 - reward_out_loss: 3.2954e-11 - val_loss: 0.0847 - val_dense_1_loss: 0.0847 - val_reward_out_loss: 9.2998e-15\n",
      "Epoch 6/100\n",
      "306/306 [==============================] - 0s 315us/step - loss: 0.1635 - dense_1_loss: 0.1635 - reward_out_loss: 3.1289e-11 - val_loss: 0.0837 - val_dense_1_loss: 0.0837 - val_reward_out_loss: 8.4638e-15\n",
      "Epoch 7/100\n",
      "306/306 [==============================] - 0s 328us/step - loss: 0.1613 - dense_1_loss: 0.1613 - reward_out_loss: 3.0060e-11 - val_loss: 0.0830 - val_dense_1_loss: 0.0830 - val_reward_out_loss: 6.0605e-15\n",
      "Epoch 8/100\n",
      "306/306 [==============================] - 0s 340us/step - loss: 0.1598 - dense_1_loss: 0.1598 - reward_out_loss: 2.9046e-11 - val_loss: 0.0827 - val_dense_1_loss: 0.0827 - val_reward_out_loss: 4.9111e-15\n",
      "Epoch 9/100\n",
      "306/306 [==============================] - 0s 328us/step - loss: 0.1594 - dense_1_loss: 0.1594 - reward_out_loss: 2.8160e-11 - val_loss: 0.0824 - val_dense_1_loss: 0.0824 - val_reward_out_loss: 4.9111e-15\n",
      "Epoch 10/100\n",
      "306/306 [==============================] - 0s 336us/step - loss: 0.1591 - dense_1_loss: 0.1591 - reward_out_loss: 2.7343e-11 - val_loss: 0.0822 - val_dense_1_loss: 0.0822 - val_reward_out_loss: 2.9258e-15\n",
      "Epoch 11/100\n",
      "306/306 [==============================] - 0s 326us/step - loss: 0.1590 - dense_1_loss: 0.1590 - reward_out_loss: 2.6608e-11 - val_loss: 0.0821 - val_dense_1_loss: 0.0821 - val_reward_out_loss: 2.9258e-15\n",
      "Epoch 12/100\n",
      "306/306 [==============================] - 0s 320us/step - loss: 0.1590 - dense_1_loss: 0.1590 - reward_out_loss: 2.5950e-11 - val_loss: 0.0819 - val_dense_1_loss: 0.0819 - val_reward_out_loss: 2.9258e-15\n",
      "Epoch 13/100\n",
      "306/306 [==============================] - 0s 352us/step - loss: 0.1589 - dense_1_loss: 0.1589 - reward_out_loss: 2.5410e-11 - val_loss: 0.0819 - val_dense_1_loss: 0.0819 - val_reward_out_loss: 1.6719e-15\n",
      "Epoch 14/100\n",
      "306/306 [==============================] - 0s 312us/step - loss: 0.1589 - dense_1_loss: 0.1589 - reward_out_loss: 2.4906e-11 - val_loss: 0.0818 - val_dense_1_loss: 0.0818 - val_reward_out_loss: 1.6719e-15\n",
      "Epoch 00014: early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 335 samples, validate on 38 samples\n",
      "Epoch 1/100\n",
      "335/335 [==============================] - 0s 778us/step - loss: 0.1684 - dense_1_loss: 0.1684 - reward_out_loss: 1.9072e-11 - val_loss: 0.1724 - val_dense_1_loss: 0.1724 - val_reward_out_loss: 8.4143e-16\n",
      "Epoch 2/100\n",
      "335/335 [==============================] - 0s 390us/step - loss: 0.1582 - dense_1_loss: 0.1582 - reward_out_loss: 1.9863e-11 - val_loss: 0.1691 - val_dense_1_loss: 0.1691 - val_reward_out_loss: 8.4143e-16\n",
      "Epoch 3/100\n",
      "335/335 [==============================] - 0s 372us/step - loss: 0.1504 - dense_1_loss: 0.1504 - reward_out_loss: 2.1344e-11 - val_loss: 0.1658 - val_dense_1_loss: 0.1658 - val_reward_out_loss: 3.7397e-16\n",
      "Epoch 4/100\n",
      "335/335 [==============================] - 0s 361us/step - loss: 0.1470 - dense_1_loss: 0.1470 - reward_out_loss: 2.3118e-11 - val_loss: 0.1632 - val_dense_1_loss: 0.1632 - val_reward_out_loss: 3.7397e-16\n",
      "Epoch 5/100\n",
      "335/335 [==============================] - 0s 367us/step - loss: 0.1478 - dense_1_loss: 0.1478 - reward_out_loss: 2.4830e-11 - val_loss: 0.1616 - val_dense_1_loss: 0.1616 - val_reward_out_loss: 3.7397e-16\n",
      "Epoch 6/100\n",
      "335/335 [==============================] - 0s 362us/step - loss: 0.1489 - dense_1_loss: 0.1489 - reward_out_loss: 2.6394e-11 - val_loss: 0.1609 - val_dense_1_loss: 0.1609 - val_reward_out_loss: 3.7397e-16\n",
      "Epoch 7/100\n",
      "335/335 [==============================] - 0s 369us/step - loss: 0.1489 - dense_1_loss: 0.1489 - reward_out_loss: 2.7740e-11 - val_loss: 0.1611 - val_dense_1_loss: 0.1611 - val_reward_out_loss: 3.7397e-16\n",
      "Epoch 8/100\n",
      "335/335 [==============================] - 0s 377us/step - loss: 0.1478 - dense_1_loss: 0.1478 - reward_out_loss: 2.8814e-11 - val_loss: 0.1618 - val_dense_1_loss: 0.1618 - val_reward_out_loss: 3.7397e-16\n",
      "Epoch 9/100\n",
      "335/335 [==============================] - 0s 366us/step - loss: 0.1466 - dense_1_loss: 0.1466 - reward_out_loss: 2.9656e-11 - val_loss: 0.1629 - val_dense_1_loss: 0.1629 - val_reward_out_loss: 3.7397e-16\n",
      "Epoch 10/100\n",
      "335/335 [==============================] - 0s 347us/step - loss: 0.1458 - dense_1_loss: 0.1458 - reward_out_loss: 3.0198e-11 - val_loss: 0.1641 - val_dense_1_loss: 0.1641 - val_reward_out_loss: 9.3492e-17\n",
      "Epoch 00010: early stopping\n",
      "Train on 347 samples, validate on 39 samples\n",
      "Epoch 1/100\n",
      "347/347 [==============================] - 0s 710us/step - loss: 0.1408 - dense_1_loss: 0.1408 - reward_out_loss: 3.3674e-11 - val_loss: 0.2469 - val_dense_1_loss: 0.2469 - val_reward_out_loss: 2.3637e-07\n",
      "Epoch 2/100\n",
      "347/347 [==============================] - 0s 334us/step - loss: 0.1405 - dense_1_loss: 0.1405 - reward_out_loss: 3.3867e-11 - val_loss: 0.2475 - val_dense_1_loss: 0.2475 - val_reward_out_loss: 2.2573e-07\n",
      "Epoch 3/100\n",
      "347/347 [==============================] - 0s 315us/step - loss: 0.1403 - dense_1_loss: 0.1403 - reward_out_loss: 3.3788e-11 - val_loss: 0.2481 - val_dense_1_loss: 0.2481 - val_reward_out_loss: 2.1536e-07\n",
      "Epoch 4/100\n",
      "347/347 [==============================] - 0s 301us/step - loss: 0.1402 - dense_1_loss: 0.1402 - reward_out_loss: 3.3580e-11 - val_loss: 0.2489 - val_dense_1_loss: 0.2489 - val_reward_out_loss: 2.0540e-07\n",
      "Epoch 5/100\n",
      "347/347 [==============================] - 0s 319us/step - loss: 0.1401 - dense_1_loss: 0.1401 - reward_out_loss: 3.3291e-11 - val_loss: 0.2499 - val_dense_1_loss: 0.2499 - val_reward_out_loss: 1.9503e-07\n",
      "Epoch 6/100\n",
      "347/347 [==============================] - 0s 324us/step - loss: 0.1401 - dense_1_loss: 0.1401 - reward_out_loss: 3.2973e-11 - val_loss: 0.2508 - val_dense_1_loss: 0.2508 - val_reward_out_loss: 1.8471e-07\n",
      "Epoch 00006: early stopping\n",
      "Train on 359 samples, validate on 40 samples\n",
      "Epoch 1/100\n",
      "359/359 [==============================] - 0s 1ms/step - loss: 0.1656 - dense_1_loss: 0.1656 - reward_out_loss: 4.0085e-11 - val_loss: 0.0931 - val_dense_1_loss: 0.0931 - val_reward_out_loss: 1.6876e-07\n",
      "Epoch 2/100\n",
      "359/359 [==============================] - 0s 327us/step - loss: 0.1558 - dense_1_loss: 0.1558 - reward_out_loss: 4.0709e-11 - val_loss: 0.0931 - val_dense_1_loss: 0.0931 - val_reward_out_loss: 1.5488e-07\n",
      "Epoch 3/100\n",
      "359/359 [==============================] - 0s 317us/step - loss: 0.1392 - dense_1_loss: 0.1392 - reward_out_loss: 4.2415e-11 - val_loss: 0.1005 - val_dense_1_loss: 0.1005 - val_reward_out_loss: 1.4054e-07\n",
      "Epoch 4/100\n",
      "359/359 [==============================] - 0s 323us/step - loss: 0.1355 - dense_1_loss: 0.1355 - reward_out_loss: 4.4876e-11 - val_loss: 0.1429 - val_dense_1_loss: 0.1429 - val_reward_out_loss: 1.2836e-07\n",
      "Epoch 5/100\n",
      "359/359 [==============================] - 0s 319us/step - loss: 0.1393 - dense_1_loss: 0.1393 - reward_out_loss: 4.7234e-11 - val_loss: 0.1750 - val_dense_1_loss: 0.1750 - val_reward_out_loss: 1.2100e-07\n",
      "Epoch 6/100\n",
      "359/359 [==============================] - 0s 322us/step - loss: 0.1440 - dense_1_loss: 0.1440 - reward_out_loss: 4.8115e-11 - val_loss: 0.1094 - val_dense_1_loss: 0.1094 - val_reward_out_loss: 1.1861e-07\n",
      "Epoch 00006: early stopping\n",
      "Train on 371 samples, validate on 42 samples\n",
      "Epoch 1/100\n",
      "371/371 [==============================] - 0s 776us/step - loss: 0.1329 - dense_1_loss: 0.1329 - reward_out_loss: 4.8340e-11 - val_loss: 0.1056 - val_dense_1_loss: 0.1056 - val_reward_out_loss: 1.1042e-07\n",
      "Epoch 2/100\n",
      "371/371 [==============================] - 0s 318us/step - loss: 0.1318 - dense_1_loss: 0.1318 - reward_out_loss: 4.7282e-11 - val_loss: 0.1040 - val_dense_1_loss: 0.1040 - val_reward_out_loss: 1.0728e-07\n",
      "Epoch 3/100\n",
      "371/371 [==============================] - 0s 313us/step - loss: 0.1313 - dense_1_loss: 0.1313 - reward_out_loss: 4.6823e-11 - val_loss: 0.1027 - val_dense_1_loss: 0.1027 - val_reward_out_loss: 1.0394e-07\n",
      "Epoch 4/100\n",
      "371/371 [==============================] - 0s 305us/step - loss: 0.1311 - dense_1_loss: 0.1311 - reward_out_loss: 4.6508e-11 - val_loss: 0.1017 - val_dense_1_loss: 0.1017 - val_reward_out_loss: 1.0031e-07\n",
      "Epoch 5/100\n",
      "371/371 [==============================] - 0s 326us/step - loss: 0.1311 - dense_1_loss: 0.1311 - reward_out_loss: 4.6265e-11 - val_loss: 0.1008 - val_dense_1_loss: 0.1008 - val_reward_out_loss: 9.6515e-08\n",
      "Epoch 6/100\n",
      "371/371 [==============================] - 0s 310us/step - loss: 0.1312 - dense_1_loss: 0.1312 - reward_out_loss: 4.6003e-11 - val_loss: 0.1001 - val_dense_1_loss: 0.1001 - val_reward_out_loss: 9.2463e-08\n",
      "Epoch 7/100\n",
      "371/371 [==============================] - 0s 308us/step - loss: 0.1316 - dense_1_loss: 0.1316 - reward_out_loss: 4.5777e-11 - val_loss: 0.0996 - val_dense_1_loss: 0.0996 - val_reward_out_loss: 8.8001e-08\n",
      "Epoch 8/100\n",
      "371/371 [==============================] - 0s 302us/step - loss: 0.1320 - dense_1_loss: 0.1320 - reward_out_loss: 4.5658e-11 - val_loss: 0.0993 - val_dense_1_loss: 0.0993 - val_reward_out_loss: 8.3195e-08\n",
      "Epoch 9/100\n",
      "371/371 [==============================] - 0s 310us/step - loss: 0.1318 - dense_1_loss: 0.1318 - reward_out_loss: 4.5746e-11 - val_loss: 0.0995 - val_dense_1_loss: 0.0995 - val_reward_out_loss: 7.8338e-08\n",
      "Epoch 10/100\n",
      "371/371 [==============================] - 0s 316us/step - loss: 0.1314 - dense_1_loss: 0.1314 - reward_out_loss: 4.6048e-11 - val_loss: 0.0998 - val_dense_1_loss: 0.0998 - val_reward_out_loss: 7.3732e-08\n",
      "Epoch 11/100\n",
      "371/371 [==============================] - 0s 301us/step - loss: 0.1311 - dense_1_loss: 0.1311 - reward_out_loss: 4.6364e-11 - val_loss: 0.1001 - val_dense_1_loss: 0.1001 - val_reward_out_loss: 6.9498e-08\n",
      "Epoch 00011: early stopping\n",
      "Train on 415 samples, validate on 47 samples\n",
      "Epoch 1/100\n",
      "415/415 [==============================] - 0s 703us/step - loss: 0.1451 - dense_1_loss: 0.1451 - reward_out_loss: 2.3351e-10 - val_loss: 0.6069 - val_dense_1_loss: 0.6069 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 2/100\n",
      "415/415 [==============================] - 0s 311us/step - loss: 0.1201 - dense_1_loss: 0.1201 - reward_out_loss: 2.2026e-10 - val_loss: 0.5314 - val_dense_1_loss: 0.5314 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 3/100\n",
      "415/415 [==============================] - 0s 317us/step - loss: 0.1193 - dense_1_loss: 0.1193 - reward_out_loss: 2.0587e-10 - val_loss: 0.4830 - val_dense_1_loss: 0.4830 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 4/100\n",
      "415/415 [==============================] - 0s 315us/step - loss: 0.1255 - dense_1_loss: 0.1255 - reward_out_loss: 1.9361e-10 - val_loss: 0.4429 - val_dense_1_loss: 0.4429 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "415/415 [==============================] - 0s 304us/step - loss: 0.1182 - dense_1_loss: 0.1182 - reward_out_loss: 1.9070e-10 - val_loss: 0.4067 - val_dense_1_loss: 0.4067 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 6/100\n",
      "415/415 [==============================] - 0s 316us/step - loss: 0.1178 - dense_1_loss: 0.1178 - reward_out_loss: 1.8809e-10 - val_loss: 0.3744 - val_dense_1_loss: 0.3744 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 7/100\n",
      "415/415 [==============================] - 0s 311us/step - loss: 0.1177 - dense_1_loss: 0.1177 - reward_out_loss: 1.8557e-10 - val_loss: 0.3458 - val_dense_1_loss: 0.3458 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 8/100\n",
      "415/415 [==============================] - 0s 301us/step - loss: 0.1176 - dense_1_loss: 0.1176 - reward_out_loss: 1.8322e-10 - val_loss: 0.3218 - val_dense_1_loss: 0.3218 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 9/100\n",
      "415/415 [==============================] - 0s 310us/step - loss: 0.1175 - dense_1_loss: 0.1175 - reward_out_loss: 1.8074e-10 - val_loss: 0.3023 - val_dense_1_loss: 0.3023 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 10/100\n",
      "415/415 [==============================] - 0s 297us/step - loss: 0.1174 - dense_1_loss: 0.1174 - reward_out_loss: 1.7814e-10 - val_loss: 0.2878 - val_dense_1_loss: 0.2878 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 11/100\n",
      "415/415 [==============================] - 0s 308us/step - loss: 0.1174 - dense_1_loss: 0.1174 - reward_out_loss: 1.7535e-10 - val_loss: 0.2778 - val_dense_1_loss: 0.2778 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 12/100\n",
      "415/415 [==============================] - 0s 325us/step - loss: 0.1173 - dense_1_loss: 0.1173 - reward_out_loss: 1.7240e-10 - val_loss: 0.2715 - val_dense_1_loss: 0.2715 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 13/100\n",
      "415/415 [==============================] - 0s 295us/step - loss: 0.1173 - dense_1_loss: 0.1173 - reward_out_loss: 1.6931e-10 - val_loss: 0.2673 - val_dense_1_loss: 0.2673 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 14/100\n",
      "415/415 [==============================] - 0s 289us/step - loss: 0.1173 - dense_1_loss: 0.1173 - reward_out_loss: 1.6607e-10 - val_loss: 0.2644 - val_dense_1_loss: 0.2644 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 15/100\n",
      "415/415 [==============================] - 0s 315us/step - loss: 0.1172 - dense_1_loss: 0.1172 - reward_out_loss: 1.6286e-10 - val_loss: 0.2623 - val_dense_1_loss: 0.2623 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 16/100\n",
      "415/415 [==============================] - 0s 312us/step - loss: 0.1172 - dense_1_loss: 0.1172 - reward_out_loss: 1.5964e-10 - val_loss: 0.2607 - val_dense_1_loss: 0.2607 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 17/100\n",
      "415/415 [==============================] - 0s 307us/step - loss: 0.1172 - dense_1_loss: 0.1172 - reward_out_loss: 1.5661e-10 - val_loss: 0.2595 - val_dense_1_loss: 0.2595 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 18/100\n",
      "415/415 [==============================] - 0s 323us/step - loss: 0.1172 - dense_1_loss: 0.1172 - reward_out_loss: 1.5349e-10 - val_loss: 0.2587 - val_dense_1_loss: 0.2587 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 19/100\n",
      "415/415 [==============================] - 0s 308us/step - loss: 0.1171 - dense_1_loss: 0.1171 - reward_out_loss: 1.5045e-10 - val_loss: 0.2582 - val_dense_1_loss: 0.2582 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 20/100\n",
      "415/415 [==============================] - 0s 336us/step - loss: 0.1171 - dense_1_loss: 0.1171 - reward_out_loss: 1.4752e-10 - val_loss: 0.2577 - val_dense_1_loss: 0.2577 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 21/100\n",
      "415/415 [==============================] - 0s 306us/step - loss: 0.1171 - dense_1_loss: 0.1171 - reward_out_loss: 1.4463e-10 - val_loss: 0.2569 - val_dense_1_loss: 0.2569 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 22/100\n",
      "415/415 [==============================] - 0s 319us/step - loss: 0.1171 - dense_1_loss: 0.1171 - reward_out_loss: 1.4178e-10 - val_loss: 0.2563 - val_dense_1_loss: 0.2563 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 23/100\n",
      "415/415 [==============================] - 0s 300us/step - loss: 0.1171 - dense_1_loss: 0.1171 - reward_out_loss: 1.3906e-10 - val_loss: 0.2558 - val_dense_1_loss: 0.2558 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 24/100\n",
      "415/415 [==============================] - 0s 304us/step - loss: 0.1171 - dense_1_loss: 0.1171 - reward_out_loss: 1.3641e-10 - val_loss: 0.2555 - val_dense_1_loss: 0.2555 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 25/100\n",
      "415/415 [==============================] - 0s 308us/step - loss: 0.1170 - dense_1_loss: 0.1170 - reward_out_loss: 1.3386e-10 - val_loss: 0.2552 - val_dense_1_loss: 0.2552 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 26/100\n",
      "415/415 [==============================] - 0s 319us/step - loss: 0.1170 - dense_1_loss: 0.1170 - reward_out_loss: 1.3137e-10 - val_loss: 0.2551 - val_dense_1_loss: 0.2551 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 27/100\n",
      "415/415 [==============================] - 0s 341us/step - loss: 0.1170 - dense_1_loss: 0.1170 - reward_out_loss: 1.2897e-10 - val_loss: 0.2550 - val_dense_1_loss: 0.2550 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 28/100\n",
      "415/415 [==============================] - 0s 313us/step - loss: 0.1170 - dense_1_loss: 0.1170 - reward_out_loss: 1.2667e-10 - val_loss: 0.2550 - val_dense_1_loss: 0.2550 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 00028: early stopping\n",
      "Train on 426 samples, validate on 48 samples\n",
      "Epoch 1/100\n",
      "426/426 [==============================] - 0s 774us/step - loss: 0.1199 - dense_1_loss: 0.1199 - reward_out_loss: 1.5183e-10 - val_loss: 0.2189 - val_dense_1_loss: 0.2189 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 2/100\n",
      "426/426 [==============================] - 0s 297us/step - loss: 0.1153 - dense_1_loss: 0.1153 - reward_out_loss: 1.4654e-10 - val_loss: 0.2357 - val_dense_1_loss: 0.2357 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 3/100\n",
      "426/426 [==============================] - 0s 306us/step - loss: 0.1142 - dense_1_loss: 0.1142 - reward_out_loss: 1.4101e-10 - val_loss: 0.2519 - val_dense_1_loss: 0.2519 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 4/100\n",
      "426/426 [==============================] - 0s 309us/step - loss: 0.1140 - dense_1_loss: 0.1140 - reward_out_loss: 1.3599e-10 - val_loss: 0.2667 - val_dense_1_loss: 0.2667 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 5/100\n",
      "426/426 [==============================] - 0s 295us/step - loss: 0.1140 - dense_1_loss: 0.1140 - reward_out_loss: 1.3147e-10 - val_loss: 0.2802 - val_dense_1_loss: 0.2802 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 6/100\n",
      "426/426 [==============================] - 0s 305us/step - loss: 0.1140 - dense_1_loss: 0.1140 - reward_out_loss: 1.2721e-10 - val_loss: 0.2925 - val_dense_1_loss: 0.2925 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 00006: early stopping\n",
      "Train on 449 samples, validate on 50 samples\n",
      "Epoch 1/100\n",
      "449/449 [==============================] - 0s 725us/step - loss: 0.1089 - dense_1_loss: 0.1089 - reward_out_loss: 2.6682e-10 - val_loss: 0.4104 - val_dense_1_loss: 0.4104 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 2/100\n",
      "449/449 [==============================] - 0s 306us/step - loss: 0.1088 - dense_1_loss: 0.1088 - reward_out_loss: 2.5844e-10 - val_loss: 0.4203 - val_dense_1_loss: 0.4203 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 3/100\n",
      "449/449 [==============================] - 0s 322us/step - loss: 0.1088 - dense_1_loss: 0.1088 - reward_out_loss: 2.5032e-10 - val_loss: 0.4294 - val_dense_1_loss: 0.4294 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 4/100\n",
      "449/449 [==============================] - 0s 313us/step - loss: 0.1087 - dense_1_loss: 0.1087 - reward_out_loss: 2.4243e-10 - val_loss: 0.4379 - val_dense_1_loss: 0.4379 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 5/100\n",
      "449/449 [==============================] - 0s 306us/step - loss: 0.1086 - dense_1_loss: 0.1086 - reward_out_loss: 2.3487e-10 - val_loss: 0.4458 - val_dense_1_loss: 0.4458 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 6/100\n",
      "449/449 [==============================] - 0s 319us/step - loss: 0.1086 - dense_1_loss: 0.1086 - reward_out_loss: 2.2749e-10 - val_loss: 0.4532 - val_dense_1_loss: 0.4532 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 00006: early stopping\n",
      "Train on 476 samples, validate on 53 samples\n",
      "Epoch 1/100\n",
      "476/476 [==============================] - 0s 683us/step - loss: 0.1349 - dense_1_loss: 0.1349 - reward_out_loss: 4.0898e-10 - val_loss: 0.1401 - val_dense_1_loss: 0.1401 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "476/476 [==============================] - 0s 305us/step - loss: 0.1082 - dense_1_loss: 0.1082 - reward_out_loss: 4.0699e-10 - val_loss: 0.1488 - val_dense_1_loss: 0.1488 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 3/100\n",
      "476/476 [==============================] - 0s 304us/step - loss: 0.1032 - dense_1_loss: 0.1032 - reward_out_loss: 4.0667e-10 - val_loss: 0.1583 - val_dense_1_loss: 0.1583 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 4/100\n",
      "476/476 [==============================] - 0s 299us/step - loss: 0.1034 - dense_1_loss: 0.1034 - reward_out_loss: 4.0401e-10 - val_loss: 0.1678 - val_dense_1_loss: 0.1678 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 5/100\n",
      "476/476 [==============================] - 0s 307us/step - loss: 0.1046 - dense_1_loss: 0.1046 - reward_out_loss: 3.9729e-10 - val_loss: 0.1767 - val_dense_1_loss: 0.1767 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 6/100\n",
      "476/476 [==============================] - 0s 301us/step - loss: 0.1048 - dense_1_loss: 0.1048 - reward_out_loss: 3.8314e-10 - val_loss: 0.1843 - val_dense_1_loss: 0.1843 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 00006: early stopping\n",
      "Train on 508 samples, validate on 57 samples\n",
      "Epoch 1/100\n",
      "508/508 [==============================] - 0s 726us/step - loss: 0.1098 - dense_1_loss: 0.1098 - reward_out_loss: 1.6493e-10 - val_loss: 0.0825 - val_dense_1_loss: 0.0825 - val_reward_out_loss: 3.3651e-13\n",
      "Epoch 2/100\n",
      "508/508 [==============================] - 0s 319us/step - loss: 0.0964 - dense_1_loss: 0.0964 - reward_out_loss: 1.5526e-10 - val_loss: 0.0816 - val_dense_1_loss: 0.0816 - val_reward_out_loss: 3.1912e-13\n",
      "Epoch 3/100\n",
      "508/508 [==============================] - 0s 302us/step - loss: 0.0958 - dense_1_loss: 0.0958 - reward_out_loss: 1.4660e-10 - val_loss: 0.0820 - val_dense_1_loss: 0.0820 - val_reward_out_loss: 3.0223e-13\n",
      "Epoch 4/100\n",
      "508/508 [==============================] - 0s 314us/step - loss: 0.0958 - dense_1_loss: 0.0958 - reward_out_loss: 1.3926e-10 - val_loss: 0.0828 - val_dense_1_loss: 0.0828 - val_reward_out_loss: 2.9419e-13\n",
      "Epoch 5/100\n",
      "508/508 [==============================] - 0s 301us/step - loss: 0.0958 - dense_1_loss: 0.0958 - reward_out_loss: 1.3291e-10 - val_loss: 0.0839 - val_dense_1_loss: 0.0839 - val_reward_out_loss: 2.8204e-13\n",
      "Epoch 6/100\n",
      "508/508 [==============================] - 0s 307us/step - loss: 0.0958 - dense_1_loss: 0.0958 - reward_out_loss: 1.2736e-10 - val_loss: 0.0853 - val_dense_1_loss: 0.0853 - val_reward_out_loss: 2.7057e-13\n",
      "Epoch 00006: early stopping\n",
      "Train on 540 samples, validate on 61 samples\n",
      "Epoch 1/100\n",
      "540/540 [==============================] - 0s 742us/step - loss: 0.0984 - dense_1_loss: 0.0984 - reward_out_loss: 2.5706e-10 - val_loss: 0.3248 - val_dense_1_loss: 0.3248 - val_reward_out_loss: 4.1029e-07\n",
      "Epoch 2/100\n",
      "540/540 [==============================] - 0s 350us/step - loss: 0.0921 - dense_1_loss: 0.0921 - reward_out_loss: 2.8067e-10 - val_loss: 0.3329 - val_dense_1_loss: 0.3329 - val_reward_out_loss: 3.7453e-07\n",
      "Epoch 3/100\n",
      "540/540 [==============================] - 0s 353us/step - loss: 0.0917 - dense_1_loss: 0.0917 - reward_out_loss: 2.2925e-10 - val_loss: 0.3403 - val_dense_1_loss: 0.3403 - val_reward_out_loss: 3.4413e-07\n",
      "Epoch 4/100\n",
      "540/540 [==============================] - 0s 348us/step - loss: 0.0908 - dense_1_loss: 0.0908 - reward_out_loss: 2.3441e-10 - val_loss: 0.3479 - val_dense_1_loss: 0.3479 - val_reward_out_loss: 3.1748e-07\n",
      "Epoch 5/100\n",
      "540/540 [==============================] - 0s 341us/step - loss: 0.0909 - dense_1_loss: 0.0909 - reward_out_loss: 2.1398e-10 - val_loss: 0.3538 - val_dense_1_loss: 0.3538 - val_reward_out_loss: 3.0268e-07\n",
      "Epoch 6/100\n",
      "540/540 [==============================] - 0s 355us/step - loss: 0.0913 - dense_1_loss: 0.0913 - reward_out_loss: 2.3123e-10 - val_loss: 0.3569 - val_dense_1_loss: 0.3569 - val_reward_out_loss: 2.8679e-07\n",
      "Epoch 00006: early stopping\n",
      "Train on 603 samples, validate on 68 samples\n",
      "Epoch 1/100\n",
      "603/603 [==============================] - 0s 456us/step - loss: 0.1419 - dense_1_loss: 0.1419 - reward_out_loss: 2.2330e-09 - val_loss: 0.7602 - val_dense_1_loss: 0.7602 - val_reward_out_loss: 1.4699e-06\n",
      "Epoch 2/100\n",
      "603/603 [==============================] - 0s 354us/step - loss: 0.1050 - dense_1_loss: 0.1050 - reward_out_loss: 1.6722e-08 - val_loss: 0.7419 - val_dense_1_loss: 0.7419 - val_reward_out_loss: 1.5318e-06\n",
      "Epoch 3/100\n",
      "603/603 [==============================] - 0s 359us/step - loss: 0.0987 - dense_1_loss: 0.0987 - reward_out_loss: 9.0795e-09 - val_loss: 0.7372 - val_dense_1_loss: 0.7372 - val_reward_out_loss: 1.5105e-06\n",
      "Epoch 4/100\n",
      "603/603 [==============================] - 0s 341us/step - loss: 0.0952 - dense_1_loss: 0.0952 - reward_out_loss: 4.4694e-09 - val_loss: 0.7444 - val_dense_1_loss: 0.7443 - val_reward_out_loss: 1.4384e-06\n",
      "Epoch 5/100\n",
      "603/603 [==============================] - 0s 349us/step - loss: 0.0926 - dense_1_loss: 0.0926 - reward_out_loss: 4.0481e-09 - val_loss: 0.7634 - val_dense_1_loss: 0.7634 - val_reward_out_loss: 1.2756e-06\n",
      "Epoch 6/100\n",
      "603/603 [==============================] - 0s 355us/step - loss: 0.0895 - dense_1_loss: 0.0895 - reward_out_loss: 4.8474e-09 - val_loss: 0.7908 - val_dense_1_loss: 0.7908 - val_reward_out_loss: 1.1467e-06\n",
      "Epoch 7/100\n",
      "603/603 [==============================] - 0s 344us/step - loss: 0.0933 - dense_1_loss: 0.0933 - reward_out_loss: 3.0886e-09 - val_loss: 0.8021 - val_dense_1_loss: 0.8021 - val_reward_out_loss: 1.0566e-06\n",
      "Epoch 8/100\n",
      "603/603 [==============================] - 0s 342us/step - loss: 0.0855 - dense_1_loss: 0.0855 - reward_out_loss: 2.8602e-09 - val_loss: 0.8086 - val_dense_1_loss: 0.8086 - val_reward_out_loss: 9.9071e-07\n",
      "Epoch 00008: early stopping\n",
      "Train on 624 samples, validate on 70 samples\n",
      "Epoch 1/100\n",
      "624/624 [==============================] - 0s 458us/step - loss: 0.1193 - dense_1_loss: 0.1193 - reward_out_loss: 3.8769e-09 - val_loss: 0.5957 - val_dense_1_loss: 0.5957 - val_reward_out_loss: 4.0792e-07\n",
      "Epoch 2/100\n",
      "624/624 [==============================] - 0s 364us/step - loss: 0.1174 - dense_1_loss: 0.1174 - reward_out_loss: 2.5782e-09 - val_loss: 0.6208 - val_dense_1_loss: 0.6208 - val_reward_out_loss: 2.8107e-07\n",
      "Epoch 3/100\n",
      "624/624 [==============================] - 0s 328us/step - loss: 0.0887 - dense_1_loss: 0.0887 - reward_out_loss: 1.9058e-09 - val_loss: 0.6462 - val_dense_1_loss: 0.6462 - val_reward_out_loss: 1.7619e-07\n",
      "Epoch 4/100\n",
      "624/624 [==============================] - 0s 336us/step - loss: 0.0896 - dense_1_loss: 0.0896 - reward_out_loss: 1.6465e-08 - val_loss: 0.6776 - val_dense_1_loss: 0.6776 - val_reward_out_loss: 1.1099e-07\n",
      "Epoch 5/100\n",
      "624/624 [==============================] - 0s 346us/step - loss: 0.0857 - dense_1_loss: 0.0857 - reward_out_loss: 1.1409e-08 - val_loss: 0.7135 - val_dense_1_loss: 0.7135 - val_reward_out_loss: 7.1930e-08\n",
      "Epoch 6/100\n",
      "624/624 [==============================] - 0s 351us/step - loss: 0.0841 - dense_1_loss: 0.0841 - reward_out_loss: 2.3786e-09 - val_loss: 0.7419 - val_dense_1_loss: 0.7419 - val_reward_out_loss: 4.3446e-08\n",
      "Epoch 00006: early stopping\n",
      "Train on 635 samples, validate on 71 samples\n",
      "Epoch 1/100\n",
      "635/635 [==============================] - 0s 470us/step - loss: 0.1225 - dense_1_loss: 0.1225 - reward_out_loss: 1.9294e-09 - val_loss: 0.5548 - val_dense_1_loss: 0.5548 - val_reward_out_loss: 3.1483e-08\n",
      "Epoch 2/100\n",
      "635/635 [==============================] - 0s 352us/step - loss: 0.0850 - dense_1_loss: 0.0850 - reward_out_loss: 1.8112e-09 - val_loss: 0.6183 - val_dense_1_loss: 0.6183 - val_reward_out_loss: 2.5613e-08\n",
      "Epoch 3/100\n",
      "635/635 [==============================] - 0s 351us/step - loss: 0.0830 - dense_1_loss: 0.0830 - reward_out_loss: 1.6318e-09 - val_loss: 0.6805 - val_dense_1_loss: 0.6805 - val_reward_out_loss: 2.2880e-08\n",
      "Epoch 4/100\n",
      "635/635 [==============================] - 0s 351us/step - loss: 0.0812 - dense_1_loss: 0.0812 - reward_out_loss: 2.5022e-09 - val_loss: 0.7380 - val_dense_1_loss: 0.7380 - val_reward_out_loss: 2.1084e-08\n",
      "Epoch 5/100\n",
      "635/635 [==============================] - 0s 358us/step - loss: 0.0811 - dense_1_loss: 0.0811 - reward_out_loss: 2.2995e-09 - val_loss: 0.7830 - val_dense_1_loss: 0.7830 - val_reward_out_loss: 1.8816e-08\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635/635 [==============================] - 0s 359us/step - loss: 0.0780 - dense_1_loss: 0.0780 - reward_out_loss: 2.0141e-09 - val_loss: 0.8212 - val_dense_1_loss: 0.8212 - val_reward_out_loss: 1.7165e-08\n",
      "Epoch 00006: early stopping\n",
      "Train on 647 samples, validate on 72 samples\n",
      "Epoch 1/100\n",
      "647/647 [==============================] - 0s 455us/step - loss: 0.1029 - dense_1_loss: 0.1029 - reward_out_loss: 2.0892e-09 - val_loss: 0.5607 - val_dense_1_loss: 0.5607 - val_reward_out_loss: 6.7504e-09\n",
      "Epoch 2/100\n",
      "647/647 [==============================] - 0s 334us/step - loss: 0.0827 - dense_1_loss: 0.0827 - reward_out_loss: 1.8171e-09 - val_loss: 0.5370 - val_dense_1_loss: 0.5370 - val_reward_out_loss: 3.9904e-09\n",
      "Epoch 3/100\n",
      "647/647 [==============================] - 0s 355us/step - loss: 0.0766 - dense_1_loss: 0.0766 - reward_out_loss: 1.3395e-09 - val_loss: 0.5215 - val_dense_1_loss: 0.5215 - val_reward_out_loss: 2.7337e-09\n",
      "Epoch 4/100\n",
      "647/647 [==============================] - 0s 353us/step - loss: 0.0762 - dense_1_loss: 0.0762 - reward_out_loss: 1.4697e-09 - val_loss: 0.5101 - val_dense_1_loss: 0.5101 - val_reward_out_loss: 2.1284e-09\n",
      "Epoch 5/100\n",
      "647/647 [==============================] - 0s 353us/step - loss: 0.0762 - dense_1_loss: 0.0762 - reward_out_loss: 1.1157e-09 - val_loss: 0.5016 - val_dense_1_loss: 0.5016 - val_reward_out_loss: 1.6976e-09\n",
      "Epoch 6/100\n",
      "647/647 [==============================] - 0s 375us/step - loss: 0.0762 - dense_1_loss: 0.0762 - reward_out_loss: 1.2443e-09 - val_loss: 0.4953 - val_dense_1_loss: 0.4953 - val_reward_out_loss: 1.4191e-09\n",
      "Epoch 7/100\n",
      "647/647 [==============================] - 0s 343us/step - loss: 0.0763 - dense_1_loss: 0.0763 - reward_out_loss: 1.1831e-09 - val_loss: 0.4900 - val_dense_1_loss: 0.4900 - val_reward_out_loss: 1.1642e-09\n",
      "Epoch 8/100\n",
      "647/647 [==============================] - 0s 345us/step - loss: 0.0754 - dense_1_loss: 0.0754 - reward_out_loss: 9.9174e-10 - val_loss: 0.4867 - val_dense_1_loss: 0.4867 - val_reward_out_loss: 9.5230e-10\n",
      "Epoch 9/100\n",
      "647/647 [==============================] - 0s 349us/step - loss: 0.0754 - dense_1_loss: 0.0754 - reward_out_loss: 1.2660e-09 - val_loss: 0.4849 - val_dense_1_loss: 0.4849 - val_reward_out_loss: 7.9869e-10\n",
      "Epoch 10/100\n",
      "647/647 [==============================] - 0s 352us/step - loss: 0.0757 - dense_1_loss: 0.0757 - reward_out_loss: 8.9712e-10 - val_loss: 0.4828 - val_dense_1_loss: 0.4828 - val_reward_out_loss: 7.0313e-10\n",
      "Epoch 11/100\n",
      "647/647 [==============================] - 0s 362us/step - loss: 0.0768 - dense_1_loss: 0.0768 - reward_out_loss: 9.7435e-10 - val_loss: 0.4790 - val_dense_1_loss: 0.4790 - val_reward_out_loss: 6.5312e-10\n",
      "Epoch 12/100\n",
      "647/647 [==============================] - 0s 347us/step - loss: 0.0769 - dense_1_loss: 0.0769 - reward_out_loss: 1.3105e-09 - val_loss: 0.4757 - val_dense_1_loss: 0.4757 - val_reward_out_loss: 6.2848e-10\n",
      "Epoch 13/100\n",
      "647/647 [==============================] - 0s 334us/step - loss: 0.0758 - dense_1_loss: 0.0758 - reward_out_loss: 7.6179e-10 - val_loss: 0.4737 - val_dense_1_loss: 0.4737 - val_reward_out_loss: 6.0586e-10\n",
      "Epoch 14/100\n",
      "647/647 [==============================] - 0s 338us/step - loss: 0.0755 - dense_1_loss: 0.0755 - reward_out_loss: 5.7657e-10 - val_loss: 0.4712 - val_dense_1_loss: 0.4712 - val_reward_out_loss: 6.1024e-10\n",
      "Epoch 15/100\n",
      "647/647 [==============================] - 0s 328us/step - loss: 0.0752 - dense_1_loss: 0.0752 - reward_out_loss: 6.6884e-10 - val_loss: 0.4683 - val_dense_1_loss: 0.4683 - val_reward_out_loss: 6.0612e-10\n",
      "Epoch 16/100\n",
      "647/647 [==============================] - 0s 335us/step - loss: 0.0752 - dense_1_loss: 0.0752 - reward_out_loss: 6.9282e-10 - val_loss: 0.4656 - val_dense_1_loss: 0.4656 - val_reward_out_loss: 6.0396e-10\n",
      "Epoch 17/100\n",
      "647/647 [==============================] - 0s 341us/step - loss: 0.0754 - dense_1_loss: 0.0754 - reward_out_loss: 5.4116e-10 - val_loss: 0.4634 - val_dense_1_loss: 0.4634 - val_reward_out_loss: 5.8446e-10\n",
      "Epoch 18/100\n",
      "647/647 [==============================] - 0s 351us/step - loss: 0.0758 - dense_1_loss: 0.0758 - reward_out_loss: 6.1190e-10 - val_loss: 0.4623 - val_dense_1_loss: 0.4623 - val_reward_out_loss: 5.5442e-10\n",
      "Epoch 19/100\n",
      "647/647 [==============================] - 0s 340us/step - loss: 0.0754 - dense_1_loss: 0.0754 - reward_out_loss: 5.2402e-10 - val_loss: 0.4622 - val_dense_1_loss: 0.4622 - val_reward_out_loss: 5.0257e-10\n",
      "Epoch 20/100\n",
      "647/647 [==============================] - 0s 349us/step - loss: 0.0751 - dense_1_loss: 0.0751 - reward_out_loss: 4.2881e-10 - val_loss: 0.4623 - val_dense_1_loss: 0.4623 - val_reward_out_loss: 4.6325e-10\n",
      "Epoch 21/100\n",
      "647/647 [==============================] - 0s 331us/step - loss: 0.0751 - dense_1_loss: 0.0751 - reward_out_loss: 5.7825e-10 - val_loss: 0.4620 - val_dense_1_loss: 0.4620 - val_reward_out_loss: 4.2005e-10\n",
      "Epoch 22/100\n",
      "647/647 [==============================] - 0s 358us/step - loss: 0.0751 - dense_1_loss: 0.0751 - reward_out_loss: 6.4637e-10 - val_loss: 0.4618 - val_dense_1_loss: 0.4618 - val_reward_out_loss: 3.9472e-10\n",
      "Epoch 23/100\n",
      "647/647 [==============================] - 0s 344us/step - loss: 0.0754 - dense_1_loss: 0.0754 - reward_out_loss: 4.8035e-10 - val_loss: 0.4612 - val_dense_1_loss: 0.4612 - val_reward_out_loss: 3.6784e-10\n",
      "Epoch 24/100\n",
      "647/647 [==============================] - 0s 349us/step - loss: 0.0753 - dense_1_loss: 0.0753 - reward_out_loss: 4.6699e-10 - val_loss: 0.4599 - val_dense_1_loss: 0.4599 - val_reward_out_loss: 3.5128e-10\n",
      "Epoch 25/100\n",
      "647/647 [==============================] - 0s 343us/step - loss: 0.0752 - dense_1_loss: 0.0752 - reward_out_loss: 4.9212e-10 - val_loss: 0.4579 - val_dense_1_loss: 0.4579 - val_reward_out_loss: 3.3069e-10\n",
      "Epoch 26/100\n",
      "647/647 [==============================] - 0s 339us/step - loss: 0.0751 - dense_1_loss: 0.0751 - reward_out_loss: 2.9505e-10 - val_loss: 0.4559 - val_dense_1_loss: 0.4559 - val_reward_out_loss: 3.1307e-10\n",
      "Epoch 27/100\n",
      "647/647 [==============================] - 0s 350us/step - loss: 0.0750 - dense_1_loss: 0.0750 - reward_out_loss: 5.0328e-10 - val_loss: 0.4541 - val_dense_1_loss: 0.4541 - val_reward_out_loss: 2.9391e-10\n",
      "Epoch 28/100\n",
      "647/647 [==============================] - 0s 342us/step - loss: 0.0750 - dense_1_loss: 0.0750 - reward_out_loss: 3.7939e-10 - val_loss: 0.4527 - val_dense_1_loss: 0.4527 - val_reward_out_loss: 2.7806e-10\n",
      "Epoch 29/100\n",
      "647/647 [==============================] - 0s 354us/step - loss: 0.0750 - dense_1_loss: 0.0750 - reward_out_loss: 1.2664e-09 - val_loss: 0.4517 - val_dense_1_loss: 0.4517 - val_reward_out_loss: 2.7084e-10\n",
      "Epoch 30/100\n",
      "647/647 [==============================] - 0s 348us/step - loss: 0.0750 - dense_1_loss: 0.0750 - reward_out_loss: 3.6913e-10 - val_loss: 0.4506 - val_dense_1_loss: 0.4506 - val_reward_out_loss: 2.5894e-10\n",
      "Epoch 31/100\n",
      "647/647 [==============================] - 0s 338us/step - loss: 0.0750 - dense_1_loss: 0.0750 - reward_out_loss: 4.3719e-10 - val_loss: 0.4494 - val_dense_1_loss: 0.4494 - val_reward_out_loss: 2.4738e-10\n",
      "Epoch 32/100\n",
      "647/647 [==============================] - 0s 333us/step - loss: 0.0750 - dense_1_loss: 0.0750 - reward_out_loss: 4.4911e-10 - val_loss: 0.4483 - val_dense_1_loss: 0.4483 - val_reward_out_loss: 2.3342e-10\n",
      "Epoch 33/100\n",
      "647/647 [==============================] - 0s 330us/step - loss: 0.0750 - dense_1_loss: 0.0750 - reward_out_loss: 3.6033e-10 - val_loss: 0.4473 - val_dense_1_loss: 0.4473 - val_reward_out_loss: 2.2005e-10\n",
      "Epoch 34/100\n",
      "647/647 [==============================] - 0s 355us/step - loss: 0.0750 - dense_1_loss: 0.0750 - reward_out_loss: 4.6563e-10 - val_loss: 0.4464 - val_dense_1_loss: 0.4464 - val_reward_out_loss: 2.0395e-10\n",
      "Epoch 35/100\n",
      "647/647 [==============================] - 0s 339us/step - loss: 0.0750 - dense_1_loss: 0.0750 - reward_out_loss: 4.2845e-10 - val_loss: 0.4457 - val_dense_1_loss: 0.4457 - val_reward_out_loss: 1.9267e-10\n",
      "Epoch 36/100\n",
      "647/647 [==============================] - 0s 354us/step - loss: 0.0750 - dense_1_loss: 0.0750 - reward_out_loss: 7.9137e-10 - val_loss: 0.4454 - val_dense_1_loss: 0.4454 - val_reward_out_loss: 1.8485e-10\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "647/647 [==============================] - 0s 340us/step - loss: 0.0749 - dense_1_loss: 0.0749 - reward_out_loss: 3.3571e-10 - val_loss: 0.4446 - val_dense_1_loss: 0.4446 - val_reward_out_loss: 1.6843e-10\n",
      "Epoch 38/100\n",
      "647/647 [==============================] - 0s 344us/step - loss: 0.0750 - dense_1_loss: 0.0750 - reward_out_loss: 1.0168e-09 - val_loss: 0.4439 - val_dense_1_loss: 0.4439 - val_reward_out_loss: 1.5693e-10\n",
      "Epoch 39/100\n",
      "647/647 [==============================] - 0s 349us/step - loss: 0.0750 - dense_1_loss: 0.0750 - reward_out_loss: 6.5063e-10 - val_loss: 0.4437 - val_dense_1_loss: 0.4437 - val_reward_out_loss: 1.4877e-10\n",
      "Epoch 40/100\n",
      "647/647 [==============================] - 0s 339us/step - loss: 0.0750 - dense_1_loss: 0.0750 - reward_out_loss: 2.2171e-10 - val_loss: 0.4430 - val_dense_1_loss: 0.4430 - val_reward_out_loss: 1.3889e-10\n",
      "Epoch 41/100\n",
      "647/647 [==============================] - 0s 355us/step - loss: 0.0749 - dense_1_loss: 0.0749 - reward_out_loss: 3.0301e-10 - val_loss: 0.4426 - val_dense_1_loss: 0.4426 - val_reward_out_loss: 1.2980e-10\n",
      "Epoch 42/100\n",
      "647/647 [==============================] - 0s 332us/step - loss: 0.0750 - dense_1_loss: 0.0750 - reward_out_loss: 1.3048e-09 - val_loss: 0.4418 - val_dense_1_loss: 0.4418 - val_reward_out_loss: 1.2233e-10\n",
      "Epoch 43/100\n",
      "647/647 [==============================] - 0s 342us/step - loss: 0.0750 - dense_1_loss: 0.0750 - reward_out_loss: 3.0514e-10 - val_loss: 0.4412 - val_dense_1_loss: 0.4412 - val_reward_out_loss: 1.1147e-10\n",
      "Epoch 44/100\n",
      "647/647 [==============================] - 0s 352us/step - loss: 0.0768 - dense_1_loss: 0.0768 - reward_out_loss: 2.4167e-10 - val_loss: 0.4561 - val_dense_1_loss: 0.4561 - val_reward_out_loss: 1.0359e-10\n",
      "Epoch 45/100\n",
      "647/647 [==============================] - 0s 343us/step - loss: 0.0751 - dense_1_loss: 0.0751 - reward_out_loss: 2.3936e-10 - val_loss: 0.5218 - val_dense_1_loss: 0.5218 - val_reward_out_loss: 9.6367e-11\n",
      "Epoch 46/100\n",
      "647/647 [==============================] - 0s 335us/step - loss: 0.0750 - dense_1_loss: 0.0750 - reward_out_loss: 1.1391e-09 - val_loss: 0.5990 - val_dense_1_loss: 0.5990 - val_reward_out_loss: 8.8627e-11\n",
      "Epoch 47/100\n",
      "647/647 [==============================] - 0s 331us/step - loss: 0.0953 - dense_1_loss: 0.0953 - reward_out_loss: 2.5463e-10 - val_loss: 0.6383 - val_dense_1_loss: 0.6383 - val_reward_out_loss: 7.9456e-11\n",
      "Epoch 00047: early stopping\n",
      "Train on 658 samples, validate on 74 samples\n",
      "Epoch 1/100\n",
      "658/658 [==============================] - 0s 475us/step - loss: 0.1077 - dense_1_loss: 0.1077 - reward_out_loss: 2.7573e-10 - val_loss: 0.5926 - val_dense_1_loss: 0.5926 - val_reward_out_loss: 1.5195e-13\n",
      "Epoch 2/100\n",
      "658/658 [==============================] - 0s 340us/step - loss: 0.1014 - dense_1_loss: 0.1014 - reward_out_loss: 3.9937e-10 - val_loss: 0.6029 - val_dense_1_loss: 0.6029 - val_reward_out_loss: 1.7826e-13\n",
      "Epoch 3/100\n",
      "658/658 [==============================] - 0s 338us/step - loss: 0.0993 - dense_1_loss: 0.0993 - reward_out_loss: 3.4603e-10 - val_loss: 0.6189 - val_dense_1_loss: 0.6189 - val_reward_out_loss: 1.9943e-13\n",
      "Epoch 4/100\n",
      "658/658 [==============================] - 0s 339us/step - loss: 0.0997 - dense_1_loss: 0.0997 - reward_out_loss: 2.9038e-10 - val_loss: 0.6334 - val_dense_1_loss: 0.6334 - val_reward_out_loss: 2.0044e-13\n",
      "Epoch 5/100\n",
      "658/658 [==============================] - 0s 333us/step - loss: 0.0985 - dense_1_loss: 0.0985 - reward_out_loss: 3.2396e-10 - val_loss: 0.5859 - val_dense_1_loss: 0.5859 - val_reward_out_loss: 2.3150e-13\n",
      "Epoch 6/100\n",
      "658/658 [==============================] - 0s 347us/step - loss: 0.0741 - dense_1_loss: 0.0741 - reward_out_loss: 3.8374e-10 - val_loss: 0.5098 - val_dense_1_loss: 0.5098 - val_reward_out_loss: 2.9656e-13\n",
      "Epoch 7/100\n",
      "658/658 [==============================] - 0s 344us/step - loss: 0.0743 - dense_1_loss: 0.0743 - reward_out_loss: 2.2170e-10 - val_loss: 0.4640 - val_dense_1_loss: 0.4640 - val_reward_out_loss: 3.6915e-13\n",
      "Epoch 8/100\n",
      "658/658 [==============================] - 0s 340us/step - loss: 0.0742 - dense_1_loss: 0.0742 - reward_out_loss: 1.8558e-10 - val_loss: 0.4384 - val_dense_1_loss: 0.4384 - val_reward_out_loss: 4.1706e-13\n",
      "Epoch 9/100\n",
      "658/658 [==============================] - 0s 331us/step - loss: 0.0740 - dense_1_loss: 0.0740 - reward_out_loss: 6.1308e-10 - val_loss: 0.4267 - val_dense_1_loss: 0.4267 - val_reward_out_loss: 4.5340e-13\n",
      "Epoch 10/100\n",
      "658/658 [==============================] - 0s 342us/step - loss: 0.0984 - dense_1_loss: 0.0984 - reward_out_loss: 1.8124e-10 - val_loss: 0.4202 - val_dense_1_loss: 0.4202 - val_reward_out_loss: 4.7314e-13\n",
      "Epoch 11/100\n",
      "658/658 [==============================] - 0s 343us/step - loss: 0.0984 - dense_1_loss: 0.0984 - reward_out_loss: 2.1481e-10 - val_loss: 0.4163 - val_dense_1_loss: 0.4163 - val_reward_out_loss: 4.7405e-13\n",
      "Epoch 12/100\n",
      "658/658 [==============================] - 0s 343us/step - loss: 0.0989 - dense_1_loss: 0.0989 - reward_out_loss: 2.2589e-10 - val_loss: 0.4158 - val_dense_1_loss: 0.4158 - val_reward_out_loss: 4.3804e-13\n",
      "Epoch 13/100\n",
      "658/658 [==============================] - 0s 331us/step - loss: 0.1029 - dense_1_loss: 0.1029 - reward_out_loss: 2.0266e-10 - val_loss: 0.4374 - val_dense_1_loss: 0.4374 - val_reward_out_loss: 3.1297e-13\n",
      "Epoch 14/100\n",
      "658/658 [==============================] - 0s 346us/step - loss: 0.0986 - dense_1_loss: 0.0986 - reward_out_loss: 5.3598e-10 - val_loss: 0.4603 - val_dense_1_loss: 0.4603 - val_reward_out_loss: 2.3539e-13\n",
      "Epoch 15/100\n",
      "658/658 [==============================] - 0s 334us/step - loss: 0.0984 - dense_1_loss: 0.0984 - reward_out_loss: 2.9539e-10 - val_loss: 0.4827 - val_dense_1_loss: 0.4827 - val_reward_out_loss: 1.8210e-13\n",
      "Epoch 16/100\n",
      "658/658 [==============================] - 0s 343us/step - loss: 0.0983 - dense_1_loss: 0.0983 - reward_out_loss: 2.8909e-10 - val_loss: 0.5023 - val_dense_1_loss: 0.5023 - val_reward_out_loss: 1.4542e-13\n",
      "Epoch 00016: early stopping\n",
      "Train on 686 samples, validate on 77 samples\n",
      "Epoch 1/100\n",
      "686/686 [==============================] - 0s 483us/step - loss: 0.1111 - dense_1_loss: 0.1111 - reward_out_loss: 2.8038e-10 - val_loss: 0.6461 - val_dense_1_loss: 0.6461 - val_reward_out_loss: 1.1116e-10\n",
      "Epoch 2/100\n",
      "686/686 [==============================] - 0s 346us/step - loss: 0.0984 - dense_1_loss: 0.0984 - reward_out_loss: 2.3834e-10 - val_loss: 0.6534 - val_dense_1_loss: 0.6534 - val_reward_out_loss: 1.1517e-10\n",
      "Epoch 3/100\n",
      "686/686 [==============================] - 0s 339us/step - loss: 0.1069 - dense_1_loss: 0.1069 - reward_out_loss: 4.3747e-10 - val_loss: 0.6552 - val_dense_1_loss: 0.6552 - val_reward_out_loss: 1.1345e-10\n",
      "Epoch 4/100\n",
      "686/686 [==============================] - 0s 339us/step - loss: 0.1012 - dense_1_loss: 0.1012 - reward_out_loss: 2.3364e-10 - val_loss: 0.6541 - val_dense_1_loss: 0.6541 - val_reward_out_loss: 1.1068e-10\n",
      "Epoch 5/100\n",
      "686/686 [==============================] - 0s 326us/step - loss: 0.0947 - dense_1_loss: 0.0947 - reward_out_loss: 3.7157e-10 - val_loss: 0.6541 - val_dense_1_loss: 0.6541 - val_reward_out_loss: 1.0926e-10\n",
      "Epoch 6/100\n",
      "686/686 [==============================] - 0s 337us/step - loss: 0.0944 - dense_1_loss: 0.0944 - reward_out_loss: 2.1702e-10 - val_loss: 0.6556 - val_dense_1_loss: 0.6556 - val_reward_out_loss: 1.0878e-10\n",
      "Epoch 00006: early stopping\n",
      "Train on 706 samples, validate on 79 samples\n",
      "Epoch 1/100\n",
      "706/706 [==============================] - 0s 486us/step - loss: 0.1214 - dense_1_loss: 0.1214 - reward_out_loss: 2.8601e-10 - val_loss: 0.6017 - val_dense_1_loss: 0.6017 - val_reward_out_loss: 6.6030e-07\n",
      "Epoch 2/100\n",
      "706/706 [==============================] - 0s 319us/step - loss: 0.1107 - dense_1_loss: 0.1107 - reward_out_loss: 3.3355e-10 - val_loss: 0.5859 - val_dense_1_loss: 0.5859 - val_reward_out_loss: 6.5057e-07\n",
      "Epoch 3/100\n",
      "706/706 [==============================] - 0s 334us/step - loss: 0.0932 - dense_1_loss: 0.0932 - reward_out_loss: 3.1730e-10 - val_loss: 0.5583 - val_dense_1_loss: 0.5583 - val_reward_out_loss: 6.2027e-07\n",
      "Epoch 4/100\n",
      "706/706 [==============================] - 0s 333us/step - loss: 0.0933 - dense_1_loss: 0.0933 - reward_out_loss: 2.4148e-10 - val_loss: 0.5403 - val_dense_1_loss: 0.5403 - val_reward_out_loss: 5.8447e-07\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "706/706 [==============================] - 0s 341us/step - loss: 0.0940 - dense_1_loss: 0.0940 - reward_out_loss: 2.5849e-10 - val_loss: 0.5289 - val_dense_1_loss: 0.5289 - val_reward_out_loss: 5.6454e-07\n",
      "Epoch 6/100\n",
      "706/706 [==============================] - 0s 334us/step - loss: 0.0922 - dense_1_loss: 0.0922 - reward_out_loss: 3.4895e-10 - val_loss: 0.5224 - val_dense_1_loss: 0.5224 - val_reward_out_loss: 5.4533e-07\n",
      "Epoch 7/100\n",
      "706/706 [==============================] - 0s 328us/step - loss: 0.0918 - dense_1_loss: 0.0918 - reward_out_loss: 1.9016e-10 - val_loss: 0.5186 - val_dense_1_loss: 0.5186 - val_reward_out_loss: 5.3257e-07\n",
      "Epoch 8/100\n",
      "706/706 [==============================] - 0s 336us/step - loss: 0.0925 - dense_1_loss: 0.0925 - reward_out_loss: 1.9836e-10 - val_loss: 0.5212 - val_dense_1_loss: 0.5212 - val_reward_out_loss: 5.2070e-07\n",
      "Epoch 9/100\n",
      "706/706 [==============================] - 0s 340us/step - loss: 0.0919 - dense_1_loss: 0.0919 - reward_out_loss: 1.7437e-10 - val_loss: 0.5253 - val_dense_1_loss: 0.5253 - val_reward_out_loss: 5.0990e-07\n",
      "Epoch 10/100\n",
      "706/706 [==============================] - 0s 335us/step - loss: 0.0918 - dense_1_loss: 0.0918 - reward_out_loss: 1.3630e-10 - val_loss: 0.5286 - val_dense_1_loss: 0.5286 - val_reward_out_loss: 5.0309e-07\n",
      "Epoch 11/100\n",
      "706/706 [==============================] - 0s 319us/step - loss: 0.0917 - dense_1_loss: 0.0917 - reward_out_loss: 1.8945e-10 - val_loss: 0.5320 - val_dense_1_loss: 0.5320 - val_reward_out_loss: 4.9265e-07\n",
      "Epoch 12/100\n",
      "706/706 [==============================] - 0s 328us/step - loss: 0.0917 - dense_1_loss: 0.0917 - reward_out_loss: 1.1369e-10 - val_loss: 0.5347 - val_dense_1_loss: 0.5347 - val_reward_out_loss: 4.8485e-07\n",
      "Epoch 00012: early stopping\n",
      "Train on 716 samples, validate on 80 samples\n",
      "Epoch 1/100\n",
      "716/716 [==============================] - 0s 467us/step - loss: 0.0910 - dense_1_loss: 0.0910 - reward_out_loss: 1.7278e-10 - val_loss: 0.5291 - val_dense_1_loss: 0.5291 - val_reward_out_loss: 4.7506e-07\n",
      "Epoch 2/100\n",
      "716/716 [==============================] - 0s 331us/step - loss: 0.0908 - dense_1_loss: 0.0908 - reward_out_loss: 2.3084e-10 - val_loss: 0.5342 - val_dense_1_loss: 0.5342 - val_reward_out_loss: 4.6604e-07\n",
      "Epoch 3/100\n",
      "716/716 [==============================] - 0s 343us/step - loss: 0.0914 - dense_1_loss: 0.0914 - reward_out_loss: 1.6546e-10 - val_loss: 0.5432 - val_dense_1_loss: 0.5432 - val_reward_out_loss: 4.6968e-07\n",
      "Epoch 4/100\n",
      "716/716 [==============================] - 0s 334us/step - loss: 0.0904 - dense_1_loss: 0.0904 - reward_out_loss: 9.1560e-11 - val_loss: 0.5558 - val_dense_1_loss: 0.5558 - val_reward_out_loss: 4.7104e-07\n",
      "Epoch 5/100\n",
      "716/716 [==============================] - 0s 342us/step - loss: 0.0911 - dense_1_loss: 0.0911 - reward_out_loss: 9.8005e-11 - val_loss: 0.5629 - val_dense_1_loss: 0.5629 - val_reward_out_loss: 4.7765e-07\n",
      "Epoch 6/100\n",
      "716/716 [==============================] - 0s 350us/step - loss: 0.0907 - dense_1_loss: 0.0907 - reward_out_loss: 1.1351e-10 - val_loss: 0.5686 - val_dense_1_loss: 0.5686 - val_reward_out_loss: 4.8351e-07\n",
      "Epoch 00006: early stopping\n",
      "Train on 727 samples, validate on 81 samples\n",
      "Epoch 1/100\n",
      "727/727 [==============================] - 0s 505us/step - loss: 0.0926 - dense_1_loss: 0.0926 - reward_out_loss: 5.8352e-11 - val_loss: 0.5513 - val_dense_1_loss: 0.5513 - val_reward_out_loss: 4.8936e-07\n",
      "Epoch 2/100\n",
      "727/727 [==============================] - 0s 345us/step - loss: 0.0898 - dense_1_loss: 0.0898 - reward_out_loss: 1.2359e-10 - val_loss: 0.5548 - val_dense_1_loss: 0.5548 - val_reward_out_loss: 5.0787e-07\n",
      "Epoch 3/100\n",
      "727/727 [==============================] - 0s 339us/step - loss: 0.0892 - dense_1_loss: 0.0892 - reward_out_loss: 2.0559e-10 - val_loss: 0.5577 - val_dense_1_loss: 0.5577 - val_reward_out_loss: 5.1688e-07\n",
      "Epoch 4/100\n",
      "727/727 [==============================] - 0s 343us/step - loss: 0.0892 - dense_1_loss: 0.0892 - reward_out_loss: 1.2491e-10 - val_loss: 0.5602 - val_dense_1_loss: 0.5602 - val_reward_out_loss: 5.2677e-07\n",
      "Epoch 5/100\n",
      "727/727 [==============================] - 0s 347us/step - loss: 0.0892 - dense_1_loss: 0.0892 - reward_out_loss: 1.3868e-10 - val_loss: 0.5620 - val_dense_1_loss: 0.5620 - val_reward_out_loss: 5.3655e-07\n",
      "Epoch 6/100\n",
      "727/727 [==============================] - 0s 332us/step - loss: 0.0892 - dense_1_loss: 0.0892 - reward_out_loss: 1.7800e-10 - val_loss: 0.5631 - val_dense_1_loss: 0.5631 - val_reward_out_loss: 5.3776e-07\n",
      "Epoch 00006: early stopping\n",
      "Train on 737 samples, validate on 82 samples\n",
      "Epoch 1/100\n",
      "737/737 [==============================] - 0s 494us/step - loss: 0.1008 - dense_1_loss: 0.1008 - reward_out_loss: 1.0647e-10 - val_loss: 0.4431 - val_dense_1_loss: 0.4431 - val_reward_out_loss: 5.4436e-07\n",
      "Epoch 2/100\n",
      "737/737 [==============================] - 0s 335us/step - loss: 0.0890 - dense_1_loss: 0.0890 - reward_out_loss: 9.8633e-11 - val_loss: 0.4176 - val_dense_1_loss: 0.4176 - val_reward_out_loss: 5.7315e-07\n",
      "Epoch 3/100\n",
      "737/737 [==============================] - 0s 389us/step - loss: 0.0896 - dense_1_loss: 0.0896 - reward_out_loss: 3.6635e-10 - val_loss: 0.4344 - val_dense_1_loss: 0.4344 - val_reward_out_loss: 5.8737e-07\n",
      "Epoch 4/100\n",
      "737/737 [==============================] - 0s 348us/step - loss: 0.0899 - dense_1_loss: 0.0899 - reward_out_loss: 1.0652e-10 - val_loss: 0.4890 - val_dense_1_loss: 0.4890 - val_reward_out_loss: 5.8595e-07\n",
      "Epoch 5/100\n",
      "737/737 [==============================] - 0s 332us/step - loss: 0.0887 - dense_1_loss: 0.0887 - reward_out_loss: 1.4113e-10 - val_loss: 0.5377 - val_dense_1_loss: 0.5377 - val_reward_out_loss: 5.6981e-07\n",
      "Epoch 6/100\n",
      "737/737 [==============================] - 0s 348us/step - loss: 0.0882 - dense_1_loss: 0.0882 - reward_out_loss: 1.1022e-10 - val_loss: 0.5708 - val_dense_1_loss: 0.5708 - val_reward_out_loss: 5.5378e-07\n",
      "Epoch 7/100\n",
      "737/737 [==============================] - 0s 347us/step - loss: 0.0885 - dense_1_loss: 0.0885 - reward_out_loss: 7.5490e-11 - val_loss: 0.5747 - val_dense_1_loss: 0.5747 - val_reward_out_loss: 5.4952e-07\n",
      "Epoch 00007: early stopping\n",
      "Train on 748 samples, validate on 84 samples\n",
      "Epoch 1/100\n",
      "748/748 [==============================] - 0s 484us/step - loss: 0.1253 - dense_1_loss: 0.1253 - reward_out_loss: 1.1117e-10 - val_loss: 0.2520 - val_dense_1_loss: 0.2520 - val_reward_out_loss: 5.4810e-07\n",
      "Epoch 2/100\n",
      "748/748 [==============================] - 0s 337us/step - loss: 0.1231 - dense_1_loss: 0.1231 - reward_out_loss: 2.0836e-10 - val_loss: 0.2470 - val_dense_1_loss: 0.2470 - val_reward_out_loss: 5.8557e-07\n",
      "Epoch 3/100\n",
      "748/748 [==============================] - 0s 359us/step - loss: 0.1159 - dense_1_loss: 0.1159 - reward_out_loss: 1.2875e-10 - val_loss: 0.2462 - val_dense_1_loss: 0.2462 - val_reward_out_loss: 6.4297e-07\n",
      "Epoch 4/100\n",
      "748/748 [==============================] - 0s 340us/step - loss: 0.1123 - dense_1_loss: 0.1123 - reward_out_loss: 2.1161e-10 - val_loss: 0.2477 - val_dense_1_loss: 0.2477 - val_reward_out_loss: 7.0131e-07\n",
      "Epoch 5/100\n",
      "748/748 [==============================] - 0s 341us/step - loss: 0.1110 - dense_1_loss: 0.1110 - reward_out_loss: 4.3178e-10 - val_loss: 0.2501 - val_dense_1_loss: 0.2501 - val_reward_out_loss: 7.6696e-07\n",
      "Epoch 6/100\n",
      "748/748 [==============================] - 0s 341us/step - loss: 0.1100 - dense_1_loss: 0.1100 - reward_out_loss: 3.0155e-10 - val_loss: 0.2522 - val_dense_1_loss: 0.2522 - val_reward_out_loss: 8.3039e-07\n",
      "Epoch 7/100\n",
      "748/748 [==============================] - 0s 358us/step - loss: 0.1095 - dense_1_loss: 0.1095 - reward_out_loss: 2.9869e-10 - val_loss: 0.2543 - val_dense_1_loss: 0.2543 - val_reward_out_loss: 8.7616e-07\n",
      "Epoch 00007: early stopping\n",
      "Train on 781 samples, validate on 87 samples\n",
      "Epoch 1/100\n",
      "781/781 [==============================] - 0s 515us/step - loss: 0.1241 - dense_1_loss: 0.1241 - reward_out_loss: 1.0659e-07 - val_loss: 0.2793 - val_dense_1_loss: 0.2793 - val_reward_out_loss: 2.2460e-07\n",
      "Epoch 2/100\n",
      "781/781 [==============================] - 0s 333us/step - loss: 0.1074 - dense_1_loss: 0.1074 - reward_out_loss: 9.9378e-08 - val_loss: 0.2757 - val_dense_1_loss: 0.2757 - val_reward_out_loss: 2.4049e-07\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/781 [==============================] - 0s 334us/step - loss: 0.1045 - dense_1_loss: 0.1045 - reward_out_loss: 4.8729e-08 - val_loss: 0.2662 - val_dense_1_loss: 0.2662 - val_reward_out_loss: 2.5704e-07\n",
      "Epoch 4/100\n",
      "781/781 [==============================] - 0s 323us/step - loss: 0.0854 - dense_1_loss: 0.0854 - reward_out_loss: 2.2763e-08 - val_loss: 0.2574 - val_dense_1_loss: 0.2574 - val_reward_out_loss: 2.7079e-07\n",
      "Epoch 5/100\n",
      "781/781 [==============================] - 0s 328us/step - loss: 0.0850 - dense_1_loss: 0.0850 - reward_out_loss: 7.9333e-09 - val_loss: 0.2534 - val_dense_1_loss: 0.2534 - val_reward_out_loss: 2.8296e-07\n",
      "Epoch 6/100\n",
      "781/781 [==============================] - 0s 336us/step - loss: 0.0842 - dense_1_loss: 0.0842 - reward_out_loss: 2.0995e-09 - val_loss: 0.2521 - val_dense_1_loss: 0.2521 - val_reward_out_loss: 2.8810e-07\n",
      "Epoch 7/100\n",
      "781/781 [==============================] - 0s 332us/step - loss: 0.0841 - dense_1_loss: 0.0841 - reward_out_loss: 1.2514e-09 - val_loss: 0.2504 - val_dense_1_loss: 0.2504 - val_reward_out_loss: 2.8689e-07\n",
      "Epoch 8/100\n",
      "781/781 [==============================] - 0s 331us/step - loss: 0.0836 - dense_1_loss: 0.0836 - reward_out_loss: 1.4495e-09 - val_loss: 0.2494 - val_dense_1_loss: 0.2494 - val_reward_out_loss: 2.8195e-07\n",
      "Epoch 9/100\n",
      "781/781 [==============================] - 0s 352us/step - loss: 0.0834 - dense_1_loss: 0.0834 - reward_out_loss: 8.5088e-10 - val_loss: 0.2488 - val_dense_1_loss: 0.2488 - val_reward_out_loss: 2.7498e-07\n",
      "Epoch 10/100\n",
      "781/781 [==============================] - 0s 342us/step - loss: 0.0832 - dense_1_loss: 0.0832 - reward_out_loss: 4.9799e-10 - val_loss: 0.2476 - val_dense_1_loss: 0.2476 - val_reward_out_loss: 2.6640e-07\n",
      "Epoch 11/100\n",
      "781/781 [==============================] - 0s 331us/step - loss: 0.0836 - dense_1_loss: 0.0836 - reward_out_loss: 4.0640e-10 - val_loss: 0.2468 - val_dense_1_loss: 0.2468 - val_reward_out_loss: 2.6091e-07\n",
      "Epoch 12/100\n",
      "781/781 [==============================] - 0s 342us/step - loss: 0.0842 - dense_1_loss: 0.0842 - reward_out_loss: 6.7979e-10 - val_loss: 0.2468 - val_dense_1_loss: 0.2468 - val_reward_out_loss: 2.5987e-07\n",
      "Epoch 13/100\n",
      "781/781 [==============================] - 0s 339us/step - loss: 0.0830 - dense_1_loss: 0.0830 - reward_out_loss: 7.2153e-10 - val_loss: 0.2477 - val_dense_1_loss: 0.2477 - val_reward_out_loss: 2.5999e-07\n",
      "Epoch 14/100\n",
      "781/781 [==============================] - 0s 333us/step - loss: 0.0829 - dense_1_loss: 0.0829 - reward_out_loss: 2.7517e-10 - val_loss: 0.2486 - val_dense_1_loss: 0.2486 - val_reward_out_loss: 2.6198e-07\n",
      "Epoch 15/100\n",
      "781/781 [==============================] - 0s 329us/step - loss: 0.0829 - dense_1_loss: 0.0829 - reward_out_loss: 4.4467e-10 - val_loss: 0.2495 - val_dense_1_loss: 0.2495 - val_reward_out_loss: 2.6212e-07\n",
      "Epoch 00015: early stopping\n",
      "Train on 791 samples, validate on 88 samples\n",
      "Epoch 1/100\n",
      "791/791 [==============================] - 0s 500us/step - loss: 0.0842 - dense_1_loss: 0.0842 - reward_out_loss: 4.8208e-10 - val_loss: 0.2322 - val_dense_1_loss: 0.2322 - val_reward_out_loss: 2.4474e-07\n",
      "Epoch 2/100\n",
      "791/791 [==============================] - 0s 356us/step - loss: 0.0830 - dense_1_loss: 0.0830 - reward_out_loss: 2.1483e-09 - val_loss: 0.2347 - val_dense_1_loss: 0.2347 - val_reward_out_loss: 2.3349e-07\n",
      "Epoch 3/100\n",
      "791/791 [==============================] - 0s 342us/step - loss: 0.0823 - dense_1_loss: 0.0823 - reward_out_loss: 4.5303e-10 - val_loss: 0.2370 - val_dense_1_loss: 0.2370 - val_reward_out_loss: 2.2215e-07\n",
      "Epoch 4/100\n",
      "791/791 [==============================] - 0s 339us/step - loss: 0.0821 - dense_1_loss: 0.0821 - reward_out_loss: 4.2912e-10 - val_loss: 0.2390 - val_dense_1_loss: 0.2390 - val_reward_out_loss: 2.0970e-07\n",
      "Epoch 5/100\n",
      "791/791 [==============================] - 0s 339us/step - loss: 0.0819 - dense_1_loss: 0.0819 - reward_out_loss: 3.3758e-10 - val_loss: 0.2406 - val_dense_1_loss: 0.2406 - val_reward_out_loss: 2.0021e-07\n",
      "Epoch 6/100\n",
      "791/791 [==============================] - 0s 328us/step - loss: 0.0820 - dense_1_loss: 0.0820 - reward_out_loss: 4.4050e-10 - val_loss: 0.2416 - val_dense_1_loss: 0.2416 - val_reward_out_loss: 1.9368e-07\n",
      "Epoch 00006: early stopping\n",
      "Train on 801 samples, validate on 90 samples\n",
      "Epoch 1/100\n",
      "801/801 [==============================] - 0s 507us/step - loss: 0.0815 - dense_1_loss: 0.0815 - reward_out_loss: 6.8469e-10 - val_loss: 0.2257 - val_dense_1_loss: 0.2257 - val_reward_out_loss: 1.8739e-07\n",
      "Epoch 2/100\n",
      "801/801 [==============================] - 0s 344us/step - loss: 0.0813 - dense_1_loss: 0.0813 - reward_out_loss: 3.6080e-10 - val_loss: 0.2255 - val_dense_1_loss: 0.2255 - val_reward_out_loss: 1.8685e-07\n",
      "Epoch 3/100\n",
      "801/801 [==============================] - 0s 342us/step - loss: 0.0813 - dense_1_loss: 0.0813 - reward_out_loss: 4.4855e-10 - val_loss: 0.2251 - val_dense_1_loss: 0.2251 - val_reward_out_loss: 1.8519e-07\n",
      "Epoch 4/100\n",
      "801/801 [==============================] - 0s 341us/step - loss: 0.0809 - dense_1_loss: 0.0809 - reward_out_loss: 4.1660e-10 - val_loss: 0.2247 - val_dense_1_loss: 0.2247 - val_reward_out_loss: 1.8266e-07\n",
      "Epoch 5/100\n",
      "801/801 [==============================] - 0s 331us/step - loss: 0.0809 - dense_1_loss: 0.0809 - reward_out_loss: 4.6203e-10 - val_loss: 0.2243 - val_dense_1_loss: 0.2243 - val_reward_out_loss: 1.8006e-07\n",
      "Epoch 6/100\n",
      "801/801 [==============================] - 0s 343us/step - loss: 0.0808 - dense_1_loss: 0.0808 - reward_out_loss: 1.5943e-10 - val_loss: 0.2240 - val_dense_1_loss: 0.2240 - val_reward_out_loss: 1.7909e-07\n",
      "Epoch 7/100\n",
      "801/801 [==============================] - 0s 347us/step - loss: 0.0811 - dense_1_loss: 0.0811 - reward_out_loss: 1.6036e-10 - val_loss: 0.2235 - val_dense_1_loss: 0.2235 - val_reward_out_loss: 1.7559e-07\n",
      "Epoch 8/100\n",
      "801/801 [==============================] - 0s 340us/step - loss: 0.0808 - dense_1_loss: 0.0808 - reward_out_loss: 1.7593e-10 - val_loss: 0.2226 - val_dense_1_loss: 0.2226 - val_reward_out_loss: 1.7217e-07\n",
      "Epoch 9/100\n",
      "801/801 [==============================] - 0s 344us/step - loss: 0.0807 - dense_1_loss: 0.0807 - reward_out_loss: 2.6452e-10 - val_loss: 0.2218 - val_dense_1_loss: 0.2218 - val_reward_out_loss: 1.6959e-07\n",
      "Epoch 10/100\n",
      "801/801 [==============================] - 0s 352us/step - loss: 0.0807 - dense_1_loss: 0.0807 - reward_out_loss: 1.3733e-10 - val_loss: 0.2212 - val_dense_1_loss: 0.2212 - val_reward_out_loss: 1.6611e-07\n",
      "Epoch 11/100\n",
      "801/801 [==============================] - 0s 341us/step - loss: 0.0807 - dense_1_loss: 0.0807 - reward_out_loss: 1.6254e-10 - val_loss: 0.2205 - val_dense_1_loss: 0.2205 - val_reward_out_loss: 1.6176e-07\n",
      "Epoch 12/100\n",
      "801/801 [==============================] - 0s 358us/step - loss: 0.0807 - dense_1_loss: 0.0807 - reward_out_loss: 3.5782e-11 - val_loss: 0.2201 - val_dense_1_loss: 0.2201 - val_reward_out_loss: 1.5843e-07\n",
      "Epoch 13/100\n",
      "801/801 [==============================] - 0s 332us/step - loss: 0.0807 - dense_1_loss: 0.0807 - reward_out_loss: 9.0304e-11 - val_loss: 0.2197 - val_dense_1_loss: 0.2197 - val_reward_out_loss: 1.5378e-07\n",
      "Epoch 14/100\n",
      "801/801 [==============================] - 0s 349us/step - loss: 0.0807 - dense_1_loss: 0.0807 - reward_out_loss: 8.1932e-11 - val_loss: 0.2194 - val_dense_1_loss: 0.2194 - val_reward_out_loss: 1.4916e-07\n",
      "Epoch 15/100\n",
      "801/801 [==============================] - 0s 346us/step - loss: 0.0807 - dense_1_loss: 0.0807 - reward_out_loss: 1.0540e-10 - val_loss: 0.2194 - val_dense_1_loss: 0.2194 - val_reward_out_loss: 1.4388e-07\n",
      "Epoch 16/100\n",
      "801/801 [==============================] - 0s 340us/step - loss: 0.0807 - dense_1_loss: 0.0807 - reward_out_loss: 8.4424e-11 - val_loss: 0.2192 - val_dense_1_loss: 0.2192 - val_reward_out_loss: 1.3906e-07\n",
      "Epoch 17/100\n",
      "801/801 [==============================] - 0s 337us/step - loss: 0.0807 - dense_1_loss: 0.0807 - reward_out_loss: 5.5852e-11 - val_loss: 0.2193 - val_dense_1_loss: 0.2193 - val_reward_out_loss: 1.3383e-07\n",
      "Epoch 00017: early stopping\n",
      "Train on 882 samples, validate on 98 samples\n",
      "Epoch 1/100\n",
      "882/882 [==============================] - 0s 529us/step - loss: 0.0981 - dense_1_loss: 0.0981 - reward_out_loss: 7.2235e-10 - val_loss: 0.4247 - val_dense_1_loss: 0.4246 - val_reward_out_loss: 3.2770e-05\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "882/882 [==============================] - 0s 335us/step - loss: 0.0854 - dense_1_loss: 0.0854 - reward_out_loss: 4.6456e-09 - val_loss: 0.4261 - val_dense_1_loss: 0.4261 - val_reward_out_loss: 3.3396e-05\n",
      "Epoch 3/100\n",
      "882/882 [==============================] - 0s 349us/step - loss: 0.0792 - dense_1_loss: 0.0792 - reward_out_loss: 4.4100e-09 - val_loss: 0.4275 - val_dense_1_loss: 0.4275 - val_reward_out_loss: 3.3218e-05\n",
      "Epoch 4/100\n",
      "882/882 [==============================] - 0s 345us/step - loss: 0.0817 - dense_1_loss: 0.0817 - reward_out_loss: 2.7285e-09 - val_loss: 0.4289 - val_dense_1_loss: 0.4289 - val_reward_out_loss: 3.2825e-05\n",
      "Epoch 5/100\n",
      "882/882 [==============================] - 0s 326us/step - loss: 0.0787 - dense_1_loss: 0.0787 - reward_out_loss: 3.9645e-09 - val_loss: 0.4307 - val_dense_1_loss: 0.4306 - val_reward_out_loss: 3.2332e-05\n",
      "Epoch 6/100\n",
      "882/882 [==============================] - 0s 345us/step - loss: 0.0774 - dense_1_loss: 0.0774 - reward_out_loss: 5.6420e-09 - val_loss: 0.4300 - val_dense_1_loss: 0.4300 - val_reward_out_loss: 3.0956e-05\n",
      "Epoch 00006: early stopping\n",
      "Train on 977 samples, validate on 109 samples\n",
      "Epoch 1/100\n",
      "977/977 [==============================] - 1s 527us/step - loss: 0.1132 - dense_1_loss: 0.1132 - reward_out_loss: 2.8816e-07 - val_loss: 0.1164 - val_dense_1_loss: 0.1164 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 2/100\n",
      "977/977 [==============================] - 0s 323us/step - loss: 0.0927 - dense_1_loss: 0.0927 - reward_out_loss: 3.3485e-07 - val_loss: 0.1173 - val_dense_1_loss: 0.1173 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 3/100\n",
      "977/977 [==============================] - 0s 320us/step - loss: 0.0834 - dense_1_loss: 0.0834 - reward_out_loss: 4.5105e-07 - val_loss: 0.1163 - val_dense_1_loss: 0.1163 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 4/100\n",
      "977/977 [==============================] - 0s 317us/step - loss: 0.0761 - dense_1_loss: 0.0761 - reward_out_loss: 4.3663e-07 - val_loss: 0.1152 - val_dense_1_loss: 0.1152 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 5/100\n",
      "977/977 [==============================] - 0s 327us/step - loss: 0.0736 - dense_1_loss: 0.0736 - reward_out_loss: 2.8322e-07 - val_loss: 0.1149 - val_dense_1_loss: 0.1149 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 6/100\n",
      "977/977 [==============================] - 0s 325us/step - loss: 0.0708 - dense_1_loss: 0.0708 - reward_out_loss: 2.1412e-07 - val_loss: 0.1147 - val_dense_1_loss: 0.1147 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 7/100\n",
      "977/977 [==============================] - 0s 316us/step - loss: 0.0688 - dense_1_loss: 0.0688 - reward_out_loss: 1.5160e-07 - val_loss: 0.1142 - val_dense_1_loss: 0.1142 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 8/100\n",
      "977/977 [==============================] - 0s 320us/step - loss: 0.0682 - dense_1_loss: 0.0682 - reward_out_loss: 1.0902e-07 - val_loss: 0.1133 - val_dense_1_loss: 0.1133 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 9/100\n",
      "977/977 [==============================] - 0s 324us/step - loss: 0.0678 - dense_1_loss: 0.0678 - reward_out_loss: 8.3901e-08 - val_loss: 0.1123 - val_dense_1_loss: 0.1123 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 10/100\n",
      "977/977 [==============================] - 0s 320us/step - loss: 0.0674 - dense_1_loss: 0.0674 - reward_out_loss: 9.7678e-08 - val_loss: 0.1112 - val_dense_1_loss: 0.1112 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 11/100\n",
      "977/977 [==============================] - 0s 318us/step - loss: 0.0673 - dense_1_loss: 0.0673 - reward_out_loss: 4.6417e-08 - val_loss: 0.1104 - val_dense_1_loss: 0.1104 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 12/100\n",
      "977/977 [==============================] - 0s 323us/step - loss: 0.0671 - dense_1_loss: 0.0671 - reward_out_loss: 3.2641e-08 - val_loss: 0.1097 - val_dense_1_loss: 0.1097 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 13/100\n",
      "977/977 [==============================] - 0s 314us/step - loss: 0.0671 - dense_1_loss: 0.0671 - reward_out_loss: 2.1704e-08 - val_loss: 0.1092 - val_dense_1_loss: 0.1092 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 14/100\n",
      "977/977 [==============================] - 0s 324us/step - loss: 0.0667 - dense_1_loss: 0.0667 - reward_out_loss: 1.9680e-08 - val_loss: 0.1087 - val_dense_1_loss: 0.1087 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 15/100\n",
      "977/977 [==============================] - 0s 333us/step - loss: 0.0667 - dense_1_loss: 0.0667 - reward_out_loss: 1.5731e-08 - val_loss: 0.1083 - val_dense_1_loss: 0.1083 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 16/100\n",
      "977/977 [==============================] - 0s 314us/step - loss: 0.0666 - dense_1_loss: 0.0666 - reward_out_loss: 2.0974e-08 - val_loss: 0.1080 - val_dense_1_loss: 0.1080 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 17/100\n",
      "977/977 [==============================] - 0s 322us/step - loss: 0.0665 - dense_1_loss: 0.0665 - reward_out_loss: 1.0396e-08 - val_loss: 0.1078 - val_dense_1_loss: 0.1078 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 18/100\n",
      "977/977 [==============================] - 0s 322us/step - loss: 0.0665 - dense_1_loss: 0.0665 - reward_out_loss: 1.1441e-08 - val_loss: 0.1075 - val_dense_1_loss: 0.1075 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 19/100\n",
      "977/977 [==============================] - 0s 318us/step - loss: 0.0664 - dense_1_loss: 0.0664 - reward_out_loss: 7.8992e-09 - val_loss: 0.1073 - val_dense_1_loss: 0.1073 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 20/100\n",
      "977/977 [==============================] - 0s 330us/step - loss: 0.0664 - dense_1_loss: 0.0664 - reward_out_loss: 1.4168e-08 - val_loss: 0.1070 - val_dense_1_loss: 0.1070 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 21/100\n",
      "977/977 [==============================] - 0s 313us/step - loss: 0.0664 - dense_1_loss: 0.0664 - reward_out_loss: 6.2629e-09 - val_loss: 0.1067 - val_dense_1_loss: 0.1067 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 22/100\n",
      "977/977 [==============================] - 0s 330us/step - loss: 0.0665 - dense_1_loss: 0.0665 - reward_out_loss: 8.1281e-09 - val_loss: 0.1064 - val_dense_1_loss: 0.1064 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 23/100\n",
      "977/977 [==============================] - 0s 325us/step - loss: 0.0663 - dense_1_loss: 0.0663 - reward_out_loss: 5.3842e-09 - val_loss: 0.1061 - val_dense_1_loss: 0.1061 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 24/100\n",
      "977/977 [==============================] - 0s 342us/step - loss: 0.0664 - dense_1_loss: 0.0664 - reward_out_loss: 9.9240e-09 - val_loss: 0.1059 - val_dense_1_loss: 0.1059 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 25/100\n",
      "977/977 [==============================] - 0s 324us/step - loss: 0.0664 - dense_1_loss: 0.0664 - reward_out_loss: 3.5440e-09 - val_loss: 0.1057 - val_dense_1_loss: 0.1057 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 26/100\n",
      "977/977 [==============================] - 0s 319us/step - loss: 0.0664 - dense_1_loss: 0.0664 - reward_out_loss: 3.7615e-09 - val_loss: 0.1056 - val_dense_1_loss: 0.1056 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 27/100\n",
      "977/977 [==============================] - 0s 314us/step - loss: 0.0665 - dense_1_loss: 0.0665 - reward_out_loss: 3.4087e-09 - val_loss: 0.1055 - val_dense_1_loss: 0.1055 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 00027: early stopping\n",
      "Train on 988 samples, validate on 110 samples\n",
      "Epoch 1/100\n",
      "988/988 [==============================] - 0s 357us/step - loss: 0.0658 - dense_1_loss: 0.0658 - reward_out_loss: 3.2665e-09 - val_loss: 0.1166 - val_dense_1_loss: 0.1166 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 2/100\n",
      "988/988 [==============================] - 0s 322us/step - loss: 0.0660 - dense_1_loss: 0.0660 - reward_out_loss: 4.2479e-09 - val_loss: 0.1170 - val_dense_1_loss: 0.1170 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 3/100\n",
      "988/988 [==============================] - 0s 312us/step - loss: 0.0657 - dense_1_loss: 0.0657 - reward_out_loss: 5.2951e-09 - val_loss: 0.1170 - val_dense_1_loss: 0.1170 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 4/100\n",
      "988/988 [==============================] - 0s 323us/step - loss: 0.0657 - dense_1_loss: 0.0657 - reward_out_loss: 5.2182e-09 - val_loss: 0.1169 - val_dense_1_loss: 0.1169 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 5/100\n",
      "988/988 [==============================] - 0s 325us/step - loss: 0.0656 - dense_1_loss: 0.0656 - reward_out_loss: 3.9060e-09 - val_loss: 0.1168 - val_dense_1_loss: 0.1168 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "988/988 [==============================] - 0s 318us/step - loss: 0.0655 - dense_1_loss: 0.0655 - reward_out_loss: 3.1476e-09 - val_loss: 0.1167 - val_dense_1_loss: 0.1167 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 00006: early stopping\n",
      "Train on 1008 samples, validate on 113 samples\n",
      "Epoch 1/100\n",
      "1008/1008 [==============================] - 0s 334us/step - loss: 0.0793 - dense_1_loss: 0.0793 - reward_out_loss: 4.5874e-09 - val_loss: 0.0269 - val_dense_1_loss: 0.0269 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 2/100\n",
      "1008/1008 [==============================] - 0s 321us/step - loss: 0.0653 - dense_1_loss: 0.0653 - reward_out_loss: 2.8884e-09 - val_loss: 0.0268 - val_dense_1_loss: 0.0268 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 3/100\n",
      "1008/1008 [==============================] - 0s 312us/step - loss: 0.0651 - dense_1_loss: 0.0651 - reward_out_loss: 4.2357e-09 - val_loss: 0.0278 - val_dense_1_loss: 0.0278 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 4/100\n",
      "1008/1008 [==============================] - 0s 304us/step - loss: 0.0646 - dense_1_loss: 0.0646 - reward_out_loss: 5.9024e-09 - val_loss: 0.0284 - val_dense_1_loss: 0.0284 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 5/100\n",
      "1008/1008 [==============================] - 0s 309us/step - loss: 0.0644 - dense_1_loss: 0.0644 - reward_out_loss: 3.0957e-09 - val_loss: 0.0289 - val_dense_1_loss: 0.0289 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 6/100\n",
      "1008/1008 [==============================] - 0s 311us/step - loss: 0.0646 - dense_1_loss: 0.0646 - reward_out_loss: 2.9914e-09 - val_loss: 0.0300 - val_dense_1_loss: 0.0300 - val_reward_out_loss: 0.0000e+00\n",
      "Epoch 00006: early stopping\n",
      "Train on 1018 samples, validate on 114 samples\n",
      "Epoch 1/100\n",
      "1018/1018 [==============================] - 1s 530us/step - loss: 0.0637 - dense_1_loss: 0.0637 - reward_out_loss: 4.6403e-09 - val_loss: 0.0483 - val_dense_1_loss: 0.0483 - val_reward_out_loss: 2.0033e-12\n",
      "Epoch 2/100\n",
      "1018/1018 [==============================] - 0s 319us/step - loss: 0.0636 - dense_1_loss: 0.0636 - reward_out_loss: 4.3606e-09 - val_loss: 0.0489 - val_dense_1_loss: 0.0489 - val_reward_out_loss: 1.8781e-12\n",
      "Epoch 3/100\n",
      "1018/1018 [==============================] - 0s 320us/step - loss: 0.0636 - dense_1_loss: 0.0636 - reward_out_loss: 3.2414e-09 - val_loss: 0.0494 - val_dense_1_loss: 0.0494 - val_reward_out_loss: 1.7728e-12\n",
      "Epoch 4/100\n",
      "1018/1018 [==============================] - 0s 326us/step - loss: 0.0636 - dense_1_loss: 0.0636 - reward_out_loss: 2.0886e-09 - val_loss: 0.0499 - val_dense_1_loss: 0.0499 - val_reward_out_loss: 1.6552e-12\n",
      "Epoch 5/100\n",
      "1018/1018 [==============================] - 0s 333us/step - loss: 0.0636 - dense_1_loss: 0.0636 - reward_out_loss: 2.9637e-09 - val_loss: 0.0502 - val_dense_1_loss: 0.0502 - val_reward_out_loss: 1.5564e-12\n",
      "Epoch 6/100\n",
      "1018/1018 [==============================] - 0s 333us/step - loss: 0.0637 - dense_1_loss: 0.0637 - reward_out_loss: 1.9724e-09 - val_loss: 0.0505 - val_dense_1_loss: 0.0505 - val_reward_out_loss: 1.4598e-12\n",
      "Epoch 00006: early stopping\n",
      "Train on 1250 samples, validate on 139 samples\n",
      "Epoch 1/100\n",
      "1250/1250 [==============================] - 1s 406us/step - loss: 0.6091 - dense_1_loss: 0.2315 - reward_out_loss: 0.3776 - val_loss: 4.5377 - val_dense_1_loss: 0.5377 - val_reward_out_loss: 4.0000\n",
      "Epoch 2/100\n",
      "1250/1250 [==============================] - 0s 322us/step - loss: 0.5463 - dense_1_loss: 0.1687 - reward_out_loss: 0.3776 - val_loss: 4.2437 - val_dense_1_loss: 0.2437 - val_reward_out_loss: 4.0000\n",
      "Epoch 3/100\n",
      "1250/1250 [==============================] - 0s 319us/step - loss: 0.5222 - dense_1_loss: 0.1446 - reward_out_loss: 0.3776 - val_loss: 4.2377 - val_dense_1_loss: 0.2377 - val_reward_out_loss: 4.0000\n",
      "Epoch 4/100\n",
      "1250/1250 [==============================] - 0s 322us/step - loss: 0.4726 - dense_1_loss: 0.0950 - reward_out_loss: 0.3776 - val_loss: 4.2881 - val_dense_1_loss: 0.2881 - val_reward_out_loss: 4.0000\n",
      "Epoch 5/100\n",
      "1250/1250 [==============================] - 0s 319us/step - loss: 0.4736 - dense_1_loss: 0.0960 - reward_out_loss: 0.3776 - val_loss: 4.2383 - val_dense_1_loss: 0.2383 - val_reward_out_loss: 4.0000\n",
      "Epoch 6/100\n",
      "1250/1250 [==============================] - 0s 325us/step - loss: 0.4628 - dense_1_loss: 0.0852 - reward_out_loss: 0.3776 - val_loss: 4.2426 - val_dense_1_loss: 0.2426 - val_reward_out_loss: 4.0000\n",
      "Epoch 7/100\n",
      "1250/1250 [==============================] - 0s 322us/step - loss: 0.4587 - dense_1_loss: 0.0811 - reward_out_loss: 0.3776 - val_loss: 4.2529 - val_dense_1_loss: 0.2529 - val_reward_out_loss: 3.9999\n",
      "Epoch 8/100\n",
      "1250/1250 [==============================] - 0s 317us/step - loss: 0.4533 - dense_1_loss: 0.0758 - reward_out_loss: 0.3775 - val_loss: 4.2358 - val_dense_1_loss: 0.2367 - val_reward_out_loss: 3.9991\n",
      "Epoch 9/100\n",
      "1250/1250 [==============================] - 0s 324us/step - loss: 0.4529 - dense_1_loss: 0.0784 - reward_out_loss: 0.3745 - val_loss: 4.1466 - val_dense_1_loss: 0.2314 - val_reward_out_loss: 3.9152\n",
      "Epoch 10/100\n",
      "1250/1250 [==============================] - 0s 331us/step - loss: 0.3881 - dense_1_loss: 0.0735 - reward_out_loss: 0.3146 - val_loss: 0.7037 - val_dense_1_loss: 0.2392 - val_reward_out_loss: 0.4645\n",
      "Epoch 11/100\n",
      "1250/1250 [==============================] - 0s 314us/step - loss: 0.1621 - dense_1_loss: 0.0757 - reward_out_loss: 0.0864 - val_loss: 0.3898 - val_dense_1_loss: 0.2636 - val_reward_out_loss: 0.1262\n",
      "Epoch 12/100\n",
      "1250/1250 [==============================] - 0s 332us/step - loss: 0.1055 - dense_1_loss: 0.0817 - reward_out_loss: 0.0238 - val_loss: 0.3828 - val_dense_1_loss: 0.2852 - val_reward_out_loss: 0.0975\n",
      "Epoch 13/100\n",
      "1250/1250 [==============================] - 0s 321us/step - loss: 0.0798 - dense_1_loss: 0.0766 - reward_out_loss: 0.0032 - val_loss: 0.4474 - val_dense_1_loss: 0.3693 - val_reward_out_loss: 0.0781\n",
      "Epoch 14/100\n",
      "1250/1250 [==============================] - 0s 323us/step - loss: 0.0787 - dense_1_loss: 0.0786 - reward_out_loss: 3.8298e-05 - val_loss: 0.3987 - val_dense_1_loss: 0.3141 - val_reward_out_loss: 0.0846\n",
      "Epoch 15/100\n",
      "1250/1250 [==============================] - 0s 326us/step - loss: 0.0758 - dense_1_loss: 0.0758 - reward_out_loss: 3.7676e-05 - val_loss: 0.4059 - val_dense_1_loss: 0.3202 - val_reward_out_loss: 0.0857\n",
      "Epoch 16/100\n",
      "1250/1250 [==============================] - 0s 313us/step - loss: 0.0729 - dense_1_loss: 0.0728 - reward_out_loss: 8.7417e-05 - val_loss: 0.4420 - val_dense_1_loss: 0.3653 - val_reward_out_loss: 0.0767\n",
      "Epoch 17/100\n",
      "1250/1250 [==============================] - 0s 319us/step - loss: 0.0829 - dense_1_loss: 0.0828 - reward_out_loss: 4.0366e-05 - val_loss: 0.3952 - val_dense_1_loss: 0.3205 - val_reward_out_loss: 0.0747\n",
      "Epoch 00017: early stopping\n",
      "Train on 1302 samples, validate on 145 samples\n",
      "Epoch 1/100\n",
      "1302/1302 [==============================] - 1s 424us/step - loss: 0.0879 - dense_1_loss: 0.0863 - reward_out_loss: 0.0016 - val_loss: 0.6559 - val_dense_1_loss: 0.6159 - val_reward_out_loss: 0.0400\n",
      "Epoch 2/100\n",
      "1302/1302 [==============================] - 0s 323us/step - loss: 0.0773 - dense_1_loss: 0.0773 - reward_out_loss: 5.0591e-06 - val_loss: 0.6005 - val_dense_1_loss: 0.5759 - val_reward_out_loss: 0.0245\n",
      "Epoch 3/100\n",
      "1302/1302 [==============================] - 0s 321us/step - loss: 0.0865 - dense_1_loss: 0.0865 - reward_out_loss: 8.6876e-07 - val_loss: 0.6471 - val_dense_1_loss: 0.6283 - val_reward_out_loss: 0.0188\n",
      "Epoch 4/100\n",
      "1302/1302 [==============================] - 0s 325us/step - loss: 0.0841 - dense_1_loss: 0.0841 - reward_out_loss: 8.0573e-07 - val_loss: 0.6130 - val_dense_1_loss: 0.5951 - val_reward_out_loss: 0.0178\n",
      "Epoch 5/100\n",
      "1302/1302 [==============================] - 0s 324us/step - loss: 0.0875 - dense_1_loss: 0.0875 - reward_out_loss: 5.4441e-07 - val_loss: 0.6128 - val_dense_1_loss: 0.5945 - val_reward_out_loss: 0.0183\n",
      "Epoch 6/100\n",
      "1302/1302 [==============================] - 0s 320us/step - loss: 0.0748 - dense_1_loss: 0.0748 - reward_out_loss: 5.8579e-07 - val_loss: 0.6662 - val_dense_1_loss: 0.6467 - val_reward_out_loss: 0.0195\n",
      "Epoch 7/100\n",
      "1302/1302 [==============================] - 0s 325us/step - loss: 0.0822 - dense_1_loss: 0.0822 - reward_out_loss: 1.3746e-06 - val_loss: 0.5817 - val_dense_1_loss: 0.5635 - val_reward_out_loss: 0.0182\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1302/1302 [==============================] - 0s 319us/step - loss: 0.0762 - dense_1_loss: 0.0762 - reward_out_loss: 1.1284e-06 - val_loss: 0.6473 - val_dense_1_loss: 0.6274 - val_reward_out_loss: 0.0199\n",
      "Epoch 9/100\n",
      "1302/1302 [==============================] - 0s 321us/step - loss: 0.0740 - dense_1_loss: 0.0740 - reward_out_loss: 1.8527e-06 - val_loss: 0.6506 - val_dense_1_loss: 0.6299 - val_reward_out_loss: 0.0207\n",
      "Epoch 10/100\n",
      "1302/1302 [==============================] - 0s 310us/step - loss: 0.0697 - dense_1_loss: 0.0697 - reward_out_loss: 2.1049e-06 - val_loss: 0.6473 - val_dense_1_loss: 0.6258 - val_reward_out_loss: 0.0215\n",
      "Epoch 11/100\n",
      "1302/1302 [==============================] - 0s 310us/step - loss: 0.0709 - dense_1_loss: 0.0709 - reward_out_loss: 2.1505e-06 - val_loss: 0.6566 - val_dense_1_loss: 0.6349 - val_reward_out_loss: 0.0217\n",
      "Epoch 12/100\n",
      "1302/1302 [==============================] - 0s 314us/step - loss: 0.0676 - dense_1_loss: 0.0676 - reward_out_loss: 1.4795e-06 - val_loss: 0.6500 - val_dense_1_loss: 0.6285 - val_reward_out_loss: 0.0215\n",
      "Epoch 00012: early stopping\n",
      "Train on 1312 samples, validate on 146 samples\n",
      "Epoch 1/100\n",
      "1312/1312 [==============================] - 1s 414us/step - loss: 0.0787 - dense_1_loss: 0.0787 - reward_out_loss: 1.8073e-06 - val_loss: 0.6131 - val_dense_1_loss: 0.5917 - val_reward_out_loss: 0.0214\n",
      "Epoch 2/100\n",
      "1312/1312 [==============================] - 0s 312us/step - loss: 0.0826 - dense_1_loss: 0.0826 - reward_out_loss: 1.3948e-06 - val_loss: 0.6187 - val_dense_1_loss: 0.5968 - val_reward_out_loss: 0.0219\n",
      "Epoch 3/100\n",
      "1312/1312 [==============================] - 0s 318us/step - loss: 0.0741 - dense_1_loss: 0.0741 - reward_out_loss: 1.5336e-06 - val_loss: 0.8208 - val_dense_1_loss: 0.7965 - val_reward_out_loss: 0.0243\n",
      "Epoch 4/100\n",
      "1312/1312 [==============================] - 0s 321us/step - loss: 0.0746 - dense_1_loss: 0.0746 - reward_out_loss: 2.0606e-06 - val_loss: 0.6149 - val_dense_1_loss: 0.5922 - val_reward_out_loss: 0.0227\n",
      "Epoch 5/100\n",
      "1312/1312 [==============================] - 0s 325us/step - loss: 0.0769 - dense_1_loss: 0.0769 - reward_out_loss: 1.5398e-06 - val_loss: 0.5971 - val_dense_1_loss: 0.5749 - val_reward_out_loss: 0.0222\n",
      "Epoch 6/100\n",
      "1312/1312 [==============================] - 0s 318us/step - loss: 0.0721 - dense_1_loss: 0.0721 - reward_out_loss: 7.1804e-07 - val_loss: 0.5779 - val_dense_1_loss: 0.5533 - val_reward_out_loss: 0.0246\n",
      "Epoch 7/100\n",
      "1312/1312 [==============================] - 0s 318us/step - loss: 0.0627 - dense_1_loss: 0.0627 - reward_out_loss: 1.6215e-06 - val_loss: 0.5520 - val_dense_1_loss: 0.5238 - val_reward_out_loss: 0.0283\n",
      "Epoch 8/100\n",
      "1312/1312 [==============================] - 0s 317us/step - loss: 0.0592 - dense_1_loss: 0.0592 - reward_out_loss: 2.4569e-06 - val_loss: 0.5093 - val_dense_1_loss: 0.4750 - val_reward_out_loss: 0.0343\n",
      "Epoch 9/100\n",
      "1312/1312 [==============================] - 0s 324us/step - loss: 0.0614 - dense_1_loss: 0.0614 - reward_out_loss: 1.7567e-06 - val_loss: 0.5394 - val_dense_1_loss: 0.4998 - val_reward_out_loss: 0.0396\n",
      "Epoch 10/100\n",
      "1312/1312 [==============================] - 0s 319us/step - loss: 0.0571 - dense_1_loss: 0.0571 - reward_out_loss: 1.7649e-06 - val_loss: 0.5731 - val_dense_1_loss: 0.5329 - val_reward_out_loss: 0.0402\n",
      "Epoch 11/100\n",
      "1312/1312 [==============================] - 0s 316us/step - loss: 0.0554 - dense_1_loss: 0.0554 - reward_out_loss: 1.4801e-06 - val_loss: 0.5353 - val_dense_1_loss: 0.4987 - val_reward_out_loss: 0.0365\n",
      "Epoch 12/100\n",
      "1312/1312 [==============================] - 0s 328us/step - loss: 0.0534 - dense_1_loss: 0.0534 - reward_out_loss: 1.5493e-06 - val_loss: 0.5292 - val_dense_1_loss: 0.4951 - val_reward_out_loss: 0.0341\n",
      "Epoch 13/100\n",
      "1312/1312 [==============================] - 0s 317us/step - loss: 0.0533 - dense_1_loss: 0.0533 - reward_out_loss: 1.0910e-06 - val_loss: 0.5385 - val_dense_1_loss: 0.5058 - val_reward_out_loss: 0.0326\n",
      "Epoch 00013: early stopping\n",
      "Train on 1322 samples, validate on 147 samples\n",
      "Epoch 1/100\n",
      "1322/1322 [==============================] - 1s 431us/step - loss: 0.0521 - dense_1_loss: 0.0520 - reward_out_loss: 1.9140e-06 - val_loss: 0.5507 - val_dense_1_loss: 0.5191 - val_reward_out_loss: 0.0315\n",
      "Epoch 2/100\n",
      "1322/1322 [==============================] - 0s 320us/step - loss: 0.0520 - dense_1_loss: 0.0520 - reward_out_loss: 8.2605e-07 - val_loss: 0.5409 - val_dense_1_loss: 0.5107 - val_reward_out_loss: 0.0302\n",
      "Epoch 3/100\n",
      "1322/1322 [==============================] - 0s 321us/step - loss: 0.0513 - dense_1_loss: 0.0513 - reward_out_loss: 9.0529e-07 - val_loss: 0.5400 - val_dense_1_loss: 0.5107 - val_reward_out_loss: 0.0293\n",
      "Epoch 4/100\n",
      "1322/1322 [==============================] - 0s 317us/step - loss: 0.0509 - dense_1_loss: 0.0509 - reward_out_loss: 7.5078e-07 - val_loss: 0.5470 - val_dense_1_loss: 0.5179 - val_reward_out_loss: 0.0291\n",
      "Epoch 5/100\n",
      "1322/1322 [==============================] - 0s 317us/step - loss: 0.0506 - dense_1_loss: 0.0506 - reward_out_loss: 5.3131e-07 - val_loss: 0.5403 - val_dense_1_loss: 0.5118 - val_reward_out_loss: 0.0285\n",
      "Epoch 6/100\n",
      "1322/1322 [==============================] - 0s 320us/step - loss: 0.0515 - dense_1_loss: 0.0515 - reward_out_loss: 4.1566e-07 - val_loss: 0.5478 - val_dense_1_loss: 0.5200 - val_reward_out_loss: 0.0278\n",
      "Epoch 7/100\n",
      "1322/1322 [==============================] - 0s 314us/step - loss: 0.0510 - dense_1_loss: 0.0510 - reward_out_loss: 5.6059e-07 - val_loss: 0.5592 - val_dense_1_loss: 0.5321 - val_reward_out_loss: 0.0271\n",
      "Epoch 00007: early stopping\n",
      "Train on 1332 samples, validate on 149 samples\n",
      "Epoch 1/100\n",
      "1332/1332 [==============================] - 1s 442us/step - loss: 0.0486 - dense_1_loss: 0.0486 - reward_out_loss: 6.1377e-07 - val_loss: 0.5552 - val_dense_1_loss: 0.5288 - val_reward_out_loss: 0.0263\n",
      "Epoch 2/100\n",
      "1332/1332 [==============================] - 0s 335us/step - loss: 0.0483 - dense_1_loss: 0.0483 - reward_out_loss: 3.2540e-07 - val_loss: 0.5659 - val_dense_1_loss: 0.5397 - val_reward_out_loss: 0.0262\n",
      "Epoch 3/100\n",
      "1332/1332 [==============================] - 0s 335us/step - loss: 0.0477 - dense_1_loss: 0.0477 - reward_out_loss: 2.6013e-07 - val_loss: 0.5786 - val_dense_1_loss: 0.5523 - val_reward_out_loss: 0.0263\n",
      "Epoch 4/100\n",
      "1332/1332 [==============================] - 0s 329us/step - loss: 0.0501 - dense_1_loss: 0.0501 - reward_out_loss: 5.3496e-07 - val_loss: 0.5877 - val_dense_1_loss: 0.5608 - val_reward_out_loss: 0.0269\n",
      "Epoch 5/100\n",
      "1332/1332 [==============================] - 0s 336us/step - loss: 0.0535 - dense_1_loss: 0.0535 - reward_out_loss: 3.4731e-07 - val_loss: 0.5814 - val_dense_1_loss: 0.5550 - val_reward_out_loss: 0.0264\n",
      "Epoch 6/100\n",
      "1332/1332 [==============================] - 0s 329us/step - loss: 0.0569 - dense_1_loss: 0.0569 - reward_out_loss: 2.7045e-07 - val_loss: 0.5713 - val_dense_1_loss: 0.5448 - val_reward_out_loss: 0.0265\n",
      "Epoch 00006: early stopping\n",
      "Train on 1343 samples, validate on 150 samples\n",
      "Epoch 1/100\n",
      "1343/1343 [==============================] - 1s 433us/step - loss: 0.0526 - dense_1_loss: 0.0526 - reward_out_loss: 1.8119e-07 - val_loss: 0.5881 - val_dense_1_loss: 0.5587 - val_reward_out_loss: 0.0294\n",
      "Epoch 2/100\n",
      "1343/1343 [==============================] - 0s 317us/step - loss: 0.0538 - dense_1_loss: 0.0538 - reward_out_loss: 1.9044e-07 - val_loss: 0.5709 - val_dense_1_loss: 0.5427 - val_reward_out_loss: 0.0282\n",
      "Epoch 3/100\n",
      "1343/1343 [==============================] - 0s 316us/step - loss: 0.0563 - dense_1_loss: 0.0563 - reward_out_loss: 7.9767e-08 - val_loss: 0.5724 - val_dense_1_loss: 0.5433 - val_reward_out_loss: 0.0291\n",
      "Epoch 4/100\n",
      "1343/1343 [==============================] - 0s 311us/step - loss: 0.0542 - dense_1_loss: 0.0542 - reward_out_loss: 2.9305e-07 - val_loss: 0.5765 - val_dense_1_loss: 0.5422 - val_reward_out_loss: 0.0342\n",
      "Epoch 5/100\n",
      "1343/1343 [==============================] - 0s 324us/step - loss: 0.0514 - dense_1_loss: 0.0514 - reward_out_loss: 8.0640e-08 - val_loss: 0.5751 - val_dense_1_loss: 0.5375 - val_reward_out_loss: 0.0376\n",
      "Epoch 6/100\n",
      "1343/1343 [==============================] - 0s 318us/step - loss: 0.0478 - dense_1_loss: 0.0478 - reward_out_loss: 9.5740e-08 - val_loss: 0.5800 - val_dense_1_loss: 0.5408 - val_reward_out_loss: 0.0391\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1343/1343 [==============================] - 0s 323us/step - loss: 0.0492 - dense_1_loss: 0.0492 - reward_out_loss: 1.7155e-07 - val_loss: 0.5942 - val_dense_1_loss: 0.5540 - val_reward_out_loss: 0.0402\n",
      "Epoch 00007: early stopping\n",
      "Train on 1354 samples, validate on 151 samples\n",
      "Epoch 1/100\n",
      "1354/1354 [==============================] - 1s 431us/step - loss: 0.0478 - dense_1_loss: 0.0478 - reward_out_loss: 6.2834e-08 - val_loss: 0.6458 - val_dense_1_loss: 0.6069 - val_reward_out_loss: 0.0389\n",
      "Epoch 2/100\n",
      "1354/1354 [==============================] - 0s 311us/step - loss: 0.0488 - dense_1_loss: 0.0488 - reward_out_loss: 1.1822e-07 - val_loss: 0.6573 - val_dense_1_loss: 0.6201 - val_reward_out_loss: 0.0373\n",
      "Epoch 3/100\n",
      "1354/1354 [==============================] - 0s 318us/step - loss: 0.0467 - dense_1_loss: 0.0467 - reward_out_loss: 1.0312e-07 - val_loss: 0.6636 - val_dense_1_loss: 0.6255 - val_reward_out_loss: 0.0381\n",
      "Epoch 4/100\n",
      "1354/1354 [==============================] - 0s 317us/step - loss: 0.0468 - dense_1_loss: 0.0468 - reward_out_loss: 1.5067e-07 - val_loss: 0.6656 - val_dense_1_loss: 0.6272 - val_reward_out_loss: 0.0385\n",
      "Epoch 5/100\n",
      "1354/1354 [==============================] - 0s 318us/step - loss: 0.0455 - dense_1_loss: 0.0455 - reward_out_loss: 8.1967e-08 - val_loss: 0.6718 - val_dense_1_loss: 0.6338 - val_reward_out_loss: 0.0380\n",
      "Epoch 6/100\n",
      "1354/1354 [==============================] - 0s 321us/step - loss: 0.0461 - dense_1_loss: 0.0461 - reward_out_loss: 1.0165e-07 - val_loss: 0.6778 - val_dense_1_loss: 0.6398 - val_reward_out_loss: 0.0380\n",
      "Epoch 00006: early stopping\n",
      "Train on 1364 samples, validate on 152 samples\n",
      "Epoch 1/100\n",
      "1364/1364 [==============================] - 1s 435us/step - loss: 0.0452 - dense_1_loss: 0.0452 - reward_out_loss: 6.0662e-08 - val_loss: 0.6902 - val_dense_1_loss: 0.6533 - val_reward_out_loss: 0.0369\n",
      "Epoch 2/100\n",
      "1364/1364 [==============================] - 0s 317us/step - loss: 0.0453 - dense_1_loss: 0.0453 - reward_out_loss: 7.2514e-08 - val_loss: 0.6971 - val_dense_1_loss: 0.6611 - val_reward_out_loss: 0.0360\n",
      "Epoch 3/100\n",
      "1364/1364 [==============================] - 0s 322us/step - loss: 0.0459 - dense_1_loss: 0.0459 - reward_out_loss: 7.1822e-08 - val_loss: 0.6973 - val_dense_1_loss: 0.6614 - val_reward_out_loss: 0.0359\n",
      "Epoch 4/100\n",
      "1364/1364 [==============================] - 0s 316us/step - loss: 0.0444 - dense_1_loss: 0.0444 - reward_out_loss: 6.5085e-08 - val_loss: 0.7017 - val_dense_1_loss: 0.6667 - val_reward_out_loss: 0.0350\n",
      "Epoch 5/100\n",
      "1364/1364 [==============================] - 0s 319us/step - loss: 0.0460 - dense_1_loss: 0.0460 - reward_out_loss: 7.4161e-08 - val_loss: 0.7081 - val_dense_1_loss: 0.6734 - val_reward_out_loss: 0.0347\n",
      "Epoch 6/100\n",
      "1364/1364 [==============================] - 0s 316us/step - loss: 0.0451 - dense_1_loss: 0.0451 - reward_out_loss: 5.5180e-08 - val_loss: 0.7175 - val_dense_1_loss: 0.6835 - val_reward_out_loss: 0.0340\n",
      "Epoch 00006: early stopping\n",
      "Train on 1376 samples, validate on 153 samples\n",
      "Epoch 1/100\n",
      "1376/1376 [==============================] - 1s 431us/step - loss: 0.0439 - dense_1_loss: 0.0439 - reward_out_loss: 7.1375e-08 - val_loss: 0.7309 - val_dense_1_loss: 0.6976 - val_reward_out_loss: 0.0333\n",
      "Epoch 2/100\n",
      "1376/1376 [==============================] - 0s 310us/step - loss: 0.0441 - dense_1_loss: 0.0441 - reward_out_loss: 7.0269e-08 - val_loss: 0.7409 - val_dense_1_loss: 0.7085 - val_reward_out_loss: 0.0323\n",
      "Epoch 3/100\n",
      "1376/1376 [==============================] - 0s 316us/step - loss: 0.0442 - dense_1_loss: 0.0442 - reward_out_loss: 4.2289e-08 - val_loss: 0.7435 - val_dense_1_loss: 0.7116 - val_reward_out_loss: 0.0319\n",
      "Epoch 4/100\n",
      "1376/1376 [==============================] - 0s 310us/step - loss: 0.0440 - dense_1_loss: 0.0440 - reward_out_loss: 5.9505e-08 - val_loss: 0.7425 - val_dense_1_loss: 0.7103 - val_reward_out_loss: 0.0322\n",
      "Epoch 5/100\n",
      "1376/1376 [==============================] - 0s 315us/step - loss: 0.0454 - dense_1_loss: 0.0454 - reward_out_loss: 4.0945e-08 - val_loss: 0.7475 - val_dense_1_loss: 0.7158 - val_reward_out_loss: 0.0317\n",
      "Epoch 6/100\n",
      "1376/1376 [==============================] - 0s 310us/step - loss: 0.0465 - dense_1_loss: 0.0465 - reward_out_loss: 3.1855e-08 - val_loss: 0.7432 - val_dense_1_loss: 0.7115 - val_reward_out_loss: 0.0317\n",
      "Epoch 00006: early stopping\n",
      "Train on 1386 samples, validate on 154 samples\n",
      "Epoch 1/100\n",
      "1386/1386 [==============================] - 1s 449us/step - loss: 0.0684 - dense_1_loss: 0.0676 - reward_out_loss: 7.8101e-04 - val_loss: 0.5489 - val_dense_1_loss: 0.5233 - val_reward_out_loss: 0.0256\n",
      "Epoch 2/100\n",
      "1386/1386 [==============================] - 0s 319us/step - loss: 0.0631 - dense_1_loss: 0.0631 - reward_out_loss: 1.5837e-07 - val_loss: 0.5680 - val_dense_1_loss: 0.5370 - val_reward_out_loss: 0.0310\n",
      "Epoch 3/100\n",
      "1386/1386 [==============================] - 0s 326us/step - loss: 0.0515 - dense_1_loss: 0.0515 - reward_out_loss: 2.8496e-07 - val_loss: 0.5748 - val_dense_1_loss: 0.5377 - val_reward_out_loss: 0.0371\n",
      "Epoch 4/100\n",
      "1386/1386 [==============================] - 0s 321us/step - loss: 0.0537 - dense_1_loss: 0.0537 - reward_out_loss: 3.5868e-07 - val_loss: 0.5846 - val_dense_1_loss: 0.5387 - val_reward_out_loss: 0.0459\n",
      "Epoch 5/100\n",
      "1386/1386 [==============================] - 0s 312us/step - loss: 0.0515 - dense_1_loss: 0.0515 - reward_out_loss: 8.3030e-07 - val_loss: 0.6075 - val_dense_1_loss: 0.5554 - val_reward_out_loss: 0.0521\n",
      "Epoch 6/100\n",
      "1386/1386 [==============================] - 0s 331us/step - loss: 0.0522 - dense_1_loss: 0.0522 - reward_out_loss: 4.6707e-07 - val_loss: 0.6124 - val_dense_1_loss: 0.5563 - val_reward_out_loss: 0.0561\n",
      "Epoch 00006: early stopping\n",
      "Train on 1395 samples, validate on 156 samples\n",
      "Epoch 1/100\n",
      "1395/1395 [==============================] - 0s 334us/step - loss: 0.0574 - dense_1_loss: 0.0574 - reward_out_loss: 1.5127e-05 - val_loss: 0.5198 - val_dense_1_loss: 0.4633 - val_reward_out_loss: 0.0566\n",
      "Epoch 2/100\n",
      "1395/1395 [==============================] - 0s 319us/step - loss: 0.0510 - dense_1_loss: 0.0509 - reward_out_loss: 4.7337e-05 - val_loss: 0.4953 - val_dense_1_loss: 0.4434 - val_reward_out_loss: 0.0519\n",
      "Epoch 3/100\n",
      "1395/1395 [==============================] - 0s 313us/step - loss: 0.0512 - dense_1_loss: 0.0511 - reward_out_loss: 2.5338e-05 - val_loss: 0.4659 - val_dense_1_loss: 0.4199 - val_reward_out_loss: 0.0461\n",
      "Epoch 4/100\n",
      "1395/1395 [==============================] - 0s 319us/step - loss: 0.0485 - dense_1_loss: 0.0485 - reward_out_loss: 1.3500e-05 - val_loss: 0.4469 - val_dense_1_loss: 0.4086 - val_reward_out_loss: 0.0383\n",
      "Epoch 5/100\n",
      "1395/1395 [==============================] - 0s 328us/step - loss: 0.0498 - dense_1_loss: 0.0498 - reward_out_loss: 9.8311e-06 - val_loss: 0.4577 - val_dense_1_loss: 0.4256 - val_reward_out_loss: 0.0320\n",
      "Epoch 6/100\n",
      "1395/1395 [==============================] - 0s 317us/step - loss: 0.0490 - dense_1_loss: 0.0490 - reward_out_loss: 5.1537e-06 - val_loss: 0.4853 - val_dense_1_loss: 0.4586 - val_reward_out_loss: 0.0266\n",
      "Epoch 7/100\n",
      "1395/1395 [==============================] - 0s 318us/step - loss: 0.0485 - dense_1_loss: 0.0485 - reward_out_loss: 2.1129e-06 - val_loss: 0.5121 - val_dense_1_loss: 0.4897 - val_reward_out_loss: 0.0224\n",
      "Epoch 8/100\n",
      "1395/1395 [==============================] - 0s 315us/step - loss: 0.0484 - dense_1_loss: 0.0484 - reward_out_loss: 1.5654e-06 - val_loss: 0.5223 - val_dense_1_loss: 0.5028 - val_reward_out_loss: 0.0196\n",
      "Epoch 9/100\n",
      "1395/1395 [==============================] - 0s 324us/step - loss: 0.0495 - dense_1_loss: 0.0495 - reward_out_loss: 6.2246e-07 - val_loss: 0.5311 - val_dense_1_loss: 0.5127 - val_reward_out_loss: 0.0184\n",
      "Epoch 00009: early stopping\n",
      "Train on 1407 samples, validate on 157 samples\n",
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 1s 447us/step - loss: 0.0661 - dense_1_loss: 0.0661 - reward_out_loss: 1.5749e-06 - val_loss: 0.3628 - val_dense_1_loss: 0.3427 - val_reward_out_loss: 0.0200\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 0s 319us/step - loss: 0.0565 - dense_1_loss: 0.0565 - reward_out_loss: 2.2476e-06 - val_loss: 0.3381 - val_dense_1_loss: 0.3192 - val_reward_out_loss: 0.0189\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 0s 316us/step - loss: 0.0523 - dense_1_loss: 0.0523 - reward_out_loss: 7.9206e-07 - val_loss: 0.3336 - val_dense_1_loss: 0.3156 - val_reward_out_loss: 0.0180\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 0s 319us/step - loss: 0.0501 - dense_1_loss: 0.0501 - reward_out_loss: 6.1234e-07 - val_loss: 0.3155 - val_dense_1_loss: 0.2973 - val_reward_out_loss: 0.0182\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 0s 316us/step - loss: 0.0479 - dense_1_loss: 0.0479 - reward_out_loss: 3.8142e-07 - val_loss: 0.3152 - val_dense_1_loss: 0.2983 - val_reward_out_loss: 0.0169\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 0s 322us/step - loss: 0.0470 - dense_1_loss: 0.0470 - reward_out_loss: 2.8465e-07 - val_loss: 0.3231 - val_dense_1_loss: 0.3073 - val_reward_out_loss: 0.0158\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 0s 319us/step - loss: 0.0483 - dense_1_loss: 0.0483 - reward_out_loss: 3.2555e-07 - val_loss: 0.3316 - val_dense_1_loss: 0.3167 - val_reward_out_loss: 0.0149\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 0s 308us/step - loss: 0.0490 - dense_1_loss: 0.0490 - reward_out_loss: 3.1743e-07 - val_loss: 0.3434 - val_dense_1_loss: 0.3284 - val_reward_out_loss: 0.0151\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 0s 308us/step - loss: 0.0465 - dense_1_loss: 0.0465 - reward_out_loss: 2.4855e-07 - val_loss: 0.3536 - val_dense_1_loss: 0.3396 - val_reward_out_loss: 0.0140\n",
      "Epoch 00009: early stopping\n",
      "Train on 1417 samples, validate on 158 samples\n",
      "Epoch 1/100\n",
      "1417/1417 [==============================] - 1s 447us/step - loss: 0.0515 - dense_1_loss: 0.0506 - reward_out_loss: 8.8329e-04 - val_loss: 0.2980 - val_dense_1_loss: 0.2976 - val_reward_out_loss: 4.5381e-04\n",
      "Epoch 2/100\n",
      "1417/1417 [==============================] - 0s 317us/step - loss: 0.0475 - dense_1_loss: 0.0475 - reward_out_loss: 4.7049e-07 - val_loss: 0.2703 - val_dense_1_loss: 0.2703 - val_reward_out_loss: 3.3462e-05\n",
      "Epoch 3/100\n",
      "1417/1417 [==============================] - 0s 311us/step - loss: 0.0475 - dense_1_loss: 0.0475 - reward_out_loss: 3.2641e-08 - val_loss: 0.2666 - val_dense_1_loss: 0.2666 - val_reward_out_loss: 4.8803e-06\n",
      "Epoch 4/100\n",
      "1417/1417 [==============================] - 0s 324us/step - loss: 0.0493 - dense_1_loss: 0.0493 - reward_out_loss: 1.2154e-08 - val_loss: 0.2686 - val_dense_1_loss: 0.2686 - val_reward_out_loss: 1.2991e-06\n",
      "Epoch 5/100\n",
      "1417/1417 [==============================] - 0s 314us/step - loss: 0.0494 - dense_1_loss: 0.0494 - reward_out_loss: 1.0020e-08 - val_loss: 0.2726 - val_dense_1_loss: 0.2726 - val_reward_out_loss: 5.3177e-07\n",
      "Epoch 6/100\n",
      "1417/1417 [==============================] - 0s 322us/step - loss: 0.0451 - dense_1_loss: 0.0451 - reward_out_loss: 2.1533e-08 - val_loss: 0.2744 - val_dense_1_loss: 0.2744 - val_reward_out_loss: 2.7089e-07\n",
      "Epoch 7/100\n",
      "1417/1417 [==============================] - 0s 313us/step - loss: 0.0473 - dense_1_loss: 0.0473 - reward_out_loss: 4.9264e-08 - val_loss: 0.2787 - val_dense_1_loss: 0.2787 - val_reward_out_loss: 1.7631e-07\n",
      "Epoch 8/100\n",
      "1417/1417 [==============================] - 0s 318us/step - loss: 0.0474 - dense_1_loss: 0.0474 - reward_out_loss: 3.8228e-08 - val_loss: 0.2814 - val_dense_1_loss: 0.2814 - val_reward_out_loss: 1.2525e-07\n",
      "Epoch 00008: early stopping\n",
      "Train on 1430 samples, validate on 159 samples\n",
      "Epoch 1/100\n",
      "1430/1430 [==============================] - 1s 449us/step - loss: 0.0516 - dense_1_loss: 0.0516 - reward_out_loss: 6.3022e-08 - val_loss: 0.3222 - val_dense_1_loss: 0.3222 - val_reward_out_loss: 1.1679e-07\n",
      "Epoch 2/100\n",
      "1430/1430 [==============================] - 0s 322us/step - loss: 0.0474 - dense_1_loss: 0.0474 - reward_out_loss: 4.7471e-08 - val_loss: 0.3516 - val_dense_1_loss: 0.3516 - val_reward_out_loss: 1.1612e-07\n",
      "Epoch 3/100\n",
      "1430/1430 [==============================] - 0s 321us/step - loss: 0.0456 - dense_1_loss: 0.0456 - reward_out_loss: 7.9212e-08 - val_loss: 0.3656 - val_dense_1_loss: 0.3656 - val_reward_out_loss: 1.1335e-07\n",
      "Epoch 4/100\n",
      "1430/1430 [==============================] - 0s 319us/step - loss: 0.0475 - dense_1_loss: 0.0475 - reward_out_loss: 1.2472e-07 - val_loss: 0.3743 - val_dense_1_loss: 0.3743 - val_reward_out_loss: 1.0697e-07\n",
      "Epoch 5/100\n",
      "1430/1430 [==============================] - 0s 322us/step - loss: 0.0455 - dense_1_loss: 0.0455 - reward_out_loss: 8.9393e-08 - val_loss: 0.3735 - val_dense_1_loss: 0.3735 - val_reward_out_loss: 1.0159e-07\n",
      "Epoch 6/100\n",
      "1430/1430 [==============================] - 0s 324us/step - loss: 0.0470 - dense_1_loss: 0.0470 - reward_out_loss: 7.4254e-08 - val_loss: 0.3745 - val_dense_1_loss: 0.3745 - val_reward_out_loss: 9.2031e-08\n",
      "Epoch 00006: early stopping\n",
      "Train on 1440 samples, validate on 160 samples\n",
      "Epoch 1/100\n",
      "1440/1440 [==============================] - 1s 452us/step - loss: 0.0512 - dense_1_loss: 0.0512 - reward_out_loss: 1.3099e-07 - val_loss: 0.3887 - val_dense_1_loss: 0.3887 - val_reward_out_loss: 4.0382e-08\n",
      "Epoch 2/100\n",
      "1440/1440 [==============================] - 0s 323us/step - loss: 0.0459 - dense_1_loss: 0.0459 - reward_out_loss: 1.3935e-07 - val_loss: 0.4219 - val_dense_1_loss: 0.4219 - val_reward_out_loss: 4.1114e-08\n",
      "Epoch 3/100\n",
      "1440/1440 [==============================] - 0s 321us/step - loss: 0.0471 - dense_1_loss: 0.0471 - reward_out_loss: 1.1926e-07 - val_loss: 0.4508 - val_dense_1_loss: 0.4508 - val_reward_out_loss: 4.0677e-08\n",
      "Epoch 4/100\n",
      "1440/1440 [==============================] - 0s 325us/step - loss: 0.0469 - dense_1_loss: 0.0469 - reward_out_loss: 1.5998e-07 - val_loss: 0.4978 - val_dense_1_loss: 0.4978 - val_reward_out_loss: 3.9433e-08\n",
      "Epoch 5/100\n",
      "1440/1440 [==============================] - 0s 324us/step - loss: 0.0459 - dense_1_loss: 0.0459 - reward_out_loss: 1.6978e-07 - val_loss: 0.5621 - val_dense_1_loss: 0.5621 - val_reward_out_loss: 3.3271e-08\n",
      "Epoch 6/100\n",
      "1440/1440 [==============================] - 0s 321us/step - loss: 0.0445 - dense_1_loss: 0.0445 - reward_out_loss: 1.1836e-07 - val_loss: 0.6063 - val_dense_1_loss: 0.6063 - val_reward_out_loss: 2.9559e-08\n",
      "Epoch 00006: early stopping\n",
      "Train on 1451 samples, validate on 162 samples\n",
      "Epoch 1/100\n",
      "1451/1451 [==============================] - 1s 467us/step - loss: 0.0532 - dense_1_loss: 0.0532 - reward_out_loss: 1.1497e-07 - val_loss: 0.4713 - val_dense_1_loss: 0.4713 - val_reward_out_loss: 3.2523e-08\n",
      "Epoch 2/100\n",
      "1451/1451 [==============================] - 0s 320us/step - loss: 0.0472 - dense_1_loss: 0.0472 - reward_out_loss: 1.7074e-07 - val_loss: 0.4136 - val_dense_1_loss: 0.4136 - val_reward_out_loss: 4.0450e-08\n",
      "Epoch 3/100\n",
      "1451/1451 [==============================] - 0s 324us/step - loss: 0.0434 - dense_1_loss: 0.0434 - reward_out_loss: 1.3328e-07 - val_loss: 0.4198 - val_dense_1_loss: 0.4198 - val_reward_out_loss: 4.6667e-08\n",
      "Epoch 4/100\n",
      "1451/1451 [==============================] - 0s 324us/step - loss: 0.0461 - dense_1_loss: 0.0461 - reward_out_loss: 8.0579e-08 - val_loss: 0.4189 - val_dense_1_loss: 0.4189 - val_reward_out_loss: 4.6385e-08\n",
      "Epoch 5/100\n",
      "1451/1451 [==============================] - 0s 324us/step - loss: 0.0444 - dense_1_loss: 0.0444 - reward_out_loss: 2.0886e-07 - val_loss: 0.4171 - val_dense_1_loss: 0.4171 - val_reward_out_loss: 4.7731e-08\n",
      "Epoch 6/100\n",
      "1451/1451 [==============================] - 0s 319us/step - loss: 0.0427 - dense_1_loss: 0.0427 - reward_out_loss: 1.0528e-07 - val_loss: 0.4212 - val_dense_1_loss: 0.4212 - val_reward_out_loss: 5.0306e-08\n",
      "Epoch 7/100\n",
      "1451/1451 [==============================] - 0s 321us/step - loss: 0.0448 - dense_1_loss: 0.0448 - reward_out_loss: 1.2444e-07 - val_loss: 0.4198 - val_dense_1_loss: 0.4198 - val_reward_out_loss: 4.7946e-08\n",
      "Epoch 00007: early stopping\n"
     ]
    }
   ],
   "source": [
    "! python mcts_agent5.py go_3blocks_1 --logdir mcts --load_model ./dagger/model/il_go_3res_block/model.h4 --num_episodes 1000000 --save_interval 100 --num_runners 12 --batch_size 512 --mcts_iters 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
