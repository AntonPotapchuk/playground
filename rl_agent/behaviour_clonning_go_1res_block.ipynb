{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/hpchome/anton95/.conda/envs/pommerman/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/gpfs/hpchome/anton95/.conda/envs/pommerman/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import error! You will not be able to render --> Cannot connect to \"None\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger, TensorBoard\n",
    "from keras.layers import Input, Dense, Flatten, Convolution2D, BatchNormalization, Activation, Add\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "from pommerman.constants import BOARD_SIZE\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2048\n",
    "epochs = 150\n",
    "early_stopping = 5\n",
    "action_space = 6\n",
    "\n",
    "log_path = './supervised_learning/logs/go_1res_block/'\n",
    "model_path = './supervised_learning/model/go_1res_block/model.h4'\n",
    "\n",
    "train_data_path    = './dataset/'\n",
    "train_data_labels  = os.path.join(train_data_path, 'labels.npy')\n",
    "train_data_reward  = os.path.join(train_data_path, 'reward.npy')\n",
    "train_data_obs_map = os.path.join(train_data_path, 'obs_map.npy')\n",
    "\n",
    "if not os.path.isdir(train_data_path):\n",
    "    os.makedirs(train_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, actions, save_path, log_path, save_best_only=True, seed=0):\n",
    "        K.clear_session()\n",
    "        self.log_path = log_path\n",
    "        self.save_path = save_path\n",
    "        self.actions = actions\n",
    "        self.save_best_only = save_best_only\n",
    "        \n",
    "        # Create model\n",
    "        self.model = self.create_model(actions)\n",
    "        # Load model if exists\n",
    "        if not os.path.isdir(os.path.dirname(save_path)):\n",
    "            os.makedirs(os.path.dirname(save_path))            \n",
    "        if os.path.isfile(self.save_path):\n",
    "            try:\n",
    "                print(\"Trying to load model\")\n",
    "                self.model.load_weights(self.save_path)\n",
    "                print(\"Model was loaded successful\")\n",
    "            except:\n",
    "                print(\"Model load failed\")\n",
    "        \n",
    "    def get_res_block(self, input):\n",
    "        # Res block 1        \n",
    "        x = Convolution2D(256, 3, padding='same')(input)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Convolution2D(256, 3, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Add()([input, x])\n",
    "        x = Activation('relu')(x)\n",
    "        return x\n",
    "        \n",
    "    def create_model(self, actions, input_shape=(11, 11, 18,)):\n",
    "        inp = Input(input_shape)\n",
    "        x = Convolution2D(256, 3, padding='same')(inp)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        \n",
    "        # 1 residual blocks\n",
    "        for i in range(1):\n",
    "            x = self.get_res_block(x)\n",
    "        \n",
    "        # Output block\n",
    "        # Should be 2 filters\n",
    "        x = Convolution2D(2, 1, padding='same')(x)\n",
    "        x = BatchNormalization()(x)   \n",
    "        x = Activation('relu')(x)\n",
    "        x = Flatten()(x)  \n",
    "        \n",
    "        probs  = Dense(actions, activation='softmax', name='actions')(x)\n",
    "        reward = Dense(1, name='reward')(x)\n",
    "        \n",
    "        model = Model(inputs = inp, outputs=[probs, reward])\n",
    "        model.compile(optimizer='adam', loss=['categorical_crossentropy', 'mae'], metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def train(self, obs, actions, rewards, batch_size=16384, epochs=100,\n",
    "              early_stopping = 10, class_weight=None, initial_epoch=0):\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='loss', patience=early_stopping)\n",
    "        checkpoint     = ModelCheckpoint(self.save_path, monitor='loss', save_best_only=self.save_best_only)\n",
    "        reduce_lr      = ReduceLROnPlateau(monitor='loss', patience=2, factor=0.1)\n",
    "        logger         = CSVLogger(self.log_path + 'log.csv', append=True)\n",
    "        tensorboard    = TensorBoard(self.log_path, batch_size=batch_size)\n",
    "        \n",
    "        history = self.model.fit(x=obs, y=[actions, rewards], batch_size=batch_size, epochs=epochs, verbose=1,\n",
    "                       callbacks=[early_stopping, checkpoint, reduce_lr, logger, tensorboard],\n",
    "                       validation_split=0.15, shuffle=True, class_weight=class_weight, initial_epoch=initial_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels       = np.load(train_data_labels)\n",
    "observations = np.load(train_data_obs_map)\n",
    "rewards      = np.load(train_data_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = to_categorical(labels, num_classes=action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((553065, 6), (553065, 11, 11, 18), (553065,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape, observations.shape, rewards.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.15941164, 0.20397964, 0.190945  , 0.20009764, 0.20339382,\n",
       "       0.04217226], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(labels, axis=0) / np.sum(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.04551126, 0.81707501, 0.87285166, 0.83292671, 0.81942839,\n",
       "       3.9520451 ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights = compute_class_weight('balanced', np.unique(np.argmax(labels, axis=1)), np.argmax(labels, axis=1))\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(action_space, model_path, log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 11, 11, 18)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 11, 11, 256)  41728       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 11, 11, 256)  1024        conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 11, 11, 256)  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 11, 11, 256)  590080      activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 11, 11, 256)  1024        conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 11, 11, 256)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 11, 11, 256)  590080      activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 11, 11, 256)  1024        conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 11, 11, 256)  0           activation_1[0][0]               \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 11, 11, 256)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 11, 11, 2)    514         activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 11, 11, 2)    8           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 11, 11, 2)    0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 242)          0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "actions (Dense)                 (None, 6)            1458        flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reward (Dense)                  (None, 1)            243         flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,227,183\n",
      "Trainable params: 1,225,643\n",
      "Non-trainable params: 1,540\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "trainer.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 470105 samples, validate on 82960 samples\n",
      "Epoch 1/150\n",
      "470105/470105 [==============================] - 62s 132us/step - loss: 2.1853 - actions_loss: 1.6554 - reward_loss: 0.5299 - actions_acc: 0.2554 - reward_acc: 0.7359 - val_loss: 2.1835 - val_actions_loss: 1.6368 - val_reward_loss: 0.5467 - val_actions_acc: 0.2584 - val_reward_acc: 0.7480\n",
      "Epoch 2/150\n",
      "470105/470105 [==============================] - 60s 128us/step - loss: 2.1107 - actions_loss: 1.6064 - reward_loss: 0.5043 - actions_acc: 0.2741 - reward_acc: 0.7544 - val_loss: 2.1451 - val_actions_loss: 1.5904 - val_reward_loss: 0.5547 - val_actions_acc: 0.2901 - val_reward_acc: 0.7480\n",
      "Epoch 3/150\n",
      "470105/470105 [==============================] - 78s 165us/step - loss: 1.9254 - actions_loss: 1.4462 - reward_loss: 0.4792 - actions_acc: 0.3586 - reward_acc: 0.7462 - val_loss: 2.3751 - val_actions_loss: 1.6297 - val_reward_loss: 0.7454 - val_actions_acc: 0.3199 - val_reward_acc: 0.4199\n",
      "Epoch 4/150\n",
      "470105/470105 [==============================] - 100s 212us/step - loss: 1.5811 - actions_loss: 1.2094 - reward_loss: 0.3717 - actions_acc: 0.4539 - reward_acc: 0.7949 - val_loss: 1.9788 - val_actions_loss: 1.2088 - val_reward_loss: 0.7700 - val_actions_acc: 0.4619 - val_reward_acc: 0.4078\n",
      "Epoch 5/150\n",
      "470105/470105 [==============================] - 108s 230us/step - loss: 1.3922 - actions_loss: 1.0782 - reward_loss: 0.3140 - actions_acc: 0.5173 - reward_acc: 0.8445 - val_loss: 2.0265 - val_actions_loss: 1.2227 - val_reward_loss: 0.8039 - val_actions_acc: 0.4666 - val_reward_acc: 0.3891\n",
      "Epoch 6/150\n",
      "470105/470105 [==============================] - 103s 219us/step - loss: 1.2771 - actions_loss: 0.9945 - reward_loss: 0.2826 - actions_acc: 0.5561 - reward_acc: 0.8698 - val_loss: 1.8189 - val_actions_loss: 1.0394 - val_reward_loss: 0.7795 - val_actions_acc: 0.5341 - val_reward_acc: 0.3615\n",
      "Epoch 7/150\n",
      "470105/470105 [==============================] - 110s 235us/step - loss: 1.1934 - actions_loss: 0.9355 - reward_loss: 0.2579 - actions_acc: 0.5802 - reward_acc: 0.8868 - val_loss: 1.7326 - val_actions_loss: 0.9937 - val_reward_loss: 0.7389 - val_actions_acc: 0.5594 - val_reward_acc: 0.4630\n",
      "Epoch 8/150\n",
      "470105/470105 [==============================] - 112s 238us/step - loss: 1.1328 - actions_loss: 0.8908 - reward_loss: 0.2419 - actions_acc: 0.5983 - reward_acc: 0.8980 - val_loss: 1.8848 - val_actions_loss: 1.0854 - val_reward_loss: 0.7994 - val_actions_acc: 0.5447 - val_reward_acc: 0.3636\n",
      "Epoch 9/150\n",
      "470105/470105 [==============================] - 109s 232us/step - loss: 1.0862 - actions_loss: 0.8587 - reward_loss: 0.2275 - actions_acc: 0.6129 - reward_acc: 0.9072 - val_loss: 1.9246 - val_actions_loss: 1.1421 - val_reward_loss: 0.7825 - val_actions_acc: 0.5390 - val_reward_acc: 0.3976\n",
      "Epoch 10/150\n",
      "470105/470105 [==============================] - 111s 236us/step - loss: 1.0472 - actions_loss: 0.8314 - reward_loss: 0.2158 - actions_acc: 0.6238 - reward_acc: 0.9143 - val_loss: 1.7553 - val_actions_loss: 0.9822 - val_reward_loss: 0.7731 - val_actions_acc: 0.5655 - val_reward_acc: 0.3836\n",
      "Epoch 11/150\n",
      "470105/470105 [==============================] - 108s 231us/step - loss: 1.0177 - actions_loss: 0.8113 - reward_loss: 0.2064 - actions_acc: 0.6318 - reward_acc: 0.9202 - val_loss: 1.7141 - val_actions_loss: 0.9319 - val_reward_loss: 0.7822 - val_actions_acc: 0.5850 - val_reward_acc: 0.3774\n",
      "Epoch 12/150\n",
      "470105/470105 [==============================] - 112s 239us/step - loss: 0.9893 - actions_loss: 0.7922 - reward_loss: 0.1971 - actions_acc: 0.6398 - reward_acc: 0.9256 - val_loss: 1.7481 - val_actions_loss: 0.9709 - val_reward_loss: 0.7772 - val_actions_acc: 0.5751 - val_reward_acc: 0.4145\n",
      "Epoch 13/150\n",
      "470105/470105 [==============================] - 109s 231us/step - loss: 0.9668 - actions_loss: 0.7759 - reward_loss: 0.1909 - actions_acc: 0.6472 - reward_acc: 0.9300 - val_loss: 1.7126 - val_actions_loss: 0.9376 - val_reward_loss: 0.7750 - val_actions_acc: 0.5846 - val_reward_acc: 0.4056\n",
      "Epoch 14/150\n",
      "470105/470105 [==============================] - 108s 231us/step - loss: 0.9468 - actions_loss: 0.7623 - reward_loss: 0.1845 - actions_acc: 0.6533 - reward_acc: 0.9336 - val_loss: 1.6895 - val_actions_loss: 0.9173 - val_reward_loss: 0.7722 - val_actions_acc: 0.5888 - val_reward_acc: 0.4005\n",
      "Epoch 15/150\n",
      "470105/470105 [==============================] - 110s 234us/step - loss: 0.9298 - actions_loss: 0.7490 - reward_loss: 0.1807 - actions_acc: 0.6601 - reward_acc: 0.9367 - val_loss: 1.7348 - val_actions_loss: 0.9707 - val_reward_loss: 0.7641 - val_actions_acc: 0.5705 - val_reward_acc: 0.4159\n",
      "Epoch 16/150\n",
      "470105/470105 [==============================] - 111s 236us/step - loss: 0.9120 - actions_loss: 0.7377 - reward_loss: 0.1743 - actions_acc: 0.6655 - reward_acc: 0.9401 - val_loss: 1.6980 - val_actions_loss: 0.9275 - val_reward_loss: 0.7705 - val_actions_acc: 0.5870 - val_reward_acc: 0.4082\n",
      "Epoch 17/150\n",
      "470105/470105 [==============================] - 111s 236us/step - loss: 0.8957 - actions_loss: 0.7253 - reward_loss: 0.1704 - actions_acc: 0.6715 - reward_acc: 0.9426 - val_loss: 1.7050 - val_actions_loss: 0.9302 - val_reward_loss: 0.7748 - val_actions_acc: 0.5846 - val_reward_acc: 0.3896\n",
      "Epoch 18/150\n",
      "470105/470105 [==============================] - 111s 236us/step - loss: 0.8820 - actions_loss: 0.7154 - reward_loss: 0.1667 - actions_acc: 0.6768 - reward_acc: 0.9451 - val_loss: 1.6819 - val_actions_loss: 0.9245 - val_reward_loss: 0.7574 - val_actions_acc: 0.5898 - val_reward_acc: 0.4354\n",
      "Epoch 19/150\n",
      "470105/470105 [==============================] - 112s 239us/step - loss: 0.8650 - actions_loss: 0.7027 - reward_loss: 0.1623 - actions_acc: 0.6830 - reward_acc: 0.9472 - val_loss: 1.6955 - val_actions_loss: 0.9154 - val_reward_loss: 0.7801 - val_actions_acc: 0.5943 - val_reward_acc: 0.3809\n",
      "Epoch 20/150\n",
      "470105/470105 [==============================] - 111s 237us/step - loss: 0.8533 - actions_loss: 0.6933 - reward_loss: 0.1600 - actions_acc: 0.6875 - reward_acc: 0.9492 - val_loss: 1.7043 - val_actions_loss: 0.9313 - val_reward_loss: 0.7729 - val_actions_acc: 0.5954 - val_reward_acc: 0.4017\n",
      "Epoch 21/150\n",
      "470105/470105 [==============================] - 114s 242us/step - loss: 0.8427 - actions_loss: 0.6853 - reward_loss: 0.1573 - actions_acc: 0.6923 - reward_acc: 0.9515 - val_loss: 1.7839 - val_actions_loss: 1.0051 - val_reward_loss: 0.7788 - val_actions_acc: 0.5746 - val_reward_acc: 0.4012\n",
      "Epoch 22/150\n",
      "470105/470105 [==============================] - 111s 236us/step - loss: 0.8238 - actions_loss: 0.6705 - reward_loss: 0.1533 - actions_acc: 0.7001 - reward_acc: 0.9536 - val_loss: 1.7101 - val_actions_loss: 0.9334 - val_reward_loss: 0.7767 - val_actions_acc: 0.5892 - val_reward_acc: 0.3799\n",
      "Epoch 23/150\n",
      "470105/470105 [==============================] - 109s 233us/step - loss: 0.8150 - actions_loss: 0.6638 - reward_loss: 0.1512 - actions_acc: 0.7027 - reward_acc: 0.9549 - val_loss: 1.7706 - val_actions_loss: 0.9948 - val_reward_loss: 0.7758 - val_actions_acc: 0.5825 - val_reward_acc: 0.4014\n",
      "Epoch 24/150\n",
      "470105/470105 [==============================] - 109s 233us/step - loss: 0.8051 - actions_loss: 0.6556 - reward_loss: 0.1495 - actions_acc: 0.7074 - reward_acc: 0.9566 - val_loss: 1.7201 - val_actions_loss: 0.9513 - val_reward_loss: 0.7688 - val_actions_acc: 0.5902 - val_reward_acc: 0.4154\n",
      "Epoch 25/150\n",
      "470105/470105 [==============================] - 111s 237us/step - loss: 0.7935 - actions_loss: 0.6465 - reward_loss: 0.1470 - actions_acc: 0.7119 - reward_acc: 0.9579 - val_loss: 1.7588 - val_actions_loss: 0.9668 - val_reward_loss: 0.7919 - val_actions_acc: 0.5910 - val_reward_acc: 0.3631\n",
      "Epoch 26/150\n",
      "470105/470105 [==============================] - 110s 233us/step - loss: 0.7797 - actions_loss: 0.6347 - reward_loss: 0.1450 - actions_acc: 0.7175 - reward_acc: 0.9594 - val_loss: 1.7910 - val_actions_loss: 1.0250 - val_reward_loss: 0.7660 - val_actions_acc: 0.5758 - val_reward_acc: 0.4071\n",
      "Epoch 27/150\n",
      "470105/470105 [==============================] - 113s 240us/step - loss: 0.7714 - actions_loss: 0.6275 - reward_loss: 0.1439 - actions_acc: 0.7218 - reward_acc: 0.9602 - val_loss: 1.7661 - val_actions_loss: 0.9948 - val_reward_loss: 0.7713 - val_actions_acc: 0.5855 - val_reward_acc: 0.3974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/150\n",
      "470105/470105 [==============================] - 111s 237us/step - loss: 0.7590 - actions_loss: 0.6167 - reward_loss: 0.1423 - actions_acc: 0.7276 - reward_acc: 0.9614 - val_loss: 1.8003 - val_actions_loss: 1.0284 - val_reward_loss: 0.7719 - val_actions_acc: 0.5852 - val_reward_acc: 0.4084\n",
      "Epoch 29/150\n",
      "470105/470105 [==============================] - 111s 235us/step - loss: 0.7475 - actions_loss: 0.6071 - reward_loss: 0.1404 - actions_acc: 0.7325 - reward_acc: 0.9626 - val_loss: 1.7904 - val_actions_loss: 1.0131 - val_reward_loss: 0.7773 - val_actions_acc: 0.5848 - val_reward_acc: 0.3863\n",
      "Epoch 30/150\n",
      "470105/470105 [==============================] - 110s 234us/step - loss: 0.7375 - actions_loss: 0.5981 - reward_loss: 0.1395 - actions_acc: 0.7359 - reward_acc: 0.9636 - val_loss: 1.8055 - val_actions_loss: 1.0401 - val_reward_loss: 0.7654 - val_actions_acc: 0.5812 - val_reward_acc: 0.4109\n",
      "Epoch 31/150\n",
      "470105/470105 [==============================] - 112s 237us/step - loss: 0.7296 - actions_loss: 0.5913 - reward_loss: 0.1382 - actions_acc: 0.7397 - reward_acc: 0.9645 - val_loss: 1.7747 - val_actions_loss: 1.0154 - val_reward_loss: 0.7594 - val_actions_acc: 0.5856 - val_reward_acc: 0.4093\n",
      "Epoch 32/150\n",
      "470105/470105 [==============================] - 109s 232us/step - loss: 0.7208 - actions_loss: 0.5832 - reward_loss: 0.1376 - actions_acc: 0.7428 - reward_acc: 0.9651 - val_loss: 1.8573 - val_actions_loss: 1.0894 - val_reward_loss: 0.7679 - val_actions_acc: 0.5730 - val_reward_acc: 0.4006\n",
      "Epoch 33/150\n",
      "470105/470105 [==============================] - 102s 218us/step - loss: 0.7069 - actions_loss: 0.5715 - reward_loss: 0.1354 - actions_acc: 0.7494 - reward_acc: 0.9660 - val_loss: 1.8484 - val_actions_loss: 1.0934 - val_reward_loss: 0.7551 - val_actions_acc: 0.5736 - val_reward_acc: 0.4262\n",
      "Epoch 34/150\n",
      "470105/470105 [==============================] - 111s 237us/step - loss: 0.6979 - actions_loss: 0.5637 - reward_loss: 0.1342 - actions_acc: 0.7534 - reward_acc: 0.9665 - val_loss: 1.8538 - val_actions_loss: 1.0976 - val_reward_loss: 0.7562 - val_actions_acc: 0.5770 - val_reward_acc: 0.4195\n",
      "Epoch 35/150\n",
      "470105/470105 [==============================] - 110s 233us/step - loss: 0.6895 - actions_loss: 0.5562 - reward_loss: 0.1332 - actions_acc: 0.7572 - reward_acc: 0.9671 - val_loss: 1.9697 - val_actions_loss: 1.1876 - val_reward_loss: 0.7821 - val_actions_acc: 0.5686 - val_reward_acc: 0.3906\n",
      "Epoch 36/150\n",
      "470105/470105 [==============================] - 109s 232us/step - loss: 0.6814 - actions_loss: 0.5488 - reward_loss: 0.1326 - actions_acc: 0.7616 - reward_acc: 0.9679 - val_loss: 1.9210 - val_actions_loss: 1.1439 - val_reward_loss: 0.7771 - val_actions_acc: 0.5757 - val_reward_acc: 0.3834\n",
      "Epoch 37/150\n",
      "470105/470105 [==============================] - 109s 231us/step - loss: 0.6743 - actions_loss: 0.5427 - reward_loss: 0.1316 - actions_acc: 0.7637 - reward_acc: 0.9681 - val_loss: 1.8785 - val_actions_loss: 1.1186 - val_reward_loss: 0.7600 - val_actions_acc: 0.5801 - val_reward_acc: 0.4247\n",
      "Epoch 38/150\n",
      "470105/470105 [==============================] - 110s 234us/step - loss: 0.6635 - actions_loss: 0.5332 - reward_loss: 0.1303 - actions_acc: 0.7683 - reward_acc: 0.9688 - val_loss: 1.9510 - val_actions_loss: 1.1821 - val_reward_loss: 0.7689 - val_actions_acc: 0.5711 - val_reward_acc: 0.4082\n",
      "Epoch 39/150\n",
      "470105/470105 [==============================] - 110s 234us/step - loss: 0.6520 - actions_loss: 0.5233 - reward_loss: 0.1286 - actions_acc: 0.7728 - reward_acc: 0.9696 - val_loss: 1.9275 - val_actions_loss: 1.1664 - val_reward_loss: 0.7611 - val_actions_acc: 0.5712 - val_reward_acc: 0.4148\n",
      "Epoch 40/150\n",
      "470105/470105 [==============================] - 108s 231us/step - loss: 0.6469 - actions_loss: 0.5180 - reward_loss: 0.1289 - actions_acc: 0.7756 - reward_acc: 0.9700 - val_loss: 1.9276 - val_actions_loss: 1.1697 - val_reward_loss: 0.7580 - val_actions_acc: 0.5699 - val_reward_acc: 0.4220\n",
      "Epoch 41/150\n",
      "470105/470105 [==============================] - 111s 236us/step - loss: 0.6388 - actions_loss: 0.5110 - reward_loss: 0.1278 - actions_acc: 0.7788 - reward_acc: 0.9703 - val_loss: 1.9655 - val_actions_loss: 1.1999 - val_reward_loss: 0.7656 - val_actions_acc: 0.5729 - val_reward_acc: 0.4060\n",
      "Epoch 42/150\n",
      "470105/470105 [==============================] - 113s 241us/step - loss: 0.6317 - actions_loss: 0.5037 - reward_loss: 0.1280 - actions_acc: 0.7819 - reward_acc: 0.9708 - val_loss: 1.9785 - val_actions_loss: 1.2306 - val_reward_loss: 0.7480 - val_actions_acc: 0.5694 - val_reward_acc: 0.4328\n",
      "Epoch 43/150\n",
      "470105/470105 [==============================] - 114s 242us/step - loss: 0.6226 - actions_loss: 0.4959 - reward_loss: 0.1267 - actions_acc: 0.7858 - reward_acc: 0.9712 - val_loss: 2.0400 - val_actions_loss: 1.2813 - val_reward_loss: 0.7587 - val_actions_acc: 0.5654 - val_reward_acc: 0.4180\n",
      "Epoch 44/150\n",
      "470105/470105 [==============================] - 113s 240us/step - loss: 0.6180 - actions_loss: 0.4918 - reward_loss: 0.1262 - actions_acc: 0.7880 - reward_acc: 0.9713 - val_loss: 1.9871 - val_actions_loss: 1.2347 - val_reward_loss: 0.7524 - val_actions_acc: 0.5707 - val_reward_acc: 0.4310\n",
      "Epoch 45/150\n",
      "470105/470105 [==============================] - 113s 240us/step - loss: 0.6076 - actions_loss: 0.4827 - reward_loss: 0.1249 - actions_acc: 0.7925 - reward_acc: 0.9718 - val_loss: 2.0289 - val_actions_loss: 1.2677 - val_reward_loss: 0.7612 - val_actions_acc: 0.5720 - val_reward_acc: 0.4149\n",
      "Epoch 46/150\n",
      "470105/470105 [==============================] - 109s 232us/step - loss: 0.6014 - actions_loss: 0.4770 - reward_loss: 0.1244 - actions_acc: 0.7952 - reward_acc: 0.9723 - val_loss: 2.0522 - val_actions_loss: 1.2977 - val_reward_loss: 0.7545 - val_actions_acc: 0.5688 - val_reward_acc: 0.4242\n",
      "Epoch 47/150\n",
      "470105/470105 [==============================] - 112s 238us/step - loss: 0.5919 - actions_loss: 0.4684 - reward_loss: 0.1235 - actions_acc: 0.7986 - reward_acc: 0.9726 - val_loss: 2.1166 - val_actions_loss: 1.3362 - val_reward_loss: 0.7804 - val_actions_acc: 0.5640 - val_reward_acc: 0.3820\n",
      "Epoch 48/150\n",
      "470105/470105 [==============================] - 110s 234us/step - loss: 0.5891 - actions_loss: 0.4653 - reward_loss: 0.1238 - actions_acc: 0.8007 - reward_acc: 0.9728 - val_loss: 2.0766 - val_actions_loss: 1.3149 - val_reward_loss: 0.7617 - val_actions_acc: 0.5691 - val_reward_acc: 0.4140\n",
      "Epoch 49/150\n",
      "470105/470105 [==============================] - 111s 236us/step - loss: 0.5791 - actions_loss: 0.4570 - reward_loss: 0.1222 - actions_acc: 0.8044 - reward_acc: 0.9733 - val_loss: 2.1179 - val_actions_loss: 1.3499 - val_reward_loss: 0.7681 - val_actions_acc: 0.5684 - val_reward_acc: 0.3987\n",
      "Epoch 50/150\n",
      "470105/470105 [==============================] - 110s 235us/step - loss: 0.5734 - actions_loss: 0.4517 - reward_loss: 0.1218 - actions_acc: 0.8076 - reward_acc: 0.9735 - val_loss: 2.1284 - val_actions_loss: 1.3514 - val_reward_loss: 0.7771 - val_actions_acc: 0.5672 - val_reward_acc: 0.3755\n",
      "Epoch 51/150\n",
      "470105/470105 [==============================] - 111s 236us/step - loss: 0.5713 - actions_loss: 0.4487 - reward_loss: 0.1226 - actions_acc: 0.8086 - reward_acc: 0.9736 - val_loss: 2.1218 - val_actions_loss: 1.3729 - val_reward_loss: 0.7489 - val_actions_acc: 0.5626 - val_reward_acc: 0.4314\n",
      "Epoch 52/150\n",
      "470105/470105 [==============================] - 99s 211us/step - loss: 0.5628 - actions_loss: 0.4422 - reward_loss: 0.1206 - actions_acc: 0.8114 - reward_acc: 0.9742 - val_loss: 2.1479 - val_actions_loss: 1.3904 - val_reward_loss: 0.7575 - val_actions_acc: 0.5628 - val_reward_acc: 0.4290\n",
      "Epoch 53/150\n",
      "470105/470105 [==============================] - 110s 234us/step - loss: 0.5544 - actions_loss: 0.4346 - reward_loss: 0.1197 - actions_acc: 0.8156 - reward_acc: 0.9744 - val_loss: 2.1488 - val_actions_loss: 1.3766 - val_reward_loss: 0.7722 - val_actions_acc: 0.5661 - val_reward_acc: 0.3879\n",
      "Epoch 54/150\n",
      "470105/470105 [==============================] - 111s 235us/step - loss: 0.5499 - actions_loss: 0.4302 - reward_loss: 0.1197 - actions_acc: 0.8177 - reward_acc: 0.9744 - val_loss: 2.1766 - val_actions_loss: 1.4188 - val_reward_loss: 0.7578 - val_actions_acc: 0.5664 - val_reward_acc: 0.4230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/150\n",
      "470105/470105 [==============================] - 109s 233us/step - loss: 0.5481 - actions_loss: 0.4280 - reward_loss: 0.1201 - actions_acc: 0.8183 - reward_acc: 0.9747 - val_loss: 2.2029 - val_actions_loss: 1.4422 - val_reward_loss: 0.7608 - val_actions_acc: 0.5641 - val_reward_acc: 0.4117\n",
      "Epoch 56/150\n",
      "470105/470105 [==============================] - 111s 236us/step - loss: 0.5390 - actions_loss: 0.4205 - reward_loss: 0.1185 - actions_acc: 0.8215 - reward_acc: 0.9751 - val_loss: 2.2637 - val_actions_loss: 1.5009 - val_reward_loss: 0.7627 - val_actions_acc: 0.5599 - val_reward_acc: 0.4095\n",
      "Epoch 57/150\n",
      "470105/470105 [==============================] - 110s 235us/step - loss: 0.5324 - actions_loss: 0.4144 - reward_loss: 0.1180 - actions_acc: 0.8240 - reward_acc: 0.9754 - val_loss: 2.2232 - val_actions_loss: 1.4675 - val_reward_loss: 0.7556 - val_actions_acc: 0.5639 - val_reward_acc: 0.4191\n",
      "Epoch 58/150\n",
      "470105/470105 [==============================] - 108s 231us/step - loss: 0.5257 - actions_loss: 0.4091 - reward_loss: 0.1167 - actions_acc: 0.8271 - reward_acc: 0.9757 - val_loss: 2.2674 - val_actions_loss: 1.5027 - val_reward_loss: 0.7647 - val_actions_acc: 0.5582 - val_reward_acc: 0.4034\n",
      "Epoch 59/150\n",
      "470105/470105 [==============================] - 110s 233us/step - loss: 0.5223 - actions_loss: 0.4057 - reward_loss: 0.1166 - actions_acc: 0.8287 - reward_acc: 0.9759 - val_loss: 2.2640 - val_actions_loss: 1.5044 - val_reward_loss: 0.7596 - val_actions_acc: 0.5597 - val_reward_acc: 0.4185\n",
      "Epoch 60/150\n",
      "470105/470105 [==============================] - 104s 221us/step - loss: 0.5167 - actions_loss: 0.4001 - reward_loss: 0.1166 - actions_acc: 0.8311 - reward_acc: 0.9757 - val_loss: 2.2759 - val_actions_loss: 1.5113 - val_reward_loss: 0.7646 - val_actions_acc: 0.5620 - val_reward_acc: 0.4000\n",
      "Epoch 61/150\n",
      "470105/470105 [==============================] - 107s 229us/step - loss: 0.5139 - actions_loss: 0.3972 - reward_loss: 0.1168 - actions_acc: 0.8323 - reward_acc: 0.9759 - val_loss: 2.3202 - val_actions_loss: 1.5631 - val_reward_loss: 0.7571 - val_actions_acc: 0.5597 - val_reward_acc: 0.4239\n",
      "Epoch 62/150\n",
      "470105/470105 [==============================] - 110s 234us/step - loss: 0.5081 - actions_loss: 0.3917 - reward_loss: 0.1163 - actions_acc: 0.8351 - reward_acc: 0.9762 - val_loss: 2.3419 - val_actions_loss: 1.5802 - val_reward_loss: 0.7617 - val_actions_acc: 0.5568 - val_reward_acc: 0.4167\n",
      "Epoch 63/150\n",
      "470105/470105 [==============================] - 109s 232us/step - loss: 0.5023 - actions_loss: 0.3867 - reward_loss: 0.1156 - actions_acc: 0.8370 - reward_acc: 0.9764 - val_loss: 2.3997 - val_actions_loss: 1.6186 - val_reward_loss: 0.7810 - val_actions_acc: 0.5542 - val_reward_acc: 0.3774\n",
      "Epoch 64/150\n",
      "470105/470105 [==============================] - 110s 233us/step - loss: 0.4972 - actions_loss: 0.3823 - reward_loss: 0.1148 - actions_acc: 0.8392 - reward_acc: 0.9768 - val_loss: 2.3805 - val_actions_loss: 1.6181 - val_reward_loss: 0.7624 - val_actions_acc: 0.5559 - val_reward_acc: 0.4082\n",
      "Epoch 65/150\n",
      "470105/470105 [==============================] - 110s 234us/step - loss: 0.4907 - actions_loss: 0.3772 - reward_loss: 0.1134 - actions_acc: 0.8415 - reward_acc: 0.9770 - val_loss: 2.3544 - val_actions_loss: 1.5898 - val_reward_loss: 0.7646 - val_actions_acc: 0.5588 - val_reward_acc: 0.4117\n",
      "Epoch 66/150\n",
      "470105/470105 [==============================] - 111s 236us/step - loss: 0.4935 - actions_loss: 0.3789 - reward_loss: 0.1146 - actions_acc: 0.8408 - reward_acc: 0.9770 - val_loss: 2.4109 - val_actions_loss: 1.6414 - val_reward_loss: 0.7695 - val_actions_acc: 0.5547 - val_reward_acc: 0.4001\n",
      "Epoch 67/150\n",
      "470105/470105 [==============================] - 109s 231us/step - loss: 0.4834 - actions_loss: 0.3701 - reward_loss: 0.1133 - actions_acc: 0.8448 - reward_acc: 0.9772 - val_loss: 2.4381 - val_actions_loss: 1.6801 - val_reward_loss: 0.7580 - val_actions_acc: 0.5536 - val_reward_acc: 0.4180\n",
      "Epoch 68/150\n",
      "290816/470105 [=================>............] - ETA: 40s - loss: 0.4664 - actions_loss: 0.3547 - reward_loss: 0.1116 - actions_acc: 0.8528 - reward_acc: 0.9777"
     ]
    }
   ],
   "source": [
    "trainer.train(observations, labels, rewards, batch_size=batch_size, \n",
    "              epochs=epochs, early_stopping=early_stopping, class_weight=[class_weights, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    }
   ],
   "source": [
    "print(\"finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save(model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
