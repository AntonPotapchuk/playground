{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/hpchome/anton95/.conda/envs/pommerman/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/gpfs/hpchome/anton95/.conda/envs/pommerman/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Flatten, Convolution2D, Input, Concatenate, Activation\n",
    "from keras.optimizers import Adam\n",
    "from pommerman.configs import ffa_v0_env\n",
    "from pommerman.envs.v0 import Pomme\n",
    "from pommerman.agents import SimpleAgent, BaseAgent\n",
    "from pommerman.constants import BOARD_SIZE\n",
    "from rl.agents import DDPGAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Env, Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint, Callback\n",
    "from rl.random import OrnsteinUhlenbeckProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_training_steps = 500000\n",
    "log_interval = 5000\n",
    "file_log_path = './dqn/rl_logs/ddgp_dense_128_1_rs/log.txt'\n",
    "tensorboard_path = './dqn/logs/ddgp_dense_128_1_rs/'\n",
    "model_path = './dqn/model/ddgp_dense_128_1_rs/model{step}.h4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(os.path.dirname(file_log_path)):\n",
    "    os.makedirs(os.path.dirname(file_log_path))\n",
    "if not os.path.isdir(os.path.dirname(model_path)):\n",
    "    os.makedirs(os.path.dirname(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorforceAgent(BaseAgent):\n",
    "    def act(self, obs, action_space):\n",
    "        pass\n",
    "\n",
    "\n",
    "class TensorboardLogger(Callback):\n",
    "    \"\"\"Logging in tensorboard without tensorflow ops.\"\"\"\n",
    "    def __init__(self, log_dir):\n",
    "        # Some algorithms compute multiple episodes at once since they are multi-threaded.\n",
    "        # We therefore use a dictionary that is indexed by the episode to separate episodes\n",
    "        # from each other.\n",
    "        self.observations = {}\n",
    "        self.rewards = {}\n",
    "        self.actions = {}\n",
    "        self.metrics = {}\n",
    "        self.step = 0\n",
    "        \"\"\"Creates a summary writer logging to log_dir.\"\"\"\n",
    "        self.writer = tf.summary.FileWriter(log_dir)\n",
    "\n",
    "    def log_scalar(self, tag, value, step):\n",
    "        \"\"\"Log a scalar variable.\n",
    "        Parameter\n",
    "        ----------\n",
    "        tag : basestring\n",
    "            Name of the scalar\n",
    "        value\n",
    "        step : int\n",
    "            training iteration\n",
    "        \"\"\"\n",
    "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])\n",
    "        self.writer.add_summary(summary, step)\n",
    "\n",
    "    def on_train_begin(self, logs):\n",
    "        self.metrics_names = self.model.metrics_names\n",
    "\n",
    "    def on_episode_begin(self, episode, logs):\n",
    "        self.observations[episode] = []\n",
    "        self.rewards[episode] = []\n",
    "        self.actions[episode] = []\n",
    "        self.metrics[episode] = []\n",
    "\n",
    "    def on_episode_end(self, episode, logs):\n",
    "        episode_steps = len(self.observations[episode])\n",
    "        variables = {\n",
    "            'step': self.step,\n",
    "            'nb_steps': self.params['nb_steps'],\n",
    "            'episode_steps': episode_steps,\n",
    "            'episode_reward': np.sum(self.rewards[episode]),\n",
    "            'reward_mean': np.mean(self.rewards[episode]),\n",
    "            'reward_min': np.min(self.rewards[episode]),\n",
    "            'reward_max': np.max(self.rewards[episode]),\n",
    "            'action_mean': np.mean(np.argmax(self.actions[episode], axis=1)),\n",
    "            'action_min': np.min(np.argmax(self.actions[episode], axis=1)),\n",
    "            'action_max': np.max(np.argmax(self.actions[episode], axis=1)),\n",
    "            'obs_mean': np.mean(self.observations[episode]),\n",
    "            'obs_min': np.min(self.observations[episode]),\n",
    "            'obs_max': np.max(self.observations[episode]),\n",
    "        }\n",
    "\n",
    "        # Format all metrics.\n",
    "        metrics = np.array(self.metrics[episode])\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings('error')\n",
    "            for idx, name in enumerate(self.metrics_names):\n",
    "                try:\n",
    "                    value = np.nanmean(metrics[:, idx])\n",
    "                except Warning:\n",
    "                    value = -1\n",
    "                variables[name] = value\n",
    "        for key, value in variables.items():\n",
    "            self.log_scalar(key, value, episode + 1)\n",
    "\n",
    "        # Free up resources.\n",
    "        del self.observations[episode]\n",
    "        del self.rewards[episode]\n",
    "        del self.actions[episode]\n",
    "        del self.metrics[episode]\n",
    "\n",
    "    def on_step_end(self, step, logs):\n",
    "        episode = logs['episode']\n",
    "        self.observations[episode].append(logs['observation'])\n",
    "        self.rewards[episode].append(logs['reward'])\n",
    "        self.actions[episode].append(logs['action'])\n",
    "        self.metrics[episode].append(logs['metrics'])\n",
    "        self.step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 2369)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               303360    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 774       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 304,134\n",
      "Trainable params: 304,134\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "action_input (InputLayer)       (None, 6)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "observation_input (InputLayer)  (None, 2369)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 2375)         0           action_input[0][0]               \n",
      "                                                                 observation_input[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          304128      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 128)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            129         activation_3[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 304,257\n",
      "Trainable params: 304,257\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the environment\n",
    "config = ffa_v0_env()\n",
    "env = Pomme(**config[\"env_kwargs\"])\n",
    "np.random.seed(0)\n",
    "env.seed(0)\n",
    "\n",
    "env.set_init_game_state(None)\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "\n",
    "def create_actor(actions, input_shape=(2369,)):\n",
    "    inp = Input(input_shape)\n",
    "    x = Dense(128)(inp)\n",
    "    x = Activation('relu')(x) \n",
    "    out = Dense(actions)(x)\n",
    "    out = Activation('softmax')(out)\n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_critic(actions, input_shape=(2369,)):\n",
    "    action_input = Input(shape=(actions,), name='action_input')\n",
    "    observation_input = Input(shape=input_shape, name='observation_input')\n",
    "    x = Concatenate()([action_input, observation_input])\n",
    "    x = Dense(128)(x)\n",
    "    x = Activation('relu')(x) \n",
    "    x = Dense(1)(x)\n",
    "    return action_input, Model(inputs=[action_input, observation_input], outputs=x)\n",
    "\n",
    "\n",
    "actor = create_actor(nb_actions)\n",
    "action_input, critic = create_critic(nb_actions)\n",
    "print(actor.summary())\n",
    "print(critic.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvWrapper(Env):\n",
    "    \"\"\"The abstract environment class that is used by all agents. This class has the exact\n",
    "        same API that OpenAI Gym uses so that integrating with it is trivial. In contrast to the\n",
    "        OpenAI Gym implementation, this class only defines the abstract methods without any actual\n",
    "        implementation.\n",
    "        To implement your own environment, you need to define the following methods:\n",
    "        - `step`\n",
    "        - `reset`\n",
    "        - `render`\n",
    "        - `close`\n",
    "        Refer to the [Gym documentation](https://gym.openai.com/docs/#environments).\n",
    "        \"\"\"\n",
    "    reward_range = (-1, 1)\n",
    "    action_space = None\n",
    "    observation_space = None\n",
    "\n",
    "    def __init__(self, gym, board_size):\n",
    "        self.gym = gym\n",
    "        self.action_space = gym.action_space\n",
    "        self.observation_space = gym.observation_space\n",
    "        self.reward_range = gym.reward_range\n",
    "        self.board_size = board_size\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Run one timestep of the environment's dynamics.\n",
    "        Accepts an action and returns a tuple (observation, reward, done, info).\n",
    "        # Arguments\n",
    "            action (object): An action provided by the environment.\n",
    "        # Returns\n",
    "            observation (object): Agent's observation of the current environment.\n",
    "            reward (float) : Amount of reward returned after previous action.\n",
    "            done (boolean): Whether the episode has ended, in which case further step() calls will return undefined results.\n",
    "            info (dict): Contains auxiliary diagnostic information (helpful for debugging, and sometimes learning).\n",
    "        \"\"\"\n",
    "        action = np.argmax(action)\n",
    "        obs = self.gym.get_observations()\n",
    "        all_actions = self.gym.act(obs)\n",
    "        all_actions.insert(self.gym.training_agent, action)\n",
    "        state, reward, terminal, info = self.gym.step(all_actions)\n",
    "        agent_state = self.featurize(state[self.gym.training_agent])\n",
    "        agent_reward = reward[self.gym.training_agent]\n",
    "        agent_reward = self.gym.train_reward\n",
    "        return agent_state, agent_reward, terminal, info\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the state of the environment and returns an initial observation.\n",
    "        # Returns\n",
    "            observation (object): The initial observation of the space. Initial reward is assumed to be 0.\n",
    "        \"\"\"\n",
    "        # Add 3 random agents\n",
    "        train_agent_pos = np.random.randint(0, 4)\n",
    "        agents = []\n",
    "        for agent_id in range(4):\n",
    "            if agent_id == train_agent_pos:\n",
    "                agents.append(TensorforceAgent(config[\"agent\"](agent_id, config[\"game_type\"])))\n",
    "            else:\n",
    "                agents.append(SimpleAgent(config[\"agent\"](agent_id, config[\"game_type\"])))\n",
    "        self.gym.set_agents(agents)\n",
    "        self.gym.set_training_agent(agents[train_agent_pos].agent_id)\n",
    "        \n",
    "        obs = self.gym.reset()\n",
    "        agent_obs = self.featurize(obs[self.gym.training_agent])\n",
    "        return agent_obs\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        \"\"\"Renders the environment.\n",
    "        The set of supported modes varies per environment. (And some\n",
    "        environments do not support rendering at all.)\n",
    "        # Arguments\n",
    "            mode (str): The mode to render with.\n",
    "            close (bool): Close all open renderings.\n",
    "        \"\"\"\n",
    "        self.gym.render(mode=mode, close=close)\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Override in your subclass to perform any necessary cleanup.\n",
    "        Environments will automatically close() themselves when\n",
    "        garbage collected or when the program exits.\n",
    "        \"\"\"\n",
    "        self.gym.close()\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        \"\"\"Sets the seed for this env's random number generator(s).\n",
    "        # Returns\n",
    "            Returns the list of seeds used in this env's random number generators\n",
    "        \"\"\"\n",
    "        raise self.gym.seed(seed)\n",
    "\n",
    "    def configure(self, *args, **kwargs):\n",
    "        \"\"\"Provides runtime configuration to the environment.\n",
    "        This configuration should consist of data that tells your\n",
    "        environment how to run (such as an address of a remote server,\n",
    "        or path to your ImageNet data). It should not affect the\n",
    "        semantics of the environment.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def featurize(self, obs):\n",
    "        shape = (self.board_size, self.board_size, 1)\n",
    "\n",
    "        def get_matrix(dict, key):\n",
    "            res = dict[key]\n",
    "            return res.reshape(shape).astype(np.float32)\n",
    "\n",
    "        def get_map(board, item):\n",
    "            map = np.zeros(shape)\n",
    "            map[board == item] = 1\n",
    "            return map\n",
    "\n",
    "        board = get_matrix(obs, 'board')\n",
    "\n",
    "        # TODO: probably not needed Passage = 0\n",
    "        rigid_map = get_map(board, 1)               # Rigid = 1\n",
    "        wood_map = get_map(board, 2)                # Wood = 2\n",
    "        bomb_map = get_map(board, 3)                # Bomb = 3\n",
    "        flames_map = get_map(board, 4)              # Flames = 4\n",
    "        fog_map = get_map(board, 5)                 # TODO: not used for first two stages Fog = 5\n",
    "        extra_bomb_map = get_map(board, 6)          # ExtraBomb = 6\n",
    "        incr_range_map = get_map(board, 7)          # IncrRange = 7\n",
    "        kick_map = get_map(board, 8)                # Kick = 8\n",
    "        skull_map = get_map(board, 9)               # Skull = 9\n",
    "\n",
    "        position = obs[\"position\"]\n",
    "        my_position = np.zeros(shape)\n",
    "        my_position[position[0], position[1], 0] = 1\n",
    "\n",
    "        team_mates = get_map(board, obs[\"teammate\"].value) # TODO during documentation it should be an array\n",
    "\n",
    "        enemies = np.zeros(shape)\n",
    "        for enemy in obs[\"enemies\"]:\n",
    "            enemies[board == enemy.value] = 1\n",
    "\n",
    "        bomb_blast_strength = get_matrix(obs, 'bomb_blast_strength')\n",
    "        bomb_life = get_matrix(obs, 'bomb_life')\n",
    "\n",
    "        ammo = obs[\"ammo\"]\n",
    "        blast_strength = obs[\"blast_strength\"]\n",
    "        can_kick = int(obs[\"can_kick\"])\n",
    "\n",
    "        obs = np.concatenate([my_position, enemies, team_mates, rigid_map,\n",
    "                              wood_map, bomb_map, flames_map,\n",
    "                              fog_map, extra_bomb_map, incr_range_map,\n",
    "                              kick_map, skull_map, bomb_blast_strength,\n",
    "                              bomb_life], axis=2).flatten()\n",
    "        obs = np.append(obs, [ammo, blast_strength, can_kick])\n",
    "        return obs\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "    def __str__(self):\n",
    "        return '<{} instance>'.format(type(self).__name__)\n",
    "\n",
    "\n",
    "class CustomProcessor(Processor):\n",
    "    def process_state_batch(self, batch):\n",
    "        \"\"\"Processes an entire batch of states and returns it.\n",
    "        # Arguments\n",
    "            batch (list): List of states\n",
    "        # Returns\n",
    "            Processed list of states\n",
    "        \"\"\"\n",
    "        batch = np.squeeze(batch, axis=1)\n",
    "        return batch\n",
    "\n",
    "    def process_info(self, info):\n",
    "        \"\"\"Processes the info as obtained from the environment for use in an agent and\n",
    "        returns it.\n",
    "        \"\"\"\n",
    "        info['result'] = info['result'].value\n",
    "        return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_wrapper = EnvWrapper(env, BOARD_SIZE)\n",
    "processor = CustomProcessor()\n",
    "\n",
    "\n",
    "memory = SequentialMemory(limit=100000, window_length=1)\n",
    "random_process = OrnsteinUhlenbeckProcess(size=nb_actions, theta=.15, mu=0., sigma=.3)\n",
    "agent = DDPGAgent(nb_actions=nb_actions, actor=actor, critic=critic, critic_action_input=action_input,\n",
    "                  memory=memory, nb_steps_warmup_critic=512, nb_steps_warmup_actor=512,\n",
    "                  random_process=random_process, gamma=.99, target_model_update=1e-3,\n",
    "                  batch_size=512, processor=processor)\n",
    "agent.compile(Adam(lr=0.0001, clipnorm=1.), metrics=['mae'])\n",
    "\n",
    "\n",
    "file_logger = FileLogger(file_log_path, interval=log_interval)\n",
    "checkpoint = ModelIntervalCheckpoint(model_path, interval=log_interval)\n",
    "tensorboard = TensorboardLogger(tensorboard_path)\n",
    "callbacks=[file_logger, checkpoint, tensorboard]\n",
    "if os.path.isfile(model_path):\n",
    "    agent.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 500000 steps ...\n",
      "    255/500000: episode: 1, duration: 4.386s, episode steps: 255, steps per second: 58, episode reward: -1.240, mean reward: -0.005 [-0.005, 0.000], mean action: 0.223 [-0.305, 0.841], mean observation: 0.050 [0.000, 24.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    374/500000: episode: 2, duration: 1.722s, episode steps: 119, steps per second: 69, episode reward: -0.575, mean reward: -0.005 [-0.005, 0.000], mean action: 0.238 [-0.356, 0.888], mean observation: 0.066 [0.000, 24.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    399/500000: episode: 3, duration: 0.383s, episode steps: 25, steps per second: 65, episode reward: -0.115, mean reward: -0.005 [-0.005, 0.000], mean action: 0.166 [-0.127, 0.483], mean observation: 0.072 [0.000, 24.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    429/500000: episode: 4, duration: 0.381s, episode steps: 30, steps per second: 79, episode reward: -0.020, mean reward: -0.001 [-0.005, 0.000], mean action: 0.173 [-0.097, 0.443], mean observation: 0.056 [0.000, 24.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    492/500000: episode: 5, duration: 0.753s, episode steps: 63, steps per second: 84, episode reward: -0.260, mean reward: -0.004 [-0.005, 0.000], mean action: 0.181 [-0.110, 0.573], mean observation: 0.067 [0.000, 24.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "    671/500000: episode: 6, duration: 8.469s, episode steps: 179, steps per second: 21, episode reward: -0.755, mean reward: -0.004 [-0.005, 0.000], mean action: 0.060 [-0.536, 0.935], mean observation: 0.048 [0.000, 24.000], loss: 0.004977, mean_absolute_error: 0.038537, mean_q: -0.058113\n",
      "    696/500000: episode: 7, duration: 1.234s, episode steps: 25, steps per second: 20, episode reward: -0.050, mean reward: -0.002 [-0.005, 0.000], mean action: 0.208 [-0.269, 0.802], mean observation: 0.071 [0.000, 24.000], loss: 0.002490, mean_absolute_error: 0.025467, mean_q: -0.015626\n",
      "    721/500000: episode: 8, duration: 1.377s, episode steps: 25, steps per second: 18, episode reward: -0.110, mean reward: -0.004 [-0.005, 0.000], mean action: 0.143 [-0.186, 0.959], mean observation: 0.072 [0.000, 24.000], loss: 0.001890, mean_absolute_error: 0.025139, mean_q: -0.018239\n",
      "    804/500000: episode: 9, duration: 4.281s, episode steps: 83, steps per second: 19, episode reward: -0.280, mean reward: -0.003 [-0.005, 0.000], mean action: 0.181 [-0.414, 1.030], mean observation: 0.064 [0.000, 24.000], loss: 0.001900, mean_absolute_error: 0.024016, mean_q: -0.030935\n",
      "    848/500000: episode: 10, duration: 2.347s, episode steps: 44, steps per second: 19, episode reward: -0.115, mean reward: -0.003 [-0.005, 0.000], mean action: 0.227 [-0.214, 1.114], mean observation: 0.067 [0.000, 24.000], loss: 0.001518, mean_absolute_error: 0.023798, mean_q: -0.033279\n",
      "    873/500000: episode: 11, duration: 1.330s, episode steps: 25, steps per second: 19, episode reward: -0.011, mean reward: -0.000 [-0.001, 0.000], mean action: 0.163 [-0.081, 1.081], mean observation: 0.058 [0.000, 24.000], loss: 0.001755, mean_absolute_error: 0.024824, mean_q: -0.046002\n",
      "   1345/500000: episode: 12, duration: 25.207s, episode steps: 472, steps per second: 19, episode reward: -0.903, mean reward: -0.002 [-0.005, 0.000], mean action: 0.173 [-0.923, 1.219], mean observation: 0.048 [0.000, 24.000], loss: 0.001238, mean_absolute_error: 0.022749, mean_q: 0.014117\n",
      "   1735/500000: episode: 13, duration: 20.129s, episode steps: 390, steps per second: 19, episode reward: 0.008, mean reward: 0.000 [-0.005, 0.399], mean action: 0.296 [-0.496, 1.571], mean observation: 0.049 [0.000, 24.000], loss: 0.001437, mean_absolute_error: 0.024423, mean_q: 0.029570\n",
      "   2792/500000: episode: 14, duration: 44.610s, episode steps: 1057, steps per second: 24, episode reward: 2.543, mean reward: 0.002 [-0.001, 0.799], mean action: -0.096 [-1.135, 1.403], mean observation: 0.043 [0.000, 24.000], loss: 0.001316, mean_absolute_error: 0.023647, mean_q: 0.018257\n",
      "   3405/500000: episode: 15, duration: 31.109s, episode steps: 613, steps per second: 20, episode reward: -0.213, mean reward: -0.000 [-0.001, 0.399], mean action: 0.404 [-0.401, 2.039], mean observation: 0.045 [0.000, 24.000], loss: 0.001366, mean_absolute_error: 0.024879, mean_q: -0.003569\n",
      "   5905/500000: episode: 16, duration: 147.737s, episode steps: 2500, steps per second: 17, episode reward: -5.602, mean reward: -0.002 [-0.005, 0.395], mean action: 0.066 [-1.531, 1.639], mean observation: 0.037 [0.000, 24.000], loss: 0.000623, mean_absolute_error: 0.017685, mean_q: 0.064216\n",
      "   8405/500000: episode: 17, duration: 124.281s, episode steps: 2500, steps per second: 20, episode reward: -4.326, mean reward: -0.002 [-0.005, 0.399], mean action: -0.019 [-1.648, 1.307], mean observation: 0.038 [0.000, 24.000], loss: 0.000335, mean_absolute_error: 0.012679, mean_q: 0.111758\n",
      "   8430/500000: episode: 18, duration: 1.304s, episode steps: 25, steps per second: 19, episode reward: -0.023, mean reward: -0.001 [-0.001, 0.000], mean action: 0.171 [-0.222, 1.101], mean observation: 0.072 [0.000, 24.000], loss: 0.000231, mean_absolute_error: 0.010709, mean_q: 0.090320\n",
      "   8455/500000: episode: 19, duration: 1.343s, episode steps: 25, steps per second: 19, episode reward: -0.102, mean reward: -0.004 [-0.005, 0.000], mean action: 0.198 [-0.064, 1.061], mean observation: 0.072 [0.000, 24.000], loss: 0.000496, mean_absolute_error: 0.013085, mean_q: 0.085193\n",
      "   8480/500000: episode: 20, duration: 1.314s, episode steps: 25, steps per second: 19, episode reward: -0.024, mean reward: -0.001 [-0.001, 0.000], mean action: 0.241 [-0.141, 1.226], mean observation: 0.072 [0.000, 24.000], loss: 0.000374, mean_absolute_error: 0.013551, mean_q: 0.086505\n",
      "   8505/500000: episode: 21, duration: 1.296s, episode steps: 25, steps per second: 19, episode reward: -0.024, mean reward: -0.001 [-0.001, 0.000], mean action: 0.209 [-0.107, 1.048], mean observation: 0.072 [0.000, 24.000], loss: 0.000458, mean_absolute_error: 0.013028, mean_q: 0.078913\n",
      "   8530/500000: episode: 22, duration: 1.376s, episode steps: 25, steps per second: 18, episode reward: -0.082, mean reward: -0.003 [-0.005, 0.000], mean action: 0.184 [-0.094, 1.068], mean observation: 0.071 [0.000, 24.000], loss: 0.000537, mean_absolute_error: 0.014885, mean_q: 0.082776\n",
      "   8779/500000: episode: 23, duration: 12.882s, episode steps: 249, steps per second: 19, episode reward: -0.558, mean reward: -0.002 [-0.005, 0.000], mean action: 0.300 [-0.358, 1.809], mean observation: 0.059 [0.000, 24.000], loss: 0.000660, mean_absolute_error: 0.016285, mean_q: 0.080610\n",
      "   8827/500000: episode: 24, duration: 2.346s, episode steps: 48, steps per second: 20, episode reward: -0.102, mean reward: -0.002 [-0.005, 0.000], mean action: 0.215 [-0.207, 1.044], mean observation: 0.066 [0.000, 24.000], loss: 0.000622, mean_absolute_error: 0.016005, mean_q: 0.076316\n",
      "   8854/500000: episode: 25, duration: 1.416s, episode steps: 27, steps per second: 19, episode reward: -0.010, mean reward: -0.000 [-0.005, 0.000], mean action: 0.221 [-0.158, 1.255], mean observation: 0.071 [0.000, 24.000], loss: 0.000858, mean_absolute_error: 0.019518, mean_q: 0.076510\n",
      "   9054/500000: episode: 26, duration: 10.569s, episode steps: 200, steps per second: 19, episode reward: -0.413, mean reward: -0.002 [-0.005, 0.000], mean action: 0.179 [-0.405, 1.538], mean observation: 0.064 [0.000, 24.000], loss: 0.000733, mean_absolute_error: 0.016923, mean_q: 0.074876\n",
      "   9079/500000: episode: 27, duration: 1.326s, episode steps: 25, steps per second: 19, episode reward: -0.116, mean reward: -0.005 [-0.005, 0.000], mean action: 0.164 [-0.248, 1.080], mean observation: 0.069 [0.000, 24.000], loss: 0.000931, mean_absolute_error: 0.018765, mean_q: 0.078123\n",
      "   9104/500000: episode: 28, duration: 1.328s, episode steps: 25, steps per second: 19, episode reward: -0.038, mean reward: -0.002 [-0.005, 0.000], mean action: 0.100 [-0.313, 0.937], mean observation: 0.072 [0.000, 24.000], loss: 0.000956, mean_absolute_error: 0.017700, mean_q: 0.076823\n",
      "   9129/500000: episode: 29, duration: 1.348s, episode steps: 25, steps per second: 19, episode reward: -0.066, mean reward: -0.003 [-0.005, 0.000], mean action: 0.199 [-0.177, 1.012], mean observation: 0.070 [0.000, 24.000], loss: 0.000882, mean_absolute_error: 0.017810, mean_q: 0.077878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   9154/500000: episode: 30, duration: 1.314s, episode steps: 25, steps per second: 19, episode reward: -0.026, mean reward: -0.001 [-0.005, 0.000], mean action: 0.230 [-0.089, 1.210], mean observation: 0.071 [0.000, 24.000], loss: 0.000916, mean_absolute_error: 0.019639, mean_q: 0.078020\n",
      "   9179/500000: episode: 31, duration: 1.439s, episode steps: 25, steps per second: 17, episode reward: -0.105, mean reward: -0.004 [-0.005, 0.000], mean action: 0.199 [-0.121, 1.081], mean observation: 0.071 [0.000, 24.000], loss: 0.000751, mean_absolute_error: 0.017957, mean_q: 0.075109\n",
      "   9205/500000: episode: 32, duration: 1.383s, episode steps: 26, steps per second: 19, episode reward: -0.006, mean reward: -0.000 [-0.005, 0.000], mean action: 0.132 [-0.175, 1.058], mean observation: 0.064 [0.000, 24.000], loss: 0.000830, mean_absolute_error: 0.016969, mean_q: 0.076077\n",
      "   9230/500000: episode: 33, duration: 1.345s, episode steps: 25, steps per second: 19, episode reward: -0.048, mean reward: -0.002 [-0.005, 0.000], mean action: 0.182 [-0.121, 1.099], mean observation: 0.072 [0.000, 24.000], loss: 0.000858, mean_absolute_error: 0.019192, mean_q: 0.074872\n",
      "   9280/500000: episode: 34, duration: 2.567s, episode steps: 50, steps per second: 19, episode reward: -0.195, mean reward: -0.004 [-0.005, 0.000], mean action: 0.248 [-0.465, 1.636], mean observation: 0.068 [0.000, 24.000], loss: 0.000779, mean_absolute_error: 0.017531, mean_q: 0.077186\n",
      "   9305/500000: episode: 35, duration: 1.348s, episode steps: 25, steps per second: 19, episode reward: -0.084, mean reward: -0.003 [-0.005, 0.000], mean action: 0.194 [-0.191, 1.124], mean observation: 0.072 [0.000, 24.000], loss: 0.000718, mean_absolute_error: 0.016766, mean_q: 0.071886\n",
      "   9330/500000: episode: 36, duration: 1.364s, episode steps: 25, steps per second: 18, episode reward: -0.023, mean reward: -0.001 [-0.001, 0.000], mean action: 0.166 [-0.189, 0.977], mean observation: 0.071 [0.000, 24.000], loss: 0.000742, mean_absolute_error: 0.016958, mean_q: 0.073516\n",
      "   9380/500000: episode: 37, duration: 2.808s, episode steps: 50, steps per second: 18, episode reward: -0.235, mean reward: -0.005 [-0.005, 0.000], mean action: 0.182 [-0.399, 1.220], mean observation: 0.068 [0.000, 24.000], loss: 0.000915, mean_absolute_error: 0.019152, mean_q: 0.077411\n",
      "   9405/500000: episode: 38, duration: 1.426s, episode steps: 25, steps per second: 18, episode reward: -0.082, mean reward: -0.003 [-0.005, 0.000], mean action: 0.213 [-0.155, 1.088], mean observation: 0.072 [0.000, 24.000], loss: 0.000887, mean_absolute_error: 0.019845, mean_q: 0.071546\n",
      "   9430/500000: episode: 39, duration: 1.403s, episode steps: 25, steps per second: 18, episode reward: -0.106, mean reward: -0.004 [-0.005, 0.000], mean action: 0.139 [-0.239, 1.043], mean observation: 0.072 [0.000, 24.000], loss: 0.000768, mean_absolute_error: 0.017843, mean_q: 0.079760\n",
      "   9455/500000: episode: 40, duration: 1.388s, episode steps: 25, steps per second: 18, episode reward: -0.120, mean reward: -0.005 [-0.005, 0.000], mean action: 0.113 [-0.439, 1.060], mean observation: 0.072 [0.000, 24.000], loss: 0.000613, mean_absolute_error: 0.017190, mean_q: 0.072380\n",
      "   9480/500000: episode: 41, duration: 1.314s, episode steps: 25, steps per second: 19, episode reward: -0.102, mean reward: -0.004 [-0.005, 0.000], mean action: 0.185 [-0.129, 1.058], mean observation: 0.072 [0.000, 24.000], loss: 0.000680, mean_absolute_error: 0.017544, mean_q: 0.078998\n",
      "   9505/500000: episode: 42, duration: 1.392s, episode steps: 25, steps per second: 18, episode reward: -0.024, mean reward: -0.001 [-0.001, 0.000], mean action: 0.168 [-0.305, 1.145], mean observation: 0.072 [0.000, 24.000], loss: 0.000683, mean_absolute_error: 0.017516, mean_q: 0.083823\n",
      "   9532/500000: episode: 43, duration: 1.411s, episode steps: 27, steps per second: 19, episode reward: -0.010, mean reward: -0.000 [-0.005, 0.000], mean action: 0.175 [-0.348, 1.241], mean observation: 0.069 [0.000, 24.000], loss: 0.000772, mean_absolute_error: 0.016845, mean_q: 0.081889\n",
      "   9557/500000: episode: 44, duration: 1.334s, episode steps: 25, steps per second: 19, episode reward: -0.025, mean reward: -0.001 [-0.005, 0.000], mean action: 0.146 [-0.191, 1.098], mean observation: 0.072 [0.000, 24.000], loss: 0.000673, mean_absolute_error: 0.017161, mean_q: 0.087822\n",
      "   9582/500000: episode: 45, duration: 1.346s, episode steps: 25, steps per second: 19, episode reward: -0.011, mean reward: -0.000 [-0.005, 0.000], mean action: 0.267 [-0.052, 1.142], mean observation: 0.058 [0.000, 24.000], loss: 0.000671, mean_absolute_error: 0.016779, mean_q: 0.082516\n",
      "   9607/500000: episode: 46, duration: 1.338s, episode steps: 25, steps per second: 19, episode reward: -0.120, mean reward: -0.005 [-0.005, 0.000], mean action: 0.177 [-0.193, 1.082], mean observation: 0.071 [0.000, 24.000], loss: 0.000731, mean_absolute_error: 0.017524, mean_q: 0.084175\n",
      "   9657/500000: episode: 47, duration: 2.738s, episode steps: 50, steps per second: 18, episode reward: -0.204, mean reward: -0.004 [-0.005, 0.000], mean action: 0.135 [-0.294, 0.998], mean observation: 0.068 [0.000, 24.000], loss: 0.000777, mean_absolute_error: 0.017243, mean_q: 0.079057\n",
      "   9683/500000: episode: 48, duration: 1.353s, episode steps: 26, steps per second: 19, episode reward: -0.115, mean reward: -0.004 [-0.005, 0.000], mean action: 0.202 [-0.151, 1.046], mean observation: 0.064 [0.000, 24.000], loss: 0.000668, mean_absolute_error: 0.016551, mean_q: 0.081406\n",
      "   9708/500000: episode: 49, duration: 1.404s, episode steps: 25, steps per second: 18, episode reward: -0.110, mean reward: -0.004 [-0.005, 0.000], mean action: 0.219 [-0.061, 1.035], mean observation: 0.072 [0.000, 24.000], loss: 0.000792, mean_absolute_error: 0.018457, mean_q: 0.082703\n",
      "   9733/500000: episode: 50, duration: 1.349s, episode steps: 25, steps per second: 19, episode reward: -0.021, mean reward: -0.001 [-0.001, 0.000], mean action: 0.144 [-0.217, 1.081], mean observation: 0.071 [0.000, 24.000], loss: 0.000759, mean_absolute_error: 0.017355, mean_q: 0.075770\n",
      "   9758/500000: episode: 51, duration: 1.303s, episode steps: 25, steps per second: 19, episode reward: -0.120, mean reward: -0.005 [-0.005, 0.000], mean action: 0.188 [-0.244, 1.078], mean observation: 0.072 [0.000, 24.000], loss: 0.000652, mean_absolute_error: 0.017608, mean_q: 0.083627\n",
      "   9783/500000: episode: 52, duration: 1.429s, episode steps: 25, steps per second: 17, episode reward: -0.120, mean reward: -0.005 [-0.005, 0.000], mean action: 0.148 [-0.172, 0.994], mean observation: 0.072 [0.000, 24.000], loss: 0.000757, mean_absolute_error: 0.017528, mean_q: 0.084319\n",
      "   9835/500000: episode: 53, duration: 2.772s, episode steps: 52, steps per second: 19, episode reward: -0.239, mean reward: -0.005 [-0.005, 0.000], mean action: 0.080 [-0.477, 1.265], mean observation: 0.061 [0.000, 24.000], loss: 0.000677, mean_absolute_error: 0.016317, mean_q: 0.083985\n",
      "   9860/500000: episode: 54, duration: 1.315s, episode steps: 25, steps per second: 19, episode reward: -0.102, mean reward: -0.004 [-0.005, 0.000], mean action: 0.209 [-0.206, 1.251], mean observation: 0.072 [0.000, 24.000], loss: 0.000616, mean_absolute_error: 0.015501, mean_q: 0.078004\n"
     ]
    }
   ],
   "source": [
    "history = agent.fit(env_wrapper, nb_steps=number_of_training_steps, visualize=False, verbose=2,\n",
    "        nb_max_episode_steps=env._max_steps, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save_weights(model_path, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
