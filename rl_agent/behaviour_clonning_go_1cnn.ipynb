{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger, TensorBoard\n",
    "from keras.layers import Input, Dense, Flatten, Convolution2D, BatchNormalization, Activation, Add\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "from pommerman.constants import BOARD_SIZE\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2048\n",
    "epochs = 150\n",
    "early_stopping = 5\n",
    "action_space = 6\n",
    "\n",
    "log_path = './supervised_learning/logs/1cnn/'\n",
    "model_path = './supervised_learning/model/1cnn/model.h4'\n",
    "\n",
    "train_data_path    = './dataset/'\n",
    "train_data_labels  = os.path.join(train_data_path, 'labels.npy')\n",
    "train_data_reward  = os.path.join(train_data_path, 'reward.npy')\n",
    "train_data_obs_map = os.path.join(train_data_path, 'obs_map.npy')\n",
    "\n",
    "if not os.path.isdir(train_data_path):\n",
    "    os.makedirs(train_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, actions, save_path, log_path, save_best_only=True, seed=0):\n",
    "        K.clear_session()\n",
    "        self.log_path = log_path\n",
    "        self.save_path = save_path\n",
    "        self.actions = actions\n",
    "        self.save_best_only = save_best_only\n",
    "        \n",
    "        # Create model\n",
    "        self.model = self.create_model(actions)\n",
    "        # Load model if exists\n",
    "        if not os.path.isdir(os.path.dirname(save_path)):\n",
    "            os.makedirs(os.path.dirname(save_path))            \n",
    "        if os.path.isfile(self.save_path):\n",
    "            try:\n",
    "                print(\"Trying to load model\")\n",
    "                self.model.load_weights(self.save_path)\n",
    "                print(\"Model was loaded successful\")\n",
    "            except:\n",
    "                print(\"Model load failed\")\n",
    "        \n",
    "    def get_res_block(self, input):\n",
    "        # Res block 1        \n",
    "        x = Convolution2D(256, 3, padding='same')(input)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Convolution2D(256, 3, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Add()([input, x])\n",
    "        x = Activation('relu')(x)\n",
    "        return x\n",
    "        \n",
    "    def create_model(self, actions, input_shape=(11, 11, 18,)):\n",
    "        inp = Input(input_shape)\n",
    "        # 5 Convs -> receptive field 11\n",
    "        x = Convolution2D(256, 3, padding='valid')(inp)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('elu')(x)\n",
    "        x = Flatten()(x)\n",
    "        # Output\n",
    "        probs  = Dense(actions, activation='softmax', name='actions')(x)\n",
    "        reward = Dense(1, activation='tanh', name='reward')(x)\n",
    "        \n",
    "        model = Model(inputs = inp, outputs=[probs, reward])\n",
    "        model.compile(optimizer='adam', loss=['categorical_crossentropy', 'mae'], metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def train(self, obs, actions, rewards, batch_size=16384, epochs=100,\n",
    "              early_stopping = 10, class_weight=None, initial_epoch=0):\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='loss', patience=early_stopping)\n",
    "        checkpoint     = ModelCheckpoint(self.save_path, monitor='loss', save_best_only=self.save_best_only)\n",
    "        reduce_lr      = ReduceLROnPlateau(monitor='loss', patience=2, factor=0.1)\n",
    "        logger         = CSVLogger(self.log_path + 'log.csv', append=True)\n",
    "        tensorboard    = TensorBoard(self.log_path, batch_size=batch_size)\n",
    "        \n",
    "        history = self.model.fit(x=obs, y=[actions, rewards], batch_size=batch_size, epochs=epochs, verbose=1,\n",
    "                       callbacks=[early_stopping, checkpoint, reduce_lr, logger, tensorboard],\n",
    "                       validation_split=0.15, shuffle=True, class_weight=class_weight, initial_epoch=initial_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels       = np.load(train_data_labels)\n",
    "observations = np.load(train_data_obs_map)\n",
    "rewards      = np.load(train_data_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = to_categorical(labels, num_classes=action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((553065, 6), (553065, 11, 11, 18), (553065,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape, observations.shape, rewards.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.15941164, 0.20397964, 0.190945  , 0.20009764, 0.20339382,\n",
       "       0.04217226], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(labels, axis=0) / np.sum(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.04551126, 0.81707501, 0.87285166, 0.83292671, 0.81942839,\n",
       "       3.9520451 ])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights = compute_class_weight('balanced', np.unique(np.argmax(labels, axis=1)), np.argmax(labels, axis=1))\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(action_space, model_path, log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 11, 11, 18)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 9, 9, 256)    41728       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 9, 9, 256)    1024        conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 9, 9, 256)    0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 20736)        0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "actions (Dense)                 (None, 6)            124422      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reward (Dense)                  (None, 1)            20737       flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 187,911\n",
      "Trainable params: 187,399\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "trainer.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 470105 samples, validate on 82960 samples\n",
      "Epoch 1/150\n",
      "470105/470105 [==============================] - 15s 32us/step - loss: 2.2064 - actions_loss: 1.7125 - reward_loss: 0.4939 - actions_acc: 0.3196 - reward_acc: 0.7518 - val_loss: 2.0711 - val_actions_loss: 1.5671 - val_reward_loss: 0.5041 - val_actions_acc: 0.3421 - val_reward_acc: 0.7480\n",
      "Epoch 2/150\n",
      "470105/470105 [==============================] - 12s 25us/step - loss: 1.9718 - actions_loss: 1.4806 - reward_loss: 0.4912 - actions_acc: 0.3683 - reward_acc: 0.7544 - val_loss: 2.0045 - val_actions_loss: 1.5005 - val_reward_loss: 0.5040 - val_actions_acc: 0.3617 - val_reward_acc: 0.7480\n",
      "Epoch 3/150\n",
      "470105/470105 [==============================] - 12s 25us/step - loss: 1.9262 - actions_loss: 1.4350 - reward_loss: 0.4912 - actions_acc: 0.3878 - reward_acc: 0.7544 - val_loss: 1.9640 - val_actions_loss: 1.4600 - val_reward_loss: 0.5040 - val_actions_acc: 0.3843 - val_reward_acc: 0.7480\n",
      "Epoch 4/150\n",
      "470105/470105 [==============================] - 12s 25us/step - loss: 1.8795 - actions_loss: 1.3883 - reward_loss: 0.4912 - actions_acc: 0.4088 - reward_acc: 0.7544 - val_loss: 1.9603 - val_actions_loss: 1.4563 - val_reward_loss: 0.5040 - val_actions_acc: 0.3837 - val_reward_acc: 0.7480\n",
      "Epoch 5/150\n",
      "470105/470105 [==============================] - 12s 25us/step - loss: 1.8262 - actions_loss: 1.3426 - reward_loss: 0.4836 - actions_acc: 0.4281 - reward_acc: 0.7577 - val_loss: 2.0452 - val_actions_loss: 1.5116 - val_reward_loss: 0.5336 - val_actions_acc: 0.3860 - val_reward_acc: 0.7301\n",
      "Epoch 6/150\n",
      "470105/470105 [==============================] - 12s 25us/step - loss: 1.7591 - actions_loss: 1.2975 - reward_loss: 0.4616 - actions_acc: 0.4459 - reward_acc: 0.7684 - val_loss: 1.9099 - val_actions_loss: 1.3779 - val_reward_loss: 0.5320 - val_actions_acc: 0.4090 - val_reward_acc: 0.7321\n",
      "Epoch 7/150\n",
      "470105/470105 [==============================] - 12s 25us/step - loss: 1.7057 - actions_loss: 1.2505 - reward_loss: 0.4552 - actions_acc: 0.4640 - reward_acc: 0.7720 - val_loss: 1.9282 - val_actions_loss: 1.3885 - val_reward_loss: 0.5397 - val_actions_acc: 0.4191 - val_reward_acc: 0.7288\n",
      "Epoch 8/150\n",
      "470105/470105 [==============================] - 12s 25us/step - loss: 1.6657 - actions_loss: 1.2139 - reward_loss: 0.4518 - actions_acc: 0.4784 - reward_acc: 0.7738 - val_loss: 2.0386 - val_actions_loss: 1.5076 - val_reward_loss: 0.5310 - val_actions_acc: 0.4049 - val_reward_acc: 0.7321\n",
      "Epoch 9/150\n",
      "470105/470105 [==============================] - 12s 25us/step - loss: 1.6197 - actions_loss: 1.1698 - reward_loss: 0.4499 - actions_acc: 0.4961 - reward_acc: 0.7748 - val_loss: 1.9692 - val_actions_loss: 1.4365 - val_reward_loss: 0.5327 - val_actions_acc: 0.4240 - val_reward_acc: 0.7313\n",
      "Epoch 10/150\n",
      "470105/470105 [==============================] - 12s 25us/step - loss: 1.5911 - actions_loss: 1.1422 - reward_loss: 0.4489 - actions_acc: 0.5054 - reward_acc: 0.7754 - val_loss: 1.8138 - val_actions_loss: 1.2801 - val_reward_loss: 0.5337 - val_actions_acc: 0.4730 - val_reward_acc: 0.7316\n",
      "Epoch 11/150\n",
      "470105/470105 [==============================] - 12s 25us/step - loss: 1.5643 - actions_loss: 1.1161 - reward_loss: 0.4482 - actions_acc: 0.5165 - reward_acc: 0.7754 - val_loss: 1.7393 - val_actions_loss: 1.1985 - val_reward_loss: 0.5408 - val_actions_acc: 0.4942 - val_reward_acc: 0.7281\n",
      "Epoch 12/150\n",
      "470105/470105 [==============================] - 12s 25us/step - loss: 1.5396 - actions_loss: 1.0930 - reward_loss: 0.4466 - actions_acc: 0.5260 - reward_acc: 0.7761 - val_loss: 1.7683 - val_actions_loss: 1.2333 - val_reward_loss: 0.5350 - val_actions_acc: 0.4842 - val_reward_acc: 0.7314\n",
      "Epoch 13/150\n",
      "470105/470105 [==============================] - 12s 25us/step - loss: 1.5153 - actions_loss: 1.0704 - reward_loss: 0.4449 - actions_acc: 0.5329 - reward_acc: 0.7771 - val_loss: 1.8635 - val_actions_loss: 1.3283 - val_reward_loss: 0.5352 - val_actions_acc: 0.4729 - val_reward_acc: 0.7316\n",
      "Epoch 14/150\n",
      "470105/470105 [==============================] - 12s 25us/step - loss: 1.5002 - actions_loss: 1.0563 - reward_loss: 0.4440 - actions_acc: 0.5384 - reward_acc: 0.7776 - val_loss: 1.8566 - val_actions_loss: 1.3171 - val_reward_loss: 0.5395 - val_actions_acc: 0.4765 - val_reward_acc: 0.7292\n",
      "Epoch 15/150\n",
      "470105/470105 [==============================] - 12s 26us/step - loss: 1.4907 - actions_loss: 1.0475 - reward_loss: 0.4432 - actions_acc: 0.5432 - reward_acc: 0.7781 - val_loss: 1.7551 - val_actions_loss: 1.2255 - val_reward_loss: 0.5296 - val_actions_acc: 0.4922 - val_reward_acc: 0.7336\n",
      "Epoch 16/150\n",
      "470105/470105 [==============================] - 12s 26us/step - loss: 1.4771 - actions_loss: 1.0341 - reward_loss: 0.4430 - actions_acc: 0.5478 - reward_acc: 0.7782 - val_loss: 1.7317 - val_actions_loss: 1.1918 - val_reward_loss: 0.5399 - val_actions_acc: 0.5103 - val_reward_acc: 0.7286\n",
      "Epoch 17/150\n",
      "470105/470105 [==============================] - 13s 27us/step - loss: 1.4636 - actions_loss: 1.0208 - reward_loss: 0.4429 - actions_acc: 0.5527 - reward_acc: 0.7782 - val_loss: 1.7053 - val_actions_loss: 1.1677 - val_reward_loss: 0.5376 - val_actions_acc: 0.5025 - val_reward_acc: 0.7298\n",
      "Epoch 18/150\n",
      "470105/470105 [==============================] - 13s 28us/step - loss: 1.4547 - actions_loss: 1.0125 - reward_loss: 0.4422 - actions_acc: 0.5552 - reward_acc: 0.7787 - val_loss: 1.8379 - val_actions_loss: 1.2994 - val_reward_loss: 0.5385 - val_actions_acc: 0.4682 - val_reward_acc: 0.7297\n",
      "Epoch 19/150\n",
      "470105/470105 [==============================] - 13s 28us/step - loss: 1.4470 - actions_loss: 1.0049 - reward_loss: 0.4421 - actions_acc: 0.5584 - reward_acc: 0.7788 - val_loss: 1.6663 - val_actions_loss: 1.1263 - val_reward_loss: 0.5399 - val_actions_acc: 0.5223 - val_reward_acc: 0.7286\n",
      "Epoch 20/150\n",
      "470105/470105 [==============================] - 13s 28us/step - loss: 1.4367 - actions_loss: 0.9947 - reward_loss: 0.4420 - actions_acc: 0.5616 - reward_acc: 0.7787 - val_loss: 1.8537 - val_actions_loss: 1.3190 - val_reward_loss: 0.5347 - val_actions_acc: 0.4784 - val_reward_acc: 0.7312\n",
      "Epoch 21/150\n",
      "470105/470105 [==============================] - 12s 26us/step - loss: 1.4339 - actions_loss: 0.9919 - reward_loss: 0.4420 - actions_acc: 0.5634 - reward_acc: 0.7788 - val_loss: 1.7327 - val_actions_loss: 1.1983 - val_reward_loss: 0.5343 - val_actions_acc: 0.5092 - val_reward_acc: 0.7315\n",
      "Epoch 22/150\n",
      "470105/470105 [==============================] - 13s 28us/step - loss: 1.4252 - actions_loss: 0.9843 - reward_loss: 0.4409 - actions_acc: 0.5656 - reward_acc: 0.7793 - val_loss: 1.7386 - val_actions_loss: 1.2015 - val_reward_loss: 0.5371 - val_actions_acc: 0.5083 - val_reward_acc: 0.7300\n",
      "Epoch 23/150\n",
      "470105/470105 [==============================] - 14s 29us/step - loss: 1.4160 - actions_loss: 0.9767 - reward_loss: 0.4393 - actions_acc: 0.5673 - reward_acc: 0.7801 - val_loss: 1.7972 - val_actions_loss: 1.2573 - val_reward_loss: 0.5400 - val_actions_acc: 0.4902 - val_reward_acc: 0.7283\n",
      "Epoch 24/150\n",
      "470105/470105 [==============================] - 13s 28us/step - loss: 1.4151 - actions_loss: 0.9767 - reward_loss: 0.4384 - actions_acc: 0.5686 - reward_acc: 0.7805 - val_loss: 1.7092 - val_actions_loss: 1.1677 - val_reward_loss: 0.5415 - val_actions_acc: 0.5173 - val_reward_acc: 0.7279\n",
      "Epoch 25/150\n",
      "470105/470105 [==============================] - 13s 28us/step - loss: 1.4105 - actions_loss: 0.9732 - reward_loss: 0.4374 - actions_acc: 0.5701 - reward_acc: 0.7810 - val_loss: 1.7218 - val_actions_loss: 1.1808 - val_reward_loss: 0.5410 - val_actions_acc: 0.5062 - val_reward_acc: 0.7280\n",
      "Epoch 26/150\n",
      "470105/470105 [==============================] - 12s 27us/step - loss: 1.4002 - actions_loss: 0.9631 - reward_loss: 0.4370 - actions_acc: 0.5735 - reward_acc: 0.7811 - val_loss: 1.7000 - val_actions_loss: 1.1599 - val_reward_loss: 0.5401 - val_actions_acc: 0.5190 - val_reward_acc: 0.7276\n",
      "Epoch 27/150\n",
      "470105/470105 [==============================] - 13s 27us/step - loss: 1.3943 - actions_loss: 0.9581 - reward_loss: 0.4362 - actions_acc: 0.5753 - reward_acc: 0.7816 - val_loss: 2.2387 - val_actions_loss: 1.7051 - val_reward_loss: 0.5336 - val_actions_acc: 0.3681 - val_reward_acc: 0.7313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/150\n",
      "470105/470105 [==============================] - 13s 27us/step - loss: 1.3920 - actions_loss: 0.9565 - reward_loss: 0.4355 - actions_acc: 0.5758 - reward_acc: 0.7820 - val_loss: 1.7044 - val_actions_loss: 1.1700 - val_reward_loss: 0.5344 - val_actions_acc: 0.5192 - val_reward_acc: 0.7308\n",
      "Epoch 29/150\n",
      "470105/470105 [==============================] - 14s 30us/step - loss: 1.3802 - actions_loss: 0.9468 - reward_loss: 0.4334 - actions_acc: 0.5794 - reward_acc: 0.7829 - val_loss: 1.8010 - val_actions_loss: 1.2677 - val_reward_loss: 0.5332 - val_actions_acc: 0.4751 - val_reward_acc: 0.7322\n",
      "Epoch 30/150\n",
      "470105/470105 [==============================] - 13s 27us/step - loss: 1.3813 - actions_loss: 0.9492 - reward_loss: 0.4321 - actions_acc: 0.5779 - reward_acc: 0.7836 - val_loss: 1.6752 - val_actions_loss: 1.1389 - val_reward_loss: 0.5363 - val_actions_acc: 0.5166 - val_reward_acc: 0.7301\n",
      "Epoch 31/150\n",
      "470105/470105 [==============================] - 14s 30us/step - loss: 1.3717 - actions_loss: 0.9408 - reward_loss: 0.4310 - actions_acc: 0.5826 - reward_acc: 0.7841 - val_loss: 1.6895 - val_actions_loss: 1.1440 - val_reward_loss: 0.5454 - val_actions_acc: 0.5272 - val_reward_acc: 0.7251\n",
      "Epoch 32/150\n",
      "470105/470105 [==============================] - 13s 27us/step - loss: 1.3715 - actions_loss: 0.9416 - reward_loss: 0.4300 - actions_acc: 0.5819 - reward_acc: 0.7846 - val_loss: 1.6837 - val_actions_loss: 1.1436 - val_reward_loss: 0.5401 - val_actions_acc: 0.5304 - val_reward_acc: 0.7284\n",
      "Epoch 33/150\n",
      "470105/470105 [==============================] - 13s 29us/step - loss: 1.3622 - actions_loss: 0.9333 - reward_loss: 0.4288 - actions_acc: 0.5836 - reward_acc: 0.7852 - val_loss: 1.9920 - val_actions_loss: 1.4604 - val_reward_loss: 0.5316 - val_actions_acc: 0.4894 - val_reward_acc: 0.7329\n",
      "Epoch 34/150\n",
      "470105/470105 [==============================] - 16s 33us/step - loss: 1.3605 - actions_loss: 0.9327 - reward_loss: 0.4278 - actions_acc: 0.5850 - reward_acc: 0.7857 - val_loss: 1.7519 - val_actions_loss: 1.2087 - val_reward_loss: 0.5432 - val_actions_acc: 0.5169 - val_reward_acc: 0.7270\n",
      "Epoch 35/150\n",
      "470105/470105 [==============================] - 14s 30us/step - loss: 1.3619 - actions_loss: 0.9353 - reward_loss: 0.4266 - actions_acc: 0.5843 - reward_acc: 0.7862 - val_loss: 1.6875 - val_actions_loss: 1.1453 - val_reward_loss: 0.5421 - val_actions_acc: 0.5241 - val_reward_acc: 0.7272\n",
      "Epoch 36/150\n",
      "470105/470105 [==============================] - 13s 27us/step - loss: 1.3609 - actions_loss: 0.9353 - reward_loss: 0.4256 - actions_acc: 0.5839 - reward_acc: 0.7868 - val_loss: 1.8064 - val_actions_loss: 1.2622 - val_reward_loss: 0.5442 - val_actions_acc: 0.4991 - val_reward_acc: 0.7257\n",
      "Epoch 37/150\n",
      "470105/470105 [==============================] - 13s 27us/step - loss: 1.2650 - actions_loss: 0.8408 - reward_loss: 0.4241 - actions_acc: 0.6216 - reward_acc: 0.7877 - val_loss: 1.5164 - val_actions_loss: 0.9792 - val_reward_loss: 0.5373 - val_actions_acc: 0.5631 - val_reward_acc: 0.7303\n",
      "Epoch 38/150\n",
      "470105/470105 [==============================] - 13s 27us/step - loss: 1.2554 - actions_loss: 0.8318 - reward_loss: 0.4236 - actions_acc: 0.6265 - reward_acc: 0.7881 - val_loss: 1.5234 - val_actions_loss: 0.9868 - val_reward_loss: 0.5366 - val_actions_acc: 0.5612 - val_reward_acc: 0.7307\n",
      "Epoch 39/150\n",
      "470105/470105 [==============================] - 13s 27us/step - loss: 1.2540 - actions_loss: 0.8306 - reward_loss: 0.4234 - actions_acc: 0.6271 - reward_acc: 0.7882 - val_loss: 1.5454 - val_actions_loss: 1.0078 - val_reward_loss: 0.5376 - val_actions_acc: 0.5575 - val_reward_acc: 0.7303\n",
      "Epoch 40/150\n",
      "470105/470105 [==============================] - 13s 28us/step - loss: 1.2530 - actions_loss: 0.8298 - reward_loss: 0.4232 - actions_acc: 0.6269 - reward_acc: 0.7883 - val_loss: 1.5359 - val_actions_loss: 0.9980 - val_reward_loss: 0.5379 - val_actions_acc: 0.5581 - val_reward_acc: 0.7301\n",
      "Epoch 41/150\n",
      "470105/470105 [==============================] - 14s 29us/step - loss: 1.2526 - actions_loss: 0.8295 - reward_loss: 0.4231 - actions_acc: 0.6274 - reward_acc: 0.7884 - val_loss: 1.5206 - val_actions_loss: 0.9821 - val_reward_loss: 0.5385 - val_actions_acc: 0.5591 - val_reward_acc: 0.7296\n",
      "Epoch 42/150\n",
      "470105/470105 [==============================] - 14s 29us/step - loss: 1.2527 - actions_loss: 0.8297 - reward_loss: 0.4231 - actions_acc: 0.6263 - reward_acc: 0.7884 - val_loss: 1.5240 - val_actions_loss: 0.9851 - val_reward_loss: 0.5388 - val_actions_acc: 0.5610 - val_reward_acc: 0.7295\n",
      "Epoch 43/150\n",
      "470105/470105 [==============================] - 14s 29us/step - loss: 1.2512 - actions_loss: 0.8282 - reward_loss: 0.4230 - actions_acc: 0.6274 - reward_acc: 0.7885 - val_loss: 1.5233 - val_actions_loss: 0.9854 - val_reward_loss: 0.5379 - val_actions_acc: 0.5633 - val_reward_acc: 0.7298\n",
      "Epoch 44/150\n",
      "470105/470105 [==============================] - 12s 26us/step - loss: 1.2507 - actions_loss: 0.8278 - reward_loss: 0.4230 - actions_acc: 0.6273 - reward_acc: 0.7885 - val_loss: 1.5286 - val_actions_loss: 0.9905 - val_reward_loss: 0.5380 - val_actions_acc: 0.5589 - val_reward_acc: 0.7299\n",
      "Epoch 45/150\n",
      "470105/470105 [==============================] - 13s 27us/step - loss: 1.2498 - actions_loss: 0.8269 - reward_loss: 0.4229 - actions_acc: 0.6278 - reward_acc: 0.7885 - val_loss: 1.5412 - val_actions_loss: 1.0026 - val_reward_loss: 0.5386 - val_actions_acc: 0.5591 - val_reward_acc: 0.7296\n",
      "Epoch 46/150\n",
      "470105/470105 [==============================] - 13s 27us/step - loss: 1.2492 - actions_loss: 0.8264 - reward_loss: 0.4228 - actions_acc: 0.6279 - reward_acc: 0.7886 - val_loss: 1.5206 - val_actions_loss: 0.9828 - val_reward_loss: 0.5378 - val_actions_acc: 0.5596 - val_reward_acc: 0.7300\n",
      "Epoch 47/150\n",
      "470105/470105 [==============================] - 13s 27us/step - loss: 1.2480 - actions_loss: 0.8252 - reward_loss: 0.4228 - actions_acc: 0.6282 - reward_acc: 0.7886 - val_loss: 1.5294 - val_actions_loss: 0.9918 - val_reward_loss: 0.5376 - val_actions_acc: 0.5620 - val_reward_acc: 0.7302\n",
      "Epoch 48/150\n",
      "470105/470105 [==============================] - 15s 33us/step - loss: 1.2473 - actions_loss: 0.8245 - reward_loss: 0.4228 - actions_acc: 0.6290 - reward_acc: 0.7886 - val_loss: 1.5174 - val_actions_loss: 0.9791 - val_reward_loss: 0.5383 - val_actions_acc: 0.5625 - val_reward_acc: 0.7297\n",
      "Epoch 49/150\n",
      "470105/470105 [==============================] - 14s 30us/step - loss: 1.2467 - actions_loss: 0.8240 - reward_loss: 0.4227 - actions_acc: 0.6285 - reward_acc: 0.7886 - val_loss: 1.5204 - val_actions_loss: 0.9821 - val_reward_loss: 0.5383 - val_actions_acc: 0.5621 - val_reward_acc: 0.7297\n",
      "Epoch 50/150\n",
      "470105/470105 [==============================] - 14s 30us/step - loss: 1.2463 - actions_loss: 0.8236 - reward_loss: 0.4227 - actions_acc: 0.6287 - reward_acc: 0.7886 - val_loss: 1.5561 - val_actions_loss: 1.0171 - val_reward_loss: 0.5390 - val_actions_acc: 0.5585 - val_reward_acc: 0.7294\n",
      "Epoch 51/150\n",
      "470105/470105 [==============================] - 14s 29us/step - loss: 1.2456 - actions_loss: 0.8229 - reward_loss: 0.4226 - actions_acc: 0.6288 - reward_acc: 0.7887 - val_loss: 1.5317 - val_actions_loss: 0.9938 - val_reward_loss: 0.5379 - val_actions_acc: 0.5582 - val_reward_acc: 0.7301\n",
      "Epoch 52/150\n",
      "470105/470105 [==============================] - 13s 27us/step - loss: 1.2443 - actions_loss: 0.8217 - reward_loss: 0.4225 - actions_acc: 0.6289 - reward_acc: 0.7887 - val_loss: 1.5603 - val_actions_loss: 1.0226 - val_reward_loss: 0.5377 - val_actions_acc: 0.5548 - val_reward_acc: 0.7301\n",
      "Epoch 53/150\n",
      "470105/470105 [==============================] - 13s 27us/step - loss: 1.2436 - actions_loss: 0.8212 - reward_loss: 0.4225 - actions_acc: 0.6300 - reward_acc: 0.7888 - val_loss: 1.5315 - val_actions_loss: 0.9933 - val_reward_loss: 0.5382 - val_actions_acc: 0.5611 - val_reward_acc: 0.7298\n",
      "Epoch 54/150\n",
      "470105/470105 [==============================] - 13s 27us/step - loss: 1.2430 - actions_loss: 0.8205 - reward_loss: 0.4224 - actions_acc: 0.6296 - reward_acc: 0.7888 - val_loss: 1.5485 - val_actions_loss: 1.0098 - val_reward_loss: 0.5387 - val_actions_acc: 0.5578 - val_reward_acc: 0.7294\n",
      "Epoch 55/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470105/470105 [==============================] - 13s 28us/step - loss: 1.2422 - actions_loss: 0.8198 - reward_loss: 0.4224 - actions_acc: 0.6297 - reward_acc: 0.7888 - val_loss: 1.5475 - val_actions_loss: 1.0083 - val_reward_loss: 0.5392 - val_actions_acc: 0.5541 - val_reward_acc: 0.7291\n",
      "Epoch 56/150\n",
      "470105/470105 [==============================] - 13s 29us/step - loss: 1.2412 - actions_loss: 0.8188 - reward_loss: 0.4223 - actions_acc: 0.6306 - reward_acc: 0.7888 - val_loss: 1.5245 - val_actions_loss: 0.9860 - val_reward_loss: 0.5386 - val_actions_acc: 0.5615 - val_reward_acc: 0.7295\n",
      "Epoch 57/150\n",
      "470105/470105 [==============================] - 13s 29us/step - loss: 1.2405 - actions_loss: 0.8182 - reward_loss: 0.4223 - actions_acc: 0.6304 - reward_acc: 0.7889 - val_loss: 1.5524 - val_actions_loss: 1.0150 - val_reward_loss: 0.5375 - val_actions_acc: 0.5520 - val_reward_acc: 0.7303\n",
      "Epoch 58/150\n",
      "470105/470105 [==============================] - 14s 29us/step - loss: 1.2401 - actions_loss: 0.8178 - reward_loss: 0.4223 - actions_acc: 0.6301 - reward_acc: 0.7889 - val_loss: 1.5569 - val_actions_loss: 1.0183 - val_reward_loss: 0.5386 - val_actions_acc: 0.5551 - val_reward_acc: 0.7295\n",
      "Epoch 59/150\n",
      "470105/470105 [==============================] - 14s 29us/step - loss: 1.2395 - actions_loss: 0.8173 - reward_loss: 0.4222 - actions_acc: 0.6310 - reward_acc: 0.7889 - val_loss: 1.5436 - val_actions_loss: 1.0050 - val_reward_loss: 0.5386 - val_actions_acc: 0.5617 - val_reward_acc: 0.7295\n",
      "Epoch 60/150\n",
      "470105/470105 [==============================] - 14s 29us/step - loss: 1.2377 - actions_loss: 0.8155 - reward_loss: 0.4222 - actions_acc: 0.6308 - reward_acc: 0.7889 - val_loss: 1.5239 - val_actions_loss: 0.9857 - val_reward_loss: 0.5382 - val_actions_acc: 0.5643 - val_reward_acc: 0.7296\n",
      "Epoch 61/150\n",
      "470105/470105 [==============================] - 14s 29us/step - loss: 1.2379 - actions_loss: 0.8158 - reward_loss: 0.4221 - actions_acc: 0.6319 - reward_acc: 0.7890 - val_loss: 1.5463 - val_actions_loss: 1.0084 - val_reward_loss: 0.5378 - val_actions_acc: 0.5551 - val_reward_acc: 0.7297\n",
      "Epoch 62/150\n",
      "147456/470105 [========>.....................] - ETA: 8s - loss: 1.2366 - actions_loss: 0.8140 - reward_loss: 0.4227 - actions_acc: 0.6326 - reward_acc: 0.7886"
     ]
    }
   ],
   "source": [
    "trainer.train(observations, labels, rewards, batch_size=batch_size, \n",
    "              epochs=epochs, early_stopping=early_stopping, class_weight=[class_weights, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"finish\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
