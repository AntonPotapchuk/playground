{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger, TensorBoard\n",
    "from keras.layers import Input, Dense, Flatten, Convolution2D, BatchNormalization, Activation, Add\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "from pommerman.constants import BOARD_SIZE\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2048\n",
    "epochs = 150\n",
    "early_stopping = 5\n",
    "action_space = 6\n",
    "\n",
    "log_path = './supervised_learning/logs/1cnn/'\n",
    "model_path = './supervised_learning/model/1cnn/model.h4'\n",
    "\n",
    "train_data_path    = './dataset/'\n",
    "train_data_labels  = os.path.join(train_data_path, 'labels.npy')\n",
    "train_data_reward  = os.path.join(train_data_path, 'reward.npy')\n",
    "train_data_obs_map = os.path.join(train_data_path, 'obs_map.npy')\n",
    "\n",
    "if not os.path.isdir(train_data_path):\n",
    "    os.makedirs(train_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, actions, save_path, log_path, save_best_only=True, seed=0):\n",
    "        K.clear_session()\n",
    "        self.log_path = log_path\n",
    "        self.save_path = save_path\n",
    "        self.actions = actions\n",
    "        self.save_best_only = save_best_only\n",
    "        \n",
    "        # Create model\n",
    "        self.model = self.create_model(actions)\n",
    "        # Load model if exists\n",
    "        if not os.path.isdir(os.path.dirname(save_path)):\n",
    "            os.makedirs(os.path.dirname(save_path))            \n",
    "        if os.path.isfile(self.save_path):\n",
    "            try:\n",
    "                print(\"Trying to load model\")\n",
    "                self.model.load_weights(self.save_path)\n",
    "                print(\"Model was loaded successful\")\n",
    "            except:\n",
    "                print(\"Model load failed\")\n",
    "        \n",
    "    def get_res_block(self, input):\n",
    "        # Res block 1        \n",
    "        x = Convolution2D(256, 3, padding='same')(input)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Convolution2D(256, 3, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Add()([input, x])\n",
    "        x = Activation('relu')(x)\n",
    "        return x\n",
    "        \n",
    "    def create_model(self, actions, input_shape=(11, 11, 18,)):\n",
    "        inp = Input(input_shape)\n",
    "        # 5 Convs -> receptive field 11\n",
    "        x = Convolution2D(256, 3, padding='valid')(inp)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('elu')(x)\n",
    "        x = Flatten()(x)\n",
    "        # Output\n",
    "        probs  = Dense(actions, activation='softmax', name='actions')(x)\n",
    "        reward = Dense(1, activation='tanh', name='reward')(x)\n",
    "        \n",
    "        model = Model(inputs = inp, outputs=[probs, reward])\n",
    "        model.compile(optimizer='adam', loss=['categorical_crossentropy', 'mae'], metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def train(self, obs, actions, rewards, batch_size=16384, epochs=100,\n",
    "              early_stopping = 10, class_weight=None, initial_epoch=0):\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='loss', patience=early_stopping)\n",
    "        checkpoint     = ModelCheckpoint(self.save_path, monitor='loss', save_best_only=self.save_best_only)\n",
    "        reduce_lr      = ReduceLROnPlateau(monitor='loss', patience=2, factor=0.1)\n",
    "        logger         = CSVLogger(self.log_path + 'log.csv', append=True)\n",
    "        tensorboard    = TensorBoard(self.log_path, batch_size=batch_size)\n",
    "        \n",
    "        history = self.model.fit(x=obs, y=[actions, rewards], batch_size=batch_size, epochs=epochs, verbose=1,\n",
    "                       callbacks=[early_stopping, checkpoint, reduce_lr, logger, tensorboard],\n",
    "                       validation_split=0.15, shuffle=True, class_weight=class_weight, initial_epoch=initial_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels       = np.load(train_data_labels)\n",
    "observations = np.load(train_data_obs_map)\n",
    "rewards      = np.load(train_data_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = to_categorical(labels, num_classes=action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((553065, 6), (553065, 11, 11, 18), (553065,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape, observations.shape, rewards.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.15941164, 0.20397964, 0.190945  , 0.20009764, 0.20339382,\n",
       "       0.04217226], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(labels, axis=0) / np.sum(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.04551126, 0.81707501, 0.87285166, 0.83292671, 0.81942839,\n",
       "       3.9520451 ])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights = compute_class_weight('balanced', np.unique(np.argmax(labels, axis=1)), np.argmax(labels, axis=1))\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(action_space, model_path, log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 11, 11, 18)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 9, 9, 256)    41728       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 9, 9, 256)    1024        conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 9, 9, 256)    0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 20736)        0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "actions (Dense)                 (None, 6)            124422      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reward (Dense)                  (None, 1)            20737       flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 187,911\n",
      "Trainable params: 187,399\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "trainer.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 470105 samples, validate on 82960 samples\n",
      "Epoch 1/150\n",
      "470105/470105 [==============================] - 15s 32us/step - loss: 2.2064 - actions_loss: 1.7125 - reward_loss: 0.4939 - actions_acc: 0.3196 - reward_acc: 0.7518 - val_loss: 2.0711 - val_actions_loss: 1.5671 - val_reward_loss: 0.5041 - val_actions_acc: 0.3421 - val_reward_acc: 0.7480\n",
      "Epoch 2/150\n",
      "470105/470105 [==============================] - 12s 25us/step - loss: 1.9718 - actions_loss: 1.4806 - reward_loss: 0.4912 - actions_acc: 0.3683 - reward_acc: 0.7544 - val_loss: 2.0045 - val_actions_loss: 1.5005 - val_reward_loss: 0.5040 - val_actions_acc: 0.3617 - val_reward_acc: 0.7480\n",
      "Epoch 3/150\n",
      "470105/470105 [==============================] - 12s 25us/step - loss: 1.9262 - actions_loss: 1.4350 - reward_loss: 0.4912 - actions_acc: 0.3878 - reward_acc: 0.7544 - val_loss: 1.9640 - val_actions_loss: 1.4600 - val_reward_loss: 0.5040 - val_actions_acc: 0.3843 - val_reward_acc: 0.7480\n",
      "Epoch 4/150\n",
      "470105/470105 [==============================] - 12s 25us/step - loss: 1.8795 - actions_loss: 1.3883 - reward_loss: 0.4912 - actions_acc: 0.4088 - reward_acc: 0.7544 - val_loss: 1.9603 - val_actions_loss: 1.4563 - val_reward_loss: 0.5040 - val_actions_acc: 0.3837 - val_reward_acc: 0.7480\n",
      "Epoch 5/150\n",
      "470105/470105 [==============================] - 12s 25us/step - loss: 1.8262 - actions_loss: 1.3426 - reward_loss: 0.4836 - actions_acc: 0.4281 - reward_acc: 0.7577 - val_loss: 2.0452 - val_actions_loss: 1.5116 - val_reward_loss: 0.5336 - val_actions_acc: 0.3860 - val_reward_acc: 0.7301\n",
      "Epoch 6/150\n",
      "470105/470105 [==============================] - 12s 25us/step - loss: 1.7591 - actions_loss: 1.2975 - reward_loss: 0.4616 - actions_acc: 0.4459 - reward_acc: 0.7684 - val_loss: 1.9099 - val_actions_loss: 1.3779 - val_reward_loss: 0.5320 - val_actions_acc: 0.4090 - val_reward_acc: 0.7321\n",
      "Epoch 7/150\n",
      "470105/470105 [==============================] - 12s 25us/step - loss: 1.7057 - actions_loss: 1.2505 - reward_loss: 0.4552 - actions_acc: 0.4640 - reward_acc: 0.7720 - val_loss: 1.9282 - val_actions_loss: 1.3885 - val_reward_loss: 0.5397 - val_actions_acc: 0.4191 - val_reward_acc: 0.7288\n",
      "Epoch 8/150\n",
      "470105/470105 [==============================] - 12s 25us/step - loss: 1.6657 - actions_loss: 1.2139 - reward_loss: 0.4518 - actions_acc: 0.4784 - reward_acc: 0.7738 - val_loss: 2.0386 - val_actions_loss: 1.5076 - val_reward_loss: 0.5310 - val_actions_acc: 0.4049 - val_reward_acc: 0.7321\n",
      "Epoch 9/150\n",
      "470105/470105 [==============================] - 12s 25us/step - loss: 1.6197 - actions_loss: 1.1698 - reward_loss: 0.4499 - actions_acc: 0.4961 - reward_acc: 0.7748 - val_loss: 1.9692 - val_actions_loss: 1.4365 - val_reward_loss: 0.5327 - val_actions_acc: 0.4240 - val_reward_acc: 0.7313\n",
      "Epoch 10/150\n",
      "470105/470105 [==============================] - 12s 25us/step - loss: 1.5911 - actions_loss: 1.1422 - reward_loss: 0.4489 - actions_acc: 0.5054 - reward_acc: 0.7754 - val_loss: 1.8138 - val_actions_loss: 1.2801 - val_reward_loss: 0.5337 - val_actions_acc: 0.4730 - val_reward_acc: 0.7316\n",
      "Epoch 11/150\n",
      "470105/470105 [==============================] - 12s 25us/step - loss: 1.5643 - actions_loss: 1.1161 - reward_loss: 0.4482 - actions_acc: 0.5165 - reward_acc: 0.7754 - val_loss: 1.7393 - val_actions_loss: 1.1985 - val_reward_loss: 0.5408 - val_actions_acc: 0.4942 - val_reward_acc: 0.7281\n",
      "Epoch 12/150\n",
      "470105/470105 [==============================] - 12s 25us/step - loss: 1.5396 - actions_loss: 1.0930 - reward_loss: 0.4466 - actions_acc: 0.5260 - reward_acc: 0.7761 - val_loss: 1.7683 - val_actions_loss: 1.2333 - val_reward_loss: 0.5350 - val_actions_acc: 0.4842 - val_reward_acc: 0.7314\n",
      "Epoch 13/150\n",
      "470105/470105 [==============================] - 12s 25us/step - loss: 1.5153 - actions_loss: 1.0704 - reward_loss: 0.4449 - actions_acc: 0.5329 - reward_acc: 0.7771 - val_loss: 1.8635 - val_actions_loss: 1.3283 - val_reward_loss: 0.5352 - val_actions_acc: 0.4729 - val_reward_acc: 0.7316\n",
      "Epoch 14/150\n",
      "470105/470105 [==============================] - 12s 25us/step - loss: 1.5002 - actions_loss: 1.0563 - reward_loss: 0.4440 - actions_acc: 0.5384 - reward_acc: 0.7776 - val_loss: 1.8566 - val_actions_loss: 1.3171 - val_reward_loss: 0.5395 - val_actions_acc: 0.4765 - val_reward_acc: 0.7292\n",
      "Epoch 15/150\n",
      "470105/470105 [==============================] - 12s 26us/step - loss: 1.4907 - actions_loss: 1.0475 - reward_loss: 0.4432 - actions_acc: 0.5432 - reward_acc: 0.7781 - val_loss: 1.7551 - val_actions_loss: 1.2255 - val_reward_loss: 0.5296 - val_actions_acc: 0.4922 - val_reward_acc: 0.7336\n",
      "Epoch 16/150\n",
      "470105/470105 [==============================] - 12s 26us/step - loss: 1.4771 - actions_loss: 1.0341 - reward_loss: 0.4430 - actions_acc: 0.5478 - reward_acc: 0.7782 - val_loss: 1.7317 - val_actions_loss: 1.1918 - val_reward_loss: 0.5399 - val_actions_acc: 0.5103 - val_reward_acc: 0.7286\n",
      "Epoch 17/150\n",
      "470105/470105 [==============================] - 13s 27us/step - loss: 1.4636 - actions_loss: 1.0208 - reward_loss: 0.4429 - actions_acc: 0.5527 - reward_acc: 0.7782 - val_loss: 1.7053 - val_actions_loss: 1.1677 - val_reward_loss: 0.5376 - val_actions_acc: 0.5025 - val_reward_acc: 0.7298\n",
      "Epoch 18/150\n",
      "470105/470105 [==============================] - 13s 28us/step - loss: 1.4547 - actions_loss: 1.0125 - reward_loss: 0.4422 - actions_acc: 0.5552 - reward_acc: 0.7787 - val_loss: 1.8379 - val_actions_loss: 1.2994 - val_reward_loss: 0.5385 - val_actions_acc: 0.4682 - val_reward_acc: 0.7297\n",
      "Epoch 19/150\n",
      "470105/470105 [==============================] - 13s 28us/step - loss: 1.4470 - actions_loss: 1.0049 - reward_loss: 0.4421 - actions_acc: 0.5584 - reward_acc: 0.7788 - val_loss: 1.6663 - val_actions_loss: 1.1263 - val_reward_loss: 0.5399 - val_actions_acc: 0.5223 - val_reward_acc: 0.7286\n",
      "Epoch 20/150\n",
      "470105/470105 [==============================] - 13s 28us/step - loss: 1.4367 - actions_loss: 0.9947 - reward_loss: 0.4420 - actions_acc: 0.5616 - reward_acc: 0.7787 - val_loss: 1.8537 - val_actions_loss: 1.3190 - val_reward_loss: 0.5347 - val_actions_acc: 0.4784 - val_reward_acc: 0.7312\n",
      "Epoch 21/150\n",
      "470105/470105 [==============================] - 12s 26us/step - loss: 1.4339 - actions_loss: 0.9919 - reward_loss: 0.4420 - actions_acc: 0.5634 - reward_acc: 0.7788 - val_loss: 1.7327 - val_actions_loss: 1.1983 - val_reward_loss: 0.5343 - val_actions_acc: 0.5092 - val_reward_acc: 0.7315\n",
      "Epoch 22/150\n",
      "470105/470105 [==============================] - 13s 28us/step - loss: 1.4252 - actions_loss: 0.9843 - reward_loss: 0.4409 - actions_acc: 0.5656 - reward_acc: 0.7793 - val_loss: 1.7386 - val_actions_loss: 1.2015 - val_reward_loss: 0.5371 - val_actions_acc: 0.5083 - val_reward_acc: 0.7300\n",
      "Epoch 23/150\n",
      "470105/470105 [==============================] - 14s 29us/step - loss: 1.4160 - actions_loss: 0.9767 - reward_loss: 0.4393 - actions_acc: 0.5673 - reward_acc: 0.7801 - val_loss: 1.7972 - val_actions_loss: 1.2573 - val_reward_loss: 0.5400 - val_actions_acc: 0.4902 - val_reward_acc: 0.7283\n",
      "Epoch 24/150\n",
      "470105/470105 [==============================] - 13s 28us/step - loss: 1.4151 - actions_loss: 0.9767 - reward_loss: 0.4384 - actions_acc: 0.5686 - reward_acc: 0.7805 - val_loss: 1.7092 - val_actions_loss: 1.1677 - val_reward_loss: 0.5415 - val_actions_acc: 0.5173 - val_reward_acc: 0.7279\n",
      "Epoch 25/150\n",
      "470105/470105 [==============================] - 13s 28us/step - loss: 1.4105 - actions_loss: 0.9732 - reward_loss: 0.4374 - actions_acc: 0.5701 - reward_acc: 0.7810 - val_loss: 1.7218 - val_actions_loss: 1.1808 - val_reward_loss: 0.5410 - val_actions_acc: 0.5062 - val_reward_acc: 0.7280\n",
      "Epoch 26/150\n",
      "470105/470105 [==============================] - 12s 27us/step - loss: 1.4002 - actions_loss: 0.9631 - reward_loss: 0.4370 - actions_acc: 0.5735 - reward_acc: 0.7811 - val_loss: 1.7000 - val_actions_loss: 1.1599 - val_reward_loss: 0.5401 - val_actions_acc: 0.5190 - val_reward_acc: 0.7276\n",
      "Epoch 27/150\n",
      "470105/470105 [==============================] - 13s 27us/step - loss: 1.3943 - actions_loss: 0.9581 - reward_loss: 0.4362 - actions_acc: 0.5753 - reward_acc: 0.7816 - val_loss: 2.2387 - val_actions_loss: 1.7051 - val_reward_loss: 0.5336 - val_actions_acc: 0.3681 - val_reward_acc: 0.7313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/150\n",
      "470105/470105 [==============================] - 13s 27us/step - loss: 1.3920 - actions_loss: 0.9565 - reward_loss: 0.4355 - actions_acc: 0.5758 - reward_acc: 0.7820 - val_loss: 1.7044 - val_actions_loss: 1.1700 - val_reward_loss: 0.5344 - val_actions_acc: 0.5192 - val_reward_acc: 0.7308\n",
      "Epoch 29/150\n",
      "470105/470105 [==============================] - 14s 30us/step - loss: 1.3802 - actions_loss: 0.9468 - reward_loss: 0.4334 - actions_acc: 0.5794 - reward_acc: 0.7829 - val_loss: 1.8010 - val_actions_loss: 1.2677 - val_reward_loss: 0.5332 - val_actions_acc: 0.4751 - val_reward_acc: 0.7322\n",
      "Epoch 30/150\n",
      "470105/470105 [==============================] - 13s 27us/step - loss: 1.3813 - actions_loss: 0.9492 - reward_loss: 0.4321 - actions_acc: 0.5779 - reward_acc: 0.7836 - val_loss: 1.6752 - val_actions_loss: 1.1389 - val_reward_loss: 0.5363 - val_actions_acc: 0.5166 - val_reward_acc: 0.7301\n",
      "Epoch 31/150\n",
      "470105/470105 [==============================] - 14s 30us/step - loss: 1.3717 - actions_loss: 0.9408 - reward_loss: 0.4310 - actions_acc: 0.5826 - reward_acc: 0.7841 - val_loss: 1.6895 - val_actions_loss: 1.1440 - val_reward_loss: 0.5454 - val_actions_acc: 0.5272 - val_reward_acc: 0.7251\n",
      "Epoch 32/150\n",
      "470105/470105 [==============================] - 13s 27us/step - loss: 1.3715 - actions_loss: 0.9416 - reward_loss: 0.4300 - actions_acc: 0.5819 - reward_acc: 0.7846 - val_loss: 1.6837 - val_actions_loss: 1.1436 - val_reward_loss: 0.5401 - val_actions_acc: 0.5304 - val_reward_acc: 0.7284\n",
      "Epoch 33/150\n",
      "470105/470105 [==============================] - 13s 29us/step - loss: 1.3622 - actions_loss: 0.9333 - reward_loss: 0.4288 - actions_acc: 0.5836 - reward_acc: 0.7852 - val_loss: 1.9920 - val_actions_loss: 1.4604 - val_reward_loss: 0.5316 - val_actions_acc: 0.4894 - val_reward_acc: 0.7329\n",
      "Epoch 34/150\n",
      "470105/470105 [==============================] - 16s 33us/step - loss: 1.3605 - actions_loss: 0.9327 - reward_loss: 0.4278 - actions_acc: 0.5850 - reward_acc: 0.7857 - val_loss: 1.7519 - val_actions_loss: 1.2087 - val_reward_loss: 0.5432 - val_actions_acc: 0.5169 - val_reward_acc: 0.7270\n",
      "Epoch 35/150\n",
      "470105/470105 [==============================] - 14s 30us/step - loss: 1.3619 - actions_loss: 0.9353 - reward_loss: 0.4266 - actions_acc: 0.5843 - reward_acc: 0.7862 - val_loss: 1.6875 - val_actions_loss: 1.1453 - val_reward_loss: 0.5421 - val_actions_acc: 0.5241 - val_reward_acc: 0.7272\n",
      "Epoch 36/150\n",
      "470105/470105 [==============================] - 13s 27us/step - loss: 1.3609 - actions_loss: 0.9353 - reward_loss: 0.4256 - actions_acc: 0.5839 - reward_acc: 0.7868 - val_loss: 1.8064 - val_actions_loss: 1.2622 - val_reward_loss: 0.5442 - val_actions_acc: 0.4991 - val_reward_acc: 0.7257\n",
      "Epoch 37/150\n",
      "470105/470105 [==============================] - 13s 27us/step - loss: 1.2650 - actions_loss: 0.8408 - reward_loss: 0.4241 - actions_acc: 0.6216 - reward_acc: 0.7877 - val_loss: 1.5164 - val_actions_loss: 0.9792 - val_reward_loss: 0.5373 - val_actions_acc: 0.5631 - val_reward_acc: 0.7303\n",
      "Epoch 38/150\n",
      "470105/470105 [==============================] - 13s 27us/step - loss: 1.2554 - actions_loss: 0.8318 - reward_loss: 0.4236 - actions_acc: 0.6265 - reward_acc: 0.7881 - val_loss: 1.5234 - val_actions_loss: 0.9868 - val_reward_loss: 0.5366 - val_actions_acc: 0.5612 - val_reward_acc: 0.7307\n",
      "Epoch 39/150\n",
      "470105/470105 [==============================] - 13s 27us/step - loss: 1.2540 - actions_loss: 0.8306 - reward_loss: 0.4234 - actions_acc: 0.6271 - reward_acc: 0.7882 - val_loss: 1.5454 - val_actions_loss: 1.0078 - val_reward_loss: 0.5376 - val_actions_acc: 0.5575 - val_reward_acc: 0.7303\n",
      "Epoch 40/150\n",
      "470105/470105 [==============================] - 13s 28us/step - loss: 1.2530 - actions_loss: 0.8298 - reward_loss: 0.4232 - actions_acc: 0.6269 - reward_acc: 0.7883 - val_loss: 1.5359 - val_actions_loss: 0.9980 - val_reward_loss: 0.5379 - val_actions_acc: 0.5581 - val_reward_acc: 0.7301\n",
      "Epoch 41/150\n",
      "470105/470105 [==============================] - 14s 29us/step - loss: 1.2526 - actions_loss: 0.8295 - reward_loss: 0.4231 - actions_acc: 0.6274 - reward_acc: 0.7884 - val_loss: 1.5206 - val_actions_loss: 0.9821 - val_reward_loss: 0.5385 - val_actions_acc: 0.5591 - val_reward_acc: 0.7296\n",
      "Epoch 42/150\n",
      "470105/470105 [==============================] - 14s 29us/step - loss: 1.2527 - actions_loss: 0.8297 - reward_loss: 0.4231 - actions_acc: 0.6263 - reward_acc: 0.7884 - val_loss: 1.5240 - val_actions_loss: 0.9851 - val_reward_loss: 0.5388 - val_actions_acc: 0.5610 - val_reward_acc: 0.7295\n",
      "Epoch 43/150\n",
      "470105/470105 [==============================] - 14s 29us/step - loss: 1.2512 - actions_loss: 0.8282 - reward_loss: 0.4230 - actions_acc: 0.6274 - reward_acc: 0.7885 - val_loss: 1.5233 - val_actions_loss: 0.9854 - val_reward_loss: 0.5379 - val_actions_acc: 0.5633 - val_reward_acc: 0.7298\n",
      "Epoch 44/150\n",
      "470105/470105 [==============================] - 12s 26us/step - loss: 1.2507 - actions_loss: 0.8278 - reward_loss: 0.4230 - actions_acc: 0.6273 - reward_acc: 0.7885 - val_loss: 1.5286 - val_actions_loss: 0.9905 - val_reward_loss: 0.5380 - val_actions_acc: 0.5589 - val_reward_acc: 0.7299\n",
      "Epoch 45/150\n",
      "470105/470105 [==============================] - 13s 27us/step - loss: 1.2498 - actions_loss: 0.8269 - reward_loss: 0.4229 - actions_acc: 0.6278 - reward_acc: 0.7885 - val_loss: 1.5412 - val_actions_loss: 1.0026 - val_reward_loss: 0.5386 - val_actions_acc: 0.5591 - val_reward_acc: 0.7296\n",
      "Epoch 46/150\n",
      "470105/470105 [==============================] - 13s 27us/step - loss: 1.2492 - actions_loss: 0.8264 - reward_loss: 0.4228 - actions_acc: 0.6279 - reward_acc: 0.7886 - val_loss: 1.5206 - val_actions_loss: 0.9828 - val_reward_loss: 0.5378 - val_actions_acc: 0.5596 - val_reward_acc: 0.7300\n",
      "Epoch 47/150\n",
      "470105/470105 [==============================] - 13s 27us/step - loss: 1.2480 - actions_loss: 0.8252 - reward_loss: 0.4228 - actions_acc: 0.6282 - reward_acc: 0.7886 - val_loss: 1.5294 - val_actions_loss: 0.9918 - val_reward_loss: 0.5376 - val_actions_acc: 0.5620 - val_reward_acc: 0.7302\n",
      "Epoch 48/150\n",
      "470105/470105 [==============================] - 15s 33us/step - loss: 1.2473 - actions_loss: 0.8245 - reward_loss: 0.4228 - actions_acc: 0.6290 - reward_acc: 0.7886 - val_loss: 1.5174 - val_actions_loss: 0.9791 - val_reward_loss: 0.5383 - val_actions_acc: 0.5625 - val_reward_acc: 0.7297\n",
      "Epoch 49/150\n",
      "470105/470105 [==============================] - 14s 30us/step - loss: 1.2467 - actions_loss: 0.8240 - reward_loss: 0.4227 - actions_acc: 0.6285 - reward_acc: 0.7886 - val_loss: 1.5204 - val_actions_loss: 0.9821 - val_reward_loss: 0.5383 - val_actions_acc: 0.5621 - val_reward_acc: 0.7297\n",
      "Epoch 50/150\n",
      "470105/470105 [==============================] - 14s 30us/step - loss: 1.2463 - actions_loss: 0.8236 - reward_loss: 0.4227 - actions_acc: 0.6287 - reward_acc: 0.7886 - val_loss: 1.5561 - val_actions_loss: 1.0171 - val_reward_loss: 0.5390 - val_actions_acc: 0.5585 - val_reward_acc: 0.7294\n",
      "Epoch 51/150\n",
      "470105/470105 [==============================] - 14s 29us/step - loss: 1.2456 - actions_loss: 0.8229 - reward_loss: 0.4226 - actions_acc: 0.6288 - reward_acc: 0.7887 - val_loss: 1.5317 - val_actions_loss: 0.9938 - val_reward_loss: 0.5379 - val_actions_acc: 0.5582 - val_reward_acc: 0.7301\n",
      "Epoch 52/150\n",
      "470105/470105 [==============================] - 13s 27us/step - loss: 1.2443 - actions_loss: 0.8217 - reward_loss: 0.4225 - actions_acc: 0.6289 - reward_acc: 0.7887 - val_loss: 1.5603 - val_actions_loss: 1.0226 - val_reward_loss: 0.5377 - val_actions_acc: 0.5548 - val_reward_acc: 0.7301\n",
      "Epoch 53/150\n",
      "470105/470105 [==============================] - 13s 27us/step - loss: 1.2436 - actions_loss: 0.8212 - reward_loss: 0.4225 - actions_acc: 0.6300 - reward_acc: 0.7888 - val_loss: 1.5315 - val_actions_loss: 0.9933 - val_reward_loss: 0.5382 - val_actions_acc: 0.5611 - val_reward_acc: 0.7298\n",
      "Epoch 54/150\n",
      "470105/470105 [==============================] - 13s 27us/step - loss: 1.2430 - actions_loss: 0.8205 - reward_loss: 0.4224 - actions_acc: 0.6296 - reward_acc: 0.7888 - val_loss: 1.5485 - val_actions_loss: 1.0098 - val_reward_loss: 0.5387 - val_actions_acc: 0.5578 - val_reward_acc: 0.7294\n",
      "Epoch 55/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470105/470105 [==============================] - 13s 28us/step - loss: 1.2422 - actions_loss: 0.8198 - reward_loss: 0.4224 - actions_acc: 0.6297 - reward_acc: 0.7888 - val_loss: 1.5475 - val_actions_loss: 1.0083 - val_reward_loss: 0.5392 - val_actions_acc: 0.5541 - val_reward_acc: 0.7291\n",
      "Epoch 56/150\n",
      "470105/470105 [==============================] - 13s 29us/step - loss: 1.2412 - actions_loss: 0.8188 - reward_loss: 0.4223 - actions_acc: 0.6306 - reward_acc: 0.7888 - val_loss: 1.5245 - val_actions_loss: 0.9860 - val_reward_loss: 0.5386 - val_actions_acc: 0.5615 - val_reward_acc: 0.7295\n",
      "Epoch 57/150\n",
      "470105/470105 [==============================] - 13s 29us/step - loss: 1.2405 - actions_loss: 0.8182 - reward_loss: 0.4223 - actions_acc: 0.6304 - reward_acc: 0.7889 - val_loss: 1.5524 - val_actions_loss: 1.0150 - val_reward_loss: 0.5375 - val_actions_acc: 0.5520 - val_reward_acc: 0.7303\n",
      "Epoch 58/150\n",
      "470105/470105 [==============================] - 14s 29us/step - loss: 1.2401 - actions_loss: 0.8178 - reward_loss: 0.4223 - actions_acc: 0.6301 - reward_acc: 0.7889 - val_loss: 1.5569 - val_actions_loss: 1.0183 - val_reward_loss: 0.5386 - val_actions_acc: 0.5551 - val_reward_acc: 0.7295\n",
      "Epoch 59/150\n",
      "470105/470105 [==============================] - 14s 29us/step - loss: 1.2395 - actions_loss: 0.8173 - reward_loss: 0.4222 - actions_acc: 0.6310 - reward_acc: 0.7889 - val_loss: 1.5436 - val_actions_loss: 1.0050 - val_reward_loss: 0.5386 - val_actions_acc: 0.5617 - val_reward_acc: 0.7295\n",
      "Epoch 60/150\n",
      "470105/470105 [==============================] - 14s 29us/step - loss: 1.2377 - actions_loss: 0.8155 - reward_loss: 0.4222 - actions_acc: 0.6308 - reward_acc: 0.7889 - val_loss: 1.5239 - val_actions_loss: 0.9857 - val_reward_loss: 0.5382 - val_actions_acc: 0.5643 - val_reward_acc: 0.7296\n",
      "Epoch 61/150\n",
      "470105/470105 [==============================] - 14s 29us/step - loss: 1.2379 - actions_loss: 0.8158 - reward_loss: 0.4221 - actions_acc: 0.6319 - reward_acc: 0.7890 - val_loss: 1.5463 - val_actions_loss: 1.0084 - val_reward_loss: 0.5378 - val_actions_acc: 0.5551 - val_reward_acc: 0.7297\n",
      "Epoch 62/150\n",
      "470105/470105 [==============================] - 14s 29us/step - loss: 1.2365 - actions_loss: 0.8144 - reward_loss: 0.4221 - actions_acc: 0.6318 - reward_acc: 0.7890 - val_loss: 1.5269 - val_actions_loss: 0.9887 - val_reward_loss: 0.5382 - val_actions_acc: 0.5632 - val_reward_acc: 0.7296\n",
      "Epoch 63/150\n",
      "470105/470105 [==============================] - 13s 28us/step - loss: 1.2358 - actions_loss: 0.8137 - reward_loss: 0.4220 - actions_acc: 0.6324 - reward_acc: 0.7890 - val_loss: 1.5293 - val_actions_loss: 0.9906 - val_reward_loss: 0.5387 - val_actions_acc: 0.5653 - val_reward_acc: 0.7292\n",
      "Epoch 64/150\n",
      "470105/470105 [==============================] - 13s 29us/step - loss: 1.2356 - actions_loss: 0.8136 - reward_loss: 0.4220 - actions_acc: 0.6319 - reward_acc: 0.7890 - val_loss: 1.5501 - val_actions_loss: 1.0121 - val_reward_loss: 0.5381 - val_actions_acc: 0.5547 - val_reward_acc: 0.7299\n",
      "Epoch 65/150\n",
      "470105/470105 [==============================] - 14s 29us/step - loss: 1.2351 - actions_loss: 0.8132 - reward_loss: 0.4219 - actions_acc: 0.6320 - reward_acc: 0.7891 - val_loss: 1.5320 - val_actions_loss: 0.9931 - val_reward_loss: 0.5389 - val_actions_acc: 0.5625 - val_reward_acc: 0.7291\n",
      "Epoch 66/150\n",
      "470105/470105 [==============================] - 13s 28us/step - loss: 1.2340 - actions_loss: 0.8121 - reward_loss: 0.4219 - actions_acc: 0.6331 - reward_acc: 0.7891 - val_loss: 1.5769 - val_actions_loss: 1.0372 - val_reward_loss: 0.5396 - val_actions_acc: 0.5557 - val_reward_acc: 0.7288\n",
      "Epoch 67/150\n",
      "470105/470105 [==============================] - 15s 32us/step - loss: 1.2337 - actions_loss: 0.8118 - reward_loss: 0.4219 - actions_acc: 0.6330 - reward_acc: 0.7891 - val_loss: 1.5523 - val_actions_loss: 1.0129 - val_reward_loss: 0.5394 - val_actions_acc: 0.5628 - val_reward_acc: 0.7291\n",
      "Epoch 68/150\n",
      "470105/470105 [==============================] - 15s 32us/step - loss: 1.2324 - actions_loss: 0.8106 - reward_loss: 0.4218 - actions_acc: 0.6330 - reward_acc: 0.7891 - val_loss: 1.5180 - val_actions_loss: 0.9795 - val_reward_loss: 0.5384 - val_actions_acc: 0.5620 - val_reward_acc: 0.7296\n",
      "Epoch 69/150\n",
      "470105/470105 [==============================] - 13s 27us/step - loss: 1.2320 - actions_loss: 0.8102 - reward_loss: 0.4218 - actions_acc: 0.6338 - reward_acc: 0.7891 - val_loss: 1.5304 - val_actions_loss: 0.9910 - val_reward_loss: 0.5395 - val_actions_acc: 0.5590 - val_reward_acc: 0.7288\n",
      "Epoch 70/150\n",
      "470105/470105 [==============================] - 13s 27us/step - loss: 1.2323 - actions_loss: 0.8106 - reward_loss: 0.4217 - actions_acc: 0.6334 - reward_acc: 0.7892 - val_loss: 1.5362 - val_actions_loss: 0.9973 - val_reward_loss: 0.5389 - val_actions_acc: 0.5612 - val_reward_acc: 0.7293\n",
      "Epoch 71/150\n",
      "470105/470105 [==============================] - 13s 27us/step - loss: 1.2303 - actions_loss: 0.8086 - reward_loss: 0.4217 - actions_acc: 0.6338 - reward_acc: 0.7892 - val_loss: 1.5197 - val_actions_loss: 0.9805 - val_reward_loss: 0.5391 - val_actions_acc: 0.5640 - val_reward_acc: 0.7291\n",
      "Epoch 72/150\n",
      "470105/470105 [==============================] - 13s 27us/step - loss: 1.2297 - actions_loss: 0.8081 - reward_loss: 0.4216 - actions_acc: 0.6341 - reward_acc: 0.7892 - val_loss: 1.5252 - val_actions_loss: 0.9862 - val_reward_loss: 0.5390 - val_actions_acc: 0.5599 - val_reward_acc: 0.7295\n",
      "Epoch 73/150\n",
      "470105/470105 [==============================] - 13s 28us/step - loss: 1.2301 - actions_loss: 0.8085 - reward_loss: 0.4216 - actions_acc: 0.6329 - reward_acc: 0.7892 - val_loss: 1.5308 - val_actions_loss: 0.9915 - val_reward_loss: 0.5393 - val_actions_acc: 0.5611 - val_reward_acc: 0.7292\n",
      "Epoch 74/150\n",
      "470105/470105 [==============================] - 14s 29us/step - loss: 1.2278 - actions_loss: 0.8063 - reward_loss: 0.4215 - actions_acc: 0.6350 - reward_acc: 0.7893 - val_loss: 1.5509 - val_actions_loss: 1.0118 - val_reward_loss: 0.5391 - val_actions_acc: 0.5579 - val_reward_acc: 0.7293\n",
      "Epoch 75/150\n",
      "470105/470105 [==============================] - 15s 32us/step - loss: 1.2284 - actions_loss: 0.8069 - reward_loss: 0.4215 - actions_acc: 0.6344 - reward_acc: 0.7893 - val_loss: 1.5430 - val_actions_loss: 1.0039 - val_reward_loss: 0.5391 - val_actions_acc: 0.5648 - val_reward_acc: 0.7292\n",
      "Epoch 76/150\n",
      "470105/470105 [==============================] - 15s 32us/step - loss: 1.2272 - actions_loss: 0.8057 - reward_loss: 0.4215 - actions_acc: 0.6354 - reward_acc: 0.7893 - val_loss: 1.5590 - val_actions_loss: 1.0206 - val_reward_loss: 0.5384 - val_actions_acc: 0.5550 - val_reward_acc: 0.7296\n",
      "Epoch 77/150\n",
      "470105/470105 [==============================] - 15s 32us/step - loss: 1.2271 - actions_loss: 0.8056 - reward_loss: 0.4215 - actions_acc: 0.6349 - reward_acc: 0.7893 - val_loss: 1.5241 - val_actions_loss: 0.9848 - val_reward_loss: 0.5392 - val_actions_acc: 0.5644 - val_reward_acc: 0.7293\n",
      "Epoch 78/150\n",
      "470105/470105 [==============================] - 13s 28us/step - loss: 1.2254 - actions_loss: 0.8040 - reward_loss: 0.4214 - actions_acc: 0.6357 - reward_acc: 0.7894 - val_loss: 1.5499 - val_actions_loss: 1.0111 - val_reward_loss: 0.5388 - val_actions_acc: 0.5524 - val_reward_acc: 0.7296\n",
      "Epoch 79/150\n",
      "470105/470105 [==============================] - 13s 28us/step - loss: 1.2252 - actions_loss: 0.8038 - reward_loss: 0.4214 - actions_acc: 0.6358 - reward_acc: 0.7894 - val_loss: 1.5405 - val_actions_loss: 1.0014 - val_reward_loss: 0.5391 - val_actions_acc: 0.5545 - val_reward_acc: 0.7292\n",
      "Epoch 80/150\n",
      "470105/470105 [==============================] - 13s 28us/step - loss: 1.2244 - actions_loss: 0.8030 - reward_loss: 0.4214 - actions_acc: 0.6361 - reward_acc: 0.7894 - val_loss: 1.5466 - val_actions_loss: 1.0066 - val_reward_loss: 0.5399 - val_actions_acc: 0.5608 - val_reward_acc: 0.7288\n",
      "Epoch 81/150\n",
      "470105/470105 [==============================] - 13s 28us/step - loss: 1.2247 - actions_loss: 0.8034 - reward_loss: 0.4213 - actions_acc: 0.6357 - reward_acc: 0.7894 - val_loss: 1.5491 - val_actions_loss: 1.0098 - val_reward_loss: 0.5394 - val_actions_acc: 0.5634 - val_reward_acc: 0.7293\n",
      "Epoch 82/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470105/470105 [==============================] - 13s 28us/step - loss: 1.2242 - actions_loss: 0.8029 - reward_loss: 0.4213 - actions_acc: 0.6357 - reward_acc: 0.7894 - val_loss: 1.5548 - val_actions_loss: 1.0156 - val_reward_loss: 0.5392 - val_actions_acc: 0.5574 - val_reward_acc: 0.7295\n",
      "Epoch 83/150\n",
      "470105/470105 [==============================] - 13s 28us/step - loss: 1.2231 - actions_loss: 0.8019 - reward_loss: 0.4212 - actions_acc: 0.6370 - reward_acc: 0.7894 - val_loss: 1.5250 - val_actions_loss: 0.9852 - val_reward_loss: 0.5399 - val_actions_acc: 0.5625 - val_reward_acc: 0.7289\n",
      "Epoch 84/150\n",
      "470105/470105 [==============================] - 15s 32us/step - loss: 1.2225 - actions_loss: 0.8014 - reward_loss: 0.4211 - actions_acc: 0.6376 - reward_acc: 0.7895 - val_loss: 1.5626 - val_actions_loss: 1.0229 - val_reward_loss: 0.5397 - val_actions_acc: 0.5554 - val_reward_acc: 0.7288\n",
      "Epoch 85/150\n",
      "470105/470105 [==============================] - 15s 33us/step - loss: 1.2221 - actions_loss: 0.8012 - reward_loss: 0.4209 - actions_acc: 0.6371 - reward_acc: 0.7895 - val_loss: 1.5573 - val_actions_loss: 1.0175 - val_reward_loss: 0.5398 - val_actions_acc: 0.5566 - val_reward_acc: 0.7290\n",
      "Epoch 86/150\n",
      "470105/470105 [==============================] - 13s 29us/step - loss: 1.2217 - actions_loss: 0.8012 - reward_loss: 0.4205 - actions_acc: 0.6368 - reward_acc: 0.7897 - val_loss: 1.5267 - val_actions_loss: 0.9855 - val_reward_loss: 0.5412 - val_actions_acc: 0.5665 - val_reward_acc: 0.7282\n",
      "Epoch 87/150\n",
      "470105/470105 [==============================] - 12s 25us/step - loss: 1.2200 - actions_loss: 0.7996 - reward_loss: 0.4204 - actions_acc: 0.6376 - reward_acc: 0.7898 - val_loss: 1.5347 - val_actions_loss: 0.9955 - val_reward_loss: 0.5392 - val_actions_acc: 0.5623 - val_reward_acc: 0.7294\n",
      "Epoch 88/150\n",
      "470105/470105 [==============================] - 13s 28us/step - loss: 1.2206 - actions_loss: 0.8003 - reward_loss: 0.4203 - actions_acc: 0.6376 - reward_acc: 0.7899 - val_loss: 1.5278 - val_actions_loss: 0.9877 - val_reward_loss: 0.5401 - val_actions_acc: 0.5625 - val_reward_acc: 0.7288\n",
      "Epoch 89/150\n",
      "470105/470105 [==============================] - 13s 27us/step - loss: 1.2190 - actions_loss: 0.7988 - reward_loss: 0.4202 - actions_acc: 0.6381 - reward_acc: 0.7900 - val_loss: 1.5385 - val_actions_loss: 0.9974 - val_reward_loss: 0.5411 - val_actions_acc: 0.5655 - val_reward_acc: 0.7283\n",
      "Epoch 90/150\n",
      "470105/470105 [==============================] - 16s 34us/step - loss: 1.2193 - actions_loss: 0.7992 - reward_loss: 0.4201 - actions_acc: 0.6374 - reward_acc: 0.7900 - val_loss: 1.5335 - val_actions_loss: 0.9939 - val_reward_loss: 0.5397 - val_actions_acc: 0.5591 - val_reward_acc: 0.7290\n",
      "Epoch 91/150\n",
      "470105/470105 [==============================] - 15s 32us/step - loss: 1.2175 - actions_loss: 0.7975 - reward_loss: 0.4200 - actions_acc: 0.6386 - reward_acc: 0.7900 - val_loss: 1.5384 - val_actions_loss: 0.9981 - val_reward_loss: 0.5403 - val_actions_acc: 0.5627 - val_reward_acc: 0.7287\n",
      "Epoch 92/150\n",
      "470105/470105 [==============================] - 13s 28us/step - loss: 1.2169 - actions_loss: 0.7969 - reward_loss: 0.4200 - actions_acc: 0.6391 - reward_acc: 0.7901 - val_loss: 1.5325 - val_actions_loss: 0.9929 - val_reward_loss: 0.5396 - val_actions_acc: 0.5600 - val_reward_acc: 0.7289\n",
      "Epoch 93/150\n",
      "470105/470105 [==============================] - 13s 28us/step - loss: 1.2172 - actions_loss: 0.7972 - reward_loss: 0.4199 - actions_acc: 0.6382 - reward_acc: 0.7901 - val_loss: 1.5585 - val_actions_loss: 1.0190 - val_reward_loss: 0.5395 - val_actions_acc: 0.5501 - val_reward_acc: 0.7293\n",
      "Epoch 94/150\n",
      "470105/470105 [==============================] - 13s 28us/step - loss: 1.2175 - actions_loss: 0.7976 - reward_loss: 0.4199 - actions_acc: 0.6384 - reward_acc: 0.7901 - val_loss: 1.5293 - val_actions_loss: 0.9890 - val_reward_loss: 0.5403 - val_actions_acc: 0.5644 - val_reward_acc: 0.7287\n",
      "Epoch 95/150\n",
      "470105/470105 [==============================] - 13s 28us/step - loss: 1.2024 - actions_loss: 0.7827 - reward_loss: 0.4197 - actions_acc: 0.6477 - reward_acc: 0.7902 - val_loss: 1.5206 - val_actions_loss: 0.9803 - val_reward_loss: 0.5403 - val_actions_acc: 0.5643 - val_reward_acc: 0.7288\n",
      "Epoch 96/150\n",
      "470105/470105 [==============================] - 15s 31us/step - loss: 1.2008 - actions_loss: 0.7811 - reward_loss: 0.4197 - actions_acc: 0.6484 - reward_acc: 0.7903 - val_loss: 1.5139 - val_actions_loss: 0.9737 - val_reward_loss: 0.5402 - val_actions_acc: 0.5678 - val_reward_acc: 0.7287\n",
      "Epoch 97/150\n",
      "470105/470105 [==============================] - 15s 32us/step - loss: 1.2007 - actions_loss: 0.7809 - reward_loss: 0.4197 - actions_acc: 0.6485 - reward_acc: 0.7903 - val_loss: 1.5129 - val_actions_loss: 0.9727 - val_reward_loss: 0.5402 - val_actions_acc: 0.5673 - val_reward_acc: 0.7289\n",
      "Epoch 98/150\n",
      "470105/470105 [==============================] - 15s 32us/step - loss: 1.2005 - actions_loss: 0.7808 - reward_loss: 0.4197 - actions_acc: 0.6485 - reward_acc: 0.7903 - val_loss: 1.5178 - val_actions_loss: 0.9775 - val_reward_loss: 0.5403 - val_actions_acc: 0.5670 - val_reward_acc: 0.7287\n",
      "Epoch 99/150\n",
      "470105/470105 [==============================] - 13s 28us/step - loss: 1.2005 - actions_loss: 0.7808 - reward_loss: 0.4197 - actions_acc: 0.6483 - reward_acc: 0.7903 - val_loss: 1.5143 - val_actions_loss: 0.9741 - val_reward_loss: 0.5402 - val_actions_acc: 0.5672 - val_reward_acc: 0.7288\n",
      "Epoch 100/150\n",
      "470105/470105 [==============================] - 13s 28us/step - loss: 1.2002 - actions_loss: 0.7805 - reward_loss: 0.4197 - actions_acc: 0.6493 - reward_acc: 0.7903 - val_loss: 1.5141 - val_actions_loss: 0.9738 - val_reward_loss: 0.5402 - val_actions_acc: 0.5681 - val_reward_acc: 0.7287\n",
      "Epoch 101/150\n",
      "470105/470105 [==============================] - 13s 28us/step - loss: 1.2004 - actions_loss: 0.7807 - reward_loss: 0.4197 - actions_acc: 0.6481 - reward_acc: 0.7903 - val_loss: 1.5178 - val_actions_loss: 0.9777 - val_reward_loss: 0.5402 - val_actions_acc: 0.5676 - val_reward_acc: 0.7289\n",
      "Epoch 102/150\n",
      "470105/470105 [==============================] - 13s 28us/step - loss: 1.2002 - actions_loss: 0.7805 - reward_loss: 0.4197 - actions_acc: 0.6488 - reward_acc: 0.7903 - val_loss: 1.5134 - val_actions_loss: 0.9729 - val_reward_loss: 0.5404 - val_actions_acc: 0.5668 - val_reward_acc: 0.7287\n",
      "Epoch 103/150\n",
      "470105/470105 [==============================] - 15s 32us/step - loss: 1.1983 - actions_loss: 0.7787 - reward_loss: 0.4196 - actions_acc: 0.6495 - reward_acc: 0.7903 - val_loss: 1.5130 - val_actions_loss: 0.9726 - val_reward_loss: 0.5404 - val_actions_acc: 0.5689 - val_reward_acc: 0.7287\n",
      "Epoch 104/150\n",
      "470105/470105 [==============================] - 15s 32us/step - loss: 1.1982 - actions_loss: 0.7785 - reward_loss: 0.4197 - actions_acc: 0.6501 - reward_acc: 0.7903 - val_loss: 1.5126 - val_actions_loss: 0.9722 - val_reward_loss: 0.5404 - val_actions_acc: 0.5681 - val_reward_acc: 0.7287\n",
      "Epoch 105/150\n",
      "470105/470105 [==============================] - 15s 32us/step - loss: 1.1981 - actions_loss: 0.7785 - reward_loss: 0.4197 - actions_acc: 0.6498 - reward_acc: 0.7903 - val_loss: 1.5125 - val_actions_loss: 0.9721 - val_reward_loss: 0.5404 - val_actions_acc: 0.5688 - val_reward_acc: 0.7287\n",
      "Epoch 106/150\n",
      "470105/470105 [==============================] - 15s 31us/step - loss: 1.1981 - actions_loss: 0.7784 - reward_loss: 0.4197 - actions_acc: 0.6501 - reward_acc: 0.7903 - val_loss: 1.5124 - val_actions_loss: 0.9720 - val_reward_loss: 0.5404 - val_actions_acc: 0.5688 - val_reward_acc: 0.7287\n",
      "Epoch 107/150\n",
      "470105/470105 [==============================] - 15s 31us/step - loss: 1.1978 - actions_loss: 0.7782 - reward_loss: 0.4197 - actions_acc: 0.6502 - reward_acc: 0.7903 - val_loss: 1.5127 - val_actions_loss: 0.9723 - val_reward_loss: 0.5404 - val_actions_acc: 0.5682 - val_reward_acc: 0.7287\n",
      "Epoch 108/150\n",
      "470105/470105 [==============================] - 12s 26us/step - loss: 1.1978 - actions_loss: 0.7782 - reward_loss: 0.4197 - actions_acc: 0.6506 - reward_acc: 0.7903 - val_loss: 1.5127 - val_actions_loss: 0.9724 - val_reward_loss: 0.5404 - val_actions_acc: 0.5682 - val_reward_acc: 0.7287\n",
      "Epoch 109/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470105/470105 [==============================] - 13s 27us/step - loss: 1.1978 - actions_loss: 0.7782 - reward_loss: 0.4196 - actions_acc: 0.6501 - reward_acc: 0.7903 - val_loss: 1.5127 - val_actions_loss: 0.9723 - val_reward_loss: 0.5404 - val_actions_acc: 0.5682 - val_reward_acc: 0.7287\n",
      "Epoch 110/150\n",
      "470105/470105 [==============================] - 14s 30us/step - loss: 1.1978 - actions_loss: 0.7782 - reward_loss: 0.4197 - actions_acc: 0.6503 - reward_acc: 0.7903 - val_loss: 1.5127 - val_actions_loss: 0.9723 - val_reward_loss: 0.5404 - val_actions_acc: 0.5684 - val_reward_acc: 0.7287\n",
      "Epoch 111/150\n",
      "470105/470105 [==============================] - 13s 28us/step - loss: 1.1979 - actions_loss: 0.7782 - reward_loss: 0.4197 - actions_acc: 0.6502 - reward_acc: 0.7903 - val_loss: 1.5127 - val_actions_loss: 0.9723 - val_reward_loss: 0.5404 - val_actions_acc: 0.5683 - val_reward_acc: 0.7287\n",
      "Epoch 112/150\n",
      "470105/470105 [==============================] - 13s 29us/step - loss: 1.1978 - actions_loss: 0.7781 - reward_loss: 0.4197 - actions_acc: 0.6509 - reward_acc: 0.7903 - val_loss: 1.5127 - val_actions_loss: 0.9724 - val_reward_loss: 0.5404 - val_actions_acc: 0.5686 - val_reward_acc: 0.7287\n",
      "Epoch 113/150\n",
      "470105/470105 [==============================] - 13s 29us/step - loss: 1.1979 - actions_loss: 0.7782 - reward_loss: 0.4197 - actions_acc: 0.6502 - reward_acc: 0.7903 - val_loss: 1.5126 - val_actions_loss: 0.9722 - val_reward_loss: 0.5404 - val_actions_acc: 0.5684 - val_reward_acc: 0.7287\n",
      "Epoch 114/150\n",
      "470105/470105 [==============================] - 14s 29us/step - loss: 1.1979 - actions_loss: 0.7783 - reward_loss: 0.4197 - actions_acc: 0.6504 - reward_acc: 0.7903 - val_loss: 1.5127 - val_actions_loss: 0.9723 - val_reward_loss: 0.5404 - val_actions_acc: 0.5682 - val_reward_acc: 0.7287\n",
      "Epoch 115/150\n",
      "470105/470105 [==============================] - 14s 30us/step - loss: 1.1978 - actions_loss: 0.7782 - reward_loss: 0.4197 - actions_acc: 0.6506 - reward_acc: 0.7903 - val_loss: 1.5127 - val_actions_loss: 0.9723 - val_reward_loss: 0.5404 - val_actions_acc: 0.5683 - val_reward_acc: 0.7287\n",
      "Epoch 116/150\n",
      "470105/470105 [==============================] - 14s 30us/step - loss: 1.1979 - actions_loss: 0.7782 - reward_loss: 0.4196 - actions_acc: 0.6504 - reward_acc: 0.7903 - val_loss: 1.5127 - val_actions_loss: 0.9723 - val_reward_loss: 0.5404 - val_actions_acc: 0.5684 - val_reward_acc: 0.7287\n",
      "Epoch 117/150\n",
      "470105/470105 [==============================] - 14s 30us/step - loss: 1.1979 - actions_loss: 0.7783 - reward_loss: 0.4197 - actions_acc: 0.6505 - reward_acc: 0.7903 - val_loss: 1.5127 - val_actions_loss: 0.9723 - val_reward_loss: 0.5404 - val_actions_acc: 0.5683 - val_reward_acc: 0.7287\n"
     ]
    }
   ],
   "source": [
    "trainer.train(observations, labels, rewards, batch_size=batch_size, \n",
    "              epochs=epochs, early_stopping=early_stopping, class_weight=[class_weights, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    }
   ],
   "source": [
    "print(\"finish\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
