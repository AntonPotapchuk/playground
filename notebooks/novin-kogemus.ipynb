{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from pommerman.agents import SimpleAgent, RandomAgent, PlayerAgent, BaseAgent\n",
    "from pommerman.configs import ffa_v0_env\n",
    "from pommerman.envs.v0 import Pomme\n",
    "from pommerman.characters import Bomber\n",
    "from pommerman import utility\n",
    "from tensorforce.execution import Runner\n",
    "from tensorforce.contrib.openai_gym import OpenAIGym\n",
    "\n",
    "dobz = None # debug observation\n",
    "stateInput = 13 # used as a global variable for the observations as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reward_shaping(obs, state, reward):\n",
    "    \"\"\" Shape the reward based on the current state \"\"\"\n",
    "    #print(reward)\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def center_view(board, mypos, viewSize=25):\n",
    "    # make sure of odd viewSize\n",
    "    viewSize = viewSize + 1 if viewSize % 2 == 0 else viewSize \n",
    "    \n",
    "    # assumed board's odd shape, dimensions must be odd!\n",
    "    wmax, hmax = board.shape[1]*2+1, board.shape[0]*2+1 \n",
    "    agentView = np.ones((wmax,hmax)) # agent centric full-world coverage\n",
    "    center = (agentView.shape[0]//2+1, agentView.shape[1]//2+1)\n",
    "    \n",
    "    # copy board to the new view\n",
    "    offset_y = center[0]-mypos[0]-1\n",
    "    offset_x = center[1]-mypos[1]-1\n",
    "    agentView[offset_y:offset_y+13, offset_x:offset_x+13] = board\n",
    "    #np.savetxt('board.txt', agentView, fmt=\"%2.i\") # save to file for debug\n",
    "    \n",
    "    # finalize view size\n",
    "    r = viewSize // 2\n",
    "    start, end = center[0]-r-1, center[0]+r\n",
    "    agentView = agentView[start:end, start:end] \n",
    "    #np.savetxt('board_cut.txt', agentView, fmt=\"%2.i\") # save to file for debug\n",
    "    \n",
    "    return np.array(agentView, dtype=np.float32)\n",
    "    \n",
    "# test    \n",
    "# center_view(obz['board'], obz['position'], 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize(obz):\n",
    "    global dobz\n",
    "    global stateInput\n",
    "    \n",
    "    dobz = obz # for debugging purposes\n",
    "    viewSize = stateInput # view/state input size\n",
    "    \n",
    "    board = obz[\"board\"]\n",
    "    bomb_blast_strength = obz[\"bomb_blast_strength\"]\n",
    "    bomb_life = obz[\"bomb_life\"]\n",
    "    mypos = obz['position']\n",
    "    \n",
    "    # my powers:\n",
    "    ammo = obz[\"ammo\"] # TODO: ? how to apply\n",
    "    blast_strength = obz[\"blast_strength\"] # TODO: only around my bombs\n",
    "    can_kick = int(obz[\"can_kick\"]) # TODO: add to all channels and maps\n",
    "\n",
    "    # agent's channel\n",
    "    ch1 = center_view(board, mypos, viewSize)\n",
    "    ch1 += center_view(bomb_life, mypos, viewSize)\n",
    "    ch1 += center_view(bomb_blast_strength, mypos, viewSize) \n",
    "    \n",
    "    # teammate channel\n",
    "    def ch2_default():\n",
    "        ch2 = center_view(board, mypos, viewSize)\n",
    "        ch2 += center_view(bomb_life, mypos, viewSize)\n",
    "        ch2 += center_view(bomb_blast_strength, mypos, viewSize)\n",
    "        return ch2\n",
    "    \n",
    "    ch2 = np.zeros((viewSize, viewSize))\n",
    "    teammate = obz[\"teammate\"]\n",
    "    teammate = teammate.value if teammate is not None else -1\n",
    "    if teammate != -1:\n",
    "        teammatePos = np.array(np.where(board == [[teammate]])).reshape(-1,)\n",
    "        if teammatePos.size != 0:\n",
    "            ch2 = center_view(board, teammatePos, viewSize)\n",
    "            ch2 += center_view(bomb_life, teammatePos, viewSize)\n",
    "            ch2 += center_view(bomb_blast_strength, teammatePos, viewSize)\n",
    "        else:\n",
    "            ch2 = ch2_default()\n",
    "    ch2 = ch2_default()\n",
    "\n",
    "    # opponents channel\n",
    "    enemies = obz[\"enemies\"]\n",
    "    enemies = [e.value for e in enemies]\n",
    "    enemies = enemies + [-1]*(3 - len(enemies)) if len(enemies) < 3 else enemies\n",
    "    ch3 = np.zeros((viewSize, viewSize))\n",
    "    for enemy in enemies:\n",
    "        if enemy == -1:\n",
    "            continue\n",
    "        enemyPos = np.array(np.where(board == [[enemy]])).reshape(-1,)\n",
    "        if enemyPos.size == 0:\n",
    "            continue\n",
    "        ch3 += center_view(board, enemyPos, viewSize)\n",
    "        ch3 += center_view(bomb_life, enemyPos, viewSize)\n",
    "        ch3 += center_view(bomb_blast_strength, enemyPos, viewSize) \n",
    "    \n",
    "#     print(\"ch1: \", np.sum(ch1))\n",
    "#     print(\"ch2: \", np.sum(ch2))\n",
    "#     print(\"ch3: \", np.sum(ch3))\n",
    "    state = np.dstack((ch1, ch2))\n",
    "    state = np.dstack((state, ch3))\n",
    "    #print(state.shape)\n",
    "    \n",
    "#     return state.reshape(-1,)\n",
    "    return state\n",
    "\n",
    "\n",
    "class TensorforceAgent(BaseAgent):\n",
    "    \"\"\" Mock class for TensorforceAgent \"\"\"\n",
    "    def act(self, obs, action_space):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/reinforceio/tensorforce/blob/master/tensorforce/tests/test_tutorial_code.py\n",
    "from tensorforce.agents import DQNAgent, PPOAgent\n",
    "\n",
    "# Instantiate the environment\n",
    "config = ffa_v0_env()\n",
    "env = Pomme(**config[\"env_kwargs\"])\n",
    "env.seed(0)\n",
    "\n",
    "\n",
    "# Network is an ordered list of layers\n",
    "network_spec = [\n",
    "    dict(type='conv2d', size=64, window=8, stride=4),\n",
    "    dict(type='conv2d', size=32, window=4, stride=2),\n",
    "    dict(type='flatten')\n",
    "#     dict(type='dense', size=64),\n",
    "#     dict(type='dense', size=64)\n",
    "]\n",
    "\n",
    "# Define a state\n",
    "states = dict(shape=(stateInput,stateInput,3), type='float')\n",
    "# states = dict(shape=(stateInput*stateInput*3,), type='float') # for linear state\n",
    "\n",
    "# Define an action\n",
    "actions = dict(type='int', num_actions=env.action_space.n)\n",
    "\n",
    "agent_dqn = DQNAgent(\n",
    "    states=states,\n",
    "    actions=actions,\n",
    "    network=network_spec,\n",
    "    update_mode=dict(\n",
    "        unit='timesteps',\n",
    "        batch_size=1,\n",
    "        frequency=1\n",
    "    ),\n",
    "    memory=dict(\n",
    "        type='latest',\n",
    "        include_next_states=True,\n",
    "        capacity=100\n",
    "    ),\n",
    "    target_sync_frequency=10\n",
    ")\n",
    "\n",
    "\n",
    "# Create a Proximal Policy Optimization agent\n",
    "agent_ppo = PPOAgent(\n",
    "    states=states,\n",
    "    actions=actions,\n",
    "    network=network_spec,\n",
    "    batching_capacity=1000,\n",
    "    step_optimizer=dict(\n",
    "        type='adam',\n",
    "        learning_rate=1e-4\n",
    "    )\n",
    ")\n",
    "\n",
    "agent = agent_ppo\n",
    "\n",
    "# Add 3 simple agents\n",
    "agents = []\n",
    "for agent_id in range(3):\n",
    "    agents.append(SimpleAgent(config[\"agent\"](agent_id, config[\"game_type\"])))\n",
    "\n",
    "# Add TensorforceAgent\n",
    "agent_id += 1\n",
    "agents.append(TensorforceAgent(config[\"agent\"](agent_id, config[\"game_type\"])))\n",
    "env.set_agents(agents)\n",
    "env.set_training_agent(agents[-1].agent_id)\n",
    "env.set_init_game_state(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedEnv(OpenAIGym):    \n",
    "    def __init__(self, gym, visualize=False):\n",
    "        self.gym = gym\n",
    "        self.visualize = visualize\n",
    "    \n",
    "    def execute(self, actions):\n",
    "        if self.visualize:\n",
    "            self.gym.render()\n",
    "\n",
    "        obs = self.gym.get_observations()\n",
    "        all_actions = self.gym.act(obs)\n",
    "        all_actions.insert(self.gym.training_agent, actions)\n",
    "        state, reward, terminal, _ = self.gym.step(all_actions)\n",
    "        reward = reward_shaping(obs, state, reward)\n",
    "        agent_state = featurize(state[self.gym.training_agent])\n",
    "        agent_reward = reward[self.gym.training_agent]\n",
    "        return agent_state, terminal, agent_reward\n",
    "    \n",
    "    def reset(self):\n",
    "        obs = self.gym.reset()\n",
    "        agent_obs = featurize(obs[3])\n",
    "        return agent_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats:  [-1, -1, -1, -1, -1] [30, 54, 27, 28, 28] [4.317109107971191, 7.399764537811279, 3.5615615844726562, 3.1949238777160645, 3.3633885383605957]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and run the environment for 5 episodes.\n",
    "wrapped_env = WrappedEnv(env, True)\n",
    "runner = Runner(agent=agent, environment=wrapped_env)\n",
    "runner.run(episodes=5, max_episode_timesteps=2000)\n",
    "print(\"Stats: \", runner.episode_rewards, runner.episode_timesteps, runner.episode_times)\n",
    "\n",
    "try:\n",
    "    runner.close()\n",
    "except AttributeError as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ammo': 1,\n",
       " 'blast_strength': 3,\n",
       " 'board': array([[ 0,  8,  1,  1,  2,  1,  1,  0,  0,  1,  1,  1,  1],\n",
       "        [ 0,  0,  0,  0,  2,  2,  2,  2,  4,  4,  4,  4,  2],\n",
       "        [ 1,  0, 11,  1,  2,  1,  1,  2,  0,  4,  1,  0,  1],\n",
       "        [ 1,  0,  1,  0,  1,  1,  2,  1,  2,  4,  1,  0,  2],\n",
       "        [ 2,  2,  2,  1,  0,  2,  0,  1,  0,  2,  2,  2,  0],\n",
       "        [ 1,  2,  1,  1,  2,  0,  2,  2,  2,  0,  0,  2,  0],\n",
       "        [ 1,  2,  1,  2,  0,  2,  0,  0,  0,  0,  2,  2,  1],\n",
       "        [ 0,  2,  2,  1,  1,  2,  0,  0,  1,  1,  0,  2,  1],\n",
       "        [ 0,  2,  0,  2,  0,  2,  0,  1,  0,  1,  0,  2,  1],\n",
       "        [ 1,  0,  0,  0,  2,  0,  0,  1,  1,  0,  0,  3,  0],\n",
       "        [ 1,  0,  1,  1,  2,  0,  2,  0, 13,  0,  0,  0,  1],\n",
       "        [ 1, 12,  0,  0,  2,  2,  2,  2,  2,  0,  0,  0,  0],\n",
       "        [ 1,  2,  1,  2,  0,  0,  1,  1,  1,  0,  1,  0,  0]], dtype=uint8),\n",
       " 'bomb_blast_strength': array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 3., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " 'bomb_life': array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " 'can_kick': False,\n",
       " 'enemies': [<Item.Agent0: 11>, <Item.Agent1: 12>, <Item.Agent2: 13>],\n",
       " 'position': (1, 9),\n",
       " 'teammate': <Item.AgentDummy: 10>}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dobz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
